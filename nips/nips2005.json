[
    {
        "id": "ac5f933617",
        "title": "A Bayes Rule for Density Matrices",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/4191ef5f6c1576762869ac49281130c9-Abstract.html",
        "author": "Manfred K. Warmuth",
        "abstract": "The classical Bayes rule computes the posterior model probability from the prior probability and the data likelihood. We generalize this rule to the case when the prior is a density matrix (symmetric positive definite and trace one) and the data likelihood a covariance matrix. The classical Bayes rule is retained as the special case when the matrices are diagonal. In the classical setting, the calculation of the probability of the data is an expected likelihood, where the expectation is over the prior distribution. In the generalized setting, this is replaced by an expected variance calculation where the variance is computed along the eigenvectors of the prior density matrix and the expectation is over the eigenvalues of the density matrix (which form a probability vector). The variances along any direction is determined by the covariance matrix. Curiously enough this expected variance calculation is a quantum measurement where the co-variance matrix specifies the instrument and the prior density matrix the mixture state of the particle. We motivate both the classical and the generalized Bayes rule with a minimum relative entropy principle, where the Kullbach-Leibler version gives the classical Bayes rule and Umegaki's quantum relative entropy the new Bayes rule for density matrices.",
        "bibtex": "@inproceedings{NIPS2005_4191ef5f,\n author = {Warmuth, Manfred K. K},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {A Bayes Rule for Density Matrices},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4191ef5f6c1576762869ac49281130c9-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/4191ef5f6c1576762869ac49281130c9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/4191ef5f6c1576762869ac49281130c9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 219098,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12701869059128602845&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Computer Science Department, University of California at Santa Cruz",
        "aff_domain": "cse.ucsc.edu",
        "email": "cse.ucsc.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of California, Santa Cruz",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.ucsc.edu",
        "aff_unique_abbr": "UCSC",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Santa Cruz",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8ae538dbfa",
        "title": "A Bayesian Framework for Tilt Perception and Confidence",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/566a9968b43628588e76be5a85a0f9e8-Abstract.html",
        "author": "Odelia Schwartz; Peter Dayan; Terrence J. Sejnowski",
        "abstract": "The misjudgement of tilt in images lies at the heart of entertaining visual illusions and rigorous perceptual psychophysics. A wealth of findings has attracted many mechanistic models, but few clear computational principles. We adopt a Bayesian approach to perceptual tilt estimation, showing how a smoothness prior offers a powerful way of addressing much confusing data. In particular, we faithfully model recent results showing that confidence in estimation can be systematically affected by the same aspects of images that affect bias. Confidence is central to Bayesian modeling approaches, and is applicable in many other perceptual domains. Perceptual anomalies and illusions, such as the misjudgements of motion and tilt evident in so many psychophysical experiments, have intrigued researchers for decades.13 A Bayesian view48 has been particularly influential in models of motion processing, treating such anomalies as the normative product of prior information (often statistically codifying Gestalt laws) with likelihood information from the actual scenes presented. Here, we expand the range of statistically normative accounts to tilt estimation, for which there are classes of results (on estimation confidence) that are so far not available for motion. The tilt illusion arises when the perceived tilt of a center target is misjudged (ie bias) in the presence of flankers. Another phenomenon, called Crowding, refers to a loss in the confidence (ie sensitivity) of perceived target tilt in the presence of flankers. Attempts have been made to formalize these phenomena quantitatively. Crowding has been modeled as compulsory feature pooling (ie averaging of orientations), ignoring spatial positions.9, 10 The tilt illusion has been explained by lateral interactions11, 12 in populations of orientationtuned units; and by calibration.13 However, most models of this form cannot explain a number of crucial aspects of the data. First, the geometry of the positional arrangement of the stimuli affects attraction versus repulsion in bias, as emphasized by Kapadia et al14 (figure 1A), and others.15, 16 Second, Solomon et al. recently measured bias and sensitivity simultaneously.11 The rich and surprising range of sensitivities, far from flat as a function of flanker angles (figure 1B), are outside the reach of standard models. Moreover, current explanations do not offer a computational account of tilt perception as the outcome of a normative inference process. Here, we demonstrate that a Bayesian framework for orientation estimation, with a prior favoring smoothness, can naturally explain a range of seemingly puzzling tilt data. We explicitly consider both the geometry of the stimuli, and the issue of confidence in the esti-",
        "bibtex": "@inproceedings{NIPS2005_566a9968,\n author = {Schwartz, Odelia and Dayan, Peter and Sejnowski, Terrence J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {A Bayesian Framework for Tilt Perception and Confidence},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/566a9968b43628588e76be5a85a0f9e8-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/566a9968b43628588e76be5a85a0f9e8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/566a9968b43628588e76be5a85a0f9e8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 102244,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=305272355885837434&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "HHMI and Salk Institute, La Jolla, CA 92014; HHMI and Salk Institute, La Jolla, CA 92014; Gatsby, UCL, 17 Queen Square, London",
        "aff_domain": "salk.edu;salk.edu;gatsby.ucl.ac.uk",
        "email": "salk.edu;salk.edu;gatsby.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Salk Institute;University College London",
        "aff_unique_dep": ";Gatsby",
        "aff_unique_url": "https://www.salk.edu;https://www.ucl.ac.uk",
        "aff_unique_abbr": "Salk;UCL",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "La Jolla;London",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "5c9ca902e1",
        "title": "A Bayesian Spatial Scan Statistic",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/28acfe2da49d2b9a7f177458256f2540-Abstract.html",
        "author": "Daniel B. Neill; Andrew W. Moore; Gregory F. Cooper",
        "abstract": "We propose a new Bayesian method for spatial cluster detection, the \u201cBayesian spatial scan statistic,\u201d and compare this method to the standard (frequentist) scan statistic approach. We demonstrate that the Bayesian statistic has several advantages over the frequentist approach, including increased power to detect clusters and (since randomization testing is unnecessary) much faster runtime. We evaluate the Bayesian and fre- quentist methods on the task of prospective disease surveillance: detect- ing spatial clusters of disease cases resulting from emerging disease out- breaks. We demonstrate that our Bayesian methods are successful in rapidly detecting outbreaks while keeping number of false positives low.",
        "bibtex": "@inproceedings{NIPS2005_28acfe2d,\n author = {Neill, Daniel and Moore, Andrew and Cooper, Gregory},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {A Bayesian Spatial Scan Statistic},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/28acfe2da49d2b9a7f177458256f2540-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/28acfe2da49d2b9a7f177458256f2540-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/28acfe2da49d2b9a7f177458256f2540-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 60193,
        "gs_citation": 180,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3410909461074524681&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University; Center for Biomedical Informatics, University of Pittsburgh",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cbmi.pitt.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cbmi.pitt.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Carnegie Mellon University;University of Pittsburgh",
        "aff_unique_dep": "School of Computer Science;Center for Biomedical Informatics",
        "aff_unique_url": "https://www.cmu.edu;https://www.pitt.edu",
        "aff_unique_abbr": "CMU;Pitt",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "280aa329bf",
        "title": "A Computational Model of Eye Movements during Object Class Detection",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/e07bceab69529b0f0b43625953fbf2a0-Abstract.html",
        "author": "Wei Zhang; Hyejin Yang; Dimitris Samaras; Gregory J. Zelinsky",
        "abstract": "We present a computational model of human eye movements in an ob- ject class detection task. The model combines state-of-the-art computer vision object class detection methods (SIFT features trained using Ad- aBoost) with a biologically plausible model of human eye movement to produce a sequence of simulated \ufb01xations, culminating with the acqui- sition of a target. We validated the model by comparing its behavior to the behavior of human observers performing the identical object class detection task (looking for a teddy bear among visually complex non- target objects). We found considerable agreement between the model and human data in multiple eye movement measures, including number of \ufb01xations, cumulative probability of \ufb01xating the target, and scanpath distance.",
        "bibtex": "@inproceedings{NIPS2005_e07bceab,\n author = {Zhang, Wei and Yang, Hyejin and Samaras, Dimitris and Zelinsky, Gregory},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {A Computational Model of Eye Movements during Object Class Detection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e07bceab69529b0f0b43625953fbf2a0-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/e07bceab69529b0f0b43625953fbf2a0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/e07bceab69529b0f0b43625953fbf2a0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 158403,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1895179032526976846&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Dept. of Computer Science\u2020; Dept. of Psychology\u2021; Dept. of Computer Science\u2020; Dept. of Psychology\u2021",
        "aff_domain": "cs.sunysb.edu;ic.sunysb.edu;cs.sunysb.edu;stonybrook.edu",
        "email": "cs.sunysb.edu;ic.sunysb.edu;cs.sunysb.edu;stonybrook.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University Affiliation Not Specified",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "53eb00576c",
        "title": "A Connectionist Model for Constructive Modal Reasoning",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/86c4ad52768c511046fea7b2d42b300c-Abstract.html",
        "author": "Artur Garcez; Luis C. Lamb; Dov M. Gabbay",
        "abstract": "We present a new connectionist model for constructive, intuitionistic modal reasoning. We use ensembles of neural networks to represent in- tuitionistic modal theories, and show that for each intuitionistic modal program there exists a corresponding neural network ensemble that com- putes the program. This provides a massively parallel model for intu- itionistic modal reasoning, and sets the scene for integrated reasoning, knowledge representation, and learning of intuitionistic theories in neural networks, since the networks in the ensemble can be trained by examples using standard neural learning algorithms.",
        "bibtex": "@inproceedings{NIPS2005_86c4ad52,\n author = {Garcez, Artur and Lamb, Luis and Gabbay, Dov M.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {A Connectionist Model for Constructive Modal Reasoning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/86c4ad52768c511046fea7b2d42b300c-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/86c4ad52768c511046fea7b2d42b300c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/86c4ad52768c511046fea7b2d42b300c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 93750,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2458486783179547305&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computing, City University London; Institute of Informatics, Federal University of Rio Grande do Sul; Department of Computer Science, King\u2019s College London",
        "aff_domain": "soi.city.ac.uk;acm.org;dcs.kcl.ac.uk",
        "email": "soi.city.ac.uk;acm.org;dcs.kcl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "City University London;Federal University of Rio Grande do Sul;King\u2019s College London",
        "aff_unique_dep": "Department of Computing;Institute of Informatics;Department of Computer Science",
        "aff_unique_url": "https://www.city.ac.uk;https://www.ufrgs.br;https://www.kcl.ac.uk",
        "aff_unique_abbr": "City, University of London;;KCL",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "London;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United Kingdom;Brazil"
    },
    {
        "id": "397b8d52dc",
        "title": "A Cortically-Plausible Inverse Problem Solving Method Applied to Recognizing Static and Kinematic 3D Objects",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/ec7f346604f518906d35ef0492709f78-Abstract.html",
        "author": "David Arathorn",
        "abstract": "Recent neurophysiological evidence suggests the ability to  interpret  biological  motion  is  facilitated  by  a  neuronal  \"mirror  system\"  which  maps  visual  inputs  to  the  pre-motor  cortex.  If the  common  architecture  and  circuitry  of  the  cortices  is  taken  to  imply  a  common  computation  across  multiple  perceptual  and  cognitive  modalities, this visual-motor interaction  might be  expected to  have  a unified computational basis.  Two  essential tasks underlying such  visual-motor  cooperation  are  shown  here  to  be  simply  expressed  and  directly  solved  as  transformation-discovery  inverse  problems:  (a)  discriminating  and  determining  the  pose  of a  primed  3D  object  in  a  real-world  scene,  and  (b)  interpreting  the  3D  configuration  of  an  articulated kinematic object in  an  image.  The recently developed  map-seeking  method  provides  tractable,  cortically-plausible  solution  to  these  and  a  variety  of other  inverse  problems  which  can be  posed  as  the  discovery  of a  composition of  transformations  between  two  patterns.  The  method  relies  on  an  ordering  property  of superpositions  and  on  decomposition  of the  transformation  spaces  inherent  in  the  generating  processes  of the  problem.",
        "bibtex": "@inproceedings{NIPS2005_ec7f3466,\n author = {Arathorn, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {A Cortically-Plausible Inverse Problem Solving Method Applied to Recognizing Static and Kinematic 3D Objects},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ec7f346604f518906d35ef0492709f78-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/ec7f346604f518906d35ef0492709f78-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/ec7f346604f518906d35ef0492709f78-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1315771,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7656696290748741615&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "31274ba157",
        "title": "A Criterion for the Convergence of Learning with Spike Timing Dependent Plasticity",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/b030afbb3a8af8fb0759241c97466ee4-Abstract.html",
        "author": "Robert A. Legenstein; Wolfgang Maass",
        "abstract": "We investigate under what conditions a neuron can learn by experimen- tally supported rules for spike timing dependent plasticity (STDP) to pre- dict the arrival times of strong \u201cteacher inputs\u201d to the same neuron. It turns out that in contrast to the famous Perceptron Convergence Theo- rem, which predicts convergence of the perceptron learning rule for a simpli\ufb01ed neuron model whenever a stable solution exists, no equally strong convergence guarantee can be given for spiking neurons with STDP. But we derive a criterion on the statistical dependency structure of input spike trains which characterizes exactly when learning with STDP will converge on average for a simple model of a spiking neuron. This criterion is reminiscent of the linear separability criterion of the Percep- tron Convergence Theorem, but it applies here to the rows of a correlation matrix related to the spike inputs. In addition we show through computer simulations for more realistic neuron models that the resulting analyti- cally predicted positive learning results not only hold for the common interpretation of STDP where STDP changes the weights of synapses, but also for a more realistic interpretation suggested by experimental data where STDP modulates the initial release probability of dynamic synapses.",
        "bibtex": "@inproceedings{NIPS2005_b030afbb,\n author = {Legenstein, Robert and Maass, Wolfgang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {A Criterion for the Convergence of Learning with Spike Timing Dependent Plasticity},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b030afbb3a8af8fb0759241c97466ee4-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/b030afbb3a8af8fb0759241c97466ee4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/b030afbb3a8af8fb0759241c97466ee4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 163801,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17144587739730013578&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Institute for Theoretical Computer Science, Technische Universitaet Graz, A-8010 Graz, Austria; Institute for Theoretical Computer Science, Technische Universitaet Graz, A-8010 Graz, Austria",
        "aff_domain": "igi.tugraz.at;igi.tugraz.at",
        "email": "igi.tugraz.at;igi.tugraz.at",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Technische Universitaet Graz",
        "aff_unique_dep": "Institute for Theoretical Computer Science",
        "aff_unique_url": "https://www.tugraz.at",
        "aff_unique_abbr": "TU Graz",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Graz",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Austria"
    },
    {
        "id": "6fc2841574",
        "title": "A Domain Decomposition Method for Fast Manifold Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/d53697441ef12a45422f6660202f9840-Abstract.html",
        "author": "Zhenyue Zhang; Hongyuan Zha",
        "abstract": "We propose a fast manifold learning algorithm based on the methodology of domain decomposition. Starting with the set of sample points partitioned into two subdomains, we develop the solution of the interface problem that can glue the embeddings on the two subdomains into an embedding on the whole domain. We provide a detailed analysis to assess the errors produced by the gluing process using matrix perturbation theory. Numerical examples are given to illustrate the efficiency and effectiveness of the proposed methods.",
        "bibtex": "@inproceedings{NIPS2005_d5369744,\n author = {Zhang, Zhenyue and Zha, Hongyuan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {A Domain Decomposition Method for Fast Manifold Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d53697441ef12a45422f6660202f9840-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/d53697441ef12a45422f6660202f9840-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/d53697441ef12a45422f6660202f9840-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 80001,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2234704685034834264&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Mathematics, Zhejiang University, Yuquan Campus, Hangzhou, 310027, P. R. China; Department of Computer Science, Pennsylvania State University, University Park, PA 16802",
        "aff_domain": "zju.edu.cn;cse.psu.edu",
        "email": "zju.edu.cn;cse.psu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Zhejiang University;Pennsylvania State University",
        "aff_unique_dep": "Department of Mathematics;Department of Computer Science",
        "aff_unique_url": "http://www.zju.edu.cn;https://www.psu.edu",
        "aff_unique_abbr": "ZJU;PSU",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Yuquan;University Park",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "111bdd8694",
        "title": "A General and Efficient Multiple Kernel Learning Algorithm",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/b4944963b5c83d545c3d3022bcf03282-Abstract.html",
        "author": "S\u00f6ren Sonnenburg; Gunnar R\u00e4tsch; Christin Sch\u00e4fer",
        "abstract": "While classical kernel-based learning algorithms are based on a single kernel, in practice it is often desirable to use multiple kernels. Lankriet et al. (2004) considered conic combinations of kernel matrices for classi- \ufb01cation, leading to a convex quadratically constraint quadratic program. We show that it can be rewritten as a semi-in\ufb01nite linear program that can be ef\ufb01ciently solved by recycling the standard SVM implementa- tions. Moreover, we generalize the formulation and our method to a larger class of problems, including regression and one-class classi\ufb01ca- tion. Experimental results show that the proposed algorithm helps for automatic model selection, improving the interpretability of the learn- ing result and works for hundred thousands of examples or hundreds of kernels to be combined.",
        "bibtex": "@inproceedings{NIPS2005_b4944963,\n author = {Sonnenburg, S\\\"{o}ren and R\\\"{a}tsch, Gunnar and Sch\\\"{a}fer, Christin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {A General and Efficient Multiple Kernel Learning Algorithm},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b4944963b5c83d545c3d3022bcf03282-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/b4944963b5c83d545c3d3022bcf03282-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/b4944963b5c83d545c3d3022bcf03282-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 187869,
        "gs_citation": 236,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9579255389032096798&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Fraunhofer FIRST; Friedrich Miescher Lab, Max Planck Society; Fraunhofer FIRST",
        "aff_domain": "first.fhg.de;tue.mpg.de;first.fhg.de",
        "email": "first.fhg.de;tue.mpg.de;first.fhg.de",
        "github": "",
        "project": "http://www.fml.tuebingen.mpg.de/raetsch/projects/mkl_silp",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Fraunhofer Institute for Software and Systems Engineering;Max Planck Society",
        "aff_unique_dep": ";Friedrich Miescher Lab",
        "aff_unique_url": "https://www.first.fraunhofer.de/;https://www.mpg.de",
        "aff_unique_abbr": "Fraunhofer FIRST;MPG",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "51d8e65577",
        "title": "A Hierarchical Compositional System for Rapid Object Detection",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/7f141cf8e7136ce8701dc6636c2a6fe4-Abstract.html",
        "author": "Long Zhu; Alan L. Yuille",
        "abstract": "We describe a hierarchical compositional system for detecting deformable objects in images. Objects are represented by graphical models. The algorithm uses a hierarchical tree where the root of the tree corresponds to the full object and lower-level elements of the tree correspond to simpler features. The algorithm proceeds by passing simple messages up and down the tree. The method works rapidly, in under a second, on 320  240 images. We demonstrate the approach on detecting cats, horses, and hands. The method works in the presence of background clutter and occlusions. Our approach is contrasted with more traditional methods such as dynamic programming and belief propagation.",
        "bibtex": "@inproceedings{NIPS2005_7f141cf8,\n author = {Zhu, Long and Yuille, Alan L},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {A Hierarchical Compositional System for Rapid Object Detection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/7f141cf8e7136ce8701dc6636c2a6fe4-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/7f141cf8e7136ce8701dc6636c2a6fe4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/7f141cf8e7136ce8701dc6636c2a6fe4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 195645,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4838575192246310313&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Statistics, University of California at Los Angeles; Department of Statistics, University of California at Los Angeles",
        "aff_domain": "stat.ucla.edu;stat.ucla.edu",
        "email": "stat.ucla.edu;stat.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Los Angeles",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.ucla.edu",
        "aff_unique_abbr": "UCLA",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a282599739",
        "title": "A PAC-Bayes approach to the Set Covering Machine",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/6f1d0705c91c2145201df18a1a0c7345-Abstract.html",
        "author": "Fran\u00e7ois Laviolette; Mario Marchand; Mohak Shah",
        "abstract": "We design a new learning algorithm for the Set Covering Ma- chine from a PAC-Bayes perspective and propose a PAC-Bayes risk bound which is minimized for classi\ufb01ers achieving a non trivial margin-sparsity trade-o\ufb00.",
        "bibtex": "@inproceedings{NIPS2005_6f1d0705,\n author = {Laviolette, Fran\\c{c}ois and Marchand, Mario and Shah, Mohak},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {A PAC-Bayes approach to the Set Covering Machine},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6f1d0705c91c2145201df18a1a0c7345-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/6f1d0705c91c2145201df18a1a0c7345-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/6f1d0705c91c2145201df18a1a0c7345-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 194508,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12099891359686257061&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "IFT-GLO, Universit\u00e9 Laval; IFT-GLO, Universit\u00e9 Laval; SITE, University of Ottawa",
        "aff_domain": "ift.ulaval.ca;ift.ulaval.ca;site.uottawa.ca",
        "email": "ift.ulaval.ca;ift.ulaval.ca;site.uottawa.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Universit\u00e9 Laval;University of Ottawa",
        "aff_unique_dep": "IFT-GLO;SITE",
        "aff_unique_url": "https://www.ulaval.ca;https://www.uottawa.ca",
        "aff_unique_abbr": "UL;U Ottawa",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Quebec;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "cb18bc82af",
        "title": "A Probabilistic Approach for Optimizing Spectral Clustering",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/1b79b52d1bf6f71b2b1eb7ca08ed0776-Abstract.html",
        "author": "Rong Jin; Feng Kang; Chris H. Ding",
        "abstract": "Spectral clustering enjoys its success in both data clustering and semisupervised learning. But, most spectral clustering algorithms cannot handle multi-class clustering problems directly. Additional strategies are needed to extend spectral clustering algorithms to multi-class clustering problems. Furthermore, most spectral clustering algorithms employ hard cluster membership, which is likely to be trapped by the local optimum. In this paper, we present a new spectral clustering algorithm, named \"Soft Cut\". It improves the normalized cut algorithm by introducing soft membership, and can be efficiently computed using a bound optimization algorithm. Our experiments with a variety of datasets have shown the promising performance of the proposed clustering algorithm.",
        "bibtex": "@inproceedings{NIPS2005_1b79b52d,\n author = {Jin, Rong and Kang, Feng and Ding, Chris},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {A Probabilistic Approach for Optimizing Spectral Clustering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/1b79b52d1bf6f71b2b1eb7ca08ed0776-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/1b79b52d1bf6f71b2b1eb7ca08ed0776-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/1b79b52d1bf6f71b2b1eb7ca08ed0776-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 61233,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18034570451195668694&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Lawrence Berkeley National Laboratory; Michigan State University; Lawrence Berkeley National Laboratory",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Lawrence Berkeley National Laboratory;Michigan State University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.lbl.gov;https://www.msu.edu",
        "aff_unique_abbr": "LBNL;MSU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7dcc56c266",
        "title": "A Probabilistic Interpretation of SVMs with an Application to Unbalanced Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/f804d21145597e42851fa736e221da3f-Abstract.html",
        "author": "Yves Grandvalet; Johnny Mariethoz; Samy Bengio",
        "abstract": "In this paper, we show that the hinge loss can be interpreted as the neg-log-likelihood of a semi-parametric model of posterior probabilities. From this point of view, SVMs represent the parametric component of a semi-parametric model \ufb01tted by a maximum a posteriori estimation pro- cedure. This connection enables to derive a mapping from SVM scores to estimated posterior probabilities. Unlike previous proposals, the sug- gested mapping is interval-valued, providing a set of posterior probabil- ities compatible with each SVM score. This framework offers a new way to adapt the SVM optimization problem to unbalanced classi\ufb01ca- tion, when decisions result in unequal (asymmetric) losses. Experiments show improvements over state-of-the-art procedures.",
        "bibtex": "@inproceedings{NIPS2005_f804d211,\n author = {Grandvalet, Yves and Mariethoz, Johnny and Bengio, Samy},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {A Probabilistic Interpretation of SVMs with an Application to Unbalanced Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f804d21145597e42851fa736e221da3f-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/f804d21145597e42851fa736e221da3f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/f804d21145597e42851fa736e221da3f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 153490,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9722994107954281997&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 21,
        "aff": "Heudiasyc, CNRS/UTC; IDIAP Research Institute; IDIAP Research Institute",
        "aff_domain": "utc.fr;idiap.ch;idiap.ch",
        "email": "utc.fr;idiap.ch;idiap.ch",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "CNRS/UTC;Idiap Research Institute",
        "aff_unique_dep": "Heudiasyc;",
        "aff_unique_url": "https://www.utc.fr;https://www.idiap.ch",
        "aff_unique_abbr": ";IDIAP",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "France;Switzerland"
    },
    {
        "id": "969cf430c7",
        "title": "A Theoretical Analysis of Robust Coding over Noisy Overcomplete Channels",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/1a99f6821980ac99136dcd2f1e9c8740-Abstract.html",
        "author": "Eizaburo Doi; Doru C. Balcan; Michael S. Lewicki",
        "abstract": "Biological sensory systems are faced with the problem of encoding a high-fidelity sensory signal with a population of noisy, low-fidelity neurons. This problem can be expressed in information theoretic terms as coding and transmitting a multi-dimensional, analog signal over a set of noisy channels. Previously, we have shown that robust, overcomplete codes can be learned by minimizing the reconstruction error with a constraint on the channel capacity. Here, we present a theoretical analysis that characterizes the optimal linear coder and decoder for one- and twodimensional data. The analysis allows for an arbitrary number of coding units, thus including both under- and over-complete representations, and provides a number of important insights into optimal coding strategies. In particular, we show how the form of the code adapts to the number of coding units and to different data and noise conditions to achieve robustness. We also report numerical solutions for robust coding of highdimensional image data and show that these codes are substantially more robust compared against other image codes such as ICA and wavelets.",
        "bibtex": "@inproceedings{NIPS2005_1a99f682,\n author = {Doi, Eizaburo and Balcan, Doru and Lewicki, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {A Theoretical Analysis of Robust Coding over Noisy Overcomplete Channels},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/1a99f6821980ac99136dcd2f1e9c8740-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/1a99f6821980ac99136dcd2f1e9c8740-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/1a99f6821980ac99136dcd2f1e9c8740-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 809730,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3498685164264452039&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Center for the Neural Basis of Cognition; Computer Science Department; Center for the Neural Basis of Cognition + Computer Science Department",
        "aff_domain": "cnbc.cmu.edu;cnbc.cmu.edu;cnbc.cmu.edu",
        "email": "cnbc.cmu.edu;cnbc.cmu.edu;cnbc.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0+1",
        "aff_unique_norm": "Center for the Neural Basis of Cognition;Computer Science Department",
        "aff_unique_dep": ";Computer Science",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "2dbc3bb1b1",
        "title": "A matching pursuit approach to sparse Gaussian process regression",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/1ec3e7af38e33222bde173fecaef6bfa-Abstract.html",
        "author": "Sathiya Keerthi; Wei Chu",
        "abstract": "In this paper we propose a new basis selection criterion for building sparse GP regression models that provides promising gains in accuracy as well as efficiency over previous methods. Our algorithm is much faster than that of Smola and Bartlett, while, in generalization it greatly outperforms the information gain approach proposed by Seeger et al, especially on the quality of predictive distributions.",
        "bibtex": "@inproceedings{NIPS2005_1ec3e7af,\n author = {Keerthi, Sathiya and Chu, Wei},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {A matching pursuit approach to sparse Gaussian process regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/1ec3e7af38e33222bde173fecaef6bfa-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/1ec3e7af38e33222bde173fecaef6bfa-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/1ec3e7af38e33222bde173fecaef6bfa-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 112016,
        "gs_citation": 106,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15605564239154967226&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Yahoo! Research Labs; Gatsby Computational Neuroscience Unit, University College London",
        "aff_domain": "yahoo-inc.com;gatsby.ucl.ac.uk",
        "email": "yahoo-inc.com;gatsby.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Yahoo!;University College London",
        "aff_unique_dep": "Research Labs;Gatsby Computational Neuroscience Unit",
        "aff_unique_url": "https://research.yahoo.com;https://www.ucl.ac.uk",
        "aff_unique_abbr": "Yahoo! Research;UCL",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";London",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "ea0998bf25",
        "title": "AER Building Blocks for Multi-Layer Multi-Chip Neuromorphic Vision Systems",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/7f018eb7b301a66658931cb8a93fd6e8-Abstract.html",
        "author": "R. Serrano-Gotarredona; M. Oster; P. Lichtsteiner; A. Linares-Barranco; R. Paz-Vicente; F. Gomez-Rodriguez; H. Kolle Riis; T. Delbruck; S. C. Liu; S. Zahnd; A. M. Whatley; R. Douglas; P. Hafliger; G. Jimenez-Moreno; A. Civit; T. Serrano-Gotarredona; A. Acosta-Jimenez; B. Linares-Barranco",
        "abstract": "A 5-layer neuromorphic vision processor whose components communicate spike events asychronously using the address-event- representation (AER) is demonstrated. The system includes a retina chip, two convolution chips, a 2D winner-take-all chip, a delay line chip, a learning classi\ufb01er chip, and a set of PCBs for computer interfacing and address space remappings. The components use a mixture of analog and digital computation and will learn to classify trajectories of a moving object. A complete experimental setup and measurements results are shown.",
        "bibtex": "@inproceedings{NIPS2005_7f018eb7,\n author = {Serrano-Gotarredona, R. and Oster, M. and Lichtsteiner, P. and Linares-Barranco, A. and Paz-Vicente, R. and Gomez-Rodriguez, F. and Kolle Riis, H. and Delbruck, T. and Liu, S. C. and Zahnd, S. and Whatley, A. M. and Douglas, R. and Hafliger, P. and Jimenez-Moreno, G. and Civit, A. and Serrano-Gotarredona, T. and Acosta-Jimenez, A. and Linares-Barranco, B.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {AER Building Blocks for Multi-Layer Multi-Chip Neuromorphic Vision Systems},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/7f018eb7b301a66658931cb8a93fd6e8-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/7f018eb7b301a66658931cb8a93fd6e8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/7f018eb7b301a66658931cb8a93fd6e8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 566441,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:16nMtGG9DB4J:scholar.google.com/&scioq=AER+Building+Blocks+for+Multi-Layer+Multi-Chip+Neuromorphic+Vision+Systems&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": ";;;;;;;;;;;;;;;;;",
        "aff_domain": ";;;;;;;;;;;;;;;;;",
        "email": ";;;;;;;;;;;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 18,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "c392f08119",
        "title": "Active Bidirectional Coupling in a Cochlear Chip",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/a78482ce76496fcf49085f2190e675b4-Abstract.html",
        "author": "Bo Wen; Kwabena A. Boahen",
        "abstract": "We present a novel cochlear model implemented in analog very large scale integration (VLSI) technology that emulates nonlinear active cochlear behavior. This silicon cochlea includes outer hair cell (OHC) electromotility through active bidirectional coupling (ABC), a mech- anism we proposed in which OHC motile forces, through the mi- croanatomical organization of the organ of Corti, realize the cochlear ampli\ufb01er. Our chip measurements demonstrate that frequency responses become larger and more sharply tuned when ABC is turned on; the de- gree of the enhancement decreases with input intensity as ABC includes saturation of OHC forces.",
        "bibtex": "@inproceedings{NIPS2005_a78482ce,\n author = {Wen, Bo and Boahen, Kwabena A},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Active Bidirectional Coupling in a Cochlear Chip},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a78482ce76496fcf49085f2190e675b4-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/a78482ce76496fcf49085f2190e675b4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/a78482ce76496fcf49085f2190e675b4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 119028,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5775317797639655009&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Bioengineering, University of Pennsylvania; Department of Bioengineering, University of Pennsylvania",
        "aff_domain": "seas.upenn.edu;seas.upenn.edu",
        "email": "seas.upenn.edu;seas.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "Department of Bioengineering",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e965dadbbc",
        "title": "Active Learning For Identifying Function Threshold Boundaries",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/8e930496927757aac0dbd2438cb3f4f6-Abstract.html",
        "author": "Brent Bryan; Robert C. Nichol; Christopher R Genovese; Jeff Schneider; Christopher J. Miller; Larry Wasserman",
        "abstract": "We present an ef\ufb01cient algorithm to actively select queries for learning the boundaries separating a function domain into regions where the func- tion is above and below a given threshold. We develop experiment selec- tion methods based on entropy, misclassi\ufb01cation rate, variance, and their combinations, and show how they perform on a number of data sets. We then show how these algorithms are used to determine simultaneously valid 1 \u2212 \u03b1 con\ufb01dence intervals for seven cosmological parameters. Ex- perimentation shows that the algorithm reduces the computation neces- sary for the parameter estimation problem by an order of magnitude.",
        "bibtex": "@inproceedings{NIPS2005_8e930496,\n author = {Bryan, Brent and Nichol, Robert C. and Genovese, Christopher R and Schneider, Jeff and Miller, Christopher J. and Wasserman, Larry},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Active Learning For Identifying Function Threshold Boundaries},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/8e930496927757aac0dbd2438cb3f4f6-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/8e930496927757aac0dbd2438cb3f4f6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/8e930496927757aac0dbd2438cb3f4f6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1002781,
        "gs_citation": 133,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16833999442540195360&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Center for Automated Learning and Discovery, Carnegie Mellon University, Pittsburgh, PA 15213; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213; Institute of Cosmology and Gravitation, University of Portsmouth, Portsmouth, PO1 2EG, UK; Observatorio Cerro Tololo, Observatorio de AURA en Chile, La Serena, Chile; Department of Statistics, Carnegie Mellon University, Pittsburgh, PA 15213; Department of Statistics, Carnegie Mellon University, Pittsburgh, PA 15213",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;port.ac.uk;noao.edu;stat.cmu.edu;stat.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;port.ac.uk;noao.edu;stat.cmu.edu;stat.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2;0;0",
        "aff_unique_norm": "Carnegie Mellon University;University of Portsmouth;Observatorio Cerro Tololo",
        "aff_unique_dep": "Center for Automated Learning and Discovery;Institute of Cosmology and Gravitation;",
        "aff_unique_url": "https://www.cmu.edu;https://www.port.ac.uk;",
        "aff_unique_abbr": "CMU;UoP;",
        "aff_campus_unique_index": "0;0;1;2;0;0",
        "aff_campus_unique": "Pittsburgh;Portsmouth;La Serena",
        "aff_country_unique_index": "0;0;1;2;0;0",
        "aff_country_unique": "United States;United Kingdom;Chile"
    },
    {
        "id": "bd0de85a54",
        "title": "Active Learning for Misspecified Models",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/0c1c995b77ea7312f887ddd9f9d35de5-Abstract.html",
        "author": "Masashi Sugiyama",
        "abstract": "expressed as",
        "bibtex": "@inproceedings{NIPS2005_0c1c995b,\n author = {Sugiyama, Masashi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Active Learning for Misspecified Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/0c1c995b77ea7312f887ddd9f9d35de5-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/0c1c995b77ea7312f887ddd9f9d35de5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/0c1c995b77ea7312f887ddd9f9d35de5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 111302,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11529136813306631542&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, Tokyo Institute of Technology",
        "aff_domain": "cs.titech.ac.jp",
        "email": "cs.titech.ac.jp",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Tokyo Institute of Technology",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.titech.ac.jp",
        "aff_unique_abbr": "Titech",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Tokyo",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "571ae73d98",
        "title": "Affine Structure From Sound",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/a7a3d70c6d17a73140918996d03c014f-Abstract.html",
        "author": "Sebastian Thrun",
        "abstract": "We consider the problem of localizing a set of microphones together with a set of external acoustic events (e.g., hand claps), emitted at un- known times and unknown locations. We propose a solution that ap- proximates this problem under a far \ufb01eld approximation de\ufb01ned in the calculus of af\ufb01ne geometry, and that relies on singular value decompo- sition (SVD) to recover the af\ufb01ne structure of the problem. We then de\ufb01ne low-dimensional optimization techniques for embedding the solu- tion into Euclidean geometry, and further techniques for recovering the locations and emission times of the acoustic events. The approach is use- ful for the calibration of ad-hoc microphone arrays and sensor networks.",
        "bibtex": "@inproceedings{NIPS2005_a7a3d70c,\n author = {Thrun, Sebastian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Affine Structure From Sound},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a7a3d70c6d17a73140918996d03c014f-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/a7a3d70c6d17a73140918996d03c014f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/a7a3d70c6d17a73140918996d03c014f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 96933,
        "gs_citation": 151,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7671205918204831039&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "5d545bf2f7",
        "title": "An Alternative Infinite Mixture Of Gaussian Process Experts",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/f499d34bd87b42948b3960b8f6b82e74-Abstract.html",
        "author": "Edward Meeds; Simon Osindero",
        "abstract": "We present an in\ufb01nite mixture model in which each component com- prises a multivariate Gaussian distribution over an input space, and a Gaussian Process model over an output space. Our model is neatly able to deal with non-stationary covariance functions, discontinuities, multi- modality and overlapping output signals. The work is similar to that by Rasmussen and Ghahramani [1]; however, we use a full generative model over input and output space rather than just a conditional model. This al- lows us to deal with incomplete data, to perform inference over inverse functional mappings as well as for regression, and also leads to a more powerful and consistent Bayesian speci\ufb01cation of the effective \u2018gating network\u2019 for the different experts.",
        "bibtex": "@inproceedings{NIPS2005_f499d34b,\n author = {Meeds, Edward and Osindero, Simon},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {An Alternative Infinite Mixture Of Gaussian Process Experts},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f499d34bd87b42948b3960b8f6b82e74-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/f499d34bd87b42948b3960b8f6b82e74-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/f499d34bd87b42948b3960b8f6b82e74-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 361197,
        "gs_citation": 193,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4444264403788787479&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "86e2877bf3",
        "title": "An Analog Visual Pre-Processing Processor Employing Cyclic Line Access in Only-Nearest-Neighbor-Interconnects Architecture",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/6624b6d8217cf71640993409df58204f-Abstract.html",
        "author": "Yusuke Nakashita; Yoshio Mita; Tadashi Shibata",
        "abstract": "An analog focal-plane processor having a 128128 photodiode array has been developed for directional edge \ufb01ltering. It can perform 44-pixel kernel convolution for entire pixels only with 256 steps of simple ana- log processing. Newly developed cyclic line access and row-parallel processing scheme in conjunction with the \u201conly-nearest-neighbor in- terconnects\u201d architecture has enabled a very simple implementation. A proof-of-concept chip was fabricated in a 0.35-(cid:0)m 2-poly 3-metal CMOS technology and the edge \ufb01ltering at a rate of 200 frames/sec. has been experimentally demonstrated.",
        "bibtex": "@inproceedings{NIPS2005_6624b6d8,\n author = {Nakashita, Yusuke and Mita, Yoshio and Shibata, Tadashi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {An Analog Visual Pre-Processing Processor Employing Cyclic Line Access in Only-Nearest-Neighbor-Interconnects Architecture},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6624b6d8217cf71640993409df58204f-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/6624b6d8217cf71640993409df58204f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/6624b6d8217cf71640993409df58204f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 144189,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3218786994363800967&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Frontier Informatics, School of Frontier Sciences, The University of Tokyo; Department of Electrical Engineering, School of Engineering, The University of Tokyo; Department of Frontier Informatics, School of Frontier Sciences, The University of Tokyo",
        "aff_domain": "else.k.u-tokyo.ac.jp;ee.t.u-tokyo.ac.jp;ee.t.u-tokyo.ac.jp",
        "email": "else.k.u-tokyo.ac.jp;ee.t.u-tokyo.ac.jp;ee.t.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Tokyo",
        "aff_unique_dep": "Department of Frontier Informatics",
        "aff_unique_url": "https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "UTokyo",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Tokyo",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "dbe9c51620",
        "title": "An Application of Markov Random Fields to Range Sensing",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/353de26971b93af88da102641069b440-Abstract.html",
        "author": "James Diebel; Sebastian Thrun",
        "abstract": "This paper describes a highly successful application of MRFs to the problem of generating high-resolution range images. A new generation of range sensors combines the capture of low-resolution range images with the acquisition of registered high-resolution camera images. The MRF in this paper exploits the fact that discontinuities in range and coloring tend to co-align. This enables it to generate high-resolution, low-noise range images by integrating regular camera images into the range data. We show that by using such an MRF, we can substantially improve over existing range imaging technology.",
        "bibtex": "@inproceedings{NIPS2005_353de269,\n author = {Diebel, James and Thrun, Sebastian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {An Application of Markov Random Fields to Range Sensing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/353de26971b93af88da102641069b440-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/353de26971b93af88da102641069b440-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/353de26971b93af88da102641069b440-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 412881,
        "gs_citation": 820,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17601871686087185988&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Stanford AI Lab; Stanford University, Stanford, CA 94305",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Artificial Intelligence Laboratory",
        "aff_unique_url": "https://ai.stanford.edu",
        "aff_unique_abbr": "Stanford AI Lab",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0b6cde740b",
        "title": "An Approximate Inference Approach for the PCA Reconstruction Error",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/a6db4ed04f1621a119799fd3d7545d3d-Abstract.html",
        "author": "Manfred Opper",
        "abstract": "The problem of computing a resample estimate for the reconstruction error in PCA is reformulated as an inference problem with the help of the replica method. Using the expectation consistent (EC) approximation, the intractable inference problem can be solved efficiently using only two variational parameters. A perturbative correction to the result is computed and an alternative simplified derivation is also presented.",
        "bibtex": "@inproceedings{NIPS2005_a6db4ed0,\n author = {Opper, Manfred},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {An Approximate Inference Approach for the PCA Reconstruction Error},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a6db4ed04f1621a119799fd3d7545d3d-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/a6db4ed04f1621a119799fd3d7545d3d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/a6db4ed04f1621a119799fd3d7545d3d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 209259,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=96532492904289860&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Electronics and Computer Science, University of Southampton",
        "aff_domain": "ecs.soton.ac.uk",
        "email": "ecs.soton.ac.uk",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Southampton",
        "aff_unique_dep": "Electronics and Computer Science",
        "aff_unique_url": "https://www.southampton.ac.uk",
        "aff_unique_abbr": "Southampton",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "15da1ce1d7",
        "title": "An aVLSI Cricket Ear Model",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/64c31821603ab476a318839606743bd6-Abstract.html",
        "author": "Andre V. Schaik; Richard Reeve; Craig Jin; Tara Hamilton",
        "abstract": "Female crickets can locate males by phonotaxis to the mating song they produce. The behaviour and underlying physiology has been studied in some depth showing that the cricket auditory system solves this complex problem in a unique manner. We present an analogue very large scale integrated (aVLSI) circuit model of this process and show that results from testing the circuit agree with simulation and what is known from the behaviour and physiology of the cricket auditory system. The aVLSI circuitry is now being extended to use on a robot along with previously modelled neural circuitry to better understand the complete sensorimotor pathway.",
        "bibtex": "@inproceedings{NIPS2005_64c31821,\n author = {Schaik, Andre and Reeve, Richard and Jin, Craig and Hamilton, Tara},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {An aVLSI Cricket Ear Model},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/64c31821603ab476a318839606743bd6-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/64c31821603ab476a318839606743bd6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/64c31821603ab476a318839606743bd6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 328891,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10732221598686344314&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "School of Electrical and Information Engineering*; Institute of Perception, Action and Behaviour+; School of Electrical and Information Engineering*; School of Electrical and Information Engineering*",
        "aff_domain": "ee.usyd.edu.au;inf.ed.ac.uk;ee.usyd.edu.au;ee.usyd.edu.au",
        "email": "ee.usyd.edu.au;inf.ed.ac.uk;ee.usyd.edu.au;ee.usyd.edu.au",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "University of Sydney;Institute of Perception, Action and Behaviour",
        "aff_unique_dep": "School of Electrical and Information Engineering;Institute of Perception, Action and Behaviour",
        "aff_unique_url": "https://www.sydney.edu.au;",
        "aff_unique_abbr": "USYD;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia;"
    },
    {
        "id": "e889e9dfdc",
        "title": "An exploration-exploitation model based on norepinepherine and dopamine activity",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/bc4e356fee1972242c8f7eabf4dff517-Abstract.html",
        "author": "Samuel M. McClure; Mark S. Gilzenrat; Jonathan D. Cohen",
        "abstract": "We propose a model by which dopamine (DA) and norepinepherine  (NE) combine to alternate behavior between relatively exploratory  and  exploitative  modes.  The  model  is  developed  for  a  target  detection  task  for  which  there  is  extant  single  neuron  recording  data  available  from  locus  coeruleus  (LC)  NE  neurons.  An  exploration-exploitation trade-off is elicited by regularly switching  which  of  the  two  stimuli  are  rewarded.  DA  functions  within  the  model  to  change  synaptic  weights  according  to  a  reinforcement  learning  algorithm.  Exploration  is  mediated  by  the  state  of  LC  firing,  with  higher  tonic  and  lower  phasic  activity  producing  greater  response  variability.  The  opposite  state  of  LC  function,  with lower baseline firing rate and greater phasic responses, favors  exploitative  behavior.  Changes  in  LC  firing  mode  result  from  combined  measures  of  response  conflict  and  reward  rate,  where  response  conflict  is  monitored  using  models  of  anterior  cingulate  cortex (ACC). Increased long-term response conflict and decreased  reward  rate,  which  occurs  following  reward  contingency  switch,  favors  the  higher  tonic  state  of  LC  function  and  NE  release.  This  increases exploration, and facilitates discovery of the new target.",
        "bibtex": "@inproceedings{NIPS2005_bc4e356f,\n author = {McClure, Samuel M. and Gilzenrat, Mark S. and Cohen, Jonathan D},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {An exploration-exploitation model based on norepinepherine and dopamine activity},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/bc4e356fee1972242c8f7eabf4dff517-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/bc4e356fee1972242c8f7eabf4dff517-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/bc4e356fee1972242c8f7eabf4dff517-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 775254,
        "gs_citation": 114,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16717842337186148646&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Center for the Study of Brain, Mind, and Behavior, Princeton University; Center for the Study of Brain, Mind, and Behavior, Princeton University; Center for the Study of Brain, Mind, and Behavior, Princeton University",
        "aff_domain": "princeton.edu;princeton.edu;princeton.edu",
        "email": "princeton.edu;princeton.edu;princeton.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Princeton University",
        "aff_unique_dep": "Center for the Study of Brain, Mind, and Behavior",
        "aff_unique_url": "https://www.princeton.edu",
        "aff_unique_abbr": "Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ae49d53a6f",
        "title": "Analysis of Spectral Kernel Design based Semi-supervised Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/35c5a2cb362c4d214156f930e7d13252-Abstract.html",
        "author": "Tong Zhang; Rie Kubota Ando",
        "abstract": "We consider a framework for semi-supervised learning using spectral decomposition based un-supervised kernel design. This approach sub- sumes a class of previously proposed semi-supervised learning methods on data graphs. We examine various theoretical properties of such meth- ods. In particular, we derive a generalization performance bound, and obtain the optimal kernel design by minimizing the bound. Based on the theoretical analysis, we are able to demonstrate why spectral kernel design based methods can often improve the predictive performance. Ex- periments are used to illustrate the main consequences of our analysis.",
        "bibtex": "@inproceedings{NIPS2005_35c5a2cb,\n author = {Zhang, Tong and Ando, Rie Kubota},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Analysis of Spectral Kernel Design based Semi-supervised Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/35c5a2cb362c4d214156f930e7d13252-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/35c5a2cb362c4d214156f930e7d13252-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/35c5a2cb362c4d214156f930e7d13252-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 101074,
        "gs_citation": 99,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5554777298901475907&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Yahoo! Inc., New York City, NY 10011; IBM T. J. Watson Research Center, Yorktown Heights, NY 10598",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Yahoo! Inc.;IBM",
        "aff_unique_dep": ";IBM T. J. Watson Research Center",
        "aff_unique_url": "https://www.yahoo.com;https://www.ibm.com/research/watson",
        "aff_unique_abbr": "Yahoo!;IBM Watson",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "New York City;Yorktown Heights",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c36531ac09",
        "title": "Analyzing Auditory Neurons by Learning Distance Functions",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/9719a00ed0c5709d80dfef33795dcef3-Abstract.html",
        "author": "Inna Weiner; Tomer Hertz; Israel Nelken; Daphna Weinshall",
        "abstract": "We present a novel approach to the characterization of complex sensory neurons. One of the main goals of characterizing sensory neurons is to characterize dimensions in stimulus space to which the neurons are highly sensitive (causing large gradients in the neural responses) or al- ternatively dimensions in stimulus space to which the neuronal response are invariant (de\ufb01ning iso-response manifolds). We formulate this prob- lem as that of learning a geometry on stimulus space that is compatible with the neural responses: the distance between stimuli should be large when the responses they evoke are very different, and small when the re- sponses they evoke are similar. Here we show how to successfully train such distance functions using rather limited amount of information. The data consisted of the responses of neurons in primary auditory cortex (A1) of anesthetized cats to 32 stimuli derived from natural sounds. For each neuron, a subset of all pairs of stimuli was selected such that the responses of the two stimuli in a pair were either very similar or very dissimilar. The distance function was trained to \ufb01t these constraints. The resulting distance functions generalized to predict the distances between the responses of a test stimulus and the trained stimuli.",
        "bibtex": "@inproceedings{NIPS2005_9719a00e,\n author = {Weiner, Inna and Hertz, Tomer and Nelken, Israel and Weinshall, Daphna},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Analyzing Auditory Neurons by Learning Distance Functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/9719a00ed0c5709d80dfef33795dcef3-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/9719a00ed0c5709d80dfef33795dcef3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/9719a00ed0c5709d80dfef33795dcef3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 86036,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=266622849518795320&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "School of Computer Science and Engineering; The Center for Neural Computation+Department of Neurobiology; The Center for Neural Computation+Department of Neurobiology; School of Computer Science and Engineering+The Center for Neural Computation",
        "aff_domain": "cs.huji.ac.il;cs.huji.ac.il;md.huji.ac.il;cs.huji.ac.il",
        "email": "cs.huji.ac.il;cs.huji.ac.il;md.huji.ac.il;cs.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2;1+2;0+1",
        "aff_unique_norm": "University Affiliation Not Specified;Center for Neural Computation;Institution not specified",
        "aff_unique_dep": "School of Computer Science and Engineering;Neural Computation;Department of Neurobiology",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";;",
        "aff_country_unique": ""
    },
    {
        "id": "7c9c9e9523",
        "title": "Analyzing Coupled Brain Sources: Distinguishing True from Spurious Interaction",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/ae587cfeea5ac21a8f1c1ea51027fef0-Abstract.html",
        "author": "Guido Nolte; Andreas Ziehe; Frank Meinecke; Klaus-Robert M\u00fcller",
        "abstract": "When trying to understand the brain, it is of fundamental importance to analyse (e.g. from EEG/MEG measurements) what parts of the cortex interact with each other in order to infer more accurate models of brain activity. Common techniques like Blind Source Separation (BSS) can estimate brain sources and single out artifacts by using the underlying assumption of source signal independence. However, physiologically interesting brain sources typically interact, so BSS will--by construction-- fail to characterize them properly. Noting that there are truly interacting sources and signals that only seemingly interact due to effects of volume conduction, this work aims to contribute by distinguishing these effects. For this a new BSS technique is proposed that uses anti-symmetrized cross-correlation matrices and subsequent diagonalization. The resulting decomposition consists of the truly interacting brain sources and suppresses any spurious interaction stemming from volume conduction. Our new concept of interacting source analysis (ISA) is successfully demonstrated on MEG data.",
        "bibtex": "@inproceedings{NIPS2005_ae587cfe,\n author = {Nolte, Guido and Ziehe, Andreas and Meinecke, Frank and M\\\"{u}ller, Klaus-Robert},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Analyzing Coupled Brain Sources: Distinguishing True from Spurious Interaction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ae587cfeea5ac21a8f1c1ea51027fef0-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/ae587cfeea5ac21a8f1c1ea51027fef0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/ae587cfeea5ac21a8f1c1ea51027fef0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 188167,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12087831722363969802&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "d52b7eca6b",
        "title": "Assessing Approximations for Gaussian Process Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/3c333aadfc3ee8ecb8d77ee31197d96a-Abstract.html",
        "author": "Malte Kuss; Carl E. Rasmussen",
        "abstract": "Gaussian processes are attractive models for probabilistic classification but unfortunately exact inference is analytically intractable. We compare Laplace's method and Expectation Propagation (EP) focusing on marginal likelihood estimates and predictive performance. We explain theoretically and corroborate empirically that EP is superior to Laplace. We also compare to a sophisticated MCMC scheme and show that EP is surprisingly accurate. In recent years models based on Gaussian process (GP) priors have attracted much attention in the machine learning community. Whereas inference in the GP regression model with Gaussian noise can be done analytically, probabilistic classification using GPs is analytically intractable. Several approaches to approximate Bayesian inference have been suggested, including Laplace's approximation, Expectation Propagation (EP), variational approximations and Markov chain Monte Carlo (MCMC) sampling, some of these in conjunction with generalisation bounds, online learning schemes and sparse approximations. Despite the abundance of recent work on probabilistic GP classifiers, most experimental studies provide only anecdotal evidence, and no clear picture has yet emerged, as to when and why which algorithm should be preferred. Thus, from a practitioners point of view probabilistic GP classification remains a jungle. In this paper, we set out to understand and compare two of the most wide-spread approximations: Laplace's method and Expectation Propagation (EP). We also compare to a sophisticated, but computationally demanding MCMC scheme to examine how close the approximations are to ground truth. We examine two aspects of the approximation schemes: Firstly the accuracy of approximations to the marginal likelihood which is of central importance for model selection and model comparison. In any practical application of GPs in classification (usually multiple) parameters of the covariance function (hyperparameters) have to be handled. Bayesian model selection provides a consistent framework for setting such parameters. Therefore, it is essential to evaluate the accuracy of the marginal likelihood approximations as a function of the hyperparameters, in order to assess the practical usefulness of the approach Secondly, we need to assess the quality of the approximate probabilistic predictions. In the past, the probabilistic nature of the GP predictions have not received much attention, the focus being mostly on classification error rates. This unfortunate state of affairs is caused primarily by typical benchmarking problems being considered outside of a realistic context. The ability of a classifier to produce class probabilities or confidences, have obvious",
        "bibtex": "@inproceedings{NIPS2005_3c333aad,\n author = {Kuss, Malte and Rasmussen, Carl},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Assessing Approximations for Gaussian Process Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/3c333aadfc3ee8ecb8d77ee31197d96a-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/3c333aadfc3ee8ecb8d77ee31197d96a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/3c333aadfc3ee8ecb8d77ee31197d96a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 205932,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15216001418729792222&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "35276f079a",
        "title": "Asymptotics of Gaussian Regularized Least Squares",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/b139aeda1c2914e3b579aafd3ceeb1bd-Abstract.html",
        "author": "Ross Lippert; Ryan Rifkin",
        "abstract": "We consider regularized least-squares (RLS) with a Gaussian kernel. We prove that if we let the Gaussian bandwidth \u03c3 \u2192 \u221e while letting the regularization parameter \u03bb \u2192 0, the RLS solution tends to a polynomial whose order is controlled by the rielative rates of decay of 1 \u03c32 and \u03bb: if \u03bb = \u03c3\u2212(2k+1), then, as \u03c3 \u2192 \u221e, the RLS solution tends to the kth order polynomial with minimal empirical error. We illustrate the result with an example.",
        "bibtex": "@inproceedings{NIPS2005_b139aeda,\n author = {Lippert, Ross and Rifkin, Ryan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Asymptotics of Gaussian Regularized Least Squares},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b139aeda1c2914e3b579aafd3ceeb1bd-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/b139aeda1c2914e3b579aafd3ceeb1bd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/b139aeda1c2914e3b579aafd3ceeb1bd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 184619,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10751253238998868058&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "M.I.T., Department of Mathematics; Honda Research Institute USA, Inc.",
        "aff_domain": "math.mit.edu;honda-ri.com",
        "email": "math.mit.edu;honda-ri.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;Honda Research Institute USA",
        "aff_unique_dep": "Department of Mathematics;Research Institute",
        "aff_unique_url": "https://web.mit.edu;https://honda-ri.com",
        "aff_unique_abbr": "MIT;HRI USA",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c817e78824",
        "title": "Augmented Rescorla-Wagner and Maximum Likelihood Estimation",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/b4fd1d2cb085390fbbadae65e07876a7-Abstract.html",
        "author": "Alan L. Yuille",
        "abstract": "We show that linear generalizations of Rescorla-Wagner can perform Maximum Likelihood estimation of the parameters of all generative models for causal reasoning. Our approach involves augmenting variables to deal with conjunctions of causes, similar to the agumented model of Rescorla. Our results involve genericity assumptions on the distributions of causes. If these assumptions are violated, for example for the Cheng causal power theory, then we show that a linear Rescorla-Wagner can estimate the parameters of the model up to a nonlinear transformtion. Moreover, a nonlinear Rescorla-Wagner is able to estimate the parameters directly to within arbitrary accuracy. Previous results can be used to determine convergence and to estimate convergence rates.",
        "bibtex": "@inproceedings{NIPS2005_b4fd1d2c,\n author = {Yuille, Alan L},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Augmented Rescorla-Wagner and Maximum Likelihood Estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b4fd1d2cb085390fbbadae65e07876a7-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/b4fd1d2cb085390fbbadae65e07876a7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/b4fd1d2cb085390fbbadae65e07876a7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 56167,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16832509044883355425&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Statistics, University of California at Los Angeles",
        "aff_domain": "stat.ucla.edu",
        "email": "stat.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of California, Los Angeles",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.ucla.edu",
        "aff_unique_abbr": "UCLA",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b8112070cd",
        "title": "Bayesian Sets",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/182e6c2d3d78eef40e5dac7da77a748f-Abstract.html",
        "author": "Zoubin Ghahramani; Katherine A. Heller",
        "abstract": "Inspired by \u201cGoogle\u2122 Sets\u201d, we consider the problem of retrieving items from a concept or cluster, given a query consisting of a few items from that cluster. We formulate this as a Bayesian inference problem and de- scribe a very simple algorithm for solving it. Our algorithm uses a model- based concept of a cluster and ranks items using a score which evaluates the marginal probability that each item belongs to a cluster containing the query items. For exponential family models with conjugate priors this marginal probability is a simple function of suf\ufb01cient statistics. We focus on sparse binary data and show that our score can be evaluated ex- actly using a single sparse matrix multiplication, making it possible to apply our algorithm to very large datasets. We evaluate our algorithm on three datasets: retrieving movies from EachMovie, \ufb01nding completions of author sets from the NIPS dataset, and \ufb01nding completions of sets of words appearing in the Grolier encyclopedia. We compare to Google\u2122 Sets and show that Bayesian Sets gives very reasonable set completions.",
        "bibtex": "@inproceedings{NIPS2005_182e6c2d,\n author = {Ghahramani, Zoubin and Heller, Katherine A},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Bayesian Sets},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/182e6c2d3d78eef40e5dac7da77a748f-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/182e6c2d3d78eef40e5dac7da77a748f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/182e6c2d3d78eef40e5dac7da77a748f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 129970,
        "gs_citation": 204,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6693586583136573191&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Gatsby Computational Neuroscience Unit, University College London; Gatsby Computational Neuroscience Unit, University College London + CALD, Carnegie Mellon University",
        "aff_domain": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk",
        "email": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk",
        "github": "",
        "project": "http://labs.google.com/sets",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "University College London;Carnegie Mellon University",
        "aff_unique_dep": "Gatsby Computational Neuroscience Unit;Computer Architecture Lab for Design Automation (CALD)",
        "aff_unique_url": "https://www.ucl.ac.uk;https://www.cmu.edu",
        "aff_unique_abbr": "UCL;CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "London;",
        "aff_country_unique_index": "0;0+1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "ad4c263877",
        "title": "Bayesian Surprise Attracts Human Attention",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/0172d289da48c48de8c5ebf3de9f7ee1-Abstract.html",
        "author": "Laurent Itti; Pierre F. Baldi",
        "abstract": "The concept of surprise is central to sensory processing, adaptation, learning, and attention. Yet, no widely-accepted mathematical theory currently exists to quantitatively characterize surprise elicited by a stimulus or event, for observers that range from single neurons to complex natural or engineered systems. We describe a formal Bayesian definition of surprise that is the only consistent formulation under minimal axiomatic assumptions. Surprise quantifies how data affects a natural or artificial observer, by measuring the difference between posterior and prior beliefs of the observer. Using this framework we measure the extent to which humans direct their gaze towards surprising items while watching television and video games. We find that subjects are strongly attracted towards surprising locations, with 72% of all human gaze shifts directed towards locations more surprising than the average, a figure which rises to 84% when considering only gaze targets simultaneously selected by all subjects. The resulting theory of surprise is applicable across different spatio-temporal scales, modalities, and levels of abstraction. Life is full of surprises, ranging from a great christmas gift or a new magic trick, to wardrobe malfunctions, reckless drivers, terrorist attacks, and tsunami waves. Key to survival is our ability to rapidly attend to, identify, and learn from surprising events, to decide on present and future courses of action [1]. Yet, little theoretical and computational understanding exists of the very essence of surprise, as evidenced by the absence from our everyday vocabulary of a quantitative unit of surprise: Qualities such as the \"wow factor\" have remained vague and elusive to mathematical analysis. Informal correlates of surprise exist at nearly all stages of neural processing. In sensory neuroscience, it has been suggested that only the unexpected at one stage is transmitted to the next stage [2]. Hence, sensory cortex may have evolved to adapt to, to predict, and to quiet down the expected statistical regularities of the world [3, 4, 5, 6], focusing instead on events that are unpredictable or surprising. Electrophysiological evidence for this early sensory emphasis onto surprising stimuli exists from studies of adaptation in visual [7, 8, 4, 9], olfactory [10, 11], and auditory cortices [12], subcortical structures like the LGN [13], and even retinal ganglion cells [14, 15] and cochlear hair cells [16]: neural response greatly attenuates with repeated or prolonged exposure to an initially novel stimulus. Surprise and novelty are also central to learning and memory formation [1], to the point that surprise is believed to be a necessary trigger for associative learning [17, 18], \nas supported by mounting evidence for a role of the hippocampus as a novelty detector [19, 20, 21]. Finally, seeking novelty is a well-identified human character trait, with possible association with the dopamine D4 receptor gene [22, 23, 24]. In the Bayesian framework, we develop the only consistent theory of surprise, in terms of the difference between the posterior and prior distributions of beliefs of an observer over the available class of models or hypotheses about the world. We show that this definition derived from first principles presents key advantages over more ad-hoc formulations, typically relying on detecting outlier stimuli. Armed with this new framework, we provide direct experimental evidence that surprise best characterizes what attracts human gaze in large amounts of natural video stimuli. We here extend a recent pilot study [25], adding more comprehensive theory, large-scale human data collection, and additional analysis.",
        "bibtex": "@inproceedings{NIPS2005_0172d289,\n author = {Itti, Laurent and Baldi, Pierre},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Bayesian Surprise Attracts Human Attention},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/0172d289da48c48de8c5ebf3de9f7ee1-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/0172d289da48c48de8c5ebf3de9f7ee1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/0172d289da48c48de8c5ebf3de9f7ee1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 342774,
        "gs_citation": 1411,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=581682650606298705&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Department of Computer Science, University of Southern California, Los Angeles, California 90089-2520, USA; Department of Computer Science, University of California, Irvine, Irvine, California 92697-3425, USA",
        "aff_domain": "usc.edu;ics.uci.edu",
        "email": "usc.edu;ics.uci.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Southern California;University of California, Irvine",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.usc.edu;https://www.uci.edu",
        "aff_unique_abbr": "USC;UCI",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Los Angeles;Irvine",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6f72f8391a",
        "title": "Bayesian model learning in human visual perception",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/8b3bac12926cc1d9fb5d68783376971d-Abstract.html",
        "author": "Gerg\u0151 Orb\u00e1n; Jozsef Fiser; Richard N Aslin; M\u00e1t\u00e9 Lengyel",
        "abstract": "Humans make optimal perceptual decisions in noisy and ambiguous conditions. Computations underlying such optimal behavior have been shown to rely on probabilistic inference according to generative models whose structure is usually taken to be known a priori. We argue that Bayesian model selection is ideal for inferring similar and even more complex model structures from experience. We \ufb01nd in experiments that humans learn subtle statistical properties of visual scenes in a completely unsupervised manner. We show that these \ufb01ndings are well captured by Bayesian model learning within a class of models that seek to explain observed variables by independent hidden causes.",
        "bibtex": "@inproceedings{NIPS2005_8b3bac12,\n author = {Orb\\'{a}n, Gerg\\H{o} and Fiser, Jozsef and Aslin, Richard N and Lengyel, M\\'{a}t\\'{e}},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Bayesian model learning in human visual perception},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/8b3bac12926cc1d9fb5d68783376971d-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/8b3bac12926cc1d9fb5d68783376971d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/8b3bac12926cc1d9fb5d68783376971d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 148700,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16646904981007450342&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Collegium Budapest Institute for Advanced Study; Department of Psychology and Volen Center for Complex Systems, Brandeis University; Department of Brain and Cognitive Sciences, Center for Visual Science, University of Rochester; Gatsby Computational Neuroscience Unit, University College London",
        "aff_domain": "colbud.hu;brandeis.edu;cvs.rochester.edu;gatsby.ucl.ac.uk",
        "email": "colbud.hu;brandeis.edu;cvs.rochester.edu;gatsby.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "Collegium Budapest Institute for Advanced Study;Brandeis University;University of Rochester;University College London",
        "aff_unique_dep": ";Department of Psychology and Volen Center for Complex Systems;Department of Brain and Cognitive Sciences;Gatsby Computational Neuroscience Unit",
        "aff_unique_url": "https://www.collegium.hu;https://www.brandeis.edu;https://www.rochester.edu;https://www.ucl.ac.uk",
        "aff_unique_abbr": ";Brandeis;U of R;UCL",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Rochester;London",
        "aff_country_unique_index": "0;1;1;2",
        "aff_country_unique": "Hungary;United States;United Kingdom"
    },
    {
        "id": "23d679e337",
        "title": "Bayesian models of human action understanding",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/f5b1b89d98b7286673128a5fb112cb9a-Abstract.html",
        "author": "Chris Baker; Rebecca Saxe; Joshua B. Tenenbaum",
        "abstract": "We present a Bayesian framework for explaining how people reason about and predict the actions of an intentional agent, based on observ- ing its behavior. Action-understanding is cast as a problem of inverting a probabilistic generative model, which assumes that agents tend to act rationally in order to achieve their goals given the constraints of their en- vironment. Working in a simple sprite-world domain, we show how this model can be used to infer the goal of an agent and predict how the agent will act in novel situations or when environmental constraints change. The model provides a qualitative account of several kinds of inferences that preverbal infants have been shown to perform, and also \ufb01ts quantita- tive predictions that adult observers make in a new experiment.",
        "bibtex": "@inproceedings{NIPS2005_f5b1b89d,\n author = {Baker, Chris and Saxe, Rebecca and Tenenbaum, Joshua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Bayesian models of human action understanding},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f5b1b89d98b7286673128a5fb112cb9a-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/f5b1b89d98b7286673128a5fb112cb9a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/f5b1b89d98b7286673128a5fb112cb9a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 83837,
        "gs_citation": 179,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=216282900969576728&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Brain and Cognitive Sciences; Department of Brain and Cognitive Sciences; Department of Brain and Cognitive Sciences",
        "aff_domain": "mit.edu;mit.edu;mit.edu",
        "email": "mit.edu;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Department of Brain and Cognitive Sciences",
        "aff_unique_url": "https://web.mit.edu/bcs/",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e660e50011",
        "title": "Benchmarking Non-Parametric Statistical Tests",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/eb9fc349601c69352c859c1faa287874-Abstract.html",
        "author": "Mikaela Keller; Samy Bengio; Siew Y. Wong",
        "abstract": "Although non-parametric tests have already been proposed for that purpose, statistical significance tests for non-standard measures (different from the classification error) are less often used in the literature. This paper is an attempt at empirically verifying how these tests compare with more classical tests, on various conditions. More precisely, using a very large dataset to estimate the whole \"population\", we analyzed the behavior of several statistical test, varying the class unbalance, the compared models, the performance measure, and the sample size. The main result is that providing big enough evaluation sets non-parametric tests are relatively reliable in all conditions.",
        "bibtex": "@inproceedings{NIPS2005_eb9fc349,\n author = {Keller, Mikaela and Bengio, Samy and Wong, Siew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Benchmarking Non-Parametric Statistical Tests},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/eb9fc349601c69352c859c1faa287874-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/eb9fc349601c69352c859c1faa287874-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/eb9fc349601c69352c859c1faa287874-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 75793,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11990704921530770065&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 20,
        "aff": "DIAP Research Institute; DIAP Research Institute; DIAP Research Institute",
        "aff_domain": "idiap.ch;idiap.ch;idiap.ch",
        "email": "idiap.ch;idiap.ch;idiap.ch",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "DIAP Research Institute",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3d12ed6bb9",
        "title": "Beyond Gaussian Processes: On the Distributions of Infinite Networks",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/e2c61965b5e23b47b77d7c51611b6d7f-Abstract.html",
        "author": "Ricky Der; Daniel D. Lee",
        "abstract": "A general analysis of the limiting distribution of neural network functions is performed, with emphasis on non-Gaussian limits. We show that with i.i.d. symmetric stable output weights, and more generally with weights distributed from the normal domain of attraction of a stable variable, that the neural functions converge in distribution to stable processes. Conditions are also investigated under which Gaussian limits do occur when the weights are independent but not identically distributed. Some particularly tractable classes of stable distributions are examined, and the possibility of learning with such processes.",
        "bibtex": "@inproceedings{NIPS2005_e2c61965,\n author = {Der, Ricky and Lee, Daniel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Beyond Gaussian Processes: On the Distributions of Infinite Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e2c61965b5e23b47b77d7c51611b6d7f-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/e2c61965b5e23b47b77d7c51611b6d7f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/e2c61965b5e23b47b77d7c51611b6d7f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 514387,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8101063994144761476&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Mathematics, University of Pennsylvania; Department of Electrical Engineering, University of Pennsylvania",
        "aff_domain": "math.upenn.edu;seas.upenn.edu",
        "email": "math.upenn.edu;seas.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "Department of Mathematics",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "325ee2209a",
        "title": "Beyond Pair-Based STDP: a Phenomenological Rule for Spike Triplet and Frequency Effects",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/a4666cd9e1ab0e4abf05a0fb232f4ad3-Abstract.html",
        "author": "Jean-pascal Pfister; Wulfram Gerstner",
        "abstract": "While classical experiments on spike-timing dependent plasticity analyzed synaptic changes as a function of the timing of pairs of pre- and postsynaptic spikes, more recent experiments also point to the effect of spike triplets. Here we develop a mathematical framework that allows us to characterize timing based learning rules. Moreover, we identify a candidate learning rule with five variables (and 5 free parameters) that captures a variety of experimental data, including the dependence of potentiation and depression upon pre- and postsynaptic firing frequencies. The relation to the Bienenstock-Cooper-Munro rule as well as to some timing-based rules is discussed.",
        "bibtex": "@inproceedings{NIPS2005_a4666cd9,\n author = {Pfister, Jean-pascal and Gerstner, Wulfram},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Beyond Pair-Based STDP: a Phenomenological Rule for Spike Triplet and Frequency Effects},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a4666cd9e1ab0e4abf05a0fb232f4ad3-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/a4666cd9e1ab0e4abf05a0fb232f4ad3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/a4666cd9e1ab0e4abf05a0fb232f4ad3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 85804,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12207268687435129511&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "41a24295eb",
        "title": "CMOL CrossNets: Possible Neuromorphic Nanoelectronic Circuits",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/b44afe91b8a427a6be2078cc89bd6f9b-Abstract.html",
        "author": "Jung Hoon Lee; Xiaolong Ma; Konstantin K. Likharev",
        "abstract": "Hybrid  \u201cCMOL\u201d  integrated  circuits,  combining  CMOS  subsystem  with  nanowire  crossbars  and  simple  two-terminal  nanodevices,  promise  to  extend  the  exponential  Moore-Law  development  of  microelectronics  into  the  sub-10-nm  range.  We  are  developing  neuromorphic  network  (\u201cCrossNet\u201d)  architectures  for  this  future  technology, in which neural cell bodies are implemented in CMOS,  nanowires  are  used  as  axons  and  dendrites,  while  nanodevices  (bistable  latching  switches)  are  used  as  elementary  synapses.  We  have  shown  how  CrossNets  may  be  trained  to  perform  pattern  recovery  and  classification  despite  the  limitations  imposed  by  the  CMOL  hardware.    Preliminary  estimates  have  shown  that  CMOL  CrossNets may be extremely dense (~107 cells per cm2) and operate  approximately a million times faster than biological neural networks,  at  manageable  power  consumption.  In  Conclusion,  we  discuss  in  brief possible short-term and long-term applications of the emerging  technology.",
        "bibtex": "@inproceedings{NIPS2005_b44afe91,\n author = {Lee, Jung Hoon and Ma, Xiaolong and Likharev, Konstantin K.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {CMOL CrossNets: Possible Neuromorphic Nanoelectronic Circuits},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b44afe91b8a427a6be2078cc89bd6f9b-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/b44afe91b8a427a6be2078cc89bd6f9b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/b44afe91b8a427a6be2078cc89bd6f9b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 255753,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11117909365567562427&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Stony Brook University; Stony Brook University; Stony Brook University",
        "aff_domain": "stonybrook.edu;stonybrook.edu;notes.cc.sunysb.edu",
        "email": "stonybrook.edu;stonybrook.edu;notes.cc.sunysb.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stony Brook University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stonybrook.edu",
        "aff_unique_abbr": "SBU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ca71de3b9a",
        "title": "Coarse sample complexity bounds for active learning",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/6e82873a32b95af115de1c414a1849cb-Abstract.html",
        "author": "Sanjoy Dasgupta",
        "abstract": "We characterize the sample complexity of active learning problems in terms of a parameter which takes into account the distribution over the input space, the specific target hypothesis, and the desired accuracy.",
        "bibtex": "@inproceedings{NIPS2005_6e82873a,\n author = {Dasgupta, Sanjoy},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Coarse sample complexity bounds for active learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6e82873a32b95af115de1c414a1849cb-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/6e82873a32b95af115de1c414a1849cb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/6e82873a32b95af115de1c414a1849cb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 83232,
        "gs_citation": 414,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4967944268445912492&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "UC SanDiego",
        "aff_domain": "cs.ucsd.edu",
        "email": "cs.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "",
        "aff_unique_url": "https://ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5a6f369958",
        "title": "Combining Graph Laplacians for Semi--Supervised Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/42853a61b26fef79e2ae788d97356799-Abstract.html",
        "author": "Andreas Argyriou; Mark Herbster; Massimiliano Pontil",
        "abstract": "A foundational problem in semi-supervised learning is the construction of a graph underlying the data. We propose to use a method which optimally combines a number of differently constructed graphs. For each of these graphs we associate a basic graph kernel. We then compute an optimal combined kernel. This kernel solves an extended regularization problem which requires a joint minimization over both the data and the set of graph kernels. We present encouraging results on different OCR tasks where the optimal combined kernel is computed from graphs constructed with a variety of distances functions and the `k ' in nearest neighbors.",
        "bibtex": "@inproceedings{NIPS2005_42853a61,\n author = {Argyriou, Andreas and Herbster, Mark and Pontil, Massimiliano},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Combining Graph Laplacians for Semi--Supervised Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/42853a61b26fef79e2ae788d97356799-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/42853a61b26fef79e2ae788d97356799-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/42853a61b26fef79e2ae788d97356799-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 291319,
        "gs_citation": 199,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=504467054050326529&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "b80c1897d3",
        "title": "Comparing the Effects of Different Weight Distributions on Finding Sparse Representations",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/d8e1344e27a5b08cdfd5d027d9b8d6de-Abstract.html",
        "author": "Bhaskar D. Rao; David P. Wipf",
        "abstract": "Given a redundant dictionary of basis vectors (or atoms), our goal is to \ufb01nd maximally sparse representations of signals. Previously, we have argued that a sparse Bayesian learning (SBL) framework is particularly well-suited for this task, showing that it has far fewer local minima than other Bayesian-inspired strategies. In this paper, we provide further evi- dence for this claim by proving a restricted equivalence condition, based on the distribution of the nonzero generating model weights, whereby the SBL solution will equal the maximally sparse representation. We also prove that if these nonzero weights are drawn from an approximate Jef- freys prior, then with probability approaching one, our equivalence con- dition is satis\ufb01ed. Finally, we motivate the worst-case scenario for SBL and demonstrate that it is still better than the most widely used sparse rep- resentation algorithms. These include Basis Pursuit (BP), which is based on a convex relaxation of the \u21130 (quasi)-norm, and Orthogonal Match- ing Pursuit (OMP), a simple greedy strategy that iteratively selects basis vectors most aligned with the current residual.",
        "bibtex": "@inproceedings{NIPS2005_d8e1344e,\n author = {Rao, Bhaskar and Wipf, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Comparing the Effects of Different Weight Distributions on Finding Sparse Representations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d8e1344e27a5b08cdfd5d027d9b8d6de-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/d8e1344e27a5b08cdfd5d027d9b8d6de-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/d8e1344e27a5b08cdfd5d027d9b8d6de-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 85818,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16371929732525587463&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Electrical and Computer Engineering, University of California, San Diego; Department of Electrical and Computer Engineering, University of California, San Diego",
        "aff_domain": "ucsd.edu;ece.ucsd.edu",
        "email": "ucsd.edu;ece.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ccdba4bb00",
        "title": "Computing the Solution Path for the Regularized Support Vector Regression",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/1397386b7a1507535c59764a15ee0c98-Abstract.html",
        "author": "Lacey Gunter; Ji Zhu",
        "abstract": "In this paper we derive an algorithm that computes the entire solu- tion path of the support vector regression, with essentially the same computational cost as \ufb01tting one SVR model. We also propose an unbiased estimate for the degrees of freedom of the SVR model, which allows convenient selection of the regularization parameter.",
        "bibtex": "@inproceedings{NIPS2005_1397386b,\n author = {Gunter, Lacey and Zhu, Ji},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Computing the Solution Path for the Regularized Support Vector Regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/1397386b7a1507535c59764a15ee0c98-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/1397386b7a1507535c59764a15ee0c98-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/1397386b7a1507535c59764a15ee0c98-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 134493,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16354072823628566490&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Statistics, University of Michigan; Department of Statistics, University of Michigan",
        "aff_domain": "umich.edu;umich.edu",
        "email": "umich.edu;umich.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b3e65a80fd",
        "title": "Conditional Visual Tracking in Kernel Space",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/801fd8c2a4e79c1d24a40dc735c051ae-Abstract.html",
        "author": "Cristian Sminchisescu; Atul Kanujia; Zhiguo Li; Dimitris Metaxas",
        "abstract": "We present a conditional temporal probabilistic framework for recon- structing 3D human motion in monocular video based on descriptors en- coding image silhouette observations. For computational ef\ufb01ciency we restrict visual inference to low-dimensional kernel induced non-linear state spaces. Our methodology (kBME) combines kernel PCA-based non-linear dimensionality reduction (kPCA) and Conditional Bayesian Mixture of Experts (BME) in order to learn complex multivalued pre- dictors between observations and model hidden states. This is necessary for accurate, inverse, visual perception inferences, where several proba- ble, distant 3D solutions exist due to noise or the uncertainty of monoc- ular perspective projection. Low-dimensional models are appropriate because many visual processes exhibit strong non-linear correlations in both the image observations and the target, hidden state variables. The learned predictors are temporally combined within a conditional graphi- cal model in order to allow a principled propagation of uncertainty. We study several predictors and empirically show that the proposed algo- rithm positively compares with techniques based on regression, Kernel Dependency Estimation (KDE) or PCA alone, and gives results competi- tive to those of high-dimensional mixture predictors at a fraction of their computational cost. We show that the method successfully reconstructs the complex 3D motion of humans in real monocular video sequences.",
        "bibtex": "@inproceedings{NIPS2005_801fd8c2,\n author = {Sminchisescu, Cristian and Kanujia, Atul and Li, Zhiguo and Metaxas, Dimitris},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Conditional Visual Tracking in Kernel Space},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/801fd8c2a4e79c1d24a40dc735c051ae-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/801fd8c2a4e79c1d24a40dc735c051ae-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/801fd8c2a4e79c1d24a40dc735c051ae-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 200796,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=541927940183904449&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "TTI-C, 1497 East 50th Street, Chicago, IL, 60637, USA + University of Toronto, Department of Computer Science, Canada + Rutgers University, Department of Computer Science, USA; Rutgers University, Department of Computer Science, USA; Rutgers University, Department of Computer Science, USA; Rutgers University, Department of Computer Science, USA",
        "aff_domain": "cs.toronto.edu;cs.rutgers.edu;cs.rutgers.edu;cs.rutgers.edu",
        "email": "cs.toronto.edu;cs.rutgers.edu;cs.rutgers.edu;cs.rutgers.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1+2;2;2;2",
        "aff_unique_norm": "Toyota Technological Institute at Chicago;University of Toronto;Rutgers University",
        "aff_unique_dep": ";Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.tti-c.org;https://www.utoronto.ca;https://www.rutgers.edu",
        "aff_unique_abbr": "TTI-C;U of T;Rutgers",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Chicago;",
        "aff_country_unique_index": "0+1+0;0;0;0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "e1c92f5224",
        "title": "Consensus Propagation",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/b31df16a88ce00fed951f24b46e08649-Abstract.html",
        "author": "Benjamin V. Roy; Ciamac C. Moallemi",
        "abstract": "We propose consensus propagation, an asynchronous distributed protocol for averaging numbers across a network. We establish convergence, characterize the convergence rate for regular graphs, and demonstrate that the protocol exhibits better scaling properties than pairwise averaging, an alternative that has received much recent attention. Consensus propagation can be viewed as a special case of belief propagation, and our results contribute to the belief propagation literature. In particular, beyond singly-connected graphs, there are very few classes of relevant problems for which belief propagation is known to converge.",
        "bibtex": "@inproceedings{NIPS2005_b31df16a,\n author = {Roy, Benjamin and Moallemi, Ciamac C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Consensus Propagation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b31df16a88ce00fed951f24b46e08649-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/b31df16a88ce00fed951f24b46e08649-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/b31df16a88ce00fed951f24b46e08649-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 156227,
        "gs_citation": 308,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4820695848502568296&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Stanford University; Stanford University",
        "aff_domain": "stanford.edu;stanford.edu",
        "email": "stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2afea5c64a",
        "title": "Consistency of one-class SVM and related algorithms",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/9a11883317fde3aef2e2432a58c86779-Abstract.html",
        "author": "R\u00e9gis Vert; Jean-philippe Vert",
        "abstract": "We determine the asymptotic limit of the function computed by support vector machines (SVM) and related algorithms that minimize a regularized empirical convex loss function in the reproducing kernel Hilbert space of the Gaussian RBF kernel, in the situation where the number of examples tends to infinity, the bandwidth of the Gaussian kernel tends to 0, and the regularization parameter is held fixed. Non-asymptotic convergence bounds to this limit in the L2 sense are provided, together with upper bounds on the classification error that is shown to converge to the Bayes risk, therefore proving the Bayes-consistency of a variety of methods although the regularization term does not vanish. These results are particularly relevant to the one-class SVM, for which the regularization can not vanish by construction, and which is shown for the first time to be a consistent density level set estimator.",
        "bibtex": "@inproceedings{NIPS2005_9a118833,\n author = {Vert, R\\'{e}gis and Vert, Jean-philippe},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Consistency of one-class SVM and related algorithms},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/9a11883317fde3aef2e2432a58c86779-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/9a11883317fde3aef2e2432a58c86779-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/9a11883317fde3aef2e2432a58c86779-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 79151,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3934088188214082644&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Laboratoire de Recherche en Informatique, Universit\u00b4e Paris-Sud, 91405, Orsay Cedex, France + Masagroup, 24 Bd de l\u2019H \u02c6opital, 75005, Paris, France; Geostatistics Center, Ecole des Mines de Paris - ParisTech, 77300 Fontainebleau, France",
        "aff_domain": "lri.fr;ensmp.fr",
        "email": "lri.fr;ensmp.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "Universit\u00e9 Paris-Sud;Masagroup;Ecole des Mines de Paris - ParisTech",
        "aff_unique_dep": "Laboratoire de Recherche en Informatique;;Geostatistics Center",
        "aff_unique_url": "https://www.universite-paris-sud.fr;;https://www.mines-paris.psl.eu",
        "aff_unique_abbr": "UPS;;Mines ParisTech",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Orsay;;Paris",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "0308d4bad0",
        "title": "Context as Filtering",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/1dba5eed8838571e1c80af145184e515-Abstract.html",
        "author": "Daichi Mochihashi; Yuji Matsumoto",
        "abstract": "Long-distance language modeling is important not only in speech recognition and machine translation, but also in high-dimensional discrete sequence modeling in general. However, the problem of context length has almost been neglected so far and a nave bag-of-words history has been i employed in natural language processing. In contrast, in this paper we view topic shifts within a text as a latent stochastic process to give an explicit probabilistic generative model that has partial exchangeability. We propose an online inference algorithm using particle filters to recognize topic shifts to employ the most appropriate length of context automatically. Experiments on the BNC corpus showed consistent improvement over previous methods involving no chronological order.",
        "bibtex": "@inproceedings{NIPS2005_1dba5eed,\n author = {Mochihashi, Daichi and Matsumoto, Yuji},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Context as Filtering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/1dba5eed8838571e1c80af145184e515-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/1dba5eed8838571e1c80af145184e515-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/1dba5eed8838571e1c80af145184e515-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 643096,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4718643803720869934&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "ATR, Spoken Language Communication Research Laboratories; Graduate School of Information Science, Nara Institute of Science and Technology",
        "aff_domain": "atr.jp;is.naist.jp",
        "email": "atr.jp;is.naist.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "ATR;Nara Institute of Science and Technology",
        "aff_unique_dep": "Spoken Language Communication Research Laboratories;Graduate School of Information Science",
        "aff_unique_url": "https://www.atr.jp;https://www.nist.go.jp",
        "aff_unique_abbr": "ATR;NIST",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Nara",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "deb3eafa3d",
        "title": "Convergence and Consistency of Regularized Boosting Algorithms with Stationary B-Mixing Observations",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/ec0bfd000f253eff3acb1043e1c06979-Abstract.html",
        "author": "Aurelie C. Lozano; Sanjeev R. Kulkarni; Robert E. Schapire",
        "abstract": "We study the statistical convergence and consistency of regularized Boosting methods, where the samples are not independent and identi- cally distributed (i.i.d.) but come from empirical processes of stationary \u03b2-mixing sequences. Utilizing a technique that constructs a sequence of independent blocks close in distribution to the original samples, we prove the consistency of the composite classi\ufb01ers resulting from a regulariza- tion achieved by restricting the 1-norm of the base classi\ufb01ers\u2019 weights. When compared to the i.i.d. case, the nature of sampling manifests in the consistency result only through generalization of the original condition on the growth of the regularization parameter.",
        "bibtex": "@inproceedings{NIPS2005_ec0bfd00,\n author = {Lozano, Aurelie C and Kulkarni, Sanjeev and Schapire, Robert E},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Convergence and Consistency of Regularized Boosting Algorithms with Stationary B-Mixing Observations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ec0bfd000f253eff3acb1043e1c06979-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/ec0bfd000f253eff3acb1043e1c06979-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/ec0bfd000f253eff3acb1043e1c06979-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 175414,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7427828306737459378&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Electrical Engineering, Princeton University; Department of Electrical Engineering, Princeton University; Department of Computer Science, Princeton University",
        "aff_domain": "princeton.edu;princeton.edu;cs.princeton.edu",
        "email": "princeton.edu;princeton.edu;cs.princeton.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Princeton University",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.princeton.edu",
        "aff_unique_abbr": "Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c2f7dd33f9",
        "title": "Convex Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/0fc170ecbb8ff1afb2c6de48ea5343e7-Abstract.html",
        "author": "Yoshua Bengio; Nicolas L. Roux; Pascal Vincent; Olivier Delalleau; Patrice Marcotte",
        "abstract": "Convexity has recently received a lot of attention in the machine learning community, and the lack of convexity has been seen as a major disadvantage of many learning algorithms, such as multi-layer artificial neural networks. We show that training multi-layer neural networks in which the number of hidden units is learned can be viewed as a convex optimization problem. This problem involves an infinite number of variables, but can be solved by incrementally inserting a hidden unit at a time, each time finding a linear classifier that minimizes a weighted sum of errors.",
        "bibtex": "@inproceedings{NIPS2005_0fc170ec,\n author = {Bengio, Yoshua and Roux, Nicolas and Vincent, Pascal and Delalleau, Olivier and Marcotte, Patrice},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Convex Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/0fc170ecbb8ff1afb2c6de48ea5343e7-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/0fc170ecbb8ff1afb2c6de48ea5343e7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/0fc170ecbb8ff1afb2c6de48ea5343e7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 85972,
        "gs_citation": 294,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6719480376751148424&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Dept. IRO, Universit \u00b4e de Montr \u00b4eal; Dept. IRO, Universit \u00b4e de Montr \u00b4eal; Dept. IRO, Universit \u00b4e de Montr \u00b4eal; Dept. IRO, Universit \u00b4e de Montr \u00b4eal; Dept. IRO, Universit \u00b4e de Montr \u00b4eal",
        "aff_domain": "iro.umontreal.ca;iro.umontreal.ca;iro.umontreal.ca;iro.umontreal.ca;iro.umontreal.ca",
        "email": "iro.umontreal.ca;iro.umontreal.ca;iro.umontreal.ca;iro.umontreal.ca;iro.umontreal.ca",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Universit\u00e9 de Montr\u00e9al",
        "aff_unique_dep": "Dept. IRO",
        "aff_unique_url": "https://www.udem.ca",
        "aff_unique_abbr": "UdeM",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Montr\u00e9al",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "d14750cc0f",
        "title": "Correcting sample selection bias in maximum entropy density estimation",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/a36b0dcd1e6384abc0e1867860ad3ee3-Abstract.html",
        "author": "Miroslav Dud\u00edk; Steven J. Phillips; Robert E. Schapire",
        "abstract": "We study the problem of maximum entropy density estimation in the presence of known sample selection bias. We propose three bias cor- rection approaches. The \ufb01rst one takes advantage of unbiased suf\ufb01cient statistics which can be obtained from biased samples. The second one es- timates the biased distribution and then factors the bias out. The third one approximates the second by only using samples from the sampling distri- bution. We provide guarantees for the \ufb01rst two approaches and evaluate the performance of all three approaches in synthetic experiments and on real data from species habitat modeling, where maxent has been success- fully applied and where sample selection bias is a signi\ufb01cant problem.",
        "bibtex": "@inproceedings{NIPS2005_a36b0dcd,\n author = {Dud\\'{\\i}k, Miroslav and Phillips, Steven and Schapire, Robert E},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Correcting sample selection bias in maximum entropy density estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a36b0dcd1e6384abc0e1867860ad3ee3-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/a36b0dcd1e6384abc0e1867860ad3ee3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/a36b0dcd1e6384abc0e1867860ad3ee3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 92902,
        "gs_citation": 323,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5245262290956354292&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Princeton University, Department of Computer Science; Princeton University, Department of Computer Science; AT&T Labs \u2212Research",
        "aff_domain": "princeton.edu;princeton.edu;research.att.com",
        "email": "princeton.edu;princeton.edu;research.att.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Princeton University;AT&T Labs",
        "aff_unique_dep": "Department of Computer Science;Research",
        "aff_unique_url": "https://www.princeton.edu;https://www.att.com/labs/research",
        "aff_unique_abbr": "Princeton;AT&T Labs",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4b45fad272",
        "title": "Correlated Topic Models",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/9e82757e9a1c12cb710ad680db11f6f1-Abstract.html",
        "author": "John D. Lafferty; David M. Blei",
        "abstract": "Topic models, such as latent Dirichlet allocation (LDA), can be useful tools for the statistical analysis of document collections and other discrete data. The LDA model assumes that the words of each document arise from a mixture of topics, each of which is a distribution over the vocabulary. A limitation of LDA is the inability to model topic correlation even though, for example, a document about genetics is more likely to also be about disease than x-ray astronomy. This limitation stems from the use of the Dirichlet distribution to model the variability among the topic proportions. In this paper we develop the correlated topic model (CTM), where the topic proportions exhibit correlation via the logistic normal distribution [1]. We derive a mean-field variational inference algorithm for approximate posterior inference in this model, which is complicated by the fact that the logistic normal is not conjugate to the multinomial. The CTM gives a better fit than LDA on a collection of OCRed articles from the journal Science. Furthermore, the CTM provides a natural way of visualizing and exploring this and other unstructured data sets.",
        "bibtex": "@inproceedings{NIPS2005_9e82757e,\n author = {Lafferty, John and Blei, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Correlated Topic Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/9e82757e9a1c12cb710ad680db11f6f1-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/9e82757e9a1c12cb710ad680db11f6f1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/9e82757e9a1c12cb710ad680db11f6f1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 180754,
        "gs_citation": 1463,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12281252645913589638&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computer Science, Princeton University; School of Computer Science, Carnegie Mellon University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Princeton University;Carnegie Mellon University",
        "aff_unique_dep": "Department of Computer Science;School of Computer Science",
        "aff_unique_url": "https://www.princeton.edu;https://www.cmu.edu",
        "aff_unique_abbr": "Princeton;CMU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "38cf46d28a",
        "title": "Cue Integration for Figure/Ground Labeling",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/8e987cf1b2f1f6ffa6a43066798b4b7f-Abstract.html",
        "author": "Xiaofeng Ren; Jitendra Malik; Charless C. Fowlkes",
        "abstract": "We present a model of edge and region grouping using a conditional random field built over a scale-invariant representation of images to integrate multiple cues. Our model includes potentials that capture low-level similarity, mid-level curvilinear continuity and high-level object shape. Maximum likelihood parameters for the model are learned from human labeled groundtruth on a large collection of horse images using belief propagation. Using held out test data, we quantify the information gained by incorporating generic mid-level cues and high-level shape.",
        "bibtex": "@inproceedings{NIPS2005_8e987cf1,\n author = {Ren, Xiaofeng and Malik, Jitendra and Fowlkes, Charless},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Cue Integration for Figure/Ground Labeling},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/8e987cf1b2f1f6ffa6a43066798b4b7f-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/8e987cf1b2f1f6ffa6a43066798b4b7f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/8e987cf1b2f1f6ffa6a43066798b4b7f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 295874,
        "gs_citation": 110,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17190256449486825797&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "2a45ddffd0",
        "title": "Cyclic Equilibria in Markov Games",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/9752d873fa71c19dc602bf2a0696f9b5-Abstract.html",
        "author": "Martin Zinkevich; Amy Greenwald; Michael L. Littman",
        "abstract": "Although variants of value iteration have been proposed for \ufb01nding Nash or correlated equilibria in general-sum Markov games, these variants have not been shown to be effective in general. In this paper, we demon- strate by construction that existing variants of value iteration cannot \ufb01nd stationary equilibrium policies in arbitrary general-sum Markov games. Instead, we propose an alternative interpretation of the output of value it- eration based on a new (non-stationary) equilibrium concept that we call \u201ccyclic equilibria.\u201d We prove that value iteration identi\ufb01es cyclic equi- libria in a class of games in which it fails to \ufb01nd stationary equilibria. We also demonstrate empirically that value iteration \ufb01nds cyclic equilibria in nearly all examples drawn from a random distribution of Markov games.",
        "bibtex": "@inproceedings{NIPS2005_9752d873,\n author = {Zinkevich, Martin and Greenwald, Amy and Littman, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Cyclic Equilibria in Markov Games},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/9752d873fa71c19dc602bf2a0696f9b5-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/9752d873fa71c19dc602bf2a0696f9b5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/9752d873fa71c19dc602bf2a0696f9b5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 74187,
        "gs_citation": 128,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=713101312313978946&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Computer Science, Brown University; Department of Computer Science, Brown University; Department of Computer Science, Rutgers, The State University of NJ",
        "aff_domain": "cs.brown.edu;cs.brown.edu;cs.rutgers.edu",
        "email": "cs.brown.edu;cs.brown.edu;cs.rutgers.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Brown University;Rutgers University",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.brown.edu;https://rutgers.edu",
        "aff_unique_abbr": "Brown;Rutgers",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0ec6ce1adc",
        "title": "Data-Driven Online to Batch Conversions",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/4a5876b450b45371f6cfe5047ac8cd45-Abstract.html",
        "author": "Ofer Dekel; Yoram Singer",
        "abstract": "Online learning algorithms are typically fast, memory efficient, and simple to implement. However, many common learning problems fit more naturally in the batch learning setting. The power of online learning algorithms can be exploited in batch settings by using online-to-batch conversions techniques which build a new batch algorithm from an existing online algorithm. We first give a unified overview of three existing online-to-batch conversion techniques which do not use training data in the conversion process. We then build upon these data-independent conversions to derive and analyze data-driven conversions. Our conversions find hypotheses with a small risk by explicitly minimizing datadependent generalization bounds. We experimentally demonstrate the usefulness of our approach and in particular show that the data-driven conversions consistently outperform the data-independent conversions.",
        "bibtex": "@inproceedings{NIPS2005_4a5876b4,\n author = {Dekel, Ofer and Singer, Yoram},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Data-Driven Online to Batch Conversions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4a5876b450b45371f6cfe5047ac8cd45-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/4a5876b450b45371f6cfe5047ac8cd45-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/4a5876b450b45371f6cfe5047ac8cd45-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 98590,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17779102389823612742&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "School of Computer Science and Engineering, The Hebrew University, Jerusalem 91904, Israel; School of Computer Science and Engineering, The Hebrew University, Jerusalem 91904, Israel",
        "aff_domain": "cs.huji.ac.il;cs.huji.ac.il",
        "email": "cs.huji.ac.il;cs.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hebrew University",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "http://www.huji.ac.il",
        "aff_unique_abbr": "HUJI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Jerusalem",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "6e649af4c6",
        "title": "Describing Visual Scenes using Transformed Dirichlet Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/92bf5e6240737e0326ea59846a83e076-Abstract.html",
        "author": "Antonio Torralba; Alan S. Willsky; Erik B. Sudderth; William T. Freeman",
        "abstract": "Motivated by the problem of learning to detect and recognize objects with minimal supervision, we develop a hierarchical probabilistic model for the spatial structure of visual scenes. In contrast with most existing models, our approach explicitly captures uncertainty in the number of object instances depicted in a given image. Our scene model is based on the transformed Dirichlet process (TDP), a novel extension of the hierarchical DP in which a set of stochastically transformed mixture components are shared between multiple groups of data. For visual scenes, mixture components describe the spatial structure of visual features in an objectcentered coordinate frame, while transformations model the object positions in a particular image. Learning and inference in the TDP, which has many potential applications beyond computer vision, is based on an empirically effective Gibbs sampler. Applied to a dataset of partially labeled street scenes, we show that the TDP's inclusion of spatial structure improves detection performance, flexibly exploiting partially labeled training images.",
        "bibtex": "@inproceedings{NIPS2005_92bf5e62,\n author = {Torralba, Antonio and Willsky, Alan and Sudderth, Erik and Freeman, William},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Describing Visual Scenes using Transformed Dirichlet Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/92bf5e6240737e0326ea59846a83e076-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/92bf5e6240737e0326ea59846a83e076-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/92bf5e6240737e0326ea59846a83e076-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 905936,
        "gs_citation": 184,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8041056499202236972&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 22,
        "aff": "Department of Electrical Engineering and Computer Science; Department of Electrical Engineering and Computer Science; Department of Electrical Engineering and Computer Science; Department of Electrical Engineering and Computer Science",
        "aff_domain": "mit.edu;csail.mit.edu;mit.edu;mit.edu",
        "email": "mit.edu;csail.mit.edu;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a281d5d786",
        "title": "Diffusion Maps, Spectral Clustering and Eigenfunctions of Fokker-Planck Operators",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/2a0f97f81755e2878b264adf39cba68e-Abstract.html",
        "author": "Boaz Nadler; Stephane Lafon; Ioannis Kevrekidis; Ronald R. Coifman",
        "abstract": "This paper presents a diffusion based probabilistic interpretation of spectral clustering and dimensionality reduction algorithms that use the eigenvectors of the normalized graph Laplacian. Given the pairwise adjacency matrix of all points, we define a diffusion distance between any two data points and show that the low dimensional representation of the data by the first few eigenvectors of the corresponding Markov matrix is optimal under a certain mean squared error criterion. Furthermore, assuming that data points are random samples from a density p(x) = e-U (x) we identify these eigenvectors as discrete approximations of eigenfunctions of a Fokker-Planck operator in a potential 2U (x) with reflecting boundary conditions. Finally, applying known results regarding the eigenvalues and eigenfunctions of the continuous Fokker-Planck operator, we provide a mathematical justification for the success of spectral clustering and dimensional reduction algorithms based on these first few eigenvectors. This analysis elucidates, in terms of the characteristics of diffusion processes, many empirical findings regarding spectral clustering algorithms. Keywords: Algorithms and architectures, learning theory.",
        "bibtex": "@inproceedings{NIPS2005_2a0f97f8,\n author = {Nadler, Boaz and Lafon, Stephane and Kevrekidis, Ioannis and Coifman, Ronald},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Diffusion Maps, Spectral Clustering and Eigenfunctions of Fokker-Planck Operators},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2a0f97f81755e2878b264adf39cba68e-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/2a0f97f81755e2878b264adf39cba68e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/2a0f97f81755e2878b264adf39cba68e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 107744,
        "gs_citation": 606,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3413568746366856983&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff": "Department of Mathematics, Yale University, New Haven, CT 06520 + Weizmann Institute of Science, Rehovot, Israel; Department of Mathematics, Yale University, New Haven, CT 06520; Department of Mathematics, Yale University, New Haven, CT 06520; Department of Chemical Engineering and Program in Applied Mathematics, Princeton University, Princeton, NJ 08544",
        "aff_domain": "yale.edu;yale.edu;yale.edu;princeton.edu",
        "email": "yale.edu;yale.edu;yale.edu;princeton.edu",
        "github": "",
        "project": "http://www.wisdom.weizmann.ac.il/~nadler",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0;2",
        "aff_unique_norm": "Yale University;Weizmann Institute of Science;Princeton University",
        "aff_unique_dep": "Department of Mathematics;;Department of Chemical Engineering",
        "aff_unique_url": "https://www.yale.edu;https://www.weizmann.org.il;https://www.princeton.edu",
        "aff_unique_abbr": "Yale;Weizmann;Princeton",
        "aff_campus_unique_index": "0+1;0;0;2",
        "aff_campus_unique": "New Haven;Rehovot;Princeton",
        "aff_country_unique_index": "0+1;0;0;0",
        "aff_country_unique": "United States;Israel"
    },
    {
        "id": "8ec63ebfc5",
        "title": "Distance Metric Learning for Large Margin Nearest Neighbor Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/a7f592cef8b130a6967a90617db5681b-Abstract.html",
        "author": "Kilian Q. Weinberger; John Blitzer; Lawrence K. Saul",
        "abstract": "We show how to learn a Mahanalobis distance metric for k -nearest neighbor (kNN) classification by semidefinite programming. The metric is trained with the goal that the k -nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. On seven data sets of varying size and difficulty, we find that metrics trained in this way lead to significant improvements in kNN classification--for example, achieving a test error rate of 1.3% on the MNIST handwritten digits. As in support vector machines (SVMs), the learning problem reduces to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our framework requires no modification or extension for problems in multiway (as opposed to binary) classification.",
        "bibtex": "@inproceedings{NIPS2005_a7f592ce,\n author = {Weinberger, Kilian Q and Blitzer, John and Saul, Lawrence},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Distance Metric Learning for Large Margin Nearest Neighbor Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a7f592cef8b130a6967a90617db5681b-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/a7f592cef8b130a6967a90617db5681b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/a7f592cef8b130a6967a90617db5681b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 296977,
        "gs_citation": 5111,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=209543676242192604&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 20,
        "aff": "Department of Computer and Information Science, University of Pennsylvania; Department of Computer and Information Science, University of Pennsylvania; Department of Computer and Information Science, University of Pennsylvania",
        "aff_domain": "cis.upenn.edu;cis.upenn.edu;cis.upenn.edu",
        "email": "cis.upenn.edu;cis.upenn.edu;cis.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "Department of Computer and Information Science",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2ceb6bdf08",
        "title": "Divergences, surrogate loss functions and experimental design",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/314450613369e0ee72d0da7f6fee773c-Abstract.html",
        "author": "Xuanlong Nguyen; Martin J. Wainwright; Michael I. Jordan",
        "abstract": "In this paper, we provide a general theorem that establishes a correspon- dence between surrogate loss functions in classi\ufb01cation and the family of f-divergences. Moreover, we provide constructive procedures for determining the f-divergence induced by a given surrogate loss, and conversely for \ufb01nding all surrogate loss functions that realize a given f-divergence. Next we introduce the notion of universal equivalence among loss functions and corresponding f-divergences, and provide nec- essary and suf\ufb01cient conditions for universal equivalence to hold. These ideas have applications to classi\ufb01cation problems that also involve a com- ponent of experiment design; in particular, we leverage our results to prove consistency of a procedure for learning a classi\ufb01er under decen- tralization requirements.",
        "bibtex": "@inproceedings{NIPS2005_31445061,\n author = {Nguyen, XuanLong and Wainwright, Martin J. and Jordan, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Divergences, surrogate loss functions and experimental design},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/314450613369e0ee72d0da7f6fee773c-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/314450613369e0ee72d0da7f6fee773c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/314450613369e0ee72d0da7f6fee773c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 92210,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12104884878527147094&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "University of California, Berkeley, CA 94720; University of California, Berkeley, CA 94720; University of California, Berkeley, CA 94720",
        "aff_domain": "cs.berkeley.edu;eecs.berkeley.edu;cs.berkeley.edu",
        "email": "cs.berkeley.edu;eecs.berkeley.edu;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5d17827110",
        "title": "Dual-Tree Fast Gauss Transforms",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/9087b0efc7c7acd1ef7e153678809c77-Abstract.html",
        "author": "Dongryeol Lee; Andrew W. Moore; Alexander G. Gray",
        "abstract": "In previous work we presented an ef\ufb01cient approach to computing ker- nel summations which arise in many machine learning methods such as kernel density estimation. This approach, dual-tree recursion with \ufb01nite- difference approximation, generalized existing methods for similar prob- lems arising in computational physics in two ways appropriate for sta- tistical problems: toward distribution sensitivity and general dimension, partly by avoiding series expansions. While this proved to be the fastest practical method for multivariate kernel density estimation at the optimal bandwidth, it is much less ef\ufb01cient at larger-than-optimal bandwidths. In this work, we explore the extent to which the dual-tree approach can be integrated with multipole-like Hermite expansions in order to achieve reasonable ef\ufb01ciency across all bandwidth scales, though only for low di- mensionalities. In the process, we derive and demonstrate the \ufb01rst truly hierarchical fast Gauss transforms, effectively combining the best tools from discrete algorithms and continuous approximation theory.",
        "bibtex": "@inproceedings{NIPS2005_9087b0ef,\n author = {Lee, Dongryeol and Moore, Andrew and Gray, Alexander},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Dual-Tree Fast Gauss Transforms},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/9087b0efc7c7acd1ef7e153678809c77-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/9087b0efc7c7acd1ef7e153678809c77-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/9087b0efc7c7acd1ef7e153678809c77-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 128886,
        "gs_citation": 88,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16897338349345278233&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Computer Science, Carnegie Mellon Univ.; Computer Science, Carnegie Mellon Univ.; Computer Science, Carnegie Mellon Univ.",
        "aff_domain": "cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d5088ee8e3",
        "title": "Dynamic Social Network Analysis using Latent Space Models",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/ec8b57b0be908301f5748fb04b0714c7-Abstract.html",
        "author": "Purnamrita Sarkar; Andrew W. Moore",
        "abstract": "This paper explores two aspects of social network modeling. First, we generalize a successful static model of relationships into a dynamic model that accounts for friendships drifting over time. Second, we show how to make it tractable to learn such models from data, even as the number of entities n gets large. The generalized model associates each entity with a point in p-dimensional Euclidian latent space. The points can move as time progresses but large moves in latent space are improb- able. Observed links between entities are more likely if the entities are close in latent space. We show how to make such a model tractable (sub- quadratic in the number of entities) by the use of appropriate kernel func- tions for similarity in latent space; the use of low dimensional kd-trees; a new ef(cid:2)cient dynamic adaptation of multidimensional scaling for a (cid:2)rst pass of approximate projection of entities into latent space; and an ef(cid:2)- cient conjugate gradient update rule for non-linear local optimization in which amortized time per entity during an update is O(log n). We use both synthetic and real-world data on upto 11,000 entities which indicate linear scaling in computation time and improved performance over four alternative approaches. We also illustrate the system operating on twelve years of NIPS co-publication data. We present a detailed version of this work in [1].",
        "bibtex": "@inproceedings{NIPS2005_ec8b57b0,\n author = {Sarkar, Purnamrita and Moore, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Dynamic Social Network Analysis using Latent Space Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ec8b57b0be908301f5748fb04b0714c7-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/ec8b57b0be908301f5748fb04b0714c7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/ec8b57b0be908301f5748fb04b0714c7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 226116,
        "gs_citation": 592,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17244581098665455084&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "86ee847ed0",
        "title": "Dynamical Synapses Give Rise to a Power-Law Distribution of Neuronal Avalanches",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/fc192b0c0d270dbf41870a63a8c76c2f-Abstract.html",
        "author": "Anna Levina; Michael Herrmann",
        "abstract": "There is experimental evidence that cortical neurons show avalanche activity with the intensity of firing events being distributed as a power-law. We present a biologically plausible extension of a neural network which exhibits a power-law avalanche distribution for a wide range of connectivity parameters.",
        "bibtex": "@inproceedings{NIPS2005_fc192b0c,\n author = {Levina, Anna and Herrmann, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Dynamical Synapses Give Rise to a Power-Law Distribution of Neuronal Avalanches},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/fc192b0c0d270dbf41870a63a8c76c2f-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/fc192b0c0d270dbf41870a63a8c76c2f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/fc192b0c0d270dbf41870a63a8c76c2f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 237963,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7630826679298076354&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "43143930e8",
        "title": "Efficient Estimation of OOMs",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/7e0a0209b929d097bd3e8ef30567a5c1-Abstract.html",
        "author": "Herbert Jaeger; Mingjie Zhao; Andreas Kolling",
        "abstract": "A standard method to obtain stochastic models for symbolic time series is to train state-emitting hidden Markov models (SE-HMMs) with the Baum-Welch algorithm. Based on observable operator models (OOMs), in the last few months a number of novel learning algorithms for similar purposes have been developed: (1,2) two versions of an \"efficiency sharpening\" (ES) algorithm, which iteratively improves the statistical efficiency of a sequence of OOM estimators, (3) a constrained gradient descent ML estimator for transition-emitting HMMs (TE-HMMs). We give an overview on these algorithms and compare them with SE-HMM/EM learning on synthetic and real-life data.",
        "bibtex": "@inproceedings{NIPS2005_7e0a0209,\n author = {Jaeger, Herbert and Zhao, Mingjie and Kolling, Andreas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Efficient Estimation of OOMs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/7e0a0209b929d097bd3e8ef30567a5c1-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/7e0a0209b929d097bd3e8ef30567a5c1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/7e0a0209b929d097bd3e8ef30567a5c1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 85680,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15891863153029848618&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "International University Bremen; International University Bremen; International University Bremen",
        "aff_domain": "iu-bremen.de;iu-bremen.de;iu-bremen.de",
        "email": "iu-bremen.de;iu-bremen.de;iu-bremen.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "International University Bremen",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iubremen.de",
        "aff_unique_abbr": "IUB",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "50cad4d6f0",
        "title": "Efficient Unsupervised Learning for Localization and Detection in Object Categories",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/e27a949795bbe863f31c3b79a2686770-Abstract.html",
        "author": "Nicolas Loeff; Himanshu Arora; Alexander Sorokin; David Forsyth",
        "abstract": "We describe a novel method for learning templates for recognition and localization of objects drawn from categories. A generative model repre- sents the con\ufb01guration of multiple object parts with respect to an object coordinate system; these parts in turn generate image features. The com- plexity of the model in the number of features is low, meaning our model is much more ef\ufb01cient to train than comparative methods. Moreover, a variational approximation is introduced that allows learning to be or- ders of magnitude faster than previous approaches while incorporating many more features. This results in both accuracy and localization im- provements. Our model has been carefully tested on standard datasets; we compare with a number of recent template models. In particular, we demonstrate state-of-the-art results for detection and localization.",
        "bibtex": "@inproceedings{NIPS2005_e27a9497,\n author = {Loeff, Nicolas and Arora, Himanshu and Sorokin, Alexander and Forsyth, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Efficient Unsupervised Learning for Localization and Detection in Object Categories},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e27a949795bbe863f31c3b79a2686770-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/e27a949795bbe863f31c3b79a2686770-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/e27a949795bbe863f31c3b79a2686770-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 317636,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13687293235874094051&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "ECE Department, University of Illinois at Urbana-Champaign; ECE Department, University of Illinois at Urbana-Champaign; Computer Science Department, University of Illinois at Urbana-Champaign; Computer Science Department, University of Illinois at Urbana-Champaign",
        "aff_domain": "uiuc.edu;uiuc.edu;uiuc.edu;uiuc.edu",
        "email": "uiuc.edu;uiuc.edu;uiuc.edu;uiuc.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign",
        "aff_unique_dep": "ECE Department",
        "aff_unique_url": "https://illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Urbana-Champaign",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5e8a940f01",
        "title": "Efficient estimation of hidden state dynamics from spike trains",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/15cf76466b97264765356fcc56d801d1-Abstract.html",
        "author": "Marton G. Danoczy; Richard H. R. Hahnloser",
        "abstract": "Neurons can have rapidly changing spike train statistics dictated by the underlying network excitability or behavioural state of an animal. To estimate the time course of such state dynamics from single- or multi- ple neuron recordings, we have developed an algorithm that maximizes the likelihood of observed spike trains by optimizing the state lifetimes and the state-conditional interspike-interval (ISI) distributions. Our non- parametric algorithm is free of time-binning and spike-counting prob- lems and has the computational complexity of a Mixed-state Markov Model operating on a state sequence of length equal to the total num- ber of recorded spikes. As an example, we \ufb01t a two-state model to paired recordings of premotor neurons in the sleeping songbird. We \ufb01nd that the two state-conditional ISI functions are highly similar to the ones mea- sured during waking and singing, respectively.",
        "bibtex": "@inproceedings{NIPS2005_15cf7646,\n author = {Danoczy, Marton G. and Hahnloser, Richard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Efficient estimation of hidden state dynamics from spike trains},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/15cf76466b97264765356fcc56d801d1-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/15cf76466b97264765356fcc56d801d1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/15cf76466b97264765356fcc56d801d1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 214176,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11987324750885993897&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Inst. for Theoretical Biology, Humboldt University, Berlin; Inst. for Neuroinformatics, UNIZH / ETHZ",
        "aff_domain": "biologie.hu-berlin.de;ini.phys.ethz.ch",
        "email": "biologie.hu-berlin.de;ini.phys.ethz.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Humboldt University;University of Zurich (UNIZH)",
        "aff_unique_dep": "Institute for Theoretical Biology;Institute for Neuroinformatics",
        "aff_unique_url": "https://www.hu-berlin.de;https://www.neuroinf.org",
        "aff_unique_abbr": "HU Berlin;UNIZH",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Berlin;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Germany;Switzerland"
    },
    {
        "id": "a2402811b0",
        "title": "Estimating the wrong Markov random field: Benefits in the computation-limited setting",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/ac34ae1fda29b8fe781ac8d6d32a6bc7-Abstract.html",
        "author": "Martin J. Wainwright",
        "abstract": "Consider the problem of joint parameter estimation and prediction in a Markov random field: i.e., the model parameters are estimated on the basis of an initial set of data, and then the fitted model is used to perform prediction (e.g., smoothing, denoising, interpolation) on a new noisy observation. Working in the computation-limited setting, we analyze a joint method in which the same convex variational relaxation is used to construct an M-estimator for fitting parameters, and to perform approximate marginalization for the prediction step. The key result of this paper is that in the computation-limited setting, using an inconsistent parameter estimator (i.e., an estimator that returns the \"wrong\" model even in the infinite data limit) is provably beneficial, since the resulting errors can partially compensate for errors made by using an approximate prediction technique. En route to this result, we analyze the asymptotic properties of M-estimators based on convex variational relaxations, and establish a Lipschitz stability property that holds for a broad class of variational methods. We show that joint estimation/prediction based on the reweighted sum-product algorithm substantially outperforms a commonly used heuristic based on ordinary sum-product. 1 Keywords: Markov random fields; variational method; message-passing algorithms; sum-product; belief propagation; parameter estimation; learning.",
        "bibtex": "@inproceedings{NIPS2005_ac34ae1f,\n author = {Wainwright, Martin J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Estimating the wrong Markov random field: Benefits in the computation-limited setting},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ac34ae1fda29b8fe781ac8d6d32a6bc7-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/ac34ae1fda29b8fe781ac8d6d32a6bc7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/ac34ae1fda29b8fe781ac8d6d32a6bc7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 132730,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12158479156372168989&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "ab4cf9c8cd",
        "title": "Estimation of Intrinsic Dimensionality Using High-Rate Vector Quantization",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/892c3b1c6dccd52936e27cbd0ff683d6-Abstract.html",
        "author": "Maxim Raginsky; Svetlana Lazebnik",
        "abstract": "We introduce a technique for dimensionality estimation based on the notion of quantization dimension, which connects the asymptotic optimal quantization error for a probability distribution on a manifold to its intrinsic dimension. The definition of quantization dimension yields a family of estimation algorithms, whose limiting case is equivalent to a recent method based on packing numbers. Using the formalism of high-rate vector quantization, we address issues of statistical consistency and analyze the behavior of our scheme in the presence of noise.",
        "bibtex": "@inproceedings{NIPS2005_892c3b1c,\n author = {Raginsky, Maxim and Lazebnik, Svetlana},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Estimation of Intrinsic Dimensionality Using High-Rate Vector Quantization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/892c3b1c6dccd52936e27cbd0ff683d6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 285515,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1679620939166167524&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "4eb81b4177",
        "title": "Extracting Dynamical Structure Embedded in Neural Activity",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/46f76a4bda9a9579eab38a8f6eabcda1-Abstract.html",
        "author": "Byron M. Yu; Afsheen Afshar; Gopal Santhanam; Stephen I. Ryu; Krishna V. Shenoy; Maneesh Sahani",
        "abstract": "Spiking activity from neurophysiological experiments often exhibits dy- namics beyond that driven by external stimulation, presumably re\ufb02ect- ing the extensive recurrence of neural circuitry. Characterizing these dynamics may reveal important features of neural computation, par- ticularly during internally-driven cognitive operations. For example, the activity of premotor cortex (PMd) neurons during an instructed de- lay period separating movement-target speci\ufb01cation and a movement- initiation cue is believed to be involved in motor planning. We show that the dynamics underlying this activity can be captured by a low- dimensional non-linear dynamical systems model, with underlying re- current structure and stochastic point-process output. We present and validate latent variable methods that simultaneously estimate the system parameters and the trial-by-trial dynamical trajectories. These meth- ods are applied to characterize the dynamics in PMd data recorded from a chronically-implanted 96-electrode array while monkeys perform delayed-reach tasks.",
        "bibtex": "@inproceedings{NIPS2005_46f76a4b,\n author = {Yu, Byron M and Afshar, Afsheen and Santhanam, Gopal and Ryu, Stephen and Shenoy, Krishna V and Sahani, Maneesh},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Extracting Dynamical Structure Embedded in Neural Activity},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/46f76a4bda9a9579eab38a8f6eabcda1-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/46f76a4bda9a9579eab38a8f6eabcda1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/46f76a4bda9a9579eab38a8f6eabcda1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 328162,
        "gs_citation": 104,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12856114712437977231&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Electrical Engineering; Department of Electrical Engineering + School of Medicine; Department of Electrical Engineering; Department of Electrical Engineering + Department of Neurosurgery; Department of Electrical Engineering + Neurosciences Program; Gatsby Computational Neuroscience Unit, UCL",
        "aff_domain": "stanford.edu;stanford.edu;stanford.edu;stanford.edu;stanford.edu;gatsby.ucl.ac.uk",
        "email": "stanford.edu;stanford.edu;stanford.edu;stanford.edu;stanford.edu;gatsby.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;0;0+2;0+3;4",
        "aff_unique_norm": "Institution not specified;School of Medicine;University of California, San Francisco;Neurosciences Program;University College London",
        "aff_unique_dep": "Department of Electrical Engineering;School of Medicine;Department of Neurosurgery;Neurosciences;Gatsby Computational Neuroscience Unit",
        "aff_unique_url": ";;https://www.ucsf.edu;;https://www.ucl.ac.uk",
        "aff_unique_abbr": ";;UCSF;;UCL",
        "aff_campus_unique_index": ";1;",
        "aff_campus_unique": ";San Francisco",
        "aff_country_unique_index": ";1;;2",
        "aff_country_unique": ";United States;United Kingdom"
    },
    {
        "id": "1a3ea41599",
        "title": "Factorial Switching Kalman Filters for Condition Monitoring in Neonatal Intensive Care",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/aeefb050911334869a7a5d9e4d0e1689-Abstract.html",
        "author": "Christopher Williams; John Quinn; Neil Mcintosh",
        "abstract": "The observed physiological dynamics of an infant receiving intensive care are affected by many possible factors, including interventions to the baby, the operation of the monitoring equipment and the state of health. The Factorial Switching Kalman Filter can be used to infer the presence of such factors from a sequence of observations, and to estimate the true values where these observations have been corrupted. We apply this model to clinical time series data and show it to be effective in identifying a number of artifactual and physiological patterns.",
        "bibtex": "@inproceedings{NIPS2005_aeefb050,\n author = {Williams, Christopher and Quinn, John and Mcintosh, Neil},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Factorial Switching Kalman Filters for Condition Monitoring in Neonatal Intensive Care},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/aeefb050911334869a7a5d9e4d0e1689-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/aeefb050911334869a7a5d9e4d0e1689-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/aeefb050911334869a7a5d9e4d0e1689-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 150862,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3446336812752695386&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "School of Informatics, University of Edinburgh; School of Informatics, University of Edinburgh; Simpson Centre for Reproductive Health",
        "aff_domain": "ed.ac.uk;ed.ac.uk;ed.ac.uk",
        "email": "ed.ac.uk;ed.ac.uk;ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Edinburgh;Simpson Centre for Reproductive Health",
        "aff_unique_dep": "School of Informatics;Reproductive Health",
        "aff_unique_url": "https://www.ed.ac.uk;",
        "aff_unique_abbr": "Edinburgh;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Edinburgh;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom;"
    },
    {
        "id": "ce8b96c707",
        "title": "Fast Gaussian Process Regression using KD-Trees",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/6775a0635c302542da2c32aa19d86be0-Abstract.html",
        "author": "Yirong Shen; Matthias Seeger; Andrew Y. Ng",
        "abstract": "1 Introduction",
        "bibtex": "@inproceedings{NIPS2005_6775a063,\n author = {Shen, Yirong and Seeger, Matthias and Ng, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Fast Gaussian Process Regression using KD-Trees},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6775a0635c302542da2c32aa19d86be0-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/6775a0635c302542da2c32aa19d86be0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/6775a0635c302542da2c32aa19d86be0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 123552,
        "gs_citation": 165,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15479407792236941120&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "6543a980ae",
        "title": "Fast Information Value for Graphical Models",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/25ef0d887bc7a2b30089a025618e1c62-Abstract.html",
        "author": "Brigham S. Anderson; Andrew W. Moore",
        "abstract": "Calculations that quantify the dependencies between variables are vital to many operations with graphical models, e.g., active learning and sen- sitivity analysis. Previously, pairwise information gain calculation has involved a cost quadratic in network size. In this work, we show how to perform a similar computation with cost linear in network size. The loss function that allows this is of a form amenable to computation by dynamic programming. The message-passing algorithm that results is described and empirical results demonstrate large speedups without de- crease in accuracy. In the cost-sensitive domains examined, superior ac- curacy is achieved.",
        "bibtex": "@inproceedings{NIPS2005_25ef0d88,\n author = {Anderson, Brigham S. and Moore, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Fast Information Value for Graphical Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/25ef0d887bc7a2b30089a025618e1c62-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/25ef0d887bc7a2b30089a025618e1c62-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/25ef0d887bc7a2b30089a025618e1c62-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 116645,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14582484633189263513&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University",
        "aff_domain": "cmu.edu;cs.cmu.edu",
        "email": "cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "012075296d",
        "title": "Fast Krylov Methods for N-Body Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/85690f81aadc1749175c187784afc9ee-Abstract.html",
        "author": "Nando D. Freitas; Yang Wang; Maryam Mahdaviani; Dustin Lang",
        "abstract": "This paper addresses the issue of numerical computation in machine learning domains based on similarity metrics, such as kernel methods, spectral techniques and Gaussian processes. It presents a general solution strategy based on Krylov subspace iteration and fast N-body learning methods. The experiments show significant gains in computation and storage on datasets arising in image segmentation, object detection and dimensionality reduction. The paper also presents theoretical bounds on the stability of these methods.",
        "bibtex": "@inproceedings{NIPS2005_85690f81,\n author = {Freitas, Nando and Wang, Yang and Mahdaviani, Maryam and Lang, Dustin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Fast Krylov Methods for N-Body Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/85690f81aadc1749175c187784afc9ee-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/85690f81aadc1749175c187784afc9ee-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/85690f81aadc1749175c187784afc9ee-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 214496,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13584936298529916968&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Computer Science, University of British Columbia; School of Computing Science, Simon Fraser University; Department of Computer Science, University of British Columbia; Department of Computer Science, University of Toronto",
        "aff_domain": "cs.ubc.ca;cs.sfu.ca;cs.ubc.ca;cs.ubc.ca",
        "email": "cs.ubc.ca;cs.sfu.ca;cs.ubc.ca;cs.ubc.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "University of British Columbia;Simon Fraser University;University of Toronto",
        "aff_unique_dep": "Department of Computer Science;School of Computing Science;Department of Computer Science",
        "aff_unique_url": "https://www.ubc.ca;https://www.sfu.ca;https://www.utoronto.ca",
        "aff_unique_abbr": "UBC;SFU;U of T",
        "aff_campus_unique_index": "0;1;0;2",
        "aff_campus_unique": "Vancouver;Burnaby;Toronto",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "76d1b028d9",
        "title": "Fast Online Policy Gradient Learning with SMD Gain Vector Adaptation",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/3fc2c60b5782f641f76bcefc39fb2392-Abstract.html",
        "author": "Jin Yu; Douglas Aberdeen; Nicol N. Schraudolph",
        "abstract": "Reinforcement learning by direct policy gradient estimation is attractive in theory but in practice leads to notoriously ill-behaved optimization problems. We improve its robustness and speed of convergence with stochastic meta-descent, a gain vector adaptation method that employs fast Hessian-vector products. In our experiments the resulting algorithms outperform previously employed online stochastic, of\ufb02ine conjugate, and natural policy gradient methods.",
        "bibtex": "@inproceedings{NIPS2005_3fc2c60b,\n author = {Yu, Jin and Aberdeen, Douglas and Schraudolph, Nicol},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Fast Online Policy Gradient Learning with SMD Gain Vector Adaptation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/3fc2c60b5782f641f76bcefc39fb2392-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/3fc2c60b5782f641f76bcefc39fb2392-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/3fc2c60b5782f641f76bcefc39fb2392-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 877497,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17045229716386059183&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 15,
        "aff": "Statistical Machine Learning, National ICT Australia, Canberra; Statistical Machine Learning, National ICT Australia, Canberra; Statistical Machine Learning, National ICT Australia, Canberra",
        "aff_domain": "nicta.com.au;nicta.com.au;nicta.com.au",
        "email": "nicta.com.au;nicta.com.au;nicta.com.au",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "National ICT Australia",
        "aff_unique_dep": "Statistical Machine Learning",
        "aff_unique_url": "https://www.nicta.com.au",
        "aff_unique_abbr": "NICTA",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Canberra",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "75a67669a9",
        "title": "Fast biped walking with a reflexive controller and real-time policy searching",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/44ac09ac6a149136a4102ee4b4103ae6-Abstract.html",
        "author": "Tao Geng; Bernd Porr; Florentin W\u00f6rg\u00f6tter",
        "abstract": "In this paper, we present our design and experiments of a planar biped robot (\"RunBot\") under pure reflexive neuronal control. The goal of this study is to combine neuronal mechanisms with biomechanics to obtain very fast speed and the on-line learning of circuit parameters. Our controller is built with biologically inspired sensor- and motor-neuron models, including local reflexes and not employing any kind of position or trajectory-tracking control algorithm. Instead, this reflexive controller allows RunBot to exploit its own natural dynamics during critical stages of its walking gait cycle. To our knowledge, this is the first time that dynamic biped walking is achieved using only a pure reflexive controller. In addition, this structure allows using a policy gradient reinforcement learning algorithm to tune the parameters of the reflexive controller in real-time during walking. This way RunBot can reach a relative speed of 3.5 leg-lengths per second after a few minutes of online learning, which is faster than that of any other biped robot, and is also comparable to the fastest relative speed of human walking. In addition, the stability domain of stable walking is quite large supporting this design strategy.",
        "bibtex": "@inproceedings{NIPS2005_44ac09ac,\n author = {Geng, Tao and Porr, Bernd and W\\\"{o}rg\\\"{o}tter, Florentin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Fast biped walking with a reflexive controller and real-time policy searching},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/44ac09ac6a149136a4102ee4b4103ae6-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/44ac09ac6a149136a4102ee4b4103ae6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/44ac09ac6a149136a4102ee4b4103ae6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 736878,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11122418789649721295&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Dept. Psychology, University of Stirling, UK; Dept. Electronics & Electrical Eng., University of Glasgow, UK; Bernstein Centre for Computational Neuroscience, University of G \u00a8ottingen + Dept. Psychology, University of Stirling, UK",
        "aff_domain": "gmail.com;elec.gla.ac.uk;chaos.gwdg.de",
        "email": "gmail.com;elec.gla.ac.uk;chaos.gwdg.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2+0",
        "aff_unique_norm": "University of Stirling;University of Glasgow;University of G\u00f6ttingen",
        "aff_unique_dep": "Dept. Psychology;Dept. Electronics & Electrical Eng.;Bernstein Centre for Computational Neuroscience",
        "aff_unique_url": "https://www.stir.ac.uk;https://www.gla.ac.uk;https://www.uni-goettingen.de",
        "aff_unique_abbr": ";UoG;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1+0",
        "aff_country_unique": "United Kingdom;Germany"
    },
    {
        "id": "e75858af8a",
        "title": "Faster Rates in Regression via Active Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/4ea6a546c19499318091a9df40a13181-Abstract.html",
        "author": "Rebecca Willett; Robert Nowak; Rui M. Castro",
        "abstract": "This paper presents a rigorous statistical analysis characterizing regimes in which active learning significantly outperforms classical passive learning. Active learning algorithms are able to make queries or select sample locations in an online fashion, depending on the results of the previous queries. In some regimes, this extra flexibility leads to significantly faster rates of error decay than those possible in classical passive learning settings. The nature of these regimes is explored by studying fundamental performance limits of active and passive learning in two illustrative nonparametric function classes. In addition to examining the theoretical potential of active learning, this paper describes a practical algorithm capable of exploiting the extra flexibility of the active setting and provably improving upon the classical passive techniques. Our active learning theory and methods show promise in a number of applications, including field estimation using wireless sensor networks and fault line detection.",
        "bibtex": "@inproceedings{NIPS2005_4ea6a546,\n author = {Willett, Rebecca and Nowak, Robert and Castro, Rui},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Faster Rates in Regression via Active Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4ea6a546c19499318091a9df40a13181-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/4ea6a546c19499318091a9df40a13181-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/4ea6a546c19499318091a9df40a13181-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 833718,
        "gs_citation": 202,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17101757848786876708&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Rice University; University of Wisconsin; University of Wisconsin",
        "aff_domain": "rice.edu;cae.wisc.edu;engr.wisc.edu",
        "email": "rice.edu;cae.wisc.edu;engr.wisc.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Rice University;University of Wisconsin",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.rice.edu;https://www.wisc.edu",
        "aff_unique_abbr": "Rice;UW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "db254e2087",
        "title": "Fixing two weaknesses of the Spectral Method",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/045cf83ab0722e782cf72d14e44adf98-Abstract.html",
        "author": "Kevin Lang",
        "abstract": "We discuss two intrinsic weaknesses of the spectral graph partitioning method, both of which have practical consequences. The first is that spectral embeddings tend to hide the best cuts from the commonly used hyperplane rounding method. Rather than cleaning up the resulting suboptimal cuts with local search, we recommend the adoption of flow-based rounding. The second weakness is that for many \"power law\" graphs, the spectral method produces cuts that are highly unbalanced, thus decreasing the usefulness of the method for visualization (see figure 4(b)) or as a basis for divide-and-conquer algorithms. These balance problems, which occur even though the spectral method's quotient-style objective function does encourage balance, can be fixed with a stricter balance constraint that turns the spectral mathematical program into an SDP that can be solved for million-node graphs by a method of Burer and Monteiro.",
        "bibtex": "@inproceedings{NIPS2005_045cf83a,\n author = {Lang, Kevin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Fixing two weaknesses of the Spectral Method},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/045cf83ab0722e782cf72d14e44adf98-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/045cf83ab0722e782cf72d14e44adf98-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/045cf83ab0722e782cf72d14e44adf98-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 357262,
        "gs_citation": 110,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17942830873070558620&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Yahoo Research",
        "aff_domain": "yahoo-inc.com",
        "email": "yahoo-inc.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Yahoo",
        "aff_unique_dep": "Yahoo Research",
        "aff_unique_url": "https://research.yahoo.com",
        "aff_unique_abbr": "Yahoo Research",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "97c40b3209",
        "title": "From Batch to Transductive Online Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/17693c91d9204b7a7646284bb3adb603-Abstract.html",
        "author": "Sham Kakade; Adam Tauman Kalai",
        "abstract": "It is well-known that everything that is learnable in the dif\ufb01cult online setting, where an arbitrary sequences of examples must be labeled one at a time, is also learnable in the batch setting, where examples are drawn independently from a distribution. We show a result in the opposite di- rection. We give an ef\ufb01cient conversion algorithm from batch to online that is transductive: it uses future unlabeled data. This demonstrates the equivalence between what is properly and ef\ufb01ciently learnable in a batch model and a transductive online model.",
        "bibtex": "@inproceedings{NIPS2005_17693c91,\n author = {Kakade, Sham and Kalai, Adam Tauman},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {From Batch to Transductive Online Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/17693c91d9204b7a7646284bb3adb603-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/17693c91d9204b7a7646284bb3adb603-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/17693c91d9204b7a7646284bb3adb603-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 203661,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17826334940670383127&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Toyota Technological Institute, Chicago, IL 60637; Toyota Technological Institute, Chicago, IL 60637",
        "aff_domain": "tti-c.org;tti-c.org",
        "email": "tti-c.org;tti-c.org",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Toyota Technological Institute at Chicago",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tti-chicago.org",
        "aff_unique_abbr": "TTI-Chicago",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chicago",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5bc1c4a42a",
        "title": "From Lasso regression to Feature vector machine",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/e6cbc650cd5798a05dfd0f51d14cde5c-Abstract.html",
        "author": "Fan Li; Yiming Yang; Eric P. Xing",
        "abstract": "Lasso regression tends to assign zero weights to most irrelevant or redundant features, and hence is a promising technique for feature selection. Its limitation, however, is that it only offers solutions to linear models. Kernel machines with feature scaling techniques have been studied for feature selection with non-linear models. However, such approaches require to solve hard non-convex optimization problems. This paper proposes a new approach named the Feature Vector Machine (FVM). It reformulates the standard Lasso regression into a form isomorphic to SVM, and this form can be easily extended for feature selection with non-linear models by introducing kernels defined on feature vectors. FVM generates sparse solutions in the nonlinear feature space and it is much more tractable compared to feature scaling kernel machines. Our experiments with FVM on simulated data show encouraging results in identifying the small number of dominating features that are non-linearly correlated to the response, a task the standard Lasso fails to complete.",
        "bibtex": "@inproceedings{NIPS2005_e6cbc650,\n author = {Li, Fan and Yang, Yiming and Xing, Eric},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {From Lasso regression to Feature vector machine},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e6cbc650cd5798a05dfd0f51d14cde5c-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/e6cbc650cd5798a05dfd0f51d14cde5c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/e6cbc650cd5798a05dfd0f51d14cde5c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 105015,
        "gs_citation": 115,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3501953249960020731&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "192fa37e81",
        "title": "From Weighted Classification to Policy Search",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/fd4f21f2556dad0ea8b7a5c04eabebda-Abstract.html",
        "author": "Doron Blatt; Alfred O. Hero",
        "abstract": "This paper proposes an algorithm to convert a T -stage stochastic decision problem with a continuous state space to a sequence of supervised learning problems. The optimization problem associated with the trajectory tree and random trajectory methods of Kearns, Mansour, and Ng, 2000, is solved using the Gauss-Seidel method. The algorithm breaks a multistage reinforcement learning problem into a sequence of single-stage reinforcement learning subproblems, each of which is solved via an exact reduction to a weighted-classification problem that can be solved using off-the-self methods. Thus the algorithm converts a reinforcement learning problem into simpler supervised learning subproblems. It is shown that the method converges in a finite number of steps to a solution that cannot be further improved by componentwise optimization. The implication of the proposed algorithm is that a plethora of classification methods can be applied to find policies in the reinforcement learning problem.",
        "bibtex": "@inproceedings{NIPS2005_fd4f21f2,\n author = {Blatt, Doron and Hero, Alfred},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {From Weighted Classification to Policy Search},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/fd4f21f2556dad0ea8b7a5c04eabebda-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/fd4f21f2556dad0ea8b7a5c04eabebda-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/fd4f21f2556dad0ea8b7a5c04eabebda-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 158118,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7497035302697577398&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Electrical Engineering and Computer Science, University of Michigan; Department of Electrical Engineering and Computer Science, University of Michigan",
        "aff_domain": "eecs.umich.edu;eecs.umich.edu",
        "email": "eecs.umich.edu;eecs.umich.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6adbcb1b55",
        "title": "Fusion of Similarity Data in Clustering",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/c61f571dbd2fb949d3fe5ae1608dd48b-Abstract.html",
        "author": "Tilman Lange; Joachim M. Buhmann",
        "abstract": "Fusing multiple information sources can yield signi\ufb01cant bene\ufb01ts to suc- cessfully accomplish learning tasks. Many studies have focussed on fus- ing information in supervised learning contexts. We present an approach to utilize multiple information sources in the form of similarity data for unsupervised learning. Based on similarity information, the clustering task is phrased as a non-negative matrix factorization problem of a mix- ture of similarity measurements. The tradeoff between the informative- ness of data sources and the sparseness of their mixture is controlled by an entropy-based weighting mechanism. For the purpose of model se- lection, a stability-based approach is employed to ensure the selection of the most self-consistent hypothesis. The experiments demonstrate the performance of the method on toy as well as real world data sets.",
        "bibtex": "@inproceedings{NIPS2005_c61f571d,\n author = {Lange, Tilman and Buhmann, Joachim},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Fusion of Similarity Data in Clustering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/c61f571dbd2fb949d3fe5ae1608dd48b-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/c61f571dbd2fb949d3fe5ae1608dd48b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/c61f571dbd2fb949d3fe5ae1608dd48b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 234135,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6921039016858854227&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 8,
        "aff": "Institute of Computational Science, Dept. of Computer Science, ETH Zurich, Switzerland; Institute of Computational Science, Dept. of Computer Science, ETH Zurich, Switzerland",
        "aff_domain": "inf.ethz.ch;inf.ethz.ch",
        "email": "inf.ethz.ch;inf.ethz.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "Dept. of Computer Science",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "0cd479083a",
        "title": "Gaussian Process Dynamical Models",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/ccd45007df44dd0f12098f486e7e8a0f-Abstract.html",
        "author": "Jack Wang; Aaron Hertzmann; David J Fleet",
        "abstract": "This paper introduces Gaussian Process Dynamical Models (GPDM) for nonlinear time series analysis. A GPDM comprises a low-dimensional latent space with associated dynamics, and a map from the latent space to an observation space. We marginalize out the model parameters in closed-form, using Gaussian Process (GP) priors for both the dynamics and the observation mappings. This results in a nonparametric model for dynamical systems that accounts for uncertainty in the model. We demonstrate the approach on human motion capture data in which each pose is 62-dimensional. Despite the use of small data sets, the GPDM learns an effective representation of the nonlinear dynamics in these spaces. Webpage: http://www.dgp.toronto.edu/",
        "bibtex": "@inproceedings{NIPS2005_ccd45007,\n author = {Wang, Jack and Hertzmann, Aaron and Fleet, David J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Gaussian Process Dynamical Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ccd45007df44dd0f12098f486e7e8a0f-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/ccd45007df44dd0f12098f486e7e8a0f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/ccd45007df44dd0f12098f486e7e8a0f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 466106,
        "gs_citation": 615,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10211804001296361261&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 22,
        "aff": "Department of Computer Science, University of Toronto, Toronto, ON M5S 3G4; Department of Computer Science, University of Toronto, Toronto, ON M5S 3G4; Department of Computer Science, University of Toronto, Toronto, ON M5S 3G4",
        "aff_domain": "dgp.toronto.edu;cs.toronto.edu;dgp.toronto.edu",
        "email": "dgp.toronto.edu;cs.toronto.edu;dgp.toronto.edu",
        "github": "",
        "project": "http://www.dgp.toronto.edu/~jmwang/gpdm/",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "b4d5cadf89",
        "title": "Gaussian Processes for Multiuser Detection in CDMA receivers",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/ad82140cafe816c41a9c9974e9240b7a-Abstract.html",
        "author": "Juan J. Murillo-fuentes; Sebastian Caro; Fernando P\u00e9rez-Cruz",
        "abstract": "In this paper we propose a new receiver for digital communications. We focus on the application of Gaussian Processes (GPs) to the multiuser detection (MUD) in code division multiple access (CDMA) systems to solve the near-far problem. Hence, we aim to reduce the interference from other users sharing the same frequency band. While usual approaches minimize the mean square error (MMSE) to linearly retrieve the user of interest, we exploit the same criteria but in the design of a nonlinear MUD. Since the optimal solution is known to be nonlinear, the performance of this novel method clearly improves that of the MMSE detectors. Furthermore, the GP based MUD achieves excellent interference suppression even for short training sequences. We also include some experiments to illustrate that other nonlinear detectors such as those based on Support Vector Machines (SVMs) exhibit a worse performance.",
        "bibtex": "@inproceedings{NIPS2005_ad82140c,\n author = {Murillo-fuentes, Juan and Caro, Sebastian and P\\'{e}rez-Cruz, Fernando},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Gaussian Processes for Multiuser Detection in CDMA receivers},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ad82140cafe816c41a9c9974e9240b7a-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/ad82140cafe816c41a9c9974e9240b7a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/ad82140cafe816c41a9c9974e9240b7a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 85432,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18128191774423535290&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Dept. Signal Theory and Communications, University of Seville; Dept. Signal Theory and Communications, University of Seville; Gatsby Computational Neuroscience, University College London",
        "aff_domain": "us.es;us.es;gatsby.ucl.ac.uk",
        "email": "us.es;us.es;gatsby.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Seville;University College London",
        "aff_unique_dep": "Dept. Signal Theory and Communications;Gatsby Computational Neuroscience",
        "aff_unique_url": "https://www.us.es;https://www.ucl.ac.uk",
        "aff_unique_abbr": ";UCL",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";London",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Spain;United Kingdom"
    },
    {
        "id": "3a2ed7c62a",
        "title": "Generalization Error Bounds for Aggregation by Mirror Descent with Averaging",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/b1300291698eadedb559786c809cc592-Abstract.html",
        "author": "Anatoli Juditsky; Alexander Nazin; Alexandre Tsybakov; Nicolas Vayatis",
        "abstract": "We consider the problem of constructing an aggregated estimator from a \ufb01nite class of base functions which approximately minimizes a con- vex risk functional under the \u21131 constraint. For this purpose, we propose a stochastic procedure, the mirror descent, which performs gradient de- scent in the dual space. The generated estimates are additionally aver- aged in a recursive fashion with speci\ufb01c weights. Mirror descent algo- rithms have been developed in different contexts and they are known to be particularly ef\ufb01cient in high dimensional problems. Moreover their implementation is adapted to the online setting. The main result of the paper is the upper bound on the convergence rate for the generalization error.",
        "bibtex": "@inproceedings{NIPS2005_b1300291,\n author = {Juditsky, Anatoli and Nazin, Alexander and Tsybakov, Alexandre and Vayatis, Nicolas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Generalization Error Bounds for Aggregation by Mirror Descent with Averaging},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b1300291698eadedb559786c809cc592-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/b1300291698eadedb559786c809cc592-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/b1300291698eadedb559786c809cc592-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 70884,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15497696622110267475&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Laboratoire de Mod\u00e9lisation et Calcul - Universit\u00e9 Grenoble I; Institute of Control Sciences - Russian Academy of Science; Laboratoire de Probabilit\u00e9s et Mod\u00e8les Al\u00e9atoires - Universit\u00e9 Paris VI; Laboratoire de Probabilit\u00e9s et Mod\u00e8les Al\u00e9atoires - Universit\u00e9 Paris VI",
        "aff_domain": "imag.fr;ipu.rssi.ru;ccr.jussieu.fr;ccr.jussieu.fr",
        "email": "imag.fr;ipu.rssi.ru;ccr.jussieu.fr;ccr.jussieu.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;2",
        "aff_unique_norm": "Universit\u00e9 Grenoble I;Institute of Control Sciences;Universit\u00e9 Paris VI",
        "aff_unique_dep": "Laboratoire de Mod\u00e9lisation et Calcul;Russian Academy of Science;Laboratoire de Probabilit\u00e9s et Mod\u00e8les Al\u00e9atoires",
        "aff_unique_url": "https://www.univ-grenoble-alpes.fr;http://ics.ras.ru;https://www.upmc.fr",
        "aff_unique_abbr": "UGI;ICS RAS;UPMC",
        "aff_campus_unique_index": "0;2;2",
        "aff_campus_unique": "Grenoble;;Paris",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "France;Russian Federation"
    },
    {
        "id": "ce1e77aec6",
        "title": "Generalization error bounds for classifiers trained with interdependent data",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/894db62f7b7a6ed2f2a277dae56a017c-Abstract.html",
        "author": "Nicolas Usunier; Massih R. Amini; Patrick Gallinari",
        "abstract": "In this paper we propose a general framework to study the generalization properties of binary classi\ufb01ers trained with data which may be depen- dent, but are deterministically generated upon a sample of independent examples. It provides generalization bounds for binary classi\ufb01cation and some cases of ranking problems, and clari\ufb01es the relationship between these learning tasks.",
        "bibtex": "@inproceedings{NIPS2005_894db62f,\n author = {Usunier, Nicolas and Amini, Massih R. and Gallinari, Patrick},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Generalization error bounds for classifiers trained with interdependent data},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/894db62f7b7a6ed2f2a277dae56a017c-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/894db62f7b7a6ed2f2a277dae56a017c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/894db62f7b7a6ed2f2a277dae56a017c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 97438,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1942941392149623230&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "0c0b506e14",
        "title": "Generalization in Clustering with Unobserved Features",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/e8f2779682fd11fa2067beffc27a9192-Abstract.html",
        "author": "Eyal Krupka; Naftali Tishby",
        "abstract": "We argue that when objects are characterized by many attributes, clustering them on the basis of a relatively small random subset of these attributes can capture information on the unobserved attributes as well. Moreover, we show that under mild technical conditions, clustering the objects on the basis of such a random subset performs almost as well as clustering with the full attribute set. We prove a finite sample generalization theorems for this novel learning scheme that extends analogous results from the supervised learning setting. The scheme is demonstrated for collaborative filtering of users with movies rating as attributes.",
        "bibtex": "@inproceedings{NIPS2005_e8f27796,\n author = {Krupka, Eyal and Tishby, Naftali},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Generalization in Clustering with Unobserved Features},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e8f2779682fd11fa2067beffc27a9192-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/e8f2779682fd11fa2067beffc27a9192-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/e8f2779682fd11fa2067beffc27a9192-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 75342,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=842993863461480296&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computer Science and Engineering, Interdisciplinary Center for Neural Computation, The Hebrew University Jerusalem, 91904, Israel; School of Computer Science and Engineering, Interdisciplinary Center for Neural Computation, The Hebrew University Jerusalem, 91904, Israel",
        "aff_domain": "cs.huji.ac.il;cs.huji.ac.il",
        "email": "cs.huji.ac.il;cs.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hebrew University of Jerusalem",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "http://www.huji.ac.il",
        "aff_unique_abbr": "HUJI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Jerusalem",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "421697f2dc",
        "title": "Generalization to Unseen Cases",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/56cb94cb34617aeadff1e79b53f38354-Abstract.html",
        "author": "Teemu Roos; Peter Gr\u00fcnwald; Petri Myllym\u00e4ki; Henry Tirri",
        "abstract": "We analyze classification error on unseen cases, i.e. cases that are different from those in the training set. Unlike standard generalization error, this off-training-set error may differ significantly from the empirical error with high probability even with large sample sizes. We derive a datadependent bound on the difference between off-training-set and standard generalization error. Our result is based on a new bound on the missing mass, which for small samples is stronger than existing bounds based on Good-Turing estimators. As we demonstrate on UCI data-sets, our bound gives nontrivial generalization guarantees in many practical cases. In light of these results, we show that certain claims made in the No Free Lunch literature are overly pessimistic.",
        "bibtex": "@inproceedings{NIPS2005_56cb94cb,\n author = {Roos, Teemu and Gr\\\"{u}nwald, Peter and Myllym\\\"{a}ki, Petri and Tirri, Henry},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Generalization to Unseen Cases},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/56cb94cb34617aeadff1e79b53f38354-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/56cb94cb34617aeadff1e79b53f38354-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/56cb94cb34617aeadff1e79b53f38354-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 119707,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9463508493773271304&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Helsinki Institute for Information Technology; CWI; Helsinki Institute for Information Technology; Nokia Research Center",
        "aff_domain": "cs.helsinki.fi;cwi.nl;cs.helsinki.fi;nokia.com",
        "email": "cs.helsinki.fi;cwi.nl;cs.helsinki.fi;nokia.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "Helsinki Institute for Information Technology;Centrum Wiskunde & Informatica;Nokia",
        "aff_unique_dep": ";;Research Center",
        "aff_unique_url": "https://hiit.fi;https://www.cwi.nl;https://www.nokia.com",
        "aff_unique_abbr": "HIIT;CWI;Nokia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "Finland;Netherlands"
    },
    {
        "id": "6aad87b3a0",
        "title": "Generalized Nonnegative Matrix Approximations with Bregman Divergences",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/d58e2f077670f4de9cd7963c857f2534-Abstract.html",
        "author": "Suvrit Sra; Inderjit S. Dhillon",
        "abstract": "Nonnegative matrix approximation (NNMA) is a recent technique for dimensionality reduction and data analysis that yields a parts based, sparse nonnegative representation for nonnegative input data. NNMA has found a wide variety of applications, including text analysis, document clustering, face/image recognition, language modeling, speech processing and many others. Despite these numerous applications, the algorithmic development for computing the NNMA factors has been relatively deficient. This paper makes algorithmic progress by modeling and solving (using multiplicative updates) new generalized NNMA problems that minimize Bregman divergences between the input matrix and its lowrank approximation. The multiplicative update formulae in the pioneering work by Lee and Seung [11] arise as a special case of our algorithms. In addition, the paper shows how to use penalty functions for incorporating constraints other than nonnegativity into the problem. Further, some interesting extensions to the use of \"link\" functions for modeling nonlinear relationships are also discussed.",
        "bibtex": "@inproceedings{NIPS2005_d58e2f07,\n author = {Sra, Suvrit and Dhillon, Inderjit},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Generalized Nonnegative Matrix Approximations with Bregman Divergences},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d58e2f077670f4de9cd7963c857f2534-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/d58e2f077670f4de9cd7963c857f2534-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/d58e2f077670f4de9cd7963c857f2534-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 81137,
        "gs_citation": 683,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10515801345833189947&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Dept. of Computer Sciences, The Univ. of Texas at Austin; Dept. of Computer Sciences, The Univ. of Texas at Austin",
        "aff_domain": "cs.utexas.edu;cs.utexas.edu",
        "email": "cs.utexas.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Department of Computer Sciences",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6015fe31bd",
        "title": "Goal-Based Imitation as Probabilistic Inference over Graphical Models",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/db5cea26ca37aa09e5365f3e7f5dd9eb-Abstract.html",
        "author": "Deepak Verma; Rajesh P. Rao",
        "abstract": "Humans are extremely adept at learning new skills by imitating the actions of others. A progression of imitative abilities has been observed in children, ranging from imitation of simple body movements to goalbased imitation based on inferring intent. In this paper, we show that the problem of goal-based imitation can be formulated as one of inferring goals and selecting actions using a learned probabilistic graphical model of the environment. We first describe algorithms for planning actions to achieve a goal state using probabilistic inference. We then describe how planning can be used to bootstrap the learning of goal-dependent policies by utilizing feedback from the environment. The resulting graphical model is then shown to be powerful enough to allow goal-based imitation. Using a simple maze navigation task, we illustrate how an agent can infer the goals of an observed teacher and imitate the teacher even when the goals are uncertain and the demonstration is incomplete.",
        "bibtex": "@inproceedings{NIPS2005_db5cea26,\n author = {Verma, Deepak and Rao, Rajesh PN},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Goal-Based Imitation as Probabilistic Inference over Graphical Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/db5cea26ca37aa09e5365f3e7f5dd9eb-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/db5cea26ca37aa09e5365f3e7f5dd9eb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/db5cea26ca37aa09e5365f3e7f5dd9eb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 116095,
        "gs_citation": 90,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9571411591749809376&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Deptt of CSE, Univ. of Washington, Seattle WA- 98195-2350; Deptt of CSE, Univ. of Washington, Seattle WA- 98195-2350",
        "aff_domain": "cs.washington.edu;cs.washington.edu",
        "email": "cs.washington.edu;cs.washington.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a9199aeb92",
        "title": "Gradient Flow Independent Component Analysis in Micropower VLSI",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/f3b7e5d3eb074cde5b76e26bc0fb5776-Abstract.html",
        "author": "Abdullah Celik; Milutin Stanacevic; Gert Cauwenberghs",
        "abstract": "We present micropower mixed-signal VLSI hardware for real-time blind separation and localization of acoustic sources. Gradient flow representation of the traveling wave signals acquired over a miniature (1cm diameter) array of four microphones yields linearly mixed instantaneous observations of the time-differentiated sources, separated and localized by independent component analysis (ICA). The gradient flow and ICA processors each measure 3mm  3mm in 0.5 m CMOS, and consume 54 W and 180 W power, respectively, from a 3 V supply at 16 ks/s sampling rate. Experiments demonstrate perceptually clear (12dB) separation and precise localization of two speech sources presented through speakers positioned at 1.5m from the array on a conference room table. Analysis of the multipath residuals shows that they are spectrally diffuse, and void of the direct path.",
        "bibtex": "@inproceedings{NIPS2005_f3b7e5d3,\n author = {Celik, Abdullah and Stanacevic, Milutin and Cauwenberghs, Gert},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Gradient Flow Independent Component Analysis in Micropower VLSI},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f3b7e5d3eb074cde5b76e26bc0fb5776-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/f3b7e5d3eb074cde5b76e26bc0fb5776-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/f3b7e5d3eb074cde5b76e26bc0fb5776-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 449875,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1599109795458202397&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Johns Hopkins University, Baltimore, MD 21218; Johns Hopkins University, Baltimore, MD 21218; Johns Hopkins University, Baltimore, MD 21218",
        "aff_domain": "jhu.edu;jhu.edu;jhu.edu",
        "email": "jhu.edu;jhu.edu;jhu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Johns Hopkins University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.jhu.edu",
        "aff_unique_abbr": "JHU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Baltimore",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ae3cb6b721",
        "title": "Group and Topic Discovery from Relations and Their Attributes",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/aee92f16efd522b9326c25cc3237ac15-Abstract.html",
        "author": "Xuerui Wang; Natasha Mohanty; Andrew McCallum",
        "abstract": "We present a probabilistic generative model of entity relationships and their attributes that simultaneously discovers groups among the entities and topics among the corresponding textual attributes. Block-models of relationship data have been studied in social network analysis for some time. Here we simultaneously cluster in several modalities at once, incor- porating the attributes (here, words) associated with certain relationships. Signi\ufb01cantly, joint inference allows the discovery of topics to be guided by the emerging groups, and vice-versa. We present experimental results on two large data sets: sixteen years of bills put before the U.S. Sen- ate, comprising their corresponding text and voting records, and thirteen years of similar data from the United Nations. We show that in compari- son with traditional, separate latent-variable models for words, or Block- structures for votes, the Group-Topic model\u2019s joint inference discovers more cohesive groups and improved topics.",
        "bibtex": "@inproceedings{NIPS2005_aee92f16,\n author = {Wang, Xuerui and Mohanty, Natasha and McCallum, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Group and Topic Discovery from Relations and Their Attributes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/aee92f16efd522b9326c25cc3237ac15-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/aee92f16efd522b9326c25cc3237ac15-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/aee92f16efd522b9326c25cc3237ac15-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 113446,
        "gs_citation": 91,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2789645985414528824&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "Department of Computer Science, University of Massachusetts; Department of Computer Science, University of Massachusetts; Department of Computer Science, University of Massachusetts",
        "aff_domain": "cs.umass.edu;cs.umass.edu;cs.umass.edu",
        "email": "cs.umass.edu;cs.umass.edu;cs.umass.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Massachusetts",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.umass.edu",
        "aff_unique_abbr": "UMass",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2caea48372",
        "title": "Hierarchical Linear/Constant Time SLAM Using Particle Filters for Dense Maps",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/b8b9c74ac526fffbeb2d39ab038d1cd7-Abstract.html",
        "author": "Austin I. Eliazar; Ronald Parr",
        "abstract": "We present an improvement to the DP-SLAM algorithm for simultane- ous localization and mapping (SLAM) that maintains multiple hypothe- ses about densely populated maps (one full map per particle in a par- ticle \ufb01lter) in time that is linear in all signi\ufb01cant algorithm parameters and takes constant (amortized) time per iteration. This means that the asymptotic complexity of the algorithm is no greater than that of a pure localization algorithm using a single map and the same number of parti- cles. We also present a hierarchical extension of DP-SLAM that uses a two level particle \ufb01lter which models drift in the particle \ufb01ltering process itself. The hierarchical approach enables recovery from the inevitable drift that results from using a \ufb01nite number of particles in a particle \ufb01lter and permits the use of DP-SLAM in more challenging domains, while maintaining linear time asymptotic complexity.",
        "bibtex": "@inproceedings{NIPS2005_b8b9c74a,\n author = {Eliazar, Austin I. and Parr, Ronald},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Hierarchical Linear/Constant Time SLAM Using Particle Filters for Dense Maps},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b8b9c74ac526fffbeb2d39ab038d1cd7-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/b8b9c74ac526fffbeb2d39ab038d1cd7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/b8b9c74ac526fffbeb2d39ab038d1cd7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 70424,
        "gs_citation": 66,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17758281097972631671&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "b048fc58ab",
        "title": "Hot Coupling: A Particle Approach to Inference and Normalization on Pairwise Undirected Graphs",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/d210cf373cf002a04ec72ee395f66306-Abstract.html",
        "author": "Firas Hamze; Nando de Freitas",
        "abstract": "This paper presents a new sampling algorithm for approximating func- tions of variables representable as undirected graphical models of arbi- trary connectivity with pairwise potentials, as well as for estimating the notoriously dif(cid:2)cult partition function of the graph. The algorithm (cid:2)ts into the framework of sequential Monte Carlo methods rather than the more widely used MCMC, and relies on constructing a sequence of in- termediate distributions which get closer to the desired one. While the idea of using (cid:147)tempered(cid:148) proposals is known, we construct a novel se- quence of target distributions where, rather than dropping a global tem- perature parameter, we sequentially couple individual pairs of variables that are, initially, sampled exactly from a spanning tree of the variables. We present experimental results on inference and estimation of the parti- tion function for sparse and densely-connected graphs.",
        "bibtex": "@inproceedings{NIPS2005_d210cf37,\n author = {Hamze, Firas and de Freitas, Nando},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Hot Coupling: A Particle Approach to Inference and Normalization on Pairwise Undirected Graphs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d210cf373cf002a04ec72ee395f66306-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/d210cf373cf002a04ec72ee395f66306-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/d210cf373cf002a04ec72ee395f66306-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 181039,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11682479359961701860&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, University of British Columbia; Department of Computer Science, University of British Columbia",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of British Columbia",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ubc.ca",
        "aff_unique_abbr": "UBC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Vancouver",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "3a81043ef3",
        "title": "How fast to work: Response vigor, motivation and tonic dopamine",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/3812f9a59b634c2a9c574610eaba5bed-Abstract.html",
        "author": "Yael Niv; Nathaniel D. Daw; Peter Dayan",
        "abstract": "Reinforcement learning models have long promised to unify computa- tional, psychological and neural accounts of appetitively conditioned be- havior. However, the bulk of data on animal conditioning comes from free-operant experiments measuring how fast animals will work for rein- forcement. Existing reinforcement learning (RL) models are silent about these tasks, because they lack any notion of vigor. They thus fail to ad- dress the simple observation that hungrier animals will work harder for food, as well as stranger facts such as their sometimes greater produc- tivity even when working for irrelevant outcomes such as water. Here, we develop an RL framework for free-operant behavior, suggesting that subjects choose how vigorously to perform selected actions by optimally balancing the costs and bene\ufb01ts of quick responding. Motivational states such as hunger shift these factors, skewing the tradeoff. This accounts normatively for the effects of motivation on response rates, as well as many other classic \ufb01ndings. Finally, we suggest that tonic levels of dopamine may be involved in the computation linking motivational state to optimal responding, thereby explaining the complex vigor-related ef- fects of pharmacological manipulation of dopamine.",
        "bibtex": "@inproceedings{NIPS2005_3812f9a5,\n author = {Niv, Yael and Daw, Nathaniel and Dayan, Peter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {How fast to work: Response vigor, motivation and tonic dopamine},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/3812f9a59b634c2a9c574610eaba5bed-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/3812f9a59b634c2a9c574610eaba5bed-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/3812f9a59b634c2a9c574610eaba5bed-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 134417,
        "gs_citation": 150,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11856389617761318905&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "ICNC, Hebrew University, Jerusalem; Gatsby Computational Neuroscience Unit, UCL; Gatsby Computational Neuroscience Unit, UCL",
        "aff_domain": "alice.nc.huji.ac.il;gatsby.ucl.ac.uk;gatsby.ucl.ac.uk",
        "email": "alice.nc.huji.ac.il;gatsby.ucl.ac.uk;gatsby.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Hebrew University;University College London",
        "aff_unique_dep": "ICNC;Gatsby Computational Neuroscience Unit",
        "aff_unique_url": "https://www.huji.ac.il;https://www.ucl.ac.uk",
        "aff_unique_abbr": "HUJI;UCL",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Jerusalem;",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Israel;United Kingdom"
    },
    {
        "id": "ddf41a44e0",
        "title": "Hyperparameter and Kernel Learning for Graph Based Semi-Supervised Classification",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/c429429bf1f2af051f2021dc92a8ebea-Abstract.html",
        "author": "Ashish Kapoor; Hyungil Ahn; Yuan Qi; Rosalind W. Picard",
        "abstract": "There have been many graph-based approaches for semi-supervised clas- si\ufb01cation. One problem is that of hyperparameter learning: performance depends greatly on the hyperparameters of the similarity graph, trans- formation of the graph Laplacian and the noise model. We present a Bayesian framework for learning hyperparameters for graph-based semi- supervised classi\ufb01cation. Given some labeled data, which can contain inaccurate labels, we pose the semi-supervised classi\ufb01cation as an in- ference problem over the unknown labels. Expectation Propagation is used for approximate inference and the mean of the posterior is used for classi\ufb01cation. The hyperparameters are learned using EM for evidence maximization. We also show that the posterior mean can be written in terms of the kernel matrix, providing a Bayesian classi\ufb01er to classify new points. Tests on synthetic and real datasets show cases where there are signi\ufb01cant improvements in performance over the existing approaches.",
        "bibtex": "@inproceedings{NIPS2005_c429429b,\n author = {Kapoor, Ashish and Ahn, Hyungil and Qi, Yuan and Picard, Rosalind},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Hyperparameter and Kernel Learning for Graph Based Semi-Supervised Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/c429429bf1f2af051f2021dc92a8ebea-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/c429429bf1f2af051f2021dc92a8ebea-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/c429429bf1f2af051f2021dc92a8ebea-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 161822,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5201688479457205986&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "MIT Media Laboratory; MIT CSAIL + MIT Media Laboratory; MIT Media Laboratory; MIT Media Laboratory",
        "aff_domain": "media.mit.edu;csail.mit.edu;media.mit.edu;media.mit.edu",
        "email": "media.mit.edu;csail.mit.edu;media.mit.edu;media.mit.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Media Laboratory",
        "aff_unique_url": "http://www.media.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0+0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "148c803cd6",
        "title": "Ideal Observers for Detecting Motion: Correspondence Noise",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/765d5fb115a9f6a3e0b23b80a5b2e4c4-Abstract.html",
        "author": "Hongjing Lu; Alan L. Yuille",
        "abstract": "We derive a Bayesian Ideal Observer (BIO) for detecting motion and solving the correspondence problem. We obtain Barlow and Tripathy\u2019s classic model as an approximation. Our psychophysical experiments show that the trends of human performance are similar to the Bayesian Ideal, but overall human performance is far worse. We investigate ways to degrade the Bayesian Ideal but show that even extreme degradations do not approach human performance. Instead we propose that humans perform motion tasks using generic, general purpose, models of motion. We perform more psychophysical experiments which are consistent with humans using a Slow-and-Smooth model and which rule out an alterna- tive model using Slowness.",
        "bibtex": "@inproceedings{NIPS2005_765d5fb1,\n author = {Lu, Hongjing and Yuille, Alan L},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Ideal Observers for Detecting Motion: Correspondence Noise},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/765d5fb115a9f6a3e0b23b80a5b2e4c4-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/765d5fb115a9f6a3e0b23b80a5b2e4c4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/765d5fb115a9f6a3e0b23b80a5b2e4c4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 305030,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7065851081659241206&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Psychology, UCLA; Department of Statistics, UCLA",
        "aff_domain": "psych.ucla.edu;stat.ucla.edu",
        "email": "psych.ucla.edu;stat.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Los Angeles",
        "aff_unique_dep": "Department of Psychology",
        "aff_unique_url": "https://www.ucla.edu",
        "aff_unique_abbr": "UCLA",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9328987fe3",
        "title": "Identifying Distributed Object Representations in Human Extrastriate Visual Cortex",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/0d9095b0d6bbe98ea0c9c02b11b59ee3-Abstract.html",
        "author": "Rory Sayres; David Ress; Kalanit Grill-spector",
        "abstract": "The category of visual stimuli has been reliably decoded from patterns of neural activity in extrastriate visual cortex [1]. It has yet to be seen whether object identity can be inferred from this activity. We present fMRI data measuring responses in human extrastriate cortex to a set of 12 distinct object images. We use a simple winner-take-all classifier, using half the data from each recording session as a training set, to evaluate encoding of object identity across fMRI voxels. Since this approach is sensitive to the inclusion of noisy voxels, we describe two methods for identifying subsets of voxels in the data which optimally distinguish object identity. One method characterizes the reliability of each voxel within subsets of the data, while another estimates the mutual information of each voxel with the stimulus set. We find that both metrics can identify subsets of the data which reliably encode object identity, even when noisy measurements are artificially added to the data. The mutual information metric is less efficient at this task, likely due to constraints in fMRI data.",
        "bibtex": "@inproceedings{NIPS2005_0d9095b0,\n author = {Sayres, Rory and Ress, David and Grill-spector, Kalanit},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Identifying Distributed Object Representations in Human Extrastriate Visual Cortex},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/0d9095b0d6bbe98ea0c9c02b11b59ee3-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/0d9095b0d6bbe98ea0c9c02b11b59ee3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/0d9095b0d6bbe98ea0c9c02b11b59ee3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 333641,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12785967211573152799&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Neuroscience, Stanford University; Department of Neuroscience, Brown University; Departments of Neuroscience and Psychology, Stanford University",
        "aff_domain": "stanford.edu;brown.edu;psych.stanford.edu",
        "email": "stanford.edu;brown.edu;psych.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Stanford University;Brown University",
        "aff_unique_dep": "Department of Neuroscience;Department of Neuroscience",
        "aff_unique_url": "https://www.stanford.edu;https://www.brown.edu",
        "aff_unique_abbr": "Stanford;Brown",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6a947ec7d4",
        "title": "Improved risk tail bounds for on-line algorithms",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/5d75b942ab4bd730bc2e819df9c9a4b5-Abstract.html",
        "author": "Nicol\u00f2 Cesa-bianchi; Claudio Gentile",
        "abstract": "We prove the strongest known  bound for the risk of hypotheses selected  from the ensemble generated by running a learning algorithm incremen(cid:173) tally on the training data. Our result is based on proof techniques that are  remarkably  different from  the  standard  risk  analysis  based  on  uniform  convergence arguments.",
        "bibtex": "@inproceedings{NIPS2005_5d75b942,\n author = {Cesa-bianchi, Nicol\\`{o} and Gentile, Claudio},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Improved risk tail bounds for on-line algorithms},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/5d75b942ab4bd730bc2e819df9c9a4b5-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/5d75b942ab4bd730bc2e819df9c9a4b5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/5d75b942ab4bd730bc2e819df9c9a4b5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1125129,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3973542617491984945&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "DSI, Universita di Milano; DICOM, Universita dell'Insubria",
        "aff_domain": "dsi.unimi.it;dsi.unimi.it",
        "email": "dsi.unimi.it;dsi.unimi.it",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Universita di Milano;Universita dell'Insubria",
        "aff_unique_dep": "DSI;DICOM",
        "aff_unique_url": "https://www.unimi.it;https://www.uninsubria.it",
        "aff_unique_abbr": "UniMi;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "2fef44d6ab",
        "title": "Inference with Minimal Communication: a Decision-Theoretic Variational Approach",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/227f6afd3b7f89b96c4bb91f95d50f6d-Abstract.html",
        "author": "O. P. Kreidl; Alan S. Willsky",
        "abstract": "Given a directed graphical model with binary-valued hidden nodes and real-valued noisy observations, consider deciding upon the maximum a-posteriori (MAP) or the maximum posterior-marginal (MPM) assignment under the restriction that each node broadcasts only to its children exactly one single-bit message. We present a variational formulation, viewing the processing rules local to all nodes as degrees-of-freedom, that minimizes the loss in expected (MAP or MPM) performance subject to such online communication constraints. The approach leads to a novel message-passing algorithm to be executed offline, or before observations are realized, which mitigates the performance loss by iteratively coupling all rules in a manner implicitly driven by global statistics. We also provide (i) illustrative examples, (ii) assumptions that guarantee convergence and efficiency and (iii) connections to active research areas.",
        "bibtex": "@inproceedings{NIPS2005_227f6afd,\n author = {Kreidl, O. and Willsky, Alan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Inference with Minimal Communication: a Decision-Theoretic Variational Approach},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/227f6afd3b7f89b96c4bb91f95d50f6d-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/227f6afd3b7f89b96c4bb91f95d50f6d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/227f6afd3b7f89b96c4bb91f95d50f6d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 102282,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1478493661367593540&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Electrical Engineering and Computer Science, MIT Laboratory for Information and Decision Systems; Department of Electrical Engineering and Computer Science, MIT Laboratory for Information and Decision Systems",
        "aff_domain": "mit.edu;mit.edu",
        "email": "mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d5159ea438",
        "title": "Inferring Motor Programs from Images of Handwritten Digits",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/8fc687aa152e8199fe9e73304d407bca-Abstract.html",
        "author": "Vinod Nair; Geoffrey E. Hinton",
        "abstract": "We describe a generative model for handwritten digits that uses two pairs of opposing springs whose stiffnesses are controlled by a motor program. We show how neural networks can be trained to infer the motor programs required to accurately reconstruct the MNIST digits. The inferred motor programs can be used directly for digit classi\ufb01cation, but they can also be used in other ways. By adding noise to the motor program inferred from an MNIST image we can generate a large set of very different images of the same class, thus enlarging the training set available to other methods. We can also use the motor programs as additional, highly informative outputs which reduce over\ufb01tting when training a feed-forward classi\ufb01er.",
        "bibtex": "@inproceedings{NIPS2005_8fc687aa,\n author = {Nair, Vinod and Hinton, Geoffrey E},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Inferring Motor Programs from Images of Handwritten Digits},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/8fc687aa152e8199fe9e73304d407bca-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/8fc687aa152e8199fe9e73304d407bca-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/8fc687aa152e8199fe9e73304d407bca-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 91708,
        "gs_citation": 88,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15772318861088476375&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "478d4822a9",
        "title": "Infinite latent feature models and the Indian buffet process",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/2ef35a8b78b572a47f56846acbeef5d3-Abstract.html",
        "author": "Zoubin Ghahramani; Thomas L. Griffiths",
        "abstract": "We define a probability distribution over equivalence classes of binary matrices with a finite number of rows and an unbounded number of columns. This distribution is suitable for use as a prior in probabilistic models that represent objects using a potentially infinite array of features. We identify a simple generative process that results in the same distribution over equivalence classes, which we call the Indian buffet process. We illustrate the use of this distribution as a prior in an infinite latent feature model, deriving a Markov chain Monte Carlo algorithm for inference in this model and applying the algorithm to an image dataset.",
        "bibtex": "@inproceedings{NIPS2005_2ef35a8b,\n author = {Ghahramani, Zoubin and Griffiths, Thomas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Infinite latent feature models and the Indian buffet process},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2ef35a8b78b572a47f56846acbeef5d3-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/2ef35a8b78b572a47f56846acbeef5d3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/2ef35a8b78b572a47f56846acbeef5d3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 158659,
        "gs_citation": 989,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8110396904817818328&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff": "Cognitive and Linguistic Sciences, Brown University, Providence RI; Gatsby Computational Neuroscience Unit, University College London, London",
        "aff_domain": "brown.edu;gatsby.ucl.ac.uk",
        "email": "brown.edu;gatsby.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Brown University;University College London",
        "aff_unique_dep": "Cognitive and Linguistic Sciences;Gatsby Computational Neuroscience Unit",
        "aff_unique_url": "https://www.brown.edu;https://www.ucl.ac.uk",
        "aff_unique_abbr": "Brown;UCL",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Providence;London",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "b3eada9c2c",
        "title": "Integrate-and-Fire models with adaptation are good enough",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/42a6845a557bef704ad8ac9cb4461d43-Abstract.html",
        "author": "Renaud Jolivet; Alexander Rauch; Hans-rudolf L\u00fcscher; Wulfram Gerstner",
        "abstract": "Integrate-and-Fire-type models are usually criticized because of their simplicity. On the other hand, the Integrate-and-Fire model is the basis of most of the theoretical studies on spiking neuron models. Here, we develop a sequential procedure to quantitatively evaluate an equivalent Integrate-and-Fire-type model based on intracellular recordings of cortical pyramidal neurons. We find that the resulting effective model is sufficient to predict the spike train of the real pyramidal neuron with high accuracy. In in vivo-like regimes, predicted and recorded traces are almost indistinguishable and a significant part of the spikes can be predicted at the correct timing. Slow processes like spike-frequency adaptation are shown to be a key feature in this context since they are necessary for the model to connect between different driving regimes.",
        "bibtex": "@inproceedings{NIPS2005_42a6845a,\n author = {Jolivet, Renaud and Rauch, Alexander and L\\\"{u}scher, Hans-rudolf and Gerstner, Wulfram},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Integrate-and-Fire models with adaptation are good enough},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/42a6845a557bef704ad8ac9cb4461d43-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/42a6845a557bef704ad8ac9cb4461d43-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/42a6845a557bef704ad8ac9cb4461d43-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 157289,
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10672508730419759817&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Brain Mind Institute, EPFL; MPI for Biological Cybernetics; Institute of Physiology; Brain Mind Institute, EPFL",
        "aff_domain": "epfl.ch;tuebingen.mpg.de;pyl.unibe.ch;epfl.ch",
        "email": "epfl.ch;tuebingen.mpg.de;pyl.unibe.ch;epfl.ch",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Ecole Polytechnique Federale de Lausanne;Max Planck Institute for Biological Cybernetics;Institute of Physiology",
        "aff_unique_dep": "Brain Mind Institute;Biological Cybernetics;",
        "aff_unique_url": "https://www.epfl.ch;https://www.biological-cybernetics.de;",
        "aff_unique_abbr": "EPFL;MPIBC;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Switzerland;Germany;"
    },
    {
        "id": "def09740dc",
        "title": "Interpolating between types and tokens by estimating power-law generators",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/4b21cf96d4cf612f239a6c322b10c8fe-Abstract.html",
        "author": "Sharon Goldwater; Mark Johnson; Thomas L. Griffiths",
        "abstract": "Standard statistical models of language fail to capture one of the most striking properties of natural languages: the power-law distribution in the frequencies of word tokens. We present a framework for developing statistical models that generically produce power-laws, augmenting stan- dard generative models with an adaptor that produces the appropriate pattern of token frequencies. We show that taking a particular stochastic process \u2013 the Pitman-Yor process \u2013 as an adaptor justi\ufb01es the appearance of type frequencies in formal analyses of natural language, and improves the performance of a model for unsupervised learning of morphology.",
        "bibtex": "@inproceedings{NIPS2005_4b21cf96,\n author = {Goldwater, Sharon and Johnson, Mark and Griffiths, Thomas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Interpolating between types and tokens by estimating power-law generators},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4b21cf96d4cf612f239a6c322b10c8fe-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/4b21cf96d4cf612f239a6c322b10c8fe-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/4b21cf96d4cf612f239a6c322b10c8fe-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 101329,
        "gs_citation": 286,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3216813151664146236&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 21,
        "aff": "Department of Cognitive and Linguistic Sciences, Brown University, Providence RI 02912, USA; Department of Cognitive and Linguistic Sciences, Brown University, Providence RI 02912, USA; Department of Cognitive and Linguistic Sciences, Brown University, Providence RI 02912, USA",
        "aff_domain": "goldwater.brown.edu;griffiths.brown.edu;johnson.brown.edu",
        "email": "goldwater.brown.edu;griffiths.brown.edu;johnson.brown.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Brown University",
        "aff_unique_dep": "Department of Cognitive and Linguistic Sciences",
        "aff_unique_url": "https://www.brown.edu",
        "aff_unique_abbr": "Brown",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Providence",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0da9abc02c",
        "title": "Is Early Vision Optimized for Extracting Higher-order Dependencies?",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/a57e8915461b83adefb011530b711704-Abstract.html",
        "author": "Yan Karklin; Michael S. Lewicki",
        "abstract": "Linear implementations of the efficient coding hypothesis, such as independent component analysis (ICA) and sparse coding models, have provided functional explanations for properties of simple cells in V1 [1, 2]. These models, however, ignore the non-linear behavior of neurons and fail to match individual and population properties of neural receptive fields in subtle but important ways. Hierarchical models, including Gaussian Scale Mixtures [3, 4] and other generative statistical models [5, 6], can capture higher-order regularities in natural images and explain nonlinear aspects of neural processing such as normalization and context effects [6, 7]. Previously, it had been assumed that the lower level representation is independent of the hierarchy, and had been fixed when training these models. Here we examine the optimal lower-level representations derived in the context of a hierarchical model and find that the resulting representations are strikingly different from those based on linear models. Unlike the the basis functions and filters learned by ICA or sparse coding, these functions individually more closely resemble simple cell receptive fields and collectively span a broad range of spatial scales. Our work unifies several related approaches and observations about natural image structure and suggests that hierarchical models might yield better representations of image structure throughout the hierarchy.",
        "bibtex": "@inproceedings{NIPS2005_a57e8915,\n author = {Karklin, Yan and Lewicki, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Is Early Vision Optimized for Extracting Higher-order Dependencies?},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a57e8915461b83adefb011530b711704-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/a57e8915461b83adefb011530b711704-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/a57e8915461b83adefb011530b711704-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 510955,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3339893998770498130&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Computer Science Department & Center for the Neural Basis of Cognition, Carnegie Mellon University; Computer Science Department & Center for the Neural Basis of Cognition, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cnbc.cmu.edu",
        "email": "cs.cmu.edu;cnbc.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Computer Science Department & Center for the Neural Basis of Cognition",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "84e9543e09",
        "title": "Kernelized Infomax Clustering",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/5dc126b503e374b0e08231344a7f493f-Abstract.html",
        "author": "David Barber; Felix V. Agakov",
        "abstract": "We propose a simple information-theoretic approach to soft clus- tering based on maximizing the mutual information I(x, y) between the unknown cluster labels y and the training patterns x with re- spect to parameters of speci\ufb01cally constrained encoding distribu- tions. The constraints are chosen such that patterns are likely to be clustered similarly if they lie close to speci\ufb01c unknown vectors in the feature space. The method may be conveniently applied to learning the optimal a\ufb03nity matrix, which corresponds to learn- ing parameters of the kernelized encoder. The procedure does not require computations of eigenvalues of the Gram matrices, which makes it potentially attractive for clustering large data sets.",
        "bibtex": "@inproceedings{NIPS2005_5dc126b5,\n author = {Barber, David and Agakov, Felix},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Kernelized Infomax Clustering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/5dc126b503e374b0e08231344a7f493f-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/5dc126b503e374b0e08231344a7f493f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/5dc126b503e374b0e08231344a7f493f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 144097,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17174472151633095090&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Edinburgh University; IDIAP Research Institute",
        "aff_domain": "inf.ed.ac.uk;idiap.ch",
        "email": "inf.ed.ac.uk;idiap.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Edinburgh;Idiap Research Institute",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ed.ac.uk;https://www.idiap.ch",
        "aff_unique_abbr": "Edinburgh;IDIAP",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United Kingdom;Switzerland"
    },
    {
        "id": "df2bdd38cc",
        "title": "Kernels for gene regulatory regions",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/bd1354624fbae3b2149878941c60df99-Abstract.html",
        "author": "Jean-philippe Vert; Robert Thurman; William S. Noble",
        "abstract": "We describe a hierarchy of motif-based kernels for multiple alignments of biological sequences, particularly suitable to process regulatory regions of genes. The kernels incorporate progressively more information, with the most complex kernel accounting for a multiple alignment of orthologous regions, the phylogenetic tree relating the species, and the prior knowledge that relevant sequence patterns occur in conserved motif blocks. These kernels can be used in the presence of a library of known transcription factor binding sites, or de novo by iterating over all k -mers of a given length. In the latter mode, a discriminative classifier built from such a kernel not only recognizes a given class of promoter regions, but as a side effect simultaneously identifies a collection of relevant, discriminative sequence motifs. We demonstrate the utility of the motif-based multiple alignment kernels by using a collection of aligned promoter regions from five yeast species to recognize classes of cell-cycle regulated genes. Supplementary data is available at http://noble.gs.washington.edu/proj/pkernel.",
        "bibtex": "@inproceedings{NIPS2005_bd135462,\n author = {Vert, Jean-philippe and Thurman, Robert and Noble, William},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Kernels for gene regulatory regions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/bd1354624fbae3b2149878941c60df99-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/bd1354624fbae3b2149878941c60df99-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/bd1354624fbae3b2149878941c60df99-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 95513,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18068143948683687595&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Ecole des Mines de Paris - ParisTech; University of Washington; University of Washington",
        "aff_domain": "ensmp.fr;u.washington.edu;gs.washington.edu",
        "email": "ensmp.fr;u.washington.edu;gs.washington.edu",
        "github": "",
        "project": "http://noble.gs.washington.edu/proj/pkernel",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Ecole des Mines de Paris;University of Washington",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.mines-paris.psl.eu;https://www.washington.edu",
        "aff_unique_abbr": "Mines ParisTech;UW",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Paris;",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "France;United States"
    },
    {
        "id": "affc23cb5e",
        "title": "Laplacian Score for Feature Selection",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/b5b03f06271f8917685d14cea7c6c50a-Abstract.html",
        "author": "Xiaofei He; Deng Cai; Partha Niyogi",
        "abstract": "In supervised learning scenarios, feature selection has been studied widely in the literature. Selecting features in unsupervised learning scenarios is a much harder problem, due to the absence of class labels that would guide the search for relevant information. And, almost all of previous unsupervised feature selection methods are \"wrapper\" techniques that require a learning algorithm to evaluate the candidate feature subsets. In this paper, we propose a \"filter\" method for feature selection which is independent of any learning algorithm. Our method can be performed in either supervised or unsupervised fashion. The proposed method is based on the observation that, in many real world classification problems, data from the same class are often close to each other. The importance of a feature is evaluated by its power of locality preserving, or, Laplacian Score. We compare our method with data variance (unsupervised) and Fisher score (supervised) on two data sets. Experimental results demonstrate the effectiveness and efficiency of our algorithm.",
        "bibtex": "@inproceedings{NIPS2005_b5b03f06,\n author = {He, Xiaofei and Cai, Deng and Niyogi, Partha},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Laplacian Score for Feature Selection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b5b03f06271f8917685d14cea7c6c50a-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/b5b03f06271f8917685d14cea7c6c50a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/b5b03f06271f8917685d14cea7c6c50a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 126245,
        "gs_citation": 2902,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14774790213415939072&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, University of Chicago; Department of Computer Science, University of Illinois at Urbana-Champaign; Department of Computer Science, University of Chicago",
        "aff_domain": "cs.uchicago.edu;uiuc.edu;cs.uchicago.edu",
        "email": "cs.uchicago.edu;uiuc.edu;cs.uchicago.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Chicago;University of Illinois Urbana-Champaign",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.uchicago.edu;https://illinois.edu",
        "aff_unique_abbr": "UChicago;UIUC",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Urbana-Champaign",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a067384924",
        "title": "Large scale networks fingerprinting and visualization using the k-core decomposition",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/b19aa25ff58940d974234b48391b9549-Abstract.html",
        "author": "J. I. Alvarez-hamelin; Luca Dall'asta; Alain Barrat; Alessandro Vespignani",
        "abstract": "We use the k-core decomposition to develop algorithms for the analysis of large scale complex networks. This decomposition, based on a re- cursive pruning of the least connected vertices, allows to disentangle the hierarchical structure of networks by progressively focusing on their cen- tral cores. By using this strategy we develop a general visualization algo- rithm that can be used to compare the structural properties of various net- works and highlight their hierarchical structure. The low computational complexity of the algorithm, O(n + e), where n is the size of the net- work, and e is the number of edges, makes it suitable for the visualization of very large sparse networks. We show how the proposed visualization tool allows to \ufb01nd speci\ufb01c structural \ufb01ngerprints of networks.",
        "bibtex": "@inproceedings{NIPS2005_b19aa25f,\n author = {Alvarez-hamelin, J. and Dall\\textquotesingle asta, Luca and Barrat, Alain and Vespignani, Alessandro},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Large scale networks fingerprinting and visualization using the k-core decomposition},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b19aa25ff58940d974234b48391b9549-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/b19aa25ff58940d974234b48391b9549-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/b19aa25ff58940d974234b48391b9549-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 327586,
        "gs_citation": 509,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15579843386763678126&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "LPT (UMR du CNRS 8627), Universit\u00b4e de Paris-Sud, 91405 ORSAY Cedex France + Facultad de Ingenier \u00b4\u0131a, Universidad de Buenos Aires, Paseo col\u00b4on 850, C 1063 ACV Buenos Aires, Argentina; LPT (UMR du CNRS 8627), Universit\u00b4e de Paris-Sud, 91405 ORSAY Cedex France; LPT (UMR du CNRS 8627), Universit\u00b4e de Paris-Sud, 91405 ORSAY Cedex France; School of Informatics, Indiana University, Bloomington, IN 47408, USA",
        "aff_domain": "lri.fr;th.u-psud.fr;th.u-psud.fr;indiana.edu",
        "email": "lri.fr;th.u-psud.fr;th.u-psud.fr;indiana.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0;2",
        "aff_unique_norm": "Universit\u00e9 de Paris-Sud;Universidad de Buenos Aires;Indiana University",
        "aff_unique_dep": "LPT (UMR du CNRS 8627);Facultad de Ingenieria;School of Informatics",
        "aff_unique_url": "https://www.universite-paris-sud.fr;https://www.uba.ar;https://www.indiana.edu",
        "aff_unique_abbr": "Paris-Sud;UBA;IU",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Buenos Aires;Bloomington",
        "aff_country_unique_index": "0+1;0;0;2",
        "aff_country_unique": "France;Argentina;United States"
    },
    {
        "id": "1d3e5ee61a",
        "title": "Large-Scale Multiclass Transduction",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/445e1050156c6ae8c082a8422bb7dfc0-Abstract.html",
        "author": "Thomas G\u00e4rtner; Quoc V. Le; Simon Burton; Alex J. Smola; Vishy Vishwanathan",
        "abstract": "We present a method for performing transductive inference on very large datasets. Our algorithm is based on multiclass Gaussian processes and is effective whenever the multiplication of the kernel matrix or its inverse with a vector can be computed suf\ufb01ciently fast. This holds, for instance, for certain graph and string kernels. Transduction is achieved by varia- tional inference over the unlabeled data subject to a balancing constraint.",
        "bibtex": "@inproceedings{NIPS2005_445e1050,\n author = {G\\\"{a}rtner, Thomas and Le, Quoc and Burton, Simon and Smola, Alex J. and Vishwanathan, Vishy},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Large-Scale Multiclass Transduction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/445e1050156c6ae8c082a8422bb7dfc0-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/445e1050156c6ae8c082a8422bb7dfc0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/445e1050156c6ae8c082a8422bb7dfc0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 77528,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17085951265331483185&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Fraunhofer AIS.KD, 53754 Sankt Augustin; Statistical Machine Learning Program, NICTA and ANU, Canberra, ACT; Statistical Machine Learning Program, NICTA and ANU, Canberra, ACT; Statistical Machine Learning Program, NICTA and ANU, Canberra, ACT; Statistical Machine Learning Program, NICTA and ANU, Canberra, ACT",
        "aff_domain": "ais.fraunhofer.de;nicta.com.au;nicta.com.au;nicta.com.au;nicta.com.au",
        "email": "ais.fraunhofer.de;nicta.com.au;nicta.com.au;nicta.com.au;nicta.com.au",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1;1",
        "aff_unique_norm": "Fraunhofer Institute for AI;Australian National University",
        "aff_unique_dep": "AI Systems and Knowledge Engineering Department;Statistical Machine Learning Program",
        "aff_unique_url": "https://www.ais.kd.fraunhofer.de/;https://www.anu.edu.au",
        "aff_unique_abbr": "Fraunhofer AIS.KD;ANU",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Canberra",
        "aff_country_unique_index": "0;1;1;1;1",
        "aff_country_unique": "Germany;Australia"
    },
    {
        "id": "59e8a357af",
        "title": "Large-scale biophysical parameter estimation in single neurons via constrained linear regression",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/488c1e0332065eb80e1129139a67d6e0-Abstract.html",
        "author": "Misha Ahrens; Liam Paninski; Quentin J. Huys",
        "abstract": "Our understanding of the input-output function of single cells has been substantially advanced by biophysically accurate multi-compartmental models. The large number of parameters needing hand tuning in these models has, however, somewhat hampered their applicability and interpretability. Here we propose a simple and well-founded method for automatic estimation of many of these key parameters: 1) the spatial distribution of channel densities on the cell's membrane; 2) the spatiotemporal pattern of synaptic input; 3) the channels' reversal potentials; 4) the intercompartmental conductances; and 5) the noise level in each compartment. We assume experimental access to: a) the spatiotemporal voltage signal in the dendrite (or some contiguous subpart thereof, e.g. via voltage sensitive imaging techniques), b) an approximate kinetic description of the channels and synapses present in each compartment, and c) the morphology of the part of the neuron under investigation. The key observation is that, given data a)-c), all of the parameters 1)-4) may be simultaneously inferred by a version of constrained linear regression; this regression, in turn, is efficiently solved using standard algorithms, without any \"local minima\" problems despite the large number of parameters and complex dynamics. The noise level 5) may also be estimated by standard techniques. We demonstrate the method's accuracy on several model datasets, and describe techniques for quantifying the uncertainty in our estimates.",
        "bibtex": "@inproceedings{NIPS2005_488c1e03,\n author = {Ahrens, Misha and Paninski, Liam and Huys, Quentin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Large-scale biophysical parameter estimation in single neurons via constrained linear regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/488c1e0332065eb80e1129139a67d6e0-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/488c1e0332065eb80e1129139a67d6e0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/488c1e0332065eb80e1129139a67d6e0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 210604,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8947875637539725503&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Gatsby Computational Neuroscience Unit; Gatsby Computational Neuroscience Unit; Gatsby Computational Neuroscience Unit",
        "aff_domain": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk;gatsby.ucl.ac.uk",
        "email": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk;gatsby.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University College London",
        "aff_unique_dep": "Gatsby Computational Neuroscience Unit",
        "aff_unique_url": "https://www.ucl.ac.uk",
        "aff_unique_abbr": "UCL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "dd5c494dff",
        "title": "Layered Dynamic Textures",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/2bc8ae25856bc2a6a1333d1331a3b7a6-Abstract.html",
        "author": "Antoni B. Chan; Nuno Vasconcelos",
        "abstract": "A dynamic texture is a video model that treats a video as a sample from a spatio-temporal stochastic process, speci\ufb01cally a linear dynamical sys- tem. One problem associated with the dynamic texture is that it cannot model video where there are multiple regions of distinct motion. In this work, we introduce the layered dynamic texture model, which addresses this problem. We also introduce a variant of the model, and present the EM algorithm for learning each of the models. Finally, we demonstrate the ef\ufb01cacy of the proposed model for the tasks of segmentation and syn- thesis of video.",
        "bibtex": "@inproceedings{NIPS2005_2bc8ae25,\n author = {Chan, Antoni and Vasconcelos, Nuno},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Layered Dynamic Textures},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2bc8ae25856bc2a6a1333d1331a3b7a6-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/2bc8ae25856bc2a6a1333d1331a3b7a6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/2bc8ae25856bc2a6a1333d1331a3b7a6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 184079,
        "gs_citation": 105,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9725001067598008170&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Electrical and Computer Engineering, University of California, San Diego; Department of Electrical and Computer Engineering, University of California, San Diego",
        "aff_domain": "ucsd.edu;ece.ucsd.edu",
        "email": "ucsd.edu;ece.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0e7783fd16",
        "title": "Learning Cue-Invariant Visual Responses",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/6c990b7aca7bc7058f5e98ea909e924b-Abstract.html",
        "author": "Jarmo Hurri",
        "abstract": "Multiple visual cues are used by the visual system to analyze a scene; achromatic cues include luminance, texture, contrast and motion. Singlecell recordings have shown that the mammalian visual cortex contains neurons that respond similarly to scene structure (e.g., orientation of a boundary), regardless of the cue type conveying this information. This paper shows that cue-invariant response properties of simple- and complex-type cells can be learned from natural image data in an unsupervised manner. In order to do this, we also extend a previous conceptual model of cue invariance so that it can be applied to model simple- and complex-cell responses. Our results relate cue-invariant response properties to natural image statistics, thereby showing how the statistical modeling approach can be used to model processing beyond the elemental response properties visual neurons. This work also demonstrates how to learn, from natural image data, more sophisticated feature detectors than those based on changes in mean luminance, thereby paving the way for new data-driven approaches to image processing and computer vision.",
        "bibtex": "@inproceedings{NIPS2005_6c990b7a,\n author = {Hurri, Jarmo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Learning Cue-Invariant Visual Responses},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6c990b7aca7bc7058f5e98ea909e924b-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/6c990b7aca7bc7058f5e98ea909e924b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/6c990b7aca7bc7058f5e98ea909e924b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 174968,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17190140237041928446&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "HIIT Basic Research Unit, University of Helsinki",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Helsinki",
        "aff_unique_dep": "HIIT Basic Research Unit",
        "aff_unique_url": "https://www.helsinki.fi",
        "aff_unique_abbr": "UH",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Finland"
    },
    {
        "id": "a7ab359f40",
        "title": "Learning Depth from Single Monocular Images",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/17d8da815fa21c57af9829fb0a869602-Abstract.html",
        "author": "Ashutosh Saxena; Sung H. Chung; Andrew Y. Ng",
        "abstract": "We consider the task of depth estimation from a single monocular image. We take a supervised learning approach to this problem, in which we begin by collecting a training set of monocular images (of unstructured outdoor environments which include forests, trees, buildings, etc.) and their corresponding ground-truth depthmaps. Then, we apply supervised learning to predict the depthmap as a function of the image. Depth estimation is a challenging problem, since local features alone are insufficient to estimate depth at a point, and one needs to consider the global context of the image. Our model uses a discriminatively-trained Markov Random Field (MRF) that incorporates multiscale local- and global-image features, and models both depths at individual points as well as the relation between depths at different points. We show that, even on unstructured scenes, our algorithm is frequently able to recover fairly accurate depthmaps.",
        "bibtex": "@inproceedings{NIPS2005_17d8da81,\n author = {Saxena, Ashutosh and Chung, Sung and Ng, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Learning Depth from Single Monocular Images},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/17d8da815fa21c57af9829fb0a869602-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/17d8da815fa21c57af9829fb0a869602-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/17d8da815fa21c57af9829fb0a869602-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 195548,
        "gs_citation": 1505,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18259224307089260510&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 21,
        "aff": "Computer Science Department, Stanford University; Computer Science Department, Stanford University; Computer Science Department, Stanford University",
        "aff_domain": "stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4dbb3ec15e",
        "title": "Learning Influence among Interacting Markov Chains",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/f6c9dc70ecfd8f90ba8598aa2401cd1a-Abstract.html",
        "author": "Dong Zhang; Daniel Gatica-perez; Samy Bengio; Deb Roy",
        "abstract": "We present a model that learns the influence of interacting Markov chains within a team. The proposed model is a dynamic Bayesian network (DBN) with a two-level structure: individual-level and group-level. Individual level models actions of each player, and the group-level models actions of the team as a whole. Experiments on synthetic multi-player games and a multi-party meeting corpus show the effectiveness of the proposed model.",
        "bibtex": "@inproceedings{NIPS2005_f6c9dc70,\n author = {Zhang, Dong and Gatica-perez, Daniel and Bengio, Samy and Roy, Deb},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Learning Influence among Interacting Markov Chains},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f6c9dc70ecfd8f90ba8598aa2401cd1a-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/f6c9dc70ecfd8f90ba8598aa2401cd1a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/f6c9dc70ecfd8f90ba8598aa2401cd1a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 109005,
        "gs_citation": 77,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14431535924956892093&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 28,
        "aff": "IDIAP Research Institute; IDIAP Research Institute; IDIAP Research Institute; Massachusetts Institute of Technology",
        "aff_domain": "idiap.ch;idiap.ch;idiap.ch;media.mit.edu",
        "email": "idiap.ch;idiap.ch;idiap.ch;media.mit.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Idiap Research Institute;Massachusetts Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.idiap.ch;https://web.mit.edu",
        "aff_unique_abbr": "IDIAP;MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "Switzerland;United States"
    },
    {
        "id": "a977d67885",
        "title": "Learning Minimum Volume Sets",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/d3d80b656929a5bc0fa34381bf42fbdd-Abstract.html",
        "author": "Clayton Scott; Robert Nowak",
        "abstract": "Given a probability measure P and a reference measure \u00b5, one is often interested in the minimum \u00b5-measure set with P -measure at least \u03b1. Minimum volume sets of this type summarize the regions of greatest probability mass of P , and are useful for detecting anoma- lies and constructing con\ufb01dence regions. This paper addresses the problem of estimating minimum volume sets based on independent samples distributed according to P . Other than these samples, no other information is available regarding P , but the reference mea- sure \u00b5 is assumed to be known. We introduce rules for estimating minimum volume sets that parallel the empirical risk minimization and structural risk minimization principles in classi\ufb01cation. As in classi\ufb01cation, we show that the performances of our estimators are controlled by the rate of uniform convergence of empirical to true probabilities over the class from which the estimator is drawn. Thus we obtain \ufb01nite sample size performance bounds in terms of VC dimension and related quantities. We also demonstrate strong universal consistency and an oracle inequality. Estimators based on histograms and dyadic partitions illustrate the proposed rules.",
        "bibtex": "@inproceedings{NIPS2005_d3d80b65,\n author = {Scott, Clayton and Nowak, Robert},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Learning Minimum Volume Sets},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d3d80b656929a5bc0fa34381bf42fbdd-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/d3d80b656929a5bc0fa34381bf42fbdd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/d3d80b656929a5bc0fa34381bf42fbdd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 128742,
        "gs_citation": 192,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6082294458973382998&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 22,
        "aff": "Statistics Department, Rice University; Electrical and Computer Engineering, University of Wisconsin",
        "aff_domain": "rice.edu;engr.wisc.edu",
        "email": "rice.edu;engr.wisc.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Rice University;University of Wisconsin",
        "aff_unique_dep": "Statistics Department;Electrical and Computer Engineering",
        "aff_unique_url": "https://www.rice.edu;https://www.wisc.edu",
        "aff_unique_abbr": "Rice;UW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ae44a37a86",
        "title": "Learning Multiple Related Tasks using Latent Independent Component Analysis",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/1cd138d0499a68f4bb72bee04bbec2d7-Abstract.html",
        "author": "Jian Zhang; Zoubin Ghahramani; Yiming Yang",
        "abstract": "We propose a probabilistic model based on Independent Component Analysis for learning multiple related tasks. In our model the task parameters are assumed to be generated from independent sources which account for the relatedness of the tasks. We use Laplace distributions to model hidden sources which makes it possible to identify the hidden, independent components instead of just modeling correlations. Furthermore, our model enjoys a sparsity property which makes it both parsimonious and robust. We also propose efficient algorithms for both empirical Bayes method and point estimation. Our experimental results on two multi-label text classification data sets show that the proposed approach is promising.",
        "bibtex": "@inproceedings{NIPS2005_1cd138d0,\n author = {Zhang, Jian and Ghahramani, Zoubin and Yang, Yiming},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Learning Multiple Related Tasks using Latent Independent Component Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/1cd138d0499a68f4bb72bee04bbec2d7-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/1cd138d0499a68f4bb72bee04bbec2d7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/1cd138d0499a68f4bb72bee04bbec2d7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 97696,
        "gs_citation": 167,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16109769994344956042&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "School of Computer Science, Carnegie Mellon University; Gatsby Computational Neuroscience Unit, University College London; School of Computer Science, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Carnegie Mellon University;University College London",
        "aff_unique_dep": "School of Computer Science;Gatsby Computational Neuroscience Unit",
        "aff_unique_url": "https://www.cmu.edu;https://www.ucl.ac.uk",
        "aff_unique_abbr": "CMU;UCL",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Pittsburgh;London",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "088a99fedf",
        "title": "Learning Rankings via Convex Hull Separation",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/4ab52371762b735317125e6446a51e8f-Abstract.html",
        "author": "Glenn Fung; R\u00f3mer Rosales; Balaji Krishnapuram",
        "abstract": "We propose ef\ufb01cient algorithms for learning ranking functions from or- der constraints between sets\u2014i.e. classes\u2014of training samples. Our al- gorithms may be used for maximizing the generalized Wilcoxon Mann Whitney statistic that accounts for the partial ordering of the classes: spe- cial cases include maximizing the area under the ROC curve for binary classi\ufb01cation and its generalization for ordinal regression. Experiments on public benchmarks indicate that: (a) the proposed algorithm is at least as accurate as the current state-of-the-art; (b) computationally, it is sev- eral orders of magnitude faster and\u2014unlike current methods\u2014it is easily able to handle even large datasets with over 20,000 samples.",
        "bibtex": "@inproceedings{NIPS2005_4ab52371,\n author = {Fung, Glenn and Rosales, R\\'{o}mer and Krishnapuram, Balaji},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Learning Rankings via Convex Hull Separation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4ab52371762b735317125e6446a51e8f-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/4ab52371762b735317125e6446a51e8f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/4ab52371762b735317125e6446a51e8f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 97037,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12226437113754866924&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "571a892df7",
        "title": "Learning Shared Latent Structure for Image Synthesis and Robotic Imitation",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/030e65da2b1c944090548d36b244b28d-Abstract.html",
        "author": "Aaron Shon; Keith Grochow; Aaron Hertzmann; Rajesh P. Rao",
        "abstract": "We propose an algorithm that uses Gaussian process regression to learn common hidden structure shared between corresponding sets of heterogenous observations. The observation spaces are linked via a single, reduced-dimensionality latent variable space. We present results from two datasets demonstrating the algorithms's ability to synthesize novel data from learned correspondences. We first show that the method can learn the nonlinear mapping between corresponding views of objects, filling in missing data as needed to synthesize novel views. We then show that the method can learn a mapping between human degrees of freedom and robotic degrees of freedom for a humanoid robot, allowing robotic imitation of human poses from motion capture data.",
        "bibtex": "@inproceedings{NIPS2005_030e65da,\n author = {Shon, Aaron and Grochow, Keith and Hertzmann, Aaron and Rao, Rajesh PN},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Learning Shared Latent Structure for Image Synthesis and Robotic Imitation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/030e65da2b1c944090548d36b244b28d-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/030e65da2b1c944090548d36b244b28d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/030e65da2b1c944090548d36b244b28d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 214481,
        "gs_citation": 267,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2084158475823015846&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Science and Engineering, University of Washington; Department of Computer Science and Engineering, University of Washington; Department of Computer Science, University of Toronto; Department of Computer Science and Engineering, University of Washington",
        "aff_domain": "cs.washington.edu;cs.washington.edu;dgp.toronto.edu;cs.washington.edu",
        "email": "cs.washington.edu;cs.washington.edu;dgp.toronto.edu;cs.washington.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Washington;University of Toronto",
        "aff_unique_dep": "Department of Computer Science and Engineering;Department of Computer Science",
        "aff_unique_url": "https://www.washington.edu;https://www.utoronto.ca",
        "aff_unique_abbr": "UW;U of T",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Seattle;Toronto",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "2b133c4040",
        "title": "Learning Topology with the Generative Gaussian Graph and the EM Algorithm",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/6d19c113404cee55b4036fce1a37c058-Abstract.html",
        "author": "Micha\u00ebl Aupetit",
        "abstract": "Given a set of points and a set of prototypes representing them, how to create a graph of the prototypes whose topology accounts for that of the points? This problem had not yet been explored in the framework of statistical learning theory. In this work, we propose a generative model based on the Delaunay graph of the prototypes and the ExpectationMaximization algorithm to learn the parameters. This work is a first step towards the construction of a topological model of a set of points grounded on statistics.",
        "bibtex": "@inproceedings{NIPS2005_6d19c113,\n author = {Aupetit, Micha\\\"{e}l},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Learning Topology with the Generative Gaussian Graph and the EM Algorithm},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6d19c113404cee55b4036fce1a37c058-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/6d19c113404cee55b4036fce1a37c058-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/6d19c113404cee55b4036fce1a37c058-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 316969,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7498657620283548965&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "CEA - DASE",
        "aff_domain": "dase.bruyeres.cea.fr",
        "email": "dase.bruyeres.cea.fr",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Commissariat \u00e0 l'\u00c9nergie Atomique et aux \u00c9nergies Alternatives (CEA)",
        "aff_unique_dep": "DASE",
        "aff_unique_url": "https://www.cea.fr",
        "aff_unique_abbr": "CEA",
        "aff_country_unique_index": "0",
        "aff_country_unique": "France"
    },
    {
        "id": "f2316807e7",
        "title": "Learning from Data of Variable Quality",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/465636eb4a7ff4b267f3b765d07a02da-Abstract.html",
        "author": "Koby Crammer; Michael Kearns; Jennifer Wortman",
        "abstract": "We initiate the study of learning from multiple sources of limited data, each of which may be corrupted at a different rate. We develop a com- plete theory of which data sources should be used for two fundamental problems: estimating the bias of a coin, and learning a classi\ufb01er in the presence of label noise. In both cases, ef\ufb01cient algorithms are provided for computing the optimal subset of data.",
        "bibtex": "@inproceedings{NIPS2005_465636eb,\n author = {Crammer, Koby and Kearns, Michael and Wortman, Jennifer},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Learning from Data of Variable Quality},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/465636eb4a7ff4b267f3b765d07a02da-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/465636eb4a7ff4b267f3b765d07a02da-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/465636eb4a7ff4b267f3b765d07a02da-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 82663,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8511953120547254031&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Computer and Information Science, University of Pennsylvania; Computer and Information Science, University of Pennsylvania; Computer and Information Science, University of Pennsylvania",
        "aff_domain": "cis.upenn.edu;cis.upenn.edu;cis.upenn.edu",
        "email": "cis.upenn.edu;cis.upenn.edu;cis.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "Computer and Information Science",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2164a9ce61",
        "title": "Learning in Silicon: Timing is Everything",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/a226e450e214f350856e2980b6e55ac9-Abstract.html",
        "author": "John V. Arthur; Kwabena Boahen",
        "abstract": "We describe a neuromorphic chip that uses binary synapses with spike timing-dependent plasticity (STDP) to learn stimulated patterns of activ- ity and to compensate for variability in excitability. Speci\ufb01cally, STDP preferentially potentiates (turns on) synapses that project from excitable neurons, which spike early, to lethargic neurons, which spike late. The additional excitatory synaptic current makes lethargic neurons spike ear- lier, thereby causing neurons that belong to the same pattern to spike in synchrony. Once learned, an entire pattern can be recalled by stimulating a subset.",
        "bibtex": "@inproceedings{NIPS2005_a226e450,\n author = {Arthur, John V. and Boahen, Kwabena},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Learning in Silicon: Timing is Everything},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a226e450e214f350856e2980b6e55ac9-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/a226e450e214f350856e2980b6e55ac9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/a226e450e214f350856e2980b6e55ac9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 289157,
        "gs_citation": 189,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7630073570986779490&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Bioengineering, University of Pennsylvania; Department of Bioengineering, University of Pennsylvania",
        "aff_domain": "seas.upenn.edu;seas.upenn.edu",
        "email": "seas.upenn.edu;seas.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "Department of Bioengineering",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "91af06c103",
        "title": "Learning to Control an Octopus Arm with Gaussian Process Temporal Difference Methods",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/5be278a9e02bed9248a4674ff62fea2c-Abstract.html",
        "author": "Yaakov Engel; Peter Szabo; Dmitry Volkinshtein",
        "abstract": "The Octopus arm is a highly versatile and complex limb. How the Octo- pus controls such a hyper-redundant arm (not to mention eight of them!) is as yet unknown. Robotic arms based on the same mechanical prin- ciples may render present day robotic arms obsolete. In this paper, we tackle this control problem using an online reinforcement learning al- gorithm, based on a Bayesian approach to policy evaluation known as Gaussian process temporal difference (GPTD) learning. Our substitute for the real arm is a computer simulation of a 2-dimensional model of an Octopus arm. Even with the simpli\ufb01cations inherent to this model, the state space we face is a high-dimensional one. We apply a GPTD- based algorithm to this domain, and demonstrate its operation on several learning tasks of varying degrees of dif\ufb01culty.",
        "bibtex": "@inproceedings{NIPS2005_5be278a9,\n author = {Engel, Yaakov and Szabo, Peter and Volkinshtein, Dmitry},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Learning to Control an Octopus Arm with Gaussian Process Temporal Difference Methods},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/5be278a9e02bed9248a4674ff62fea2c-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/5be278a9e02bed9248a4674ff62fea2c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/5be278a9e02bed9248a4674ff62fea2c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 164213,
        "gs_citation": 99,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1731890821667747861&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "AICML, Dept. of Computing Science, University of Alberta, Edmonton, Canada; Dept. of Electrical Engineering, Technion Institute of Technology, Haifa, Israel; Dept. of Electrical Engineering, Technion Institute of Technology, Haifa, Israel",
        "aff_domain": "cs.ualberta.ca;gmail.com;gmail.com",
        "email": "cs.ualberta.ca;gmail.com;gmail.com",
        "github": "",
        "project": "www.cs.ualberta.ca/~yaki",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Alberta;Technion Institute of Technology",
        "aff_unique_dep": "Dept. of Computing Science;Dept. of Electrical Engineering",
        "aff_unique_url": "https://www.ualberta.ca;https://www.technion.ac.il",
        "aff_unique_abbr": "UAlberta;Technion",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Edmonton;Haifa",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Canada;Israel"
    },
    {
        "id": "fcbf00aefe",
        "title": "Learning vehicular dynamics, with application to modeling helicopters",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/09b69adcd7cbae914c6204984097d2da-Abstract.html",
        "author": "Pieter Abbeel; Varun Ganapathi; Andrew Y. Ng",
        "abstract": "We consider the problem of modeling a helicopter's dynamics based on state-action trajectories collected from it. The contribution of this paper is two-fold. First, we consider the linear models such as learned by C I F E R (the industry standard in helicopter identification), and show that the linear parameterization makes certain properties of dynamical systems, such as inertia, fundamentally difficult to capture. We propose an alternative, acceleration based, parameterization that does not suffer from this deficiency, and that can be learned as efficiently from data. Second, a Markov decision process model of a helicopter's dynamics would explicitly model only the one-step transitions, but we are often interested in a model's predictive performance over longer timescales. In this paper, we present an efficient algorithm for (approximately) minimizing the prediction error over long time scales. We present empirical results on two different helicopters. Although this work was motivated by the problem of modeling helicopters, the ideas presented here are general, and can be applied to modeling large classes of vehicular dynamics.",
        "bibtex": "@inproceedings{NIPS2005_09b69adc,\n author = {Abbeel, Pieter and Ganapathi, Varun and Ng, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Learning vehicular dynamics, with application to modeling helicopters},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/09b69adcd7cbae914c6204984097d2da-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/09b69adcd7cbae914c6204984097d2da-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/09b69adcd7cbae914c6204984097d2da-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 113697,
        "gs_citation": 126,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17004556581006134833&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "81157c81f2",
        "title": "Location-based activity recognition",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/ef67f7c2d86352c2c42e19d20f881f53-Abstract.html",
        "author": "Lin Liao; Dieter Fox; Henry Kautz",
        "abstract": "Learning patterns of human behavior from sensor data is extremely important for high-level activity inference. We show how to extract and label a person's activities and significant places from traces of GPS data. In contrast to existing techniques, our approach simultaneously detects and classifies the significant locations of a person and takes the highlevel context into account. Our system uses relational Markov networks to represent the hierarchical activity model that encodes the complex relations among GPS readings, activities and significant places. We apply FFT-based message passing to perform efficient summation over large numbers of nodes in the networks. We present experiments that show significant improvements over existing techniques.",
        "bibtex": "@inproceedings{NIPS2005_ef67f7c2,\n author = {Liao, Lin and Fox, Dieter and Kautz, Henry},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Location-based activity recognition},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ef67f7c2d86352c2c42e19d20f881f53-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/ef67f7c2d86352c2c42e19d20f881f53-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/ef67f7c2d86352c2c42e19d20f881f53-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 204449,
        "gs_citation": 300,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1241748203913935519&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "b0ae3d6261",
        "title": "Logic and MRF Circuitry for Labeling Occluding and Thinline Visual Contours",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/6fe131632103526e3a6e8114c78eb1e1-Abstract.html",
        "author": "Eric Saund",
        "abstract": "This paper presents representation and logic for labeling contrast edges and ridges in visual scenes in terms of both surface occlusion (border ownership) and thinline objects. In natural scenes, thinline objects in- clude sticks and wires, while in human graphical communication thin- lines include connectors, dividers, and other abstract devices. Our analy- sis is directed at both natural and graphical domains. The basic problem is to formulate the logic of the interactions among local image events, speci\ufb01cally contrast edges, ridges, junctions, and alignment relations, such as to encode the natural constraints among these events in visual scenes. In a sparse heterogeneous Markov Random Field framework, we de\ufb01ne a set of interpretation nodes and energy/potential functions among them. The minimum energy con\ufb01guration found by Loopy Belief Prop- agation is shown to correspond to preferred human interpretation across a wide range of prototypical examples including important illusory con- tour \ufb01gures such as the Kanizsa Triangle, as well as more dif\ufb01cult ex- amples. In practical terms, the approach delivers correct interpretations of inherently ambiguous hand-drawn box-and-connector diagrams at low computational cost.",
        "bibtex": "@inproceedings{NIPS2005_6fe13163,\n author = {Saund, Eric},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Logic and MRF Circuitry for Labeling Occluding and Thinline Visual Contours},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6fe131632103526e3a6e8114c78eb1e1-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/6fe131632103526e3a6e8114c78eb1e1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/6fe131632103526e3a6e8114c78eb1e1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 236446,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14404713854339238500&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Palo Alto Research Center",
        "aff_domain": "parc.com",
        "email": "parc.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Palo Alto Research Center",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.parc.com",
        "aff_unique_abbr": "PARC",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Palo Alto",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "cc81fbc990",
        "title": "Maximum Margin Semi-Supervised Learning for Structured Variables",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/e833e042f509c996b1b25324d56659fb-Abstract.html",
        "author": "Y. Altun; D. McAllester; M. Belkin",
        "abstract": "Many real-world classi\ufb01cation problems involve the prediction of multiple inter-dependent variables forming some structural depen- dency. Recent progress in machine learning has mainly focused on supervised classi\ufb01cation of such structured variables. In this paper, we investigate structured classi\ufb01cation in a semi-supervised setting. We present a discriminative approach that utilizes the intrinsic ge- ometry of input patterns revealed by unlabeled data points and we derive a maximum-margin formulation of semi-supervised learning for structured variables. Unlike transductive algorithms, our for- mulation naturally extends to new test points.",
        "bibtex": "@inproceedings{NIPS2005_e833e042,\n author = {Altun, Y. and McAllester, D. and Belkin, M.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Maximum Margin Semi-Supervised Learning for Structured Variables},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e833e042f509c996b1b25324d56659fb-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/e833e042f509c996b1b25324d56659fb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/e833e042f509c996b1b25324d56659fb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 113176,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "TTI at Chicago; TTI at Chicago; Department of Computer Science, University of Chicago",
        "aff_domain": "tti-c.org;tti-c.org;cs.uchicago.edu",
        "email": "tti-c.org;tti-c.org;cs.uchicago.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Toyota Technological Institute at Chicago;University of Chicago",
        "aff_unique_dep": ";Department of Computer Science",
        "aff_unique_url": "https://www.tti-chicago.org;https://www.uchicago.edu",
        "aff_unique_abbr": "TTI;UChicago",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chicago;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8b5fb0e3f6",
        "title": "Measuring Shared Information and Coordinated Activity in Neuronal Networks",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/12a1d073d5ed3fa12169c67c4e2ce415-Abstract.html",
        "author": "Kristina Klinkner; Cosma Shalizi; Marcelo Camperi",
        "abstract": "Most nervous systems encode information about stimuli in the responding activity of large neuronal networks. This activity often manifests itself as dynamically coordinated sequences of action potentials. Since multiple electrode recordings are now a standard tool in neuroscience research, it is important to have a measure of such network-wide behavioral coordination and information sharing, applicable to multiple neural spike train data. We propose a new statistic, informational coherence, which measures how much better one unit can be predicted by knowing the dynamical state of another. We argue informational coherence is a measure of association and shared information which is superior to traditional pairwise measures of synchronization and correlation. To find the dynamical states, we use a recently-introduced algorithm which reconstructs effective state spaces from stochastic time series. We then extend the pairwise measure to a multivariate analysis of the network by estimating the network multi-information. We illustrate our method by testing it on a detailed model of the transition from gamma to beta rhythms. Much of the most important information in neural systems is shared over multiple neurons or cortical areas, in such forms as population codes and distributed representations [1]. On behavioral time scales, neural information is stored in temporal patterns of activity as opposed to static markers; therefore, as information is shared between neurons or brain regions, it is physically instantiated as coordination between entire sequences of neural spikes. Furthermore, neural systems and regions of the brain often require coordinated neural activity to perform important functions; acting in concert requires multiple neurons or cortical areas to share information [2]. Thus, if we want to measure the dynamic network-wide behavior of neurons and test hypotheses about them, we need reliable, practical methods to detect and quantify behavioral coordination and the associated information sharing across multiple neural units. These would be especially useful in testing ideas about how particular forms of coordination relate to distributed coding (e.g., that of [3]). Current techniques to analyze relations among spike trains handle only pairs of neurons, so we further need a method which is extendible to analyze the coordination in the network, system, or region as a whole. Here we propose a new measure of behavioral coordination and information sharing, informational coherence, based on the notion of dynamical state. Section 1 argues that coordinated behavior in neural systems is often not captured by exist-",
        "bibtex": "@inproceedings{NIPS2005_12a1d073,\n author = {Klinkner, Kristina and Shalizi, Cosma and Camperi, Marcelo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Measuring Shared Information and Coordinated Activity in Neuronal Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/12a1d073d5ed3fa12169c67c4e2ce415-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/12a1d073d5ed3fa12169c67c4e2ce415-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/12a1d073d5ed3fa12169c67c4e2ce415-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 148580,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11391586310582129232&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Statistics Department, University of Michigan; Statistics Department, Carnegie Mellon University; Physics Department, University of San Francisco",
        "aff_domain": "umich.edu;stat.cmu.edu;usfca.edu",
        "email": "umich.edu;stat.cmu.edu;usfca.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Michigan;Carnegie Mellon University;University of San Francisco",
        "aff_unique_dep": "Statistics Department;Statistics Department;Physics Department",
        "aff_unique_url": "https://www.umich.edu;https://www.cmu.edu;https://www.usfca.edu",
        "aff_unique_abbr": "UM;CMU;USF",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Ann Arbor;;San Francisco",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b929c37a4e",
        "title": "Message passing for task redistribution on sparse graphs",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/dc20d1211f3e7a99d775b26052e0163e-Abstract.html",
        "author": "K. Y. Michael Wong; David Saad; Zhuo Gao",
        "abstract": "The problem of resource allocation in sparse graphs with real variables is studied using methods of statistical physics. An ef\ufb01cient distributed algorithm is devised on the basis of insight gained from the analysis and is examined using numerical simulations, showing excellent performance and full agreement with the theoretical results.",
        "bibtex": "@inproceedings{NIPS2005_dc20d121,\n author = {Wong, K. Y. Michael and Saad, David and Gao, Zhuo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Message passing for task redistribution on sparse graphs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/dc20d1211f3e7a99d775b26052e0163e-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/dc20d1211f3e7a99d775b26052e0163e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/dc20d1211f3e7a99d775b26052e0163e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 125436,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2553662250950157647&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Hong Kong U. of Science & Technology, Clear Water Bay, Hong Kong, China; NCRG, Aston University, Birmingham B4 7ET, UK; Hong Kong U. of Science & Technology, Clear Water Bay, Hong Kong, China + Dept. of Physics, Beijing Normal Univ., Beijing 100875, China",
        "aff_domain": "ust.hk;aston.ac.uk;bnu.edu.cn",
        "email": "ust.hk;aston.ac.uk;bnu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0+2",
        "aff_unique_norm": "Hong Kong University of Science and Technology;Aston University;Beijing Normal University",
        "aff_unique_dep": ";NCRG;Department of Physics",
        "aff_unique_url": "https://www.ust.hk;https://www.aston.ac.uk;http://www.bnu.edu.cn",
        "aff_unique_abbr": "HKUST;;BNU",
        "aff_campus_unique_index": "0;1;0+2",
        "aff_campus_unique": "Clear Water Bay;Birmingham;Beijing",
        "aff_country_unique_index": "0;1;0+0",
        "aff_country_unique": "China;United Kingdom"
    },
    {
        "id": "a9915951f0",
        "title": "Metric Learning by Collapsing Classes",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/ad8e88c0f76fa4fc8e5474384142a00a-Abstract.html",
        "author": "Amir Globerson; Sam T. Roweis",
        "abstract": "We present an algorithm for learning a quadratic Gaussian metric (Maha- lanobis distance) for use in classi\ufb01cation tasks. Our method relies on the simple geometric intuition that a good metric is one under which points in the same class are simultaneously near each other and far from points in the other classes. We construct a convex optimization problem whose solution generates such a metric by trying to collapse all examples in the same class to a single point and push examples in other classes in\ufb01nitely far away. We show that when the metric we learn is used in simple clas- si\ufb01ers, it yields substantial improvements over standard alternatives on a variety of problems. We also discuss how the learned metric may be used to obtain a compact low dimensional feature representation of the original input space, allowing more ef\ufb01cient classi\ufb01cation with very little reduction in performance.",
        "bibtex": "@inproceedings{NIPS2005_ad8e88c0,\n author = {Globerson, Amir and Roweis, Sam},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Metric Learning by Collapsing Classes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ad8e88c0f76fa4fc8e5474384142a00a-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/ad8e88c0f76fa4fc8e5474384142a00a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/ad8e88c0f76fa4fc8e5474384142a00a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 141219,
        "gs_citation": 922,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=663809489195562938&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "School of Computer Science and Engineering, Interdisciplinary Center for Neural Computation, The Hebrew University Jerusalem, 91904, Israel; Machine Learning Group, Department of Computer Science, University of Toronto, Canada",
        "aff_domain": "cs.huji.ac.il;cs.toronto.edu",
        "email": "cs.huji.ac.il;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Hebrew University of Jerusalem;University of Toronto",
        "aff_unique_dep": "School of Computer Science and Engineering;Department of Computer Science",
        "aff_unique_url": "http://www.huji.ac.il;https://www.utoronto.ca",
        "aff_unique_abbr": "HUJI;U of T",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Jerusalem;Toronto",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Israel;Canada"
    },
    {
        "id": "e704a5648e",
        "title": "Mixture Modeling by Affinity Propagation",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/327708dd10d68b1361ad3addbaca01f2-Abstract.html",
        "author": "Brendan J. Frey; Delbert Dueck",
        "abstract": "Clustering is a fundamental problem in machine learning and has been approached in many ways. Two general and quite different approaches include iteratively \ufb01tting a mixture model (e.g., using EM) and linking to- gether pairs of training cases that have high af\ufb01nity (e.g., using spectral methods). Pair-wise clustering algorithms need not compute suf\ufb01cient statistics and avoid poor solutions by directly placing similar examples in the same cluster. However, many applications require that each cluster of data be accurately described by a prototype or model, so af\ufb01nity-based clustering \u2013 and its bene\ufb01ts \u2013 cannot be directly realized. We describe a technique called \u201caf\ufb01nity propagation\u201d, which combines the advantages of both approaches. The method learns a mixture model of the data by recursively propagating af\ufb01nity messages. We demonstrate af\ufb01nity prop- agation on the problems of clustering image patches for image segmen- tation and learning mixtures of gene expression models from microar- ray data. We \ufb01nd that af\ufb01nity propagation obtains better solutions than mixtures of Gaussians, the K-medoids algorithm, spectral clustering and hierarchical clustering, and is both able to \ufb01nd a pre-speci\ufb01ed number of clusters and is able to automatically determine the number of clusters. Interestingly, af\ufb01nity propagation can be viewed as belief propagation in a graphical model that accounts for pairwise training case likelihood functions and the identi\ufb01cation of cluster centers.",
        "bibtex": "@inproceedings{NIPS2005_327708dd,\n author = {Frey, Brendan J and Dueck, Delbert},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Mixture Modeling by Affinity Propagation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/327708dd10d68b1361ad3addbaca01f2-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/327708dd10d68b1361ad3addbaca01f2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/327708dd10d68b1361ad3addbaca01f2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 267040,
        "gs_citation": 153,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3482611368722222694&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Toronto; University of Toronto",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "www.psi.toronto.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "e4d51b80e0",
        "title": "Modeling Memory Transfer and Saving in Cerebellar Motor Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/efd7e9ed0e5e694ba6df444d84dfa37d-Abstract.html",
        "author": "Naoki Masuda; Shun-ichi Amari",
        "abstract": "There is a long-standing controversy on the site of the cerebellar motor learning. Different theories and experimental results suggest that either the cerebellar flocculus or the brainstem learns the task and stores the memory. With a dynamical system approach, we clarify the mechanism of transferring the memory generated in the flocculus to the brainstem and that of so-called savings phenomena. The brainstem learning must comply with a sort of Hebbian rule depending on Purkinje-cell activities. In contrast to earlier numerical models, our model is simple but it accommodates explanations and predictions of experimental situations as qualitative features of trajectories in the phase space of synaptic weights, without fine parameter tuning.",
        "bibtex": "@inproceedings{NIPS2005_efd7e9ed,\n author = {Masuda, Naoki and Amari, Shun-ichi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Modeling Memory Transfer and Saving in Cerebellar Motor Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/efd7e9ed0e5e694ba6df444d84dfa37d-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/efd7e9ed0e5e694ba6df444d84dfa37d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/efd7e9ed0e5e694ba6df444d84dfa37d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 160484,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1903827059373011358&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "RIKEN Brain Science Institute; RIKEN Brain Science Institute",
        "aff_domain": "brain.riken.jp;brain.riken.jp",
        "email": "brain.riken.jp;brain.riken.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "RIKEN",
        "aff_unique_dep": "Brain Science Institute",
        "aff_unique_url": "https://briken.org",
        "aff_unique_abbr": "RIKEN",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "aeb2c744e3",
        "title": "Modeling Neural Population Spiking Activity with Gibbs Distributions",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/6244b2ba957c48bc64582cf2bcec3d04-Abstract.html",
        "author": "Frank Wood; Stefan Roth; Michael J. Black",
        "abstract": "Probabilistic modeling of correlated neural population firing activity is central to understanding the neural code and building practical decoding algorithms. No parametric models currently exist for modeling multivariate correlated neural data and the high dimensional nature of the data makes fully non-parametric methods impractical. To address these problems we propose an energy-based model in which the joint probability of neural activity is represented using learned functions of the 1D marginal histograms of the data. The parameters of the model are learned using contrastive divergence and an optimization procedure for finding appropriate marginal directions. We evaluate the method using real data recorded from a population of motor cortical neurons. In particular, we model the joint probability of population spiking times and 2D hand position and show that the likelihood of test data under our model is significantly higher than under other models. These results suggest that our model captures correlations in the firing activity. Our rich probabilistic model of neural population activity is a step towards both measurement of the importance of correlations in neural coding and improved decoding of population activity.",
        "bibtex": "@inproceedings{NIPS2005_6244b2ba,\n author = {Wood, Frank and Roth, Stefan and Black, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Modeling Neural Population Spiking Activity with Gibbs Distributions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6244b2ba957c48bc64582cf2bcec3d04-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/6244b2ba957c48bc64582cf2bcec3d04-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/6244b2ba957c48bc64582cf2bcec3d04-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 89478,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4801468529069602880&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "Department of Computer Science, Brown University; Department of Computer Science, Brown University; Department of Computer Science, Brown University",
        "aff_domain": "cs.brown.edu;cs.brown.edu;cs.brown.edu",
        "email": "cs.brown.edu;cs.brown.edu;cs.brown.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Brown University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.brown.edu",
        "aff_unique_abbr": "Brown",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b855c595b8",
        "title": "Modeling Neuronal Interactivity using Dynamic Bayesian Networks",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/1a6727711b84fd1efbb87fc565199d13-Abstract.html",
        "author": "Lei Zhang; Dimitris Samaras; Nelly Alia-klein; Nora Volkow; Rita Goldstein",
        "abstract": "Functional Magnetic Resonance Imaging (fMRI) has enabled scientists to look into the active brain. However, interactivity between functional brain regions, is still little studied. In this paper, we contribute a novel framework for modeling the interactions between multiple active brain regions, using Dynamic Bayesian Networks (DBNs) as generative mod- els for brain activation patterns. This framework is applied to modeling of neuronal circuits associated with reward. The novelty of our frame- work from a Machine Learning perspective lies in the use of DBNs to reveal the brain connectivity and interactivity. Such interactivity mod- els which are derived from fMRI data are then validated through a group classi\ufb01cation task. We employ and compare four different types of DBNs: Parallel Hidden Markov Models, Coupled Hidden Markov Models, Fully-linked Hidden Markov Models and Dynamically Multi- Linked HMMs (DML-HMM). Moreover, we propose and compare two schemes of learning DML-HMMs. Experimental results show that by using DBNs, group classi\ufb01cation can be performed even if the DBNs are constructed from as few as 5 brain regions. We also demonstrate that, by using the proposed learning algorithms, different DBN structures charac- terize drug addicted subjects vs. control subjects. This \ufb01nding provides an independent test for the effect of psychopathology on brain function. In general, we demonstrate that incorporation of computer science prin- ciples into functional neuroimaging clinical studies provides a novel ap- proach for probing human brain function.",
        "bibtex": "@inproceedings{NIPS2005_1a672771,\n author = {Zhang, Lei and Samaras, Dimitris and Alia-klein, Nelly and Volkow, Nora and Goldstein, Rita},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Modeling Neuronal Interactivity using Dynamic Bayesian Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/1a6727711b84fd1efbb87fc565199d13-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/1a6727711b84fd1efbb87fc565199d13-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/1a6727711b84fd1efbb87fc565199d13-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 230373,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7029889297597346786&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Computer Science Department, SUNY at Stony Brook, Stony Brook, NY; Computer Science Department, SUNY at Stony Brook, Stony Brook, NY; Medical Department, Brookhaven National Laboratory, Upton, NY; Medical Department, Brookhaven National Laboratory, Upton, NY; Medical Department, Brookhaven National Laboratory, Upton, NY",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;1;1",
        "aff_unique_norm": "State University of New York at Stony Brook;Brookhaven National Laboratory",
        "aff_unique_dep": "Computer Science Department;Medical Department",
        "aff_unique_url": "https://www.stonybrook.edu;https://www.bnl.gov",
        "aff_unique_abbr": "SUNY Stony Brook;BNL",
        "aff_campus_unique_index": "0;0;1;1;1",
        "aff_campus_unique": "Stony Brook;Upton",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "206cc194ce",
        "title": "Multiple Instance Boosting for Object Detection",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/56c82ccd658e09e829f16bb99457bcbc-Abstract.html",
        "author": "Cha Zhang; John C. Platt; Paul A. Viola",
        "abstract": "A good image object detection algorithm is accurate, fast, and does not require exact locations of objects in a training set. We can create such an object detector by taking the architecture of the Viola-Jones detector cascade and training it with a new variant of boosting that we call MILBoost. MILBoost uses cost functions from the Multiple Instance Learning literature combined with the AnyBoost framework. We adapt the feature selection criterion of MILBoost to optimize the performance of the Viola-Jones cascade. Experiments show that the detection rate is up to 1.6 times better using MILBoost. This increased detection rate shows the advantage of simultaneously learning the locations and scales of the objects in the training set along with the parameters of the classifier.",
        "bibtex": "@inproceedings{NIPS2005_56c82ccd,\n author = {Zhang, Cha and Platt, John and Viola, Paul},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Multiple Instance Boosting for Object Detection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/56c82ccd658e09e829f16bb99457bcbc-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/56c82ccd658e09e829f16bb99457bcbc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/56c82ccd658e09e829f16bb99457bcbc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 288538,
        "gs_citation": 1008,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13310043595207109757&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Microsoft Research; Microsoft Research; Microsoft Research",
        "aff_domain": "microsoft.com;microsoft.com; ",
        "email": "microsoft.com;microsoft.com; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Microsoft Research",
        "aff_unique_url": "https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7a22802576",
        "title": "Nearest Neighbor Based Feature Selection for Regression and its Application to Neural Activity",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/5a2756a3cb9cde852cad3c97e120b656-Abstract.html",
        "author": "Amir Navot; Lavi Shpigelman; Naftali Tishby; Eilon Vaadia",
        "abstract": "We present a non-linear, simple, yet effective, feature subset selection method for regression and use it in analyzing cortical neural activity. Our algorithm involves a feature-weighted version of the k-nearest-neighbor algorithm. It is able to capture complex dependency of the target func- tion on its input and makes use of the leave-one-out error as a natural regularization. We explain the characteristics of our algorithm on syn- thetic problems and use it in the context of predicting hand velocity from spikes recorded in motor cortex of a behaving monkey. By applying fea- ture selection we are able to improve prediction quality and suggest a novel way of exploring neural data.",
        "bibtex": "@inproceedings{NIPS2005_5a2756a3,\n author = {Navot, Amir and Shpigelman, Lavi and Tishby, Naftali and Vaadia, Eilon},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Nearest Neighbor Based Feature Selection for Regression and its Application to Neural Activity},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/5a2756a3cb9cde852cad3c97e120b656-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/5a2756a3cb9cde852cad3c97e120b656-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/5a2756a3cb9cde852cad3c97e120b656-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 300476,
        "gs_citation": 146,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9274319416896077915&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "School of computer Science and Engineering; Interdisciplinary Center for Neural Computation; Dept. of Physiology, Hadassah Medical School; The Hebrew University Jerusalem, 91904, Israel",
        "aff_domain": "cs.huji.ac.il;cs.huji.ac.il; ; ",
        "email": "cs.huji.ac.il;cs.huji.ac.il; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "School of Computer Science and Engineering;Interdisciplinary Center for Neural Computation;Hadassah Medical School;Hebrew University of Jerusalem",
        "aff_unique_dep": "Computer Science and Engineering;Neural Computation;Dept. of Physiology;",
        "aff_unique_url": ";;https://www.hadassah.org.il;https://www.huji.ac.il",
        "aff_unique_abbr": ";;;HUJI",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Jerusalem",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";Israel"
    },
    {
        "id": "93c0647ab5",
        "title": "Nested sampling for Potts models",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/9dc372713683fd865d366d5d9ee810ba-Abstract.html",
        "author": "Iain Murray; David MacKay; Zoubin Ghahramani; John Skilling",
        "abstract": "Nested sampling is a new Monte Carlo method by Skilling [1] intended for general Bayesian computation. Nested sampling provides a robust alternative to annealing-based methods for computing normalizing constants. It can also generate estimates of other quantities such as posterior expectations. The key technical requirement is an ability to draw samples uniformly from the prior sub ject to a constraint on the likelihood. We provide a demonstration with the Potts model, an undirected graphical model.",
        "bibtex": "@inproceedings{NIPS2005_9dc37271,\n author = {Murray, Iain and MacKay, David and Ghahramani, Zoubin and Skilling, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Nested sampling for Potts models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/9dc372713683fd865d366d5d9ee810ba-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/9dc372713683fd865d366d5d9ee810ba-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/9dc372713683fd865d366d5d9ee810ba-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 403345,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4161018666263080836&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Gatsby Computational Neuroscience Unit, University College London; Cavendish Laboratory, University of Cambridge; Gatsby Computational Neuroscience Unit, University College London; Maximum Entropy Data Consultants Ltd.",
        "aff_domain": "gatsby.ucl.ac.uk;mrao.cam.ac.uk;gatsby.ucl.ac.uk;eircom.net",
        "email": "gatsby.ucl.ac.uk;mrao.cam.ac.uk;gatsby.ucl.ac.uk;eircom.net",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "University College London;University of Cambridge;Maximum Entropy Data Consultants",
        "aff_unique_dep": "Gatsby Computational Neuroscience Unit;Cavendish Laboratory;",
        "aff_unique_url": "https://www.ucl.ac.uk;https://www.cam.ac.uk;",
        "aff_unique_abbr": "UCL;Cambridge;",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "London;Cambridge;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "ba57ebef12",
        "title": "Neural mechanisms of contrast dependent receptive field size in V1",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/e98741479a7b998f88b8f8c9f0b6b6f1-Abstract.html",
        "author": "Jim Wielaard; Paul Sajda",
        "abstract": "Based on a large scale spiking neuron model of the input layers 4C and  of macaque, we identify neural mechanisms for the observed contrast dependent receptive field size of V1 cells. We observe a rich variety of mechanisms for the phenomenon and analyze them based on the relative gain of excitatory and inhibitory synaptic inputs. We observe an average growth in the spatial extent of excitation and inhibition for low contrast, as predicted from phenomenological models. However, contrary to phenomenological models, our simulation results suggest this is neither sufficient nor necessary to explain the phenomenon.",
        "bibtex": "@inproceedings{NIPS2005_e9874147,\n author = {Wielaard, Jim and Sajda, Paul},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Neural mechanisms of contrast dependent receptive field size in V1},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e98741479a7b998f88b8f8c9f0b6b6f1-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/e98741479a7b998f88b8f8c9f0b6b6f1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/e98741479a7b998f88b8f8c9f0b6b6f1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 151442,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11585732714806275198&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Biomedical Engineering, Columbia University; Department of Biomedical Engineering, Columbia University",
        "aff_domain": "columbia.edu;columbia.edu",
        "email": "columbia.edu;columbia.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "Department of Biomedical Engineering",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b576fdebb4",
        "title": "Neuronal Fiber Delineation in Area of Edema from Diffusion Weighted MRI",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/dfb84a11f431c62436cfb760e30a34fe-Abstract.html",
        "author": "Ofer Pasternak; Nathan Intrator; Nir Sochen; Yaniv Assaf",
        "abstract": "Diffusion Tensor Magnetic Resonance Imaging (DT-MRI) is a non inva- sive method for brain neuronal \ufb01bers delineation. Here we show a mod- i\ufb01cation for DT-MRI that allows delineation of neuronal \ufb01bers which are in\ufb01ltrated by edema. We use the Muliple Tensor Variational (MTV) framework which replaces the diffusion model of DT-MRI with a mul- tiple component model and \ufb01ts it to the signal attenuation with a vari- ational regularization mechanism. In order to reduce free water con- tamination we estimate the free water compartment volume fraction in each voxel, remove it, and then calculate the anisotropy of the remaining compartment. The variational framework was applied on data collected with conventional clinical parameters, containing only six diffusion di- rections. By using the variational framework we were able to overcome the highly ill posed \ufb01tting. The results show that we were able to \ufb01nd \ufb01bers that were not found by DT-MRI.",
        "bibtex": "@inproceedings{NIPS2005_dfb84a11,\n author = {Pasternak, Ofer and Intrator, Nathan and Sochen, Nir and Assaf, Yaniv},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Neuronal Fiber Delineation in Area of Edema from Diffusion Weighted MRI},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/dfb84a11f431c62436cfb760e30a34fe-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/dfb84a11f431c62436cfb760e30a34fe-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/dfb84a11f431c62436cfb760e30a34fe-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 77876,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15857446985962490&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 16,
        "aff": "School of Computer Science, Tel-Aviv University; Department of Applied Mathematics, Tel-Aviv University; School of Computer Science, Tel-Aviv University; Department of Neurobiochemistry, Faculty of Life Science, Tel-Aviv University",
        "aff_domain": "post.tau.ac.il;post.tau.ac.il;post.tau.ac.il;post.tau.ac.il",
        "email": "post.tau.ac.il;post.tau.ac.il;post.tau.ac.il;post.tau.ac.il",
        "github": "",
        "project": "http://www.cs.tau.ac.il/~oferpas",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Tel-Aviv University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.tau.ac.il",
        "aff_unique_abbr": "TAU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Tel-Aviv",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "d9794e5a1d",
        "title": "Noise and the two-thirds power Law",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/e6af401c28c1790eaef7d55c92ab6ab6-Abstract.html",
        "author": "Uri Maoz; Elon Portugaly; Tamar Flash; Yair Weiss",
        "abstract": "The two-thirds power law, an empirical law stating an inverse non-linear relationship between the tangential hand speed and the curvature of its trajectory during curved motion, is widely acknowledged to be an invariant of upper-limb movement. It has also been shown to exist in eyemotion, locomotion and was even demonstrated in motion perception and prediction. This ubiquity has fostered various attempts to uncover the origins of this empirical relationship. In these it was generally attributed either to smoothness in hand- or joint-space or to the result of mechanisms that damp noise inherent in the motor system to produce the smooth trajectories evident in healthy human motion. We show here that white Gaussian noise also obeys this power-law. Analysis of signal and noise combinations shows that trajectories that were synthetically created not to comply with the power-law are transformed to power-law compliant ones after combination with low levels of noise. Furthermore, there exist colored noise types that drive non-power-law trajectories to power-law compliance and are not affected by smoothing. These results suggest caution when running experiments aimed at verifying the power-law or assuming its underlying existence without proper analysis of the noise. Our results could also suggest that the power-law might be derived not from smoothness or smoothness-inducing mechanisms operating on the noise inherent in our motor system but rather from the correlated noise which is inherent in this motor system.",
        "bibtex": "@inproceedings{NIPS2005_e6af401c,\n author = {Maoz, Uri and Portugaly, Elon and Flash, Tamar and Weiss, Yair},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Noise and the two-thirds power Law},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e6af401c28c1790eaef7d55c92ab6ab6-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/e6af401c28c1790eaef7d55c92ab6ab6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/e6af401c28c1790eaef7d55c92ab6ab6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1205936,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15138795887409647111&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "256ee75324",
        "title": "Non-Gaussian Component Analysis: a Semi-parametric Framework for Linear Dimension Reduction",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/3070e6addcd702cb58de5d7897bfdae1-Abstract.html",
        "author": "Gilles Blanchard; Masashi Sugiyama; Motoaki Kawanabe; Vladimir Spokoiny; Klaus-Robert M\u00fcller",
        "abstract": "We propose a new linear method for dimension reduction to identify nonGaussian components in high dimensional data. Our method, NGCA (non-Gaussian component analysis), uses a very general semi-parametric framework. In contrast to existing projection methods we define what is uninteresting (Gaussian): by projecting out uninterestingness, we can estimate the relevant non-Gaussian subspace. We show that the estimation error of finding the non-Gaussian components tends to zero at a parametric rate. Once NGCA components are identified and extracted, various tasks can be applied in the data analysis process, like data visualization, clustering, denoising or classification. A numerical study demonstrates the usefulness of our method.",
        "bibtex": "@inproceedings{NIPS2005_3070e6ad,\n author = {Blanchard, Gilles and Sugiyama, Masashi and Kawanabe, Motoaki and Spokoiny, Vladimir and M\\\"{u}ller, Klaus-Robert},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Non-Gaussian Component Analysis: a Semi-parametric Framework for Linear Dimension Reduction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/3070e6addcd702cb58de5d7897bfdae1-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/3070e6addcd702cb58de5d7897bfdae1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/3070e6addcd702cb58de5d7897bfdae1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 197287,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=456792672884564521&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Fraunhofer FIRST.IDA; Fraunhofer FIRST.IDA + Dept. of CS, Tokyo Inst. of Tech.; Fraunhofer FIRST.IDA; Weierstrass Institute and Humboldt University; Fraunhofer FIRST.IDA + Dept. of CS, University of Potsdam",
        "aff_domain": "first.fhg.de;first.fhg.de;first.fhg.de;wias-berlin.de;first.fhg.de",
        "email": "first.fhg.de;first.fhg.de;first.fhg.de;wias-berlin.de;first.fhg.de",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;0;2;0+3",
        "aff_unique_norm": "Fraunhofer Institute for Software and Systems Engineering;Tokyo Institute of Technology;Weierstrass Institute;University of Potsdam",
        "aff_unique_dep": "FIRST.IDA;Department of Computer Science;;Dept. of CS",
        "aff_unique_url": "https://www.first.ida.fraunhofer.de/;https://www.titech.ac.jp;https://www.wiener-stitut.de;https://www.uni-potsdam.de",
        "aff_unique_abbr": "Fraunhofer FIRST.IDA;Tokyo Tech;WIAS;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+1;0;0;0+0",
        "aff_country_unique": "Germany;Japan"
    },
    {
        "id": "bb67e72905",
        "title": "Non-Local Manifold Parzen Windows",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/17eb7ecc4c38e4705361cccd903ad8c6-Abstract.html",
        "author": "Yoshua Bengio; Hugo Larochelle; Pascal Vincent",
        "abstract": "To escape from the curse of dimensionality, we claim that one can learn non-local functions, in the sense that the value and shape of the learned function at x must be inferred using examples that may be far from x. With this objective, we present a non-local non-parametric density esti- mator. It builds upon previously proposed Gaussian mixture models with regularized covariance matrices to take into account the local shape of the manifold. It also builds upon recent work on non-local estimators of the tangent plane of a manifold, which are able to generalize in places with little training data, unlike traditional, local, non-parametric models.",
        "bibtex": "@inproceedings{NIPS2005_17eb7ecc,\n author = {Bengio, Yoshua and Larochelle, Hugo and Vincent, Pascal},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Non-Local Manifold Parzen Windows},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/17eb7ecc4c38e4705361cccd903ad8c6-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/17eb7ecc4c38e4705361cccd903ad8c6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/17eb7ecc4c38e4705361cccd903ad8c6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 206255,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=340187095118487201&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "37d5d9bb11",
        "title": "Non-iterative Estimation with Perturbed Gaussian Markov Processes",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/73b817090081cef1bca77232f4532c5d-Abstract.html",
        "author": "Yunsong Huang; B. Keith Jenkins",
        "abstract": "We develop an approach for estimation with Gaussian Markov processes that imposes a smoothness prior while allowing for discontinuities. In- stead of propagating information laterally between neighboring nodes in a graph, we study the posterior distribution of the hidden nodes as a whole\u2014how it is perturbed by invoking discontinuities, or weakening the edges, in the graph. We show that the resulting computation amounts to feed-forward fan-in operations reminiscent of V1 neurons. Moreover, using suitable matrix preconditioners, the incurred matrix inverse and determinant can be approximated, without iteration, in the same compu- tational style. Simulation results illustrate the merits of this approach.",
        "bibtex": "@inproceedings{NIPS2005_73b81709,\n author = {Huang, Yunsong and Jenkins, B. Keith},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Non-iterative Estimation with Perturbed Gaussian Markov Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/73b817090081cef1bca77232f4532c5d-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/73b817090081cef1bca77232f4532c5d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/73b817090081cef1bca77232f4532c5d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 138800,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10279097312382252061&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Signal and Image Processing Institute, Department of Electrical Engineering-Systems, University of Southern California; Signal and Image Processing Institute, Department of Electrical Engineering-Systems, University of Southern California",
        "aff_domain": "sipi.usc.edu;sipi.usc.edu",
        "email": "sipi.usc.edu;sipi.usc.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Southern California",
        "aff_unique_dep": "Department of Electrical Engineering-Systems",
        "aff_unique_url": "https://www.usc.edu",
        "aff_unique_abbr": "USC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7f647543db",
        "title": "Nonparametric inference of prior probabilities from Bayes-optimal behavior",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/f9fd2624beefbc7808e4e405d73f57ab-Abstract.html",
        "author": "Liam Paninski",
        "abstract": "We discuss a method for obtaining a subject\u2019s a priori beliefs from his/her behavior in a psychophysics context, under the assumption that the behavior is (nearly) optimal from a Bayesian perspective. The method is nonparametric in the sense that we do not assume that the prior belongs to any \ufb01xed class of distributions (e.g., Gaussian). Despite this increased generality, the method is relatively simple to implement, being based in the simplest case on a linear programming algorithm, and more generally on a straightforward maximum likelihood or maximum a posteriori formulation, which turns out to be a convex optimization problem (with no non-global local maxima) in many important cases. In addition, we develop methods for analyzing the uncertainty of these esti- mates. We demonstrate the accuracy of the method in a simple simulated coin-\ufb02ipping setting; in particular, the method is able to precisely track the evolution of the subject\u2019s posterior distribution as more and more data are observed. We close by brie\ufb02y discussing an interesting connection to recent models of neural population coding.",
        "bibtex": "@inproceedings{NIPS2005_f9fd2624,\n author = {Paninski, Liam},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Nonparametric inference of prior probabilities from Bayes-optimal behavior},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f9fd2624beefbc7808e4e405d73f57ab-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/f9fd2624beefbc7808e4e405d73f57ab-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/f9fd2624beefbc7808e4e405d73f57ab-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 146276,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2266110193455259752&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Statistics, Columbia University",
        "aff_domain": "stat.columbia.edu",
        "email": "stat.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "bd3d7cd71b",
        "title": "Norepinephrine and Neural Interrupts",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/b2ea5e977c5fc1ccfa74171a9723dd61-Abstract.html",
        "author": "Peter Dayan; Angela J. Yu",
        "abstract": "Angela J. Yu",
        "bibtex": "@inproceedings{NIPS2005_b2ea5e97,\n author = {Dayan, Peter and Yu, Angela J.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Norepinephrine and Neural Interrupts},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b2ea5e977c5fc1ccfa74171a9723dd61-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/b2ea5e977c5fc1ccfa74171a9723dd61-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/b2ea5e977c5fc1ccfa74171a9723dd61-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 109697,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16560961520645505924&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Gatsby Computational Neuroscience Unit, University College London; Center for Brain, Mind & Behavior, Princeton University",
        "aff_domain": "gatsby.ucl.ac.uk;princeton.edu",
        "email": "gatsby.ucl.ac.uk;princeton.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University College London;Princeton University",
        "aff_unique_dep": "Gatsby Computational Neuroscience Unit;Center for Brain, Mind & Behavior",
        "aff_unique_url": "https://www.ucl.ac.uk;https://www.princeton.edu",
        "aff_unique_abbr": "UCL;Princeton",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "London;Princeton",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "81536c6bbe",
        "title": "Oblivious Equilibrium: A Mean Field Approximation for Large-Scale Dynamic Games",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/fb3f76858cb38e5b7fd113e0bc1c0721-Abstract.html",
        "author": "Gabriel Y. Weintraub; Lanier Benkard; Benjamin Van Roy",
        "abstract": "We propose a mean-\ufb01eld approximation that dramatically reduces the computational complexity of solving stochastic dynamic games. We pro- vide conditions that guarantee our method approximates an equilibrium as the number of agents grow. We then derive a performance bound to assess how well the approximation performs for any given number of agents. We apply our method to an important class of problems in ap- plied microeconomics. We show with numerical experiments that we are able to greatly expand the set of economic problems that can be analyzed computationally.",
        "bibtex": "@inproceedings{NIPS2005_fb3f7685,\n author = {Weintraub, Gabriel Y. and Benkard, Lanier and Van Roy, Benjamin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Oblivious Equilibrium: A Mean Field Approximation for Large-Scale Dynamic Games},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/fb3f76858cb38e5b7fd113e0bc1c0721-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/fb3f76858cb38e5b7fd113e0bc1c0721-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/fb3f76858cb38e5b7fd113e0bc1c0721-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 167829,
        "gs_citation": 162,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1985316851862067825&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Stanford University; Stanford University; Stanford University",
        "aff_domain": "stanford.edu;stanford.edu;stanford.edu",
        "email": "stanford.edu;stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4aa01f7350",
        "title": "Off-Road Obstacle Avoidance through End-to-End Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/fdf1bc5669e8ff5ba45d02fded729feb-Abstract.html",
        "author": "Urs Muller; Jan Ben; Eric Cosatto; Beat Flepp; Yann L. Cun",
        "abstract": "We describe a vision-based obstacle avoidance system for off-road mobile robots. The system is trained from end to end to map raw in put images to steering angles. It is trained in supervised mode to predict the steering angles provided by a human driver during training r uns collected in a wide variety of terrains, weather conditions, lighting conditions, and obstacle types. The robot is a 50cm off-road truck, with two f orwardpointing wireless color cameras. A remote computer process es the video and controls the robot via radio. The learning system is a lar ge 6-layer convolutional network whose input is a single left/right pair of unprocessed low-resolution images. The robot exhibits an excellent ability to detect obstacles and navigate around them in real time at speeds of 2 m/s.",
        "bibtex": "@inproceedings{NIPS2005_fdf1bc56,\n author = {Muller, Urs and Ben, Jan and Cosatto, Eric and Flepp, Beat and Cun, Yann},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Off-Road Obstacle Avoidance through End-to-End Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/fdf1bc5669e8ff5ba45d02fded729feb-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/fdf1bc5669e8ff5ba45d02fded729feb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/fdf1bc5669e8ff5ba45d02fded729feb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 216461,
        "gs_citation": 782,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9788131855215156897&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Courant Institute of Mathematical Sciences, New York University, New York, NY 10004, USA; Net-Scale Technologies, Morganville, NJ 07751, USA; Net-Scale Technologies, Morganville, NJ 07751, USA; NECLaboratories, Princeton, NJ 08540; Net-Scale Technologies, Morganville, NJ 07751, USA",
        "aff_domain": "http://yann.lecun.com;net-scale.com; ; ;net-scale.com",
        "email": "http://yann.lecun.com;net-scale.com; ; ;net-scale.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;2;1",
        "aff_unique_norm": "New York University;Net-Scale Technologies;NEC Laboratories",
        "aff_unique_dep": "Courant Institute of Mathematical Sciences;;",
        "aff_unique_url": "https://www.nyu.edu;;https://www.nec-labs.com",
        "aff_unique_abbr": "NYU;;NEC Labs",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "New York;;Princeton",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f6b0b902ab",
        "title": "Off-policy Learning with Options and Recognizers",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/f75526659f31040afeb61cb7133e4e6d-Abstract.html",
        "author": "Doina Precup; Cosmin Paduraru; Anna Koop; Richard S. Sutton; Satinder P. Singh",
        "abstract": "We introduce a new algorithm for off-policy temporal-difference learning with function approximation that has lower variance and requires less knowledge of the behavior policy than prior methods. We develop the notion of a recognizer, a filter on actions that distorts the behavior policy to produce a related target policy with low-variance importance-sampling corrections. We also consider target policies that are deviations from the state distribution of the behavior policy, such as potential temporally abstract options, which further reduces variance. This paper introduces recognizers and their potential advantages, then develops a full algorithm for linear function approximation and proves that its updates are in the same direction as on-policy TD updates, which implies asymptotic convergence. Even though our algorithm is based on importance sampling, we prove that it requires absolutely no knowledge of the behavior policy for the case of state-aggregation function approximators.",
        "bibtex": "@inproceedings{NIPS2005_f7552665,\n author = {Precup, Doina and Paduraru, Cosmin and Koop, Anna and Sutton, Richard S and Singh, Satinder},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Off-policy Learning with Options and Recognizers},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f75526659f31040afeb61cb7133e4e6d-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/f75526659f31040afeb61cb7133e4e6d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/f75526659f31040afeb61cb7133e4e6d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 96117,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12226561511147853505&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "McGill University, Montreal, QC, Canada; University of Alberta, Edmonton, AB, Canada; University of Alberta, Edmonton, AB, Canada; University of Alberta, Edmonton, AB, Canada; University of Michigan, Ann Arbor, MI, USA",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1;2",
        "aff_unique_norm": "McGill University;University of Alberta;University of Michigan",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.mcgill.ca;https://www.ualberta.ca;https://www.umich.edu",
        "aff_unique_abbr": "McGill;UAlberta;UM",
        "aff_campus_unique_index": "0;1;1;1;2",
        "aff_campus_unique": "Montreal;Edmonton;Ann Arbor",
        "aff_country_unique_index": "0;0;0;0;1",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "00f911449f",
        "title": "On Local Rewards and Scaling Distributed Reinforcement Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/02180771a9b609a26dcea07f272e141f-Abstract.html",
        "author": "Drew Bagnell; Andrew Y. Ng",
        "abstract": "We consider the scaling of the number of examples necessary to achieve good performance in distributed, cooperative, multi-agent reinforcement learning, as a function of the the number of agents n. We prove a worstcase lower bound showing that algorithms that rely solely on a global reward signal to learn policies confront a fundamental limit: They require a number of real-world examples that scales roughly linearly in the number of agents. For settings of interest with a very large number of agents, this is impractical. We demonstrate, however, that there is a class of algorithms that, by taking advantage of local reward signals in large distributed Markov Decision Processes, are able to ensure good performance with a number of samples that scales as O(log n). This makes them applicable even in settings with a very large number of agents n.",
        "bibtex": "@inproceedings{NIPS2005_02180771,\n author = {Bagnell, Drew and Ng, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {On Local Rewards and Scaling Distributed Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/02180771a9b609a26dcea07f272e141f-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/02180771a9b609a26dcea07f272e141f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/02180771a9b609a26dcea07f272e141f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 880965,
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12782169336403918870&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Robotics Institute, Carnegie Mellon University; Computer Science Department, Stanford University",
        "aff_domain": "ri.cmu.edu;cs.stanford.edu",
        "email": "ri.cmu.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Carnegie Mellon University;Stanford University",
        "aff_unique_dep": "Robotics Institute;Computer Science Department",
        "aff_unique_url": "https://www.cmu.edu;https://www.stanford.edu",
        "aff_unique_abbr": "CMU;Stanford",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Pittsburgh;Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a4229a983a",
        "title": "On the Accuracy of Bounded Rationality: How Far from Optimal Is Fast and Frugal?",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/39d352b0395ba768e18f042c6e2a8621-Abstract.html",
        "author": "Michael Schmitt; Laura Martignon",
        "abstract": "Fast and frugal heuristics are well studied models of bounded rationality. Psychological research has proposed the take-the-best heuristic as a successful strategy in decision making with limited resources. Take-thebest searches for a sufficiently good ordering of cues (features) in a task where objects are to be compared lexicographically. We investigate the complexity of the problem of approximating optimal cue permutations for lexicographic strategies. We show that no efficient algorithm can approximate the optimum to within any constant factor, if P = NP. We further consider a greedy approach for building lexicographic strategies and derive tight bounds for the performance ratio of a new and simple algorithm. This algorithm is proven to perform better than take-the-best.",
        "bibtex": "@inproceedings{NIPS2005_39d352b0,\n author = {Schmitt, Michael and Martignon, Laura},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {On the Accuracy of Bounded Rationality: How Far from Optimal Is Fast and Frugal?},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/39d352b0395ba768e18f042c6e2a8621-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/39d352b0395ba768e18f042c6e2a8621-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/39d352b0395ba768e18f042c6e2a8621-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 60228,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=566406275959022485&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Ludwig-Marum-Gymnasium; Institut f\u00a8ur Mathematik und Informatik P\u00a8adagogische Hochschule Ludwigsburg",
        "aff_domain": "googlemail.com;ph-ludwigsburg.de",
        "email": "googlemail.com;ph-ludwigsburg.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Ludwig-Marum-Gymnasium;P\u00e4dagogische Hochschule Ludwigsburg",
        "aff_unique_dep": ";Institut f\u00fcr Mathematik und Informatik",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "1a76306301",
        "title": "On the Convergence of Eigenspaces in Kernel Principal Component Analysis",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/2b6921f2c64dee16ba21ebf17f3c2c92-Abstract.html",
        "author": "Laurent Zwald; Gilles Blanchard",
        "abstract": "This paper presents a non-asymptotic statistical analysis of Kernel-PCA with a focus different from the one proposed in previous work on this topic. Here instead of considering the reconstruction error of KPCA we are interested in approximation error bounds for the eigenspaces themselves. We prove an upper bound depending on the spacing between eigenvalues but not on the dimensionality of the eigenspace. As a consequence this allows to infer stability results for these estimated spaces.",
        "bibtex": "@inproceedings{NIPS2005_2b6921f2,\n author = {Zwald, Laurent and Blanchard, Gilles},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {On the Convergence of Eigenspaces in Kernel Principal Component Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2b6921f2c64dee16ba21ebf17f3c2c92-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/2b6921f2c64dee16ba21ebf17f3c2c92-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/2b6921f2c64dee16ba21ebf17f3c2c92-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 80573,
        "gs_citation": 173,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12742740378181008870&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "D\u00b4epartement de Math \u00b4ematiques, Universit \u00b4e Paris-Sud, B\u02c6at. 425, F-91405 Orsay, France; Fraunhofer First (IDA), K\u00b4ekul\u00b4estr. 7, D-12489 Berlin, Germany",
        "aff_domain": "math.u-psud.fr;first.fhg.de",
        "email": "math.u-psud.fr;first.fhg.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Universit\u00e9 Paris-Sud;Fraunhofer Institute for Digital Data Space (IDA)",
        "aff_unique_dep": "D\u00e9partement de Math\u00e9matiques;",
        "aff_unique_url": "https://www.universite-paris-sud.fr;https://www.iza.berlin/",
        "aff_unique_abbr": "UPS;Fraunhofer IDA",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Orsay;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "France;Germany"
    },
    {
        "id": "7e67d992c1",
        "title": "Online Discovery and Learning of Predictive State Representations",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/c3008b2c6f5370b744850a98a95b73ad-Abstract.html",
        "author": "Peter Mccracken; Michael Bowling",
        "abstract": "Predictive state representations (PSRs) are a method of modeling dynamical systems using only observable data, such as actions and observations, to describe their model. PSRs use predictions about the outcome of future tests to summarize the system state. The best existing techniques for discovery and learning of PSRs use a Monte Carlo approach to explicitly estimate these outcome probabilities. In this paper, we present a new algorithm for discovery and learning of PSRs that uses a gradient descent approach to compute the predictions for the current state. The algorithm takes advantage of the large amount of structure inherent in a valid prediction matrix to constrain its predictions. Furthermore, the algorithm can be used online by an agent to constantly improve its prediction quality; something that current state of the art discovery and learning algorithms are unable to do. We give empirical results to show that our constrained gradient algorithm is able to discover core tests using very small amounts of data, and with larger amounts of data can compute accurate predictions of the system dynamics.",
        "bibtex": "@inproceedings{NIPS2005_c3008b2c,\n author = {Mccracken, Peter and Bowling, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Online Discovery and Learning of Predictive State Representations},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/c3008b2c6f5370b744850a98a95b73ad-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/c3008b2c6f5370b744850a98a95b73ad-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/c3008b2c6f5370b744850a98a95b73ad-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 126641,
        "gs_citation": 86,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12822051606863413359&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computing Science, University of Alberta; Department of Computing Science, University of Alberta",
        "aff_domain": "cs.ualberta.ca;cs.ualberta.ca",
        "email": "cs.ualberta.ca;cs.ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "Department of Computing Science",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "97207e840d",
        "title": "Optimal cue selection strategy",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/2eb5657d37f474e4c4cf01e4882b8962-Abstract.html",
        "author": "Vidhya Navalpakkam; Laurent Itti",
        "abstract": "Survival in the natural world demands the selection of relevant visual cues to rapidly and reliably guide attention towards prey an d predators in cluttered environments. We investigate whether our visu al system selects cues that guide search in an optimal manner. We formall y obtain the optimal cue selection strategy by maximizing the signal to noise ratio (S N R) between a search target and surrounding distractors. This optimal strategy successfully accounts for several phenom ena in visual search behavior, including the effect of target-distracto r discriminability, uncertainty in target's features, distractor heterogenei ty, and linear separability. Furthermore, the theory generates a new predict ion, which we verify through psychophysical experiments with human subj ects. Our results provide direct experimental evidence that humans sel ect visual cues so as to maximize S N R between the targets and surrounding clutter.",
        "bibtex": "@inproceedings{NIPS2005_2eb5657d,\n author = {Navalpakkam, Vidhya and Itti, Laurent},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Optimal cue selection strategy},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2eb5657d37f474e4c4cf01e4882b8962-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/2eb5657d37f474e4c4cf01e4882b8962-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/2eb5657d37f474e4c4cf01e4882b8962-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 102009,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4785141175450196706&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, USC, Los Angeles; Department of Computer Science, USC, Los Angeles",
        "aff_domain": "usc.edu;usc.edu",
        "email": "usc.edu;usc.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Southern California",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.usc.edu",
        "aff_unique_abbr": "USC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ad3fc7b626",
        "title": "Optimizing spatio-temporal filters for improving Brain-Computer Interfacing",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/6e0e24295e8a86282cb559b860416812-Abstract.html",
        "author": "Guido Dornhege; Benjamin Blankertz; Matthias Krauledat; Florian Losch; Gabriel Curio; Klaus-Robert M\u00fcller",
        "abstract": "Brain-Computer Interface (BCI) systems create a novel communication channel from the brain to an output device by bypassing conventional motor output pathways of nerves and muscles. Therefore they could provide a new communication and control option for paralyzed patients. Modern BCI technology is essentially based on techniques for the clas- si\ufb01cation of single-trial brain signals. Here we present a novel technique that allows the simultaneous optimization of a spatial and a spectral \ufb01lter enhancing discriminability of multi-channel EEG single-trials. The eval- uation of 60 experiments involving 22 different subjects demonstrates the superiority of the proposed algorithm. Apart from the enhanced clas- si\ufb01cation, the spatial and/or the spectral \ufb01lter that are determined by the algorithm can also be used for further analysis of the data, e.g., for source localization of the respective brain rhythms.",
        "bibtex": "@inproceedings{NIPS2005_6e0e2429,\n author = {Dornhege, Guido and Blankertz, Benjamin and Krauledat, Matthias and Losch, Florian and Curio, Gabriel and M\\\"{u}ller, Klaus-Robert},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Optimizing spatio-temporal filters for improving Brain-Computer Interfacing},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6e0e24295e8a86282cb559b860416812-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/6e0e24295e8a86282cb559b860416812-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/6e0e24295e8a86282cb559b860416812-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 73867,
        "gs_citation": 96,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11480818217356820953&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Fraunhofer FIRST.IDA; Fraunhofer FIRST.IDA; Fraunhofer FIRST.IDA + University of Potsdam; Campus Benjamin Franklin, Charit\u00e9 University Medicine Berlin; Campus Benjamin Franklin, Charit\u00e9 University Medicine Berlin; Fraunhofer FIRST.IDA + University of Potsdam",
        "aff_domain": "first.fhg.de;first.fhg.de;first.fhg.de;charite.de;charite.de;first.fhg.de",
        "email": "first.fhg.de;first.fhg.de;first.fhg.de;charite.de;charite.de;first.fhg.de",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+1;2;2;0+1",
        "aff_unique_norm": "Fraunhofer Institute for Software and Systems Engineering;University of Potsdam;Charit\u00e9 University Medicine Berlin",
        "aff_unique_dep": "FIRST.IDA;;",
        "aff_unique_url": "https://www.first.ida.fraunhofer.de/;https://www.uni-potsdam.de;https://www.charite.de",
        "aff_unique_abbr": "Fraunhofer FIRST.IDA;UP;Charit\u00e9",
        "aff_campus_unique_index": ";1;1;",
        "aff_campus_unique": ";Benjamin Franklin",
        "aff_country_unique_index": "0;0;0+0;0;0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "3c4df8761a",
        "title": "Pattern Recognition from One Example by Chopping",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/d0bb8259d8fe3c7df4554dab9d7da3c9-Abstract.html",
        "author": "Francois Fleuret; Gilles Blanchard",
        "abstract": "We investigate the learning of the appearance of an object from a single image of it. Instead of using a large number of pictures of the object to recognize, we use a labeled reference database of pictures of other ob- jects to learn invariance to noise and variations in pose and illumination. This acquired knowledge is then used to predict if two pictures of new objects, which do not appear on the training pictures, actually display the same object. We propose a generic scheme called chopping to address this task. It relies on hundreds of random binary splits of the training set chosen to keep together the images of any given object. Those splits are extended to the complete image space with a simple learning algorithm. Given two images, the responses of the split predictors are combined with a Bayesian rule into a posterior probability of similarity. Experiments with the COIL-100 database and with a database of 150 de- graded LATEX symbols compare our method to a classical learning with several examples of the positive class and to a direct learning of the sim- ilarity.",
        "bibtex": "@inproceedings{NIPS2005_d0bb8259,\n author = {Fleuret, Francois and Blanchard, Gilles},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Pattern Recognition from One Example by Chopping},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d0bb8259d8fe3c7df4554dab9d7da3c9-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/d0bb8259d8fe3c7df4554dab9d7da3c9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/d0bb8259d8fe3c7df4554dab9d7da3c9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 71297,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17791294927875434422&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "VLAB/LCN \u2013 EPFL Lausanne, Switzerland; Fraunhofer FIRST Berlin, Germany",
        "aff_domain": "epfl.ch;first.fhg.de",
        "email": "epfl.ch;first.fhg.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "EPFL;Fraunhofer Institute for Software and Systems Engineering",
        "aff_unique_dep": "VLAB/LCN;",
        "aff_unique_url": "https://www.epfl.ch;https://www.first.fraunhofer.de/",
        "aff_unique_abbr": "EPFL;Fraunhofer FIRST",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Lausanne;Berlin",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Switzerland;Germany"
    },
    {
        "id": "62851ab99c",
        "title": "Phase Synchrony Rate for the Recognition of Motor Imagery in Brain-Computer Interface",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/2217ad1dd50c1017d3df6b44b7c45508-Abstract.html",
        "author": "Le Song; Evian Gordon; Elly Gysels",
        "abstract": "Motor imagery attenuates EEG  and  rhythms over sensorimotor cortices. These amplitude changes are most successfully captured by the method of Common Spatial Patterns (CSP) and widely used in braincomputer interfaces (BCI). BCI methods based on amplitude information, however, have not incoporated the rich phase dynamics in the EEG rhythm. This study reports on a BCI method based on phase synchrony rate (SR). SR, computed from binarized phase locking value, describes the number of discrete synchronization events within a window. Statistical nonparametric tests show that SRs contain significant differences between 2 types of motor imageries. Classifiers trained on SRs consistently demonstrate satisfactory results for all 5 subjects. It is further observed that, for 3 subjects, phase is more discriminative than amplitude in the first 1.5-2.0 s, which suggests that phase has the potential to boost the information transfer rate in BCIs.",
        "bibtex": "@inproceedings{NIPS2005_2217ad1d,\n author = {Song, Le and Gordon, Evian and Gysels, Elly},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Phase Synchrony Rate for the Recognition of Motor Imagery in Brain-Computer Interface},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2217ad1dd50c1017d3df6b44b7c45508-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/2217ad1dd50c1017d3df6b44b7c45508-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/2217ad1dd50c1017d3df6b44b7c45508-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 233766,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11804989129465435194&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "80004698ea",
        "title": "Policy-Gradient Methods for Planning",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/dba31bb5c75992690f20c2d3b370ec7c-Abstract.html",
        "author": "Douglas Aberdeen",
        "abstract": "Probabilistic temporal planning attempts to \ufb01nd good policies for acting in domains with concurrent durative tasks, multiple uncertain outcomes, and limited resources. These domains are typically modelled as Markov decision problems and solved using dynamic programming methods. This paper demonstrates the application of reinforcement learning \u2014 in the form of a policy-gradient method \u2014 to these domains. Our emphasis is large domains that are infeasible for dynamic programming. Our ap- proach is to construct simple policies, or agents, for each planning task. The result is a general probabilistic temporal planner, named the Factored Policy-Gradient Planner (FPG-Planner), which can handle hundreds of tasks, optimising for probability of success, duration, and resource use.",
        "bibtex": "@inproceedings{NIPS2005_dba31bb5,\n author = {Aberdeen, Douglas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Policy-Gradient Methods for Planning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/dba31bb5c75992690f20c2d3b370ec7c-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/dba31bb5c75992690f20c2d3b370ec7c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/dba31bb5c75992690f20c2d3b370ec7c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 138399,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15700679613003753779&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Statistical Machine Learning, National ICT Australia, Canberra",
        "aff_domain": "anu.edu.au",
        "email": "anu.edu.au",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "National ICT Australia",
        "aff_unique_dep": "Statistical Machine Learning",
        "aff_unique_url": "https://www.nicta.com.au",
        "aff_unique_abbr": "NICTA",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Canberra",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "ad56bc1484",
        "title": "Preconditioner Approximations for Probabilistic Graphical Models",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/e2f9247929b404b2fe98ba6f32301e3b-Abstract.html",
        "author": "John D. Lafferty; Pradeep K. Ravikumar",
        "abstract": "We present a family of approximation techniques for probabilistic graphical models, based on the use of graphical preconditioners developed in the scientific computing literature. Our framework yields rigorous upper and lower bounds on event probabilities and the log partition function of undirected graphical models, using non-iterative procedures that have low time complexity. As in mean field approaches, the approximations are built upon tractable subgraphs; however, we recast the problem of optimizing the tractable distribution parameters and approximate inference in terms of the well-studied linear systems problem of obtaining a good matrix preconditioner. Experiments are presented that compare the new approximation schemes to variational methods.",
        "bibtex": "@inproceedings{NIPS2005_e2f92479,\n author = {Lafferty, John and Ravikumar, Pradeep},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Preconditioner Approximations for Probabilistic Graphical Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e2f9247929b404b2fe98ba6f32301e3b-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/e2f9247929b404b2fe98ba6f32301e3b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/e2f9247929b404b2fe98ba6f32301e3b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 67989,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16228179273522240164&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "School of Computer Science; School of Computer Science",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "School of Computer Science",
        "aff_unique_dep": "Computer Science",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "6751423201",
        "title": "Predicting EMG Data from M1 Neurons with Variational Bayesian Least Squares",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/d71fa38b648d86602d14ac610f2e6194-Abstract.html",
        "author": "Jo-anne Ting; Aaron D'souza; Kenji Yamamoto; Toshinori Yoshioka; Donna Hoffman; Shinji Kakei; Lauren Sergio; John Kalaska; Mitsuo Kawato",
        "abstract": "An increasing number of projects in neuroscience requires the sta- tistical analysis of high dimensional data sets, as, for instance, in predicting behavior from neural \ufb01ring or in operating arti\ufb01cial de- vices from brain recordings in brain-machine interfaces. Linear analysis techniques remain prevalent in such cases, but classical linear regression approaches are often numerically too fragile in high dimensions. In this paper, we address the question of whether EMG data collected from arm movements of monkeys can be faith- fully reconstructed with linear approaches from neural activity in primary motor cortex (M1). To achieve robust data analysis, we develop a full Bayesian approach to linear regression that auto- matically detects and excludes irrelevant features in the data, reg- ularizing against over\ufb01tting. In comparison with ordinary least squares, stepwise regression, partial least squares, LASSO regres- sion and a brute force combinatorial search for the most predictive input features in the data, we demonstrate that the new Bayesian method o\ufb00ers a superior mixture of characteristics in terms of reg- ularization against over\ufb01tting, computational e\ufb03ciency and ease of use, demonstrating its potential as a drop-in replacement for other linear regression techniques. As neuroscienti\ufb01c results, our anal- yses demonstrate that EMG data can be well predicted from M1 neurons, further opening the path for possible real-time interfaces between brains and machines.",
        "bibtex": "@inproceedings{NIPS2005_d71fa38b,\n author = {Ting, Jo-anne and D\\textquotesingle souza, Aaron and Yamamoto, Kenji and Yoshioka, Toshinori and Hoffman, Donna and Kakei, Shinji and Sergio, Lauren and Kalaska, John and Kawato, Mitsuo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Predicting EMG Data from M1 Neurons with Variational Bayesian Least Squares},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d71fa38b648d86602d14ac610f2e6194-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/d71fa38b648d86602d14ac610f2e6194-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/d71fa38b648d86602d14ac610f2e6194-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 163703,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8656306337408864353&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": ";;;;;;;;",
        "aff_domain": ";;;;;;;;",
        "email": ";;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 9,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "994e459909",
        "title": "Prediction and Change Detection",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/c9efe5f26cd17ba6216bbe2a7d26d490-Abstract.html",
        "author": "Mark Steyvers; Scott Brown",
        "abstract": "We measure the ability of human observers to predict the next datum  in a sequence that is generated by a simple statistical process  undergoing change at random points in time. Accurate performance  in this task requires the identification of changepoints. We assess  individual differences between observers both empirically, and  using two kinds of models: a Bayesian approach for change detection  and a family of cognitively plausible fast and frugal models. Some  individuals detect  too many changes and hence perform  sub-optimally due to excess variability. Other individuals do not  detect enough changes, and perform sub-optimally because they fail  to notice short-term temporal trends.",
        "bibtex": "@inproceedings{NIPS2005_c9efe5f2,\n author = {Steyvers, Mark and Brown, Scott},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Prediction and Change Detection},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/c9efe5f26cd17ba6216bbe2a7d26d490-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/c9efe5f26cd17ba6216bbe2a7d26d490-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/c9efe5f26cd17ba6216bbe2a7d26d490-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 226834,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=754455012791394194&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "University of California, Irvine; University of California, Irvine",
        "aff_domain": "uci.edu;uci.edu",
        "email": "uci.edu;uci.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Irvine",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uci.edu",
        "aff_unique_abbr": "UCI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Irvine",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "47c28647d0",
        "title": "Principles of real-time computing with feedback applied to cortical microcircuit models",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/ef0d17b3bdb4ee2aa741ba28c7255c53-Abstract.html",
        "author": "Wolfgang Maass; Prashant Joshi; Eduardo D. Sontag",
        "abstract": "The network topology of neurons in the brain exhibits an abundance of feedback connections, but the computational function of these feedback connections is largely unknown. We present a computational theory that characterizes the gain in computational power achieved through feedback in dynamical systems with fading memory. It implies that many such systems acquire through feedback universal computational capabilities for analog computing with a non-fading memory. In particular, we show that feedback enables such systems to process time-varying input streams in diverse ways according to rules that are implemented through internal states of the dynamical system. In contrast to previous attractor-based computational models for neural networks, these \ufb02exible internal states are high-dimensional attractors of the circuit dynamics, that still allow the circuit state to absorb new information from online input streams. In this way one arrives at novel models for working memory, integration of evidence, and reward expectation in cortical circuits. We show that they are applicable to circuits of conductance-based Hodgkin-Huxley (HH) neurons with high levels of noise that re\ufb02ect experimental data on in- vivo conditions.",
        "bibtex": "@inproceedings{NIPS2005_ef0d17b3,\n author = {Maass, Wolfgang and Joshi, Prashant and Sontag, Eduardo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Principles of real-time computing with feedback applied to cortical microcircuit models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ef0d17b3bdb4ee2aa741ba28c7255c53-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/ef0d17b3bdb4ee2aa741ba28c7255c53-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/ef0d17b3bdb4ee2aa741ba28c7255c53-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 503945,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11182720399100731288&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Institute for Theoretical Computer Science, Technische Universitaet Graz, A-8010 Graz, Austria; Institute for Theoretical Computer Science, Technische Universitaet Graz, A-8010 Graz, Austria; Department of Mathematics, Rutgers, The State University of New Jersey, Piscataway, NJ 08854-8019, USA",
        "aff_domain": "igi.tugraz.at;igi.tugraz.at;cs.rutgers.edu",
        "email": "igi.tugraz.at;igi.tugraz.at;cs.rutgers.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Technische Universitaet Graz;Rutgers University",
        "aff_unique_dep": "Institute for Theoretical Computer Science;Department of Mathematics",
        "aff_unique_url": "https://www.tugraz.at;https://www.rutgers.edu",
        "aff_unique_abbr": "TU Graz;Rutgers",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Graz;Piscataway",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Austria;United States"
    },
    {
        "id": "a515914a12",
        "title": "Products of ``Edge-perts",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/74378afe5e8b20910cf1f939e57f0480-Abstract.html",
        "author": "Max Welling; Peter V. Gehler",
        "abstract": "Images represent an important and abundant source of data. Understanding their statistical structure has important applications such as image compression and restoration. In this paper we propose a particular kind of probabilistic model, dubbed the \"products of edge-perts model\" to describe the structure of wavelet transformed images. We develop a practical denoising algorithm based on a single edge-pert and show state-ofthe-art denoising performance on benchmark images.",
        "bibtex": "@inproceedings{NIPS2005_74378afe,\n author = {Welling, Max and Gehler, Peter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Products of \\textasciigrave \\textasciigrave Edge-perts},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/74378afe5e8b20910cf1f939e57f0480-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/74378afe5e8b20910cf1f939e57f0480-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/74378afe5e8b20910cf1f939e57f0480-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 221189,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18002924807484153792&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Max Planck Institute for Biological Cybernetics; Department of Computer Science, University of California Irvine",
        "aff_domain": "tuebingen.mpg.de;ics.uci.edu",
        "email": "tuebingen.mpg.de;ics.uci.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Max Planck Institute for Biological Cybernetics;University of California, Irvine",
        "aff_unique_dep": "Biological Cybernetics;Department of Computer Science",
        "aff_unique_url": "https://www.biocybernetics.mpg.de;https://www.uci.edu",
        "aff_unique_abbr": "MPIBC;UCI",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Irvine",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Germany;United States"
    },
    {
        "id": "be45d47259",
        "title": "Q-Clustering",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/b0bef4c9a6e50d43880191492d4fc827-Abstract.html",
        "author": "Mukund Narasimhan; Nebojsa Jojic; Jeff A. Bilmes",
        "abstract": "We show that Queyranne's algorithm for minimizing symmetric submodular functions can be used for clustering with a variety of different objective functions. Two specific criteria that we consider in this paper are the single linkage and the minimum description length criteria. The first criterion tries to maximize the minimum distance between elements of different clusters, and is inherently \"discriminative\". It is known that optimal clusterings into k clusters for any given k in polynomial time for this criterion can be computed. The second criterion seeks to minimize the description length of the clusters given a probabilistic generative model. We show that the optimal partitioning into 2 clusters, and approximate partitioning (guaranteed to be within a factor of 2 of the the optimal) for more clusters can be computed. To the best of our knowledge, this is the first time that a tractable algorithm for finding the optimal clustering with respect to the MDL criterion for 2 clusters has been given. Besides the optimality result for the MDL criterion, the chief contribution of this paper is to show that the same algorithm can be used to optimize a broad class of criteria, and hence can be used for many application specific criterion for which efficient algorithm are not known.",
        "bibtex": "@inproceedings{NIPS2005_b0bef4c9,\n author = {Narasimhan, Mukund and Jojic, Nebojsa and Bilmes, Jeff A},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Q-Clustering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b0bef4c9a6e50d43880191492d4fc827-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/b0bef4c9a6e50d43880191492d4fc827-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/b0bef4c9a6e50d43880191492d4fc827-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 81482,
        "gs_citation": 107,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1816343084988101452&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Dept of Electrical Engineering, University of Washington, Seattle WA; Microsoft Research, Microsoft Corporation, Redmond WA; Dept of Electrical Engineering, University of Washington, Seattle WA",
        "aff_domain": "ee.washington.edu;microsoft.com;ee.washington.edu",
        "email": "ee.washington.edu;microsoft.com;ee.washington.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Washington;Microsoft",
        "aff_unique_dep": "Dept of Electrical Engineering;Microsoft Research",
        "aff_unique_url": "https://www.washington.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "UW;Microsoft",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Seattle;Redmond",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3f4c03caaf",
        "title": "Query by Committee Made Real",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/340a39045c40d50dda207bcfdece883a-Abstract.html",
        "author": "Ran Gilad-bachrach; Amir Navot; Naftali Tishby",
        "abstract": "Training a learning algorithm is a costly task. A major goal of active learning is to reduce this cost. In this paper we introduce a new algorithm, KQBC, which is capable of actively learning large scale problems by using selective sampling. The algorithm overcomes the costly sampling step of the well known Query By Committee (QBC) algorithm by projecting onto a low dimensional space. KQBC also enables the use of kernels, providing a simple way of extending QBC to the non-linear scenario. Sampling the low dimension space is done using the hit and run random walk. We demonstrate the success of this novel algorithm by applying it to both artificial and a real world problems.",
        "bibtex": "@inproceedings{NIPS2005_340a3904,\n author = {Gilad-bachrach, Ran and Navot, Amir and Tishby, Naftali},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Query by Committee Made Real},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/340a39045c40d50dda207bcfdece883a-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/340a39045c40d50dda207bcfdece883a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/340a39045c40d50dda207bcfdece883a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 105025,
        "gs_citation": 156,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4089857844677964712&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "f652dfa1c0",
        "title": "Radial Basis Function Network for Multi-task Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/c7558e9d1f956b016d1fdba7ea132378-Abstract.html",
        "author": "Xuejun Liao; Lawrence Carin",
        "abstract": "We extend radial basis function (RBF) networks to the scenario in which multiple correlated tasks are learned simultaneously, and present the cor- responding learning algorithms. We develop the algorithms for learn- ing the network structure, in either a supervised or unsupervised manner. Training data may also be actively selected to improve the network\u2019s gen- eralization to test data. Experimental results based on real data demon- strate the advantage of the proposed algorithms and support our conclu- sions.",
        "bibtex": "@inproceedings{NIPS2005_c7558e9d,\n author = {Liao, Xuejun and Carin, Lawrence},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Radial Basis Function Network for Multi-task Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/c7558e9d1f956b016d1fdba7ea132378-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/c7558e9d1f956b016d1fdba7ea132378-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/c7558e9d1f956b016d1fdba7ea132378-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 107129,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7334182135334655363&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of ECE, Duke University; Department of ECE, Duke University",
        "aff_domain": "ee.duke.edu;ee.duke.edu",
        "email": "ee.duke.edu;ee.duke.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Duke University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.duke.edu",
        "aff_unique_abbr": "Duke",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "234d1fda54",
        "title": "Rate Distortion Codes in Sensor Networks: A System-level Analysis",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/a3788c8c64fd65c470e23e7534c3ebc8-Abstract.html",
        "author": "Tatsuto Murayama; Peter Davis",
        "abstract": "This paper provides a system-level analysis of a scalable distributed sens- ing model for networked sensors. In our system model, a data center ac- quires data from a bunch of L sensors which each independently encode their noisy observations of an original binary sequence, and transmit their encoded data sequences to the data center at a combined rate R, which is limited. Supposing that the sensors use independent LDGM rate dis- tortion codes, we show that the system performance can be evaluated for any given \ufb01nite R when the number of sensors L goes to in\ufb01nity. The analysis shows how the optimal strategy for the distributed sensing prob- lem changes at critical values of the data rate R or the noise level.",
        "bibtex": "@inproceedings{NIPS2005_a3788c8c,\n author = {Murayama, Tatsuto and Davis, Peter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Rate Distortion Codes in Sensor Networks: A System-level Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a3788c8c64fd65c470e23e7534c3ebc8-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/a3788c8c64fd65c470e23e7534c3ebc8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/a3788c8c64fd65c470e23e7534c3ebc8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 114184,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=482552552022304322&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation, \u201cKeihanna Science City\u201d, Kyoto 619-0237, Japan; NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation, \u201cKeihanna Science City\u201d, Kyoto 619-0237, Japan",
        "aff_domain": "cslab.kecl.ntt.co.jp;cslab.kecl.ntt.co.jp",
        "email": "cslab.kecl.ntt.co.jp;cslab.kecl.ntt.co.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Nippon Telegraph and Telephone Corporation",
        "aff_unique_dep": "Communication Science Laboratories",
        "aff_unique_url": "https://www.ntt.co.jp",
        "aff_unique_abbr": "NTT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Keihanna Science City",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "78412ada4a",
        "title": "Recovery of Jointly Sparse Signals from Few Random Projections",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/044a23cadb567653eb51d4eb40acaa88-Abstract.html",
        "author": "Michael B. Wakin; Marco F. Duarte; Shriram Sarvotham; Dror Baron; Richard G. Baraniuk",
        "abstract": "Compressed sensing is an emerging \ufb01eld based on the revelation that a small group of linear projections of a sparse signal contains enough information for reconstruc- tion. In this paper we introduce a new theory for distributed compressed sensing (DCS) that enables new distributed coding algorithms for multi-signal ensembles that exploit both intra- and inter-signal correlation structures. The DCS theory rests on a new concept that we term the joint sparsity of a signal ensemble. We study three simple models for jointly sparse signals, propose algorithms for joint recov- ery of multiple signals from incoherent projections, and characterize theoretically and empirically the number of measurements per sensor required for accurate re- construction. In some sense DCS is a framework for distributed compression of sources with memory, which has remained a challenging problem in information theory for some time. DCS is immediately applicable to a range of problems in sensor networks and arrays.",
        "bibtex": "@inproceedings{NIPS2005_044a23ca,\n author = {Wakin, Michael B. and Duarte, Marco and Sarvotham, Shriram and Baron, Dror and Baraniuk, Richard G.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Recovery of Jointly Sparse Signals from Few Random Projections},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/044a23cadb567653eb51d4eb40acaa88-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/044a23cadb567653eb51d4eb40acaa88-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/044a23cadb567653eb51d4eb40acaa88-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 76290,
        "gs_citation": 97,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14407193822294200922&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "ECE Department, Rice University; ECE Department, Rice University; ECE Department, Rice University; ECE Department, Rice University; ECE Department, Rice University",
        "aff_domain": "rice.edu;rice.edu;rice.edu;rice.edu;rice.edu",
        "email": "rice.edu;rice.edu;rice.edu;rice.edu;rice.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Rice University",
        "aff_unique_dep": "ECE Department",
        "aff_unique_url": "https://www.rice.edu",
        "aff_unique_abbr": "Rice",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b539dc5926",
        "title": "Representing Part-Whole Relationships in Recurrent Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/370bfb31abd222b582245b977ea5f25a-Abstract.html",
        "author": "Viren Jain; Valentin Zhigulin; H. S. Seung",
        "abstract": "There is little consensus about the computational function of top-down synaptic connections in the visual system. Here we explore the hypothesis that top-down connections, like bottom-up connections, reflect partwhole relationships. We analyze a recurrent network with bidirectional synaptic interactions between a layer of neurons representing parts and a layer of neurons representing wholes. Within each layer, there is lateral inhibition. When the network detects a whole, it can rigorously enforce part-whole relationships by ignoring parts that do not belong. The network can complete the whole by filling in missing parts. The network can refuse to recognize a whole, if the activated parts do not conform to a stored part-whole relationship. Parameter regimes in which these behaviors happen are identified using the theory of permitted and forbidden sets [3, 4]. The network behaviors are illustrated by recreating Rumelhart and McClelland's \"interactive activation\" model [7]. In neural network models of visual object recognition [2, 6, 8], patterns of synaptic connectivity often reflect part-whole relationships between the features that are represented by neurons. For example, the connections of Figure 1 reflect the fact that feature B both contains simpler features A1, A2, and A3, and is contained in more complex features C1, C2, and C3. Such connectivity allows neurons to follow the rule that existence of the part is evidence for existence of the whole. By combining synaptic input from multiple sources of evidence for a feature, a neuron can \"decide\" whether that feature is present. 1 The synapses shown in Figure 1 are purely bottom-up, directed from simple to complex features. However, there are also top-down connections in the visual system, and there is little consensus about their function. One possibility is that top-down connections also reflect part-whole relationships. They allow feature detectors to make decisions using the rule that existence of the whole is evidence for existence of its parts. In this paper, we analyze the dynamics of a recurrent network in which part-whole relationships are stored as bidirectional synaptic interactions, rather than the unidirectional interactions of Figure 1. The network has a number of interesting computational capabilities. When the network detects a whole, it can rigorously enforce part-whole relationships Synaptic connectivity may reflect other relationships besides part-whole. For example, invariances can be implemented by connecting detectors of several instances of the same feature to the same target, which is consequently an invariant detector of the feature. 1",
        "bibtex": "@inproceedings{NIPS2005_370bfb31,\n author = {Jain, Viren and Zhigulin, Valentin and Seung, H.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Representing Part-Whole Relationships in Recurrent Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/370bfb31abd222b582245b977ea5f25a-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/370bfb31abd222b582245b977ea5f25a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/370bfb31abd222b582245b977ea5f25a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 694179,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10682177448838477048&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Howard Hughes Medical Institute + Brain & Cog. Sci. Dept., MIT; Howard Hughes Medical Institute + Brain & Cog. Sci. Dept., MIT; Howard Hughes Medical Institute + Brain & Cog. Sci. Dept., MIT",
        "aff_domain": "mit.edu;mit.edu;mit.edu",
        "email": "mit.edu;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "Howard Hughes Medical Institute;Massachusetts Institute of Technology",
        "aff_unique_dep": ";Department of Brain and Cognitive Sciences",
        "aff_unique_url": "https://www.hhmi.org;https://web.mit.edu/",
        "aff_unique_abbr": "HHMI;MIT",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f95d6bf5e8",
        "title": "Response Analysis of Neuronal Population with Synaptic Depression",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/4b29fa4efe4fb7bc667c7b301b74d52d-Abstract.html",
        "author": "Wentao Huang; Licheng Jiao; Shan Tan; Maoguo Gong",
        "abstract": "In this paper, we aim at analyzing the characteristic of neuronal population responses to instantaneous or time-dependent inputs and the role of synapses in neural information processing. We have derived an evolution equation of the membrane potential density function with synaptic depression, and obtain the formulas for analytic computing the response of instantaneous re rate. Through a technical analysis, we arrive at several signi cant conclusions: The background inputs play an important role in information processing and act as a switch betwee temporal integration and coincidence detection. the role of synapses can be regarded as a spatio-temporal lter; it is important in neural information processing for the spatial distribution of synapses and the spatial and temporal relation of inputs. The instantaneous input frequency can affect the response amplitude and phase delay.",
        "bibtex": "@inproceedings{NIPS2005_4b29fa4e,\n author = {Huang, Wentao and Jiao, Licheng and Tan, Shan and Gong, Maoguo},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Response Analysis of Neuronal Population with Synaptic Depression},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4b29fa4efe4fb7bc667c7b301b74d52d-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/4b29fa4efe4fb7bc667c7b301b74d52d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/4b29fa4efe4fb7bc667c7b301b74d52d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 197541,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2395351810345568088&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Institute of Intelligent Information Processing, Xidian University, Xi'an 710071, China; Institute of Intelligent Information Processing, Xidian University, Xi'an 710071, China; Institute of Intelligent Information Processing, Xidian University, Xi'an 710071, China; Institute of Intelligent Information Processing, Xidian University, Xi'an 710071, China",
        "aff_domain": "mail.xidian.edu.cn;mail.xidian.edu.cn;mail.xidian.edu.cn;mail.xidian.edu.cn",
        "email": "mail.xidian.edu.cn;mail.xidian.edu.cn;mail.xidian.edu.cn;mail.xidian.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Xidian University",
        "aff_unique_dep": "Institute of Intelligent Information Processing",
        "aff_unique_url": "http://www.xidian.edu.cn/",
        "aff_unique_abbr": "Xidian",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Xi'an",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2689a8d180",
        "title": "Robust Fisher Discriminant Analysis",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/1264a061d82a2edae1574b07249800d6-Abstract.html",
        "author": "Seung-jean Kim; Alessandro Magnani; Stephen Boyd",
        "abstract": "Fisher linear discriminant analysis (LDA) can be sensitive to the problem data. Robust Fisher LDA can systematically alleviate the sensitivity problem by explicitly incorporating a model of data uncertainty in a classification problem and optimizing for the worst-case scenario under this model. The main contribution of this paper is show that with general convex uncertainty models on the problem data, robust Fisher LDA can be carried out using convex optimization. For a certain type of product form uncertainty model, robust Fisher LDA can be carried out at a cost comparable to standard Fisher LDA. The method is demonstrated with some numerical examples. Finally, we show how to extend these results to robust kernel Fisher discriminant analysis, i.e., robust Fisher LDA in a high dimensional feature space.",
        "bibtex": "@inproceedings{NIPS2005_1264a061,\n author = {Kim, Seung-jean and Magnani, Alessandro and Boyd, Stephen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Robust Fisher Discriminant Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/1264a061d82a2edae1574b07249800d6-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/1264a061d82a2edae1574b07249800d6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/1264a061d82a2edae1574b07249800d6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 75454,
        "gs_citation": 164,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1848667694313218556&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Information Systems Laboratory; Electrical Engineering Department, Stanford University; Electrical Engineering Department, Stanford University",
        "aff_domain": "stanford.edu;stanford.edu;stanford.edu",
        "email": "stanford.edu;stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Information Systems Laboratory;Stanford University",
        "aff_unique_dep": ";Electrical Engineering Department",
        "aff_unique_url": ";https://www.stanford.edu",
        "aff_unique_abbr": ";Stanford",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "d0855be1a0",
        "title": "Robust design of biological experiments",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/3953630da28e5181cffca1278517e3cf-Abstract.html",
        "author": "Patrick Flaherty; Adam Arkin; Michael I. Jordan",
        "abstract": "We address the problem of robust, computationally-efficient design of biological experiments. Classical optimal experiment design methods have not been widely adopted in biological practice, in part because the resulting designs can be very brittle if the nominal parameter estimates for the model are poor, and in part because of computational constraints. We present a method for robust experiment design based on a semidefinite programming relaxation. We present an application of this method to the design of experiments for a complex calcium signal transduction pathway, where we have found that the parameter estimates obtained from the robust design are better than those obtained from an \"optimal\" design.",
        "bibtex": "@inproceedings{NIPS2005_3953630d,\n author = {Flaherty, Patrick and Arkin, Adam and Jordan, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Robust design of biological experiments},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/3953630da28e5181cffca1278517e3cf-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/3953630da28e5181cffca1278517e3cf-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/3953630da28e5181cffca1278517e3cf-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 89046,
        "gs_citation": 102,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7741688608859902306&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "EECS Department, University of California, Berkeley; Computer Science and Statistics, University of California, Berkeley; Bioengineering Department, LBL, Howard Hughes Medical Institute, University of California, Berkeley",
        "aff_domain": "berkeley.edu;cs.berkeley.edu;lbl.gov",
        "email": "berkeley.edu;cs.berkeley.edu;lbl.gov",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "EECS Department",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8291223263",
        "title": "Rodeo: Sparse Nonparametric Regression in High Dimensions",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/d0010a6f34908640a4a6da2389772a78-Abstract.html",
        "author": "Larry Wasserman; John D. Lafferty",
        "abstract": "We present a method for nonparametric regression that performs bandwidth selection and variable selection simultaneously. The approach is based on the technique of incrementally decreasing the bandwidth in directions where the gradient of the estimator with respect to bandwidth is large. When the unknown function satisfies a sparsity condition, our approach avoids the curse of dimensionality, achieving the optimal minimax rate of convergence, up to logarithmic factors, as if the relevant variables were known in advance. The method--called rodeo (regularization of derivative expectation operator)--conducts a sequence of hypothesis tests, and is easy to implement. A modified version that replaces hard with soft thresholding effectively solves a sequence of lasso problems.",
        "bibtex": "@inproceedings{NIPS2005_d0010a6f,\n author = {Wasserman, Larry and Lafferty, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Rodeo: Sparse Nonparametric Regression in High Dimensions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d0010a6f34908640a4a6da2389772a78-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/d0010a6f34908640a4a6da2389772a78-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/d0010a6f34908640a4a6da2389772a78-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 92165,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13739271277903918840&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "School of Computer Science, Carnegie Mellon University; Department of Statistics, Carnegie Mellon University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Pittsburgh;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a336434a7c",
        "title": "Saliency Based on Information Maximization",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/0738069b244a1c43c83112b735140a16-Abstract.html",
        "author": "Neil Bruce; John Tsotsos",
        "abstract": "A model of bottom-up overt attention is proposed based on the principle  of maximizing information sampled from  a scene.  The proposed opera(cid:173) tion  is  based on  Shannon's self-information  measure and is  achieved in  a neural circuit, which is demonstrated as having close ties with  the cir(cid:173) cuitry existent in  the primate visual cortex.  It is  further shown  that the  proposed saliency measure may  be extended  to  address  issues that cur(cid:173) rently elude explanation in the domain of saliency based models.  Results  on  natural  images are compared with experimental eye tracking data re(cid:173) vealing  the  efficacy of the  model  in  predicting  the  deployment of overt  attention as compared with existing efforts.",
        "bibtex": "@inproceedings{NIPS2005_0738069b,\n author = {Bruce, Neil and Tsotsos, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Saliency Based on Information Maximization},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/0738069b244a1c43c83112b735140a16-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/0738069b244a1c43c83112b735140a16-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/0738069b244a1c43c83112b735140a16-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1534981,
        "gs_citation": 1635,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11401787876863620599&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science and Centre for Vision Research, York University, Toronto, ON, M2N 5X8; Department of Computer Science and Centre for Vision Research, York University, Toronto, ON, M2N 5X8",
        "aff_domain": "cs.yorku.ca;cs.yorku.ca",
        "email": "cs.yorku.ca;cs.yorku.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "York University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.yorku.ca",
        "aff_unique_abbr": "York U",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "16943e69be",
        "title": "Scaling Laws in Natural Scenes and the Inference of 3D Shape",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/2b64c2f19d868305aa8bbc2d72902cc5-Abstract.html",
        "author": "Tai-sing Lee; Brian R. Potetz",
        "abstract": "This paper explores the statistical relationship between natural images and their underlying range (depth) images. We look at how this relationship changes over scale, and how this information can be used to enhance low resolution range data using a full resolution intensity image. Based on our findings, we propose an extension to an existing technique known as shape recipes [3], and the success of the two methods are compared using images and laser scans of real scenes. Our extension is shown to provide a two-fold improvement over the current method. Furthermore, we demonstrate that ideal linear shape-from-shading filters, when learned from natural scenes, may derive even more strength from shadow cues than from the traditional linear-Lambertian shading cues.",
        "bibtex": "@inproceedings{NIPS2005_2b64c2f1,\n author = {Lee, Tai-sing and Potetz, Brian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Scaling Laws in Natural Scenes and the Inference of 3D Shape},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2b64c2f19d868305aa8bbc2d72902cc5-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/2b64c2f19d868305aa8bbc2d72902cc5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/2b64c2f19d868305aa8bbc2d72902cc5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 115981,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6981609639512528725&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computer Science, Center for the Neural Basis of Cognition, Carnegie Mellon University; Department of Computer Science, Center for the Neural Basis of Cognition, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cnbc.cmu.edu",
        "email": "cs.cmu.edu;cnbc.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6979485b4b",
        "title": "Searching for Character Models",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/6271faadeedd7626d661856b7a004e27-Abstract.html",
        "author": "Jaety Edwards; David Forsyth",
        "abstract": "We introduce a method to automatically improve character models for a handwritten script without the use of transcriptions and using a minimum of document speci\ufb01c training data. We show that we can use searches for the words in a dictionary to identify portions of the document whose transcriptions are unambiguous. Using templates extracted from those regions, we retrain our character prediction model to drastically improve our search retrieval performance for words in the document.",
        "bibtex": "@inproceedings{NIPS2005_6271faad,\n author = {Edwards, Jaety and Forsyth, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Searching for Character Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6271faadeedd7626d661856b7a004e27-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/6271faadeedd7626d661856b7a004e27-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/6271faadeedd7626d661856b7a004e27-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 116858,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4135488409554099208&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, UCBerkeley; Department of Computer Science, UCBerkeley",
        "aff_domain": "cs.berkeley.edu;cs.berkeley.edu",
        "email": "cs.berkeley.edu;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "eb2f1dc6b9",
        "title": "Selecting Landmark Points for Sparse Manifold Learning",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/780965ae22ea6aee11935f3fb73da841-Abstract.html",
        "author": "Jorge Silva; Jorge Marques; Jo\u00e3o Lemos",
        "abstract": "There has been a surge of interest in learning non-linear manifold models to approximate high-dimensional data. Both for computational complexity reasons and for generalization capability, sparsity is a desired feature in such models. This usually means dimensionality reduction, which naturally implies estimating the intrinsic dimension, but it can also mean selecting a subset of the data to use as landmarks, which is especially important because many existing algorithms have quadratic complexity in the number of observations. This paper presents an algorithm for selecting landmarks, based on LASSO regression, which is well known to favor sparse approximations because it uses regularization with an l1 norm. As an added benefit, a continuous manifold parameterization, based on the landmarks, is also found. Experimental results with synthetic and real data illustrate the algorithm.",
        "bibtex": "@inproceedings{NIPS2005_780965ae,\n author = {Silva, Jorge and Marques, Jorge and Lemos, Jo\\~{a}o},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Selecting Landmark Points for Sparse Manifold Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/780965ae22ea6aee11935f3fb73da841-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/780965ae22ea6aee11935f3fb73da841-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/780965ae22ea6aee11935f3fb73da841-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 154988,
        "gs_citation": 83,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15343150026041301878&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "ISEL/ISR; IST/ISR; INESC-ID/IST",
        "aff_domain": "isel.ipl.pt;isr.ist.utl.pt;inesc-id.pt",
        "email": "isel.ipl.pt;isr.ist.utl.pt;inesc-id.pt",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Instituto Superior de Engenharia de Lisboa / Instituto de Sistemas e Robotica;Instituto Superior T\u00e9cnico",
        "aff_unique_dep": ";Instituto de Sistemas e Robotica",
        "aff_unique_url": "https://www.isel.pt;https://www.ist.utl.pt",
        "aff_unique_abbr": "ISEL/ISR;IST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Portugal"
    },
    {
        "id": "5c3e62719d",
        "title": "Sensory Adaptation within a Bayesian Framework for Perception",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/c68c9c8258ea7d85472dd6fd0015f047-Abstract.html",
        "author": "Alan Stocker; Eero P. Simoncelli",
        "abstract": "We extend a previously developed Bayesian framework for perception to account for sensory adaptation. We \ufb01rst note that the perceptual ef- fects of adaptation seems inconsistent with an adjustment of the inter- nally represented prior distribution. Instead, we postulate that adaptation increases the signal-to-noise ratio of the measurements by adapting the operational range of the measurement stage to the input range. We show that this changes the likelihood function in such a way that the Bayesian estimator model can account for reported perceptual behavior. In particu- lar, we compare the model\u2019s predictions to human motion discrimination data and demonstrate that the model accounts for the commonly observed perceptual adaptation effects of repulsion and enhanced discriminability.",
        "bibtex": "@inproceedings{NIPS2005_c68c9c82,\n author = {Stocker, Alan A and Simoncelli, Eero},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Sensory Adaptation within a Bayesian Framework for Perception},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/c68c9c8258ea7d85472dd6fd0015f047-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/c68c9c8258ea7d85472dd6fd0015f047-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/c68c9c8258ea7d85472dd6fd0015f047-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 210579,
        "gs_citation": 215,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11025820016678625254&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "fcb07bce83",
        "title": "Separation of Music Signals by Harmonic Structure Modeling",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/d3aeec875c479e55d1cdeea161842ec6-Abstract.html",
        "author": "Yun-gang Zhang; Chang-shui Zhang",
        "abstract": "Separation of music signals is an interesting but difficult problem. It is helpful for many other music researches such as audio content analysis. In this paper, a new music signal separation method is proposed, which is based on harmonic structure modeling. The main idea of harmonic structure modeling is that the harmonic structure of a music signal is stable, so a music signal can be represented by a harmonic structure model. Accordingly, a corresponding separation algorithm is proposed. The main idea is to learn a harmonic structure model for each music signal in the mixture, and then separate signals by using these models to distinguish harmonic structures of different signals. Experimental results show that the algorithm can separate signals and obtain not only a very high Signalto-Noise Ratio (SNR) but also a rather good subjective audio quality.",
        "bibtex": "@inproceedings{NIPS2005_d3aeec87,\n author = {Zhang, Yun-gang and Zhang, Chang-shui},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Separation of Music Signals by Harmonic Structure Modeling},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d3aeec875c479e55d1cdeea161842ec6-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/d3aeec875c479e55d1cdeea161842ec6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/d3aeec875c479e55d1cdeea161842ec6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 637787,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=627292508318315388&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Automation, Tsinghua University; Department of Automation, Tsinghua University",
        "aff_domain": "mails.tsinghua.edu.cn;mail.tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn;mail.tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Tsinghua University",
        "aff_unique_dep": "Department of Automation",
        "aff_unique_url": "https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "THU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "486e45d1fc",
        "title": "Sequence and Tree Kernels with Statistical Feature Mining",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/4e2a6330465c8ffcaa696a5a16639176-Abstract.html",
        "author": "Jun Suzuki; Hideki Isozaki",
        "abstract": "This paper proposes a new approach to feature selection based on a sta- tistical feature mining technique for sequence and tree kernels. Since natural language data take discrete structures, convolution kernels, such as sequence and tree kernels, are advantageous for both the concept and accuracy of many natural language processing tasks. However, experi- ments have shown that the best results can only be achieved when lim- ited small sub-structures are dealt with by these kernels. This paper dis- cusses this issue of convolution kernels and then proposes a statistical feature selection that enable us to use larger sub-structures effectively. The proposed method, in order to execute ef\ufb01ciently, can be embedded into an original kernel calculation process by using sub-structure min- ing algorithms. Experiments on real NLP tasks con\ufb01rm the problem in the conventional method and compare the performance of a conventional method to that of the proposed method.",
        "bibtex": "@inproceedings{NIPS2005_4e2a6330,\n author = {Suzuki, Jun and Isozaki, Hideki},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Sequence and Tree Kernels with Statistical Feature Mining},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4e2a6330465c8ffcaa696a5a16639176-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/4e2a6330465c8ffcaa696a5a16639176-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/4e2a6330465c8ffcaa696a5a16639176-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 221387,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1791306209765865735&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "9ae3a940a9",
        "title": "Silicon growth cones map silicon retina",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/f565bb9efccaf6986443db0bf01018bc-Abstract.html",
        "author": "Brian Taba; Kwabena Boahen",
        "abstract": "We demonstrate the \ufb01rst fully hardware implementation of retinotopic self-organization, from photon transduction to neural map formation. A silicon retina transduces patterned illumination into correlated spike trains that drive a population of silicon growth cones to automatically wire a topographic mapping by migrating toward sources of a diffusible guidance cue that is released by postsynaptic spikes. We varied the pat- tern of illumination to steer growth cones projected by different retinal ganglion cell types to self-organize segregated or coordinated retinotopic maps.",
        "bibtex": "@inproceedings{NIPS2005_f565bb9e,\n author = {Taba, Brian and Boahen, Kwabena},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Silicon growth cones map silicon retina},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f565bb9efccaf6986443db0bf01018bc-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/f565bb9efccaf6986443db0bf01018bc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/f565bb9efccaf6986443db0bf01018bc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 292007,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18167495966477053800&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Bioengineering, University of Pennsylvania; Department of Bioengineering, University of Pennsylvania",
        "aff_domain": "seas.upenn.edu;seas.upenn.edu",
        "email": "seas.upenn.edu;seas.upenn.edu",
        "github": "",
        "project": "www.neuroengineering.upenn.edu/boahen",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "Department of Bioengineering",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b14b3c3615",
        "title": "Size Regularized Cut for Data Clustering",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/379a7ba015d8bf1c70b8add2c287c6fa-Abstract.html",
        "author": "Yixin Chen; Ya Zhang; Xiang Ji",
        "abstract": "We present a novel spectral clustering method that enables users to incorporate prior knowledge of the size of clusters into the clustering process. The cost function, which is named size regularized cut (SRcut), is defined as the sum of the inter-cluster similarity and a regularization term measuring the relative size of two clusters. Finding a partition of the data set to minimize SRcut is proved to be NP-complete. An approximation algorithm is proposed to solve a relaxed version of the optimization problem as an eigenvalue problem. Evaluations over different data sets demonstrate that the method is not sensitive to outliers and performs better than normalized cut.",
        "bibtex": "@inproceedings{NIPS2005_379a7ba0,\n author = {Chen, Yixin and Zhang, Ya and Ji, Xiang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Size Regularized Cut for Data Clustering},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/379a7ba015d8bf1c70b8add2c287c6fa-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/379a7ba015d8bf1c70b8add2c287c6fa-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/379a7ba015d8bf1c70b8add2c287c6fa-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 129137,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4123035993458904020&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 8,
        "aff": "Department of CS, Univ. of New Orleans; Department of EECS, Uinv. of Kansas; NEC-Labs America, Inc.",
        "aff_domain": "cs.uno.edu;ittc.ku.edu;sv.nec-labs.com",
        "email": "cs.uno.edu;ittc.ku.edu;sv.nec-labs.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of New Orleans;University of Kansas;NEC-Labs America, Inc.",
        "aff_unique_dep": "Department of Computer Science;Department of Electrical Engineering and Computer Science;",
        "aff_unique_url": "https://www.uno.edu;https://www.ku.edu;https://www.nec-labs.com",
        "aff_unique_abbr": "UNO;KU;NEC-Labs",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c659ad2a88",
        "title": "Soft Clustering on Graphs",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/15c00b5250ddedaabc203b67f8b034fd-Abstract.html",
        "author": "Kai Yu; Shipeng Yu; Volker Tresp",
        "abstract": "We propose a simple clustering framework on graphs encoding pairwise data similarities. Unlike usual similarity-based methods, the approach softly assigns data to clusters in a probabilistic way. More importantly, a hierarchical clustering is naturally derived in this framework to gradually merge lower-level clusters into higher-level ones. A random walk analysis indicates that the algorithm exposes clustering structures in various resolutions, i.e., a higher level statistically models a longer-term diffusion on graphs and thus discovers a more global clustering structure. Finally we provide very encouraging experimental results.",
        "bibtex": "@inproceedings{NIPS2005_15c00b52,\n author = {Yu, Kai and Yu, Shipeng and Tresp, Volker},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Soft Clustering on Graphs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/15c00b5250ddedaabc203b67f8b034fd-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/15c00b5250ddedaabc203b67f8b034fd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/15c00b5250ddedaabc203b67f8b034fd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 228044,
        "gs_citation": 151,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7252946563692032549&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Siemens AG, Corporate Technology; Institute for Computer Science, University of Munich; Siemens AG, Corporate Technology",
        "aff_domain": "siemens.com;dbs.informatik.uni-muenchen.de;siemens.com",
        "email": "siemens.com;dbs.informatik.uni-muenchen.de;siemens.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Siemens AG;University of Munich",
        "aff_unique_dep": "Corporate Technology;Institute for Computer Science",
        "aff_unique_url": "https://www.siemens.com;https://www.lmu.de",
        "aff_unique_abbr": "Siemens;LMU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Munich",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "b3124236c2",
        "title": "Sparse Gaussian Processes using Pseudo-inputs",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/4491777b1aa8b5b32c2e8666dbe1a495-Abstract.html",
        "author": "Edward Snelson; Zoubin Ghahramani",
        "abstract": "We present a new Gaussian process (GP) regression model whose covariance is parameterized by the the locations of M pseudo-input points, which we learn by a gradient based optimization. We take M N, where N is the number of real data points, and hence obtain a sparse regression method which has O(M 2 N ) training cost and O(M 2 ) prediction cost per test case. We also find hyperparameters of the covariance function in the same joint optimization. The method can be viewed as a Bayesian regression model with particular input dependent noise. The method turns out to be closely related to several other sparse GP approaches, and we discuss the relation in detail. We finally demonstrate its performance on some large data sets, and make a direct comparison to other sparse GP methods. We show that our method can match full GP performance with small M , i.e. very sparse solutions, and it significantly outperforms other approaches in this regime.",
        "bibtex": "@inproceedings{NIPS2005_4491777b,\n author = {Snelson, Edward and Ghahramani, Zoubin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Sparse Gaussian Processes using Pseudo-inputs},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4491777b1aa8b5b32c2e8666dbe1a495-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/4491777b1aa8b5b32c2e8666dbe1a495-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/4491777b1aa8b5b32c2e8666dbe1a495-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 551614,
        "gs_citation": 2419,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17256260653985668610&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "742151384f",
        "title": "Spectral Bounds for Sparse PCA: Exact and Greedy Algorithms",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/d47844673f2db74d78da8687d794523d-Abstract.html",
        "author": "Baback Moghaddam; Yair Weiss; Shai Avidan",
        "abstract": "Sparse PCA seeks approximate sparse \"eigenvectors\" whose projections capture the maximal variance of data. As a cardinality-constrained and non-convex optimization problem, it is NP-hard and is encountered in a wide range of applied fields, from bio-informatics to finance. Recent progress has focused mainly on continuous approximation and convex relaxation of the hard cardinality constraint. In contrast, we consider an alternative discrete spectral formulation based on variational eigenvalue bounds and provide an effective greedy strategy as well as provably optimal solutions using branch-and-bound search. Moreover, the exact methodology used reveals a simple renormalization step that improves approximate solutions obtained by any continuous method. The resulting performance gain of discrete algorithms is demonstrated on real-world benchmark data and in extensive Monte Carlo evaluation trials.",
        "bibtex": "@inproceedings{NIPS2005_d4784467,\n author = {Moghaddam, Baback and Weiss, Yair and Avidan, Shai},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Spectral Bounds for Sparse PCA: Exact and Greedy Algorithms},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/d47844673f2db74d78da8687d794523d-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/d47844673f2db74d78da8687d794523d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/d47844673f2db74d78da8687d794523d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 120217,
        "gs_citation": 342,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2725474228519252087&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "c37e7e683f",
        "title": "Spiking Inputs to a Winner-take-all Network",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/881c6efa917cff1c97a74e03e15f43e8-Abstract.html",
        "author": "Matthias Oster; Shih-Chii Liu",
        "abstract": "Recurrent networks that perform a winner-take-all computation have been studied extensively. Although some of these studies include spik- ing networks, they consider only analog input rates. We present results of this winner-take-all computation on a network of integrate-and-\ufb01re neurons which receives spike trains as inputs. We show how we can con- \ufb01gure the connectivity in the network so that the winner is selected after a pre-determined number of input spikes. We discuss spiking inputs with both regular frequencies and Poisson-distributed rates. The robustness of the computation was tested by implementing the winner-take-all network on an analog VLSI array of 64 integrate-and-\ufb01re neurons which have an innate variance in their operating parameters.",
        "bibtex": "@inproceedings{NIPS2005_881c6efa,\n author = {Oster, Matthias and Liu, Shih-Chii},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Spiking Inputs to a Winner-take-all Network},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/881c6efa917cff1c97a74e03e15f43e8-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/881c6efa917cff1c97a74e03e15f43e8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/881c6efa917cff1c97a74e03e15f43e8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 370315,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11498577129898966496&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "f14c0e68ab",
        "title": "Statistical Convergence of Kernel CCA",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/7a006957be65e608e863301eb98e1808-Abstract.html",
        "author": "Kenji Fukumizu; Arthur Gretton; Francis R. Bach",
        "abstract": "While kernel canonical correlation analysis (kernel CCA) has been applied in many problems, the asymptotic convergence of the functions estimated from a finite sample to the true functions has not yet been established. This paper gives a rigorous proof of the statistical convergence of kernel CCA and a related method (NOCCO), which provides a theoretical justification for these methods. The result also gives a sufficient condition on the decay of the regularization coefficient in the methods to ensure convergence.",
        "bibtex": "@inproceedings{NIPS2005_7a006957,\n author = {Fukumizu, Kenji and Gretton, Arthur and Bach, Francis},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Statistical Convergence of Kernel CCA},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/7a006957be65e608e863301eb98e1808-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/7a006957be65e608e863301eb98e1808-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/7a006957be65e608e863301eb98e1808-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 119346,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13108725436272458908&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Institute of Statistical Mathematics, Tokyo 106-8569 Japan; Centre de Morphologie Mathematique, Ecole des Mines de Paris, France; Max Planck Institute for Biological Cybernetics, 72076 T\u00fcbingen, Germany",
        "aff_domain": "ism.ac.jp;mines.org;tuebingen.mpg.de",
        "email": "ism.ac.jp;mines.org;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Institute of Statistical Mathematics;Ecole des Mines de Paris;Max Planck Institute for Biological Cybernetics",
        "aff_unique_dep": ";Centre de Morphologie Mathematique;",
        "aff_unique_url": ";https://www.mines-paris.psl.eu;https://www.biocybernetics.mpg.de",
        "aff_unique_abbr": ";;MPIBC",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Tokyo;;T\u00fcbingen",
        "aff_country_unique_index": "0;1;2",
        "aff_country_unique": "Japan;France;Germany"
    },
    {
        "id": "4a538e8291",
        "title": "Stimulus Evoked Independent Factor Analysis of MEG Data with Large Background Activity",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/b4a721cfb62f5d19ec61575114d8a2d1-Abstract.html",
        "author": "Kenneth Hild; Kensuke Sekihara; Hagai T. Attias; Srikantan S. Nagarajan",
        "abstract": "This paper presents a novel technique for analyzing electromagnetic imaging data obtained using the stimulus evoked experimental paradigm. The technique is based on a probabilistic graphical model, which describes the data in terms of underlying evoked and interference sources, and explicitly models the stimulus evoked paradigm. A variational Bayesian EM algorithm infers the model from data, suppresses interference sources, and reconstructs the activity of separated individual brain sources. The new algorithm outperforms existing techniques on two real datasets, as well as on simulated data.",
        "bibtex": "@inproceedings{NIPS2005_b4a721cf,\n author = {Hild, Kenneth and Sekihara, Kensuke and Attias, Hagai and Nagarajan, Srikantan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Stimulus Evoked Independent Factor Analysis of MEG Data with Large Background Activity},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b4a721cfb62f5d19ec61575114d8a2d1-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/b4a721cfb62f5d19ec61575114d8a2d1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/b4a721cfb62f5d19ec61575114d8a2d1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 202935,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16915570161474435610&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Biomagnetic Imaging Laboratory, Department of Radiology, University of California, San Francisco; Golden Metallic, Inc.; Biomagnetic Imaging Laboratory, Department of Radiology, University of California, San Francisco; Dept. of Systems Design and Engineering, Tokyo Metropolitan University",
        "aff_domain": "radiology.ucsf.edu;goldenmetallic.com;mrsc.ucsf.edu;cc.tmit.ac.jp",
        "email": "radiology.ucsf.edu;goldenmetallic.com;mrsc.ucsf.edu;cc.tmit.ac.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "University of California, San Francisco;Meta;Tokyo Metropolitan University",
        "aff_unique_dep": "Department of Radiology;Golden Metallic, Inc.;Dept. of Systems Design and Engineering",
        "aff_unique_url": "https://www.ucsf.edu;;https://www.tmuedu.net",
        "aff_unique_abbr": "UCSF;;TMU",
        "aff_campus_unique_index": "0;0;2",
        "aff_campus_unique": "San Francisco;;Tokyo",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "United States;Japan"
    },
    {
        "id": "68240bab08",
        "title": "Structured Prediction via the Extragradient Method",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/e465ae46b07058f4ab5e96b98f101756-Abstract.html",
        "author": "Ben Taskar; Simon Lacoste-Julien; Michael I. Jordan",
        "abstract": "We present a simple and scalable algorithm for large-margin estima- tion of structured models, including an important class of Markov net- works and combinatorial models. We formulate the estimation problem as a convex-concave saddle-point problem and apply the extragradient method, yielding an algorithm with linear convergence using simple gra- dient and projection calculations. The projection step can be solved us- ing combinatorial algorithms for min-cost quadratic \ufb02ow. This makes the approach an ef\ufb01cient alternative to formulations based on reductions to a quadratic program (QP). We present experiments on two very different structured prediction tasks: 3D image segmentation and word alignment, illustrating the favorable scaling properties of our algorithm.",
        "bibtex": "@inproceedings{NIPS2005_e465ae46,\n author = {Taskar, Ben and Lacoste-Julien, Simon and Jordan, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Structured Prediction via the Extragradient Method},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e465ae46b07058f4ab5e96b98f101756-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/e465ae46b07058f4ab5e96b98f101756-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/e465ae46b07058f4ab5e96b98f101756-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 120161,
        "gs_citation": 87,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6294824040970091069&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Computer Science, UC Berkeley, Berkeley, CA 94720; Computer Science, UC Berkeley, Berkeley, CA 94720; Computer Science and Statistics, UC Berkeley, Berkeley, CA 94720",
        "aff_domain": "cs.berkeley.edu;cs.berkeley.edu;cs.berkeley.edu",
        "email": "cs.berkeley.edu;cs.berkeley.edu;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e30062902c",
        "title": "Subsequence Kernels for Relation Extraction",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/2e0bff759d057e28460eaa5b2cb118e5-Abstract.html",
        "author": "Raymond J. Mooney; Razvan C. Bunescu",
        "abstract": "We present a new kernel method for extracting semantic relations between entities in natural language text, based on a generalization of subsequence kernels. This kernel uses three types of subsequence patterns that are typically employed in natural language to assert relationships between two entities. Experiments on extracting protein interactions from biomedical corpora and top-level relations from newspaper corpora demonstrate the advantages of this approach.",
        "bibtex": "@inproceedings{NIPS2005_2e0bff75,\n author = {Mooney, Raymond and Bunescu, Razvan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Subsequence Kernels for Relation Extraction},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2e0bff759d057e28460eaa5b2cb118e5-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/2e0bff759d057e28460eaa5b2cb118e5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/2e0bff759d057e28460eaa5b2cb118e5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 79152,
        "gs_citation": 701,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13090592267991197593&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Sciences, University of Texas at Austin; Department of Computer Sciences, University of Texas at Austin",
        "aff_domain": "cs.utexas.edu;cs.utexas.edu",
        "email": "cs.utexas.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Department of Computer Sciences",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3cab9f3f48",
        "title": "TD(0) Leads to Better Policies than Approximate Value Iteration",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/f076073b2082f8741a9cd07b789c77a0-Abstract.html",
        "author": "Benjamin V. Roy",
        "abstract": "We consider approximate value iteration with a parameterized approximator in which the state space is partitioned and the optimal cost-to-go function over each partition is approximated by a constant. We establish performance loss bounds for policies derived from approximations associated with fixed points. These bounds identify benefits to having projection weights equal to the invariant distribution of the resulting policy. Such projection weighting leads to the same fixed points as TD(0). Our analysis also leads to the first performance loss bound for approximate value iteration with an average cost objective.",
        "bibtex": "@inproceedings{NIPS2005_f076073b,\n author = {Roy, Benjamin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {TD(0) Leads to Better Policies than Approximate Value Iteration},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/f076073b2082f8741a9cd07b789c77a0-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/f076073b2082f8741a9cd07b789c77a0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/f076073b2082f8741a9cd07b789c77a0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 163679,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5260117525091133080&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Management Science and Engineering and Electrical Engineering, Stanford University",
        "aff_domain": "stanford.edu",
        "email": "stanford.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Management Science and Engineering, Electrical Engineering",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "709d6358f2",
        "title": "Temporal Abstraction in Temporal-difference Networks",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/12311d05c9aa67765703984239511212-Abstract.html",
        "author": "Eddie Rafols; Anna Koop; Richard S. Sutton",
        "abstract": "Part of",
        "bibtex": "@inproceedings{NIPS2005_12311d05,\n author = {Rafols, Eddie and Koop, Anna and Sutton, Richard S},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Temporal Abstraction in Temporal-difference Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/12311d05c9aa67765703984239511212-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/12311d05c9aa67765703984239511212-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/12311d05c9aa67765703984239511212-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 164828,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "206e263792",
        "title": "Temporally changing synaptic plasticity",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/60243f9b1ac2dba11ff8131c8f4431e0-Abstract.html",
        "author": "Minija Tamosiunaite; Bernd Porr; Florentin W\u00f6rg\u00f6tter",
        "abstract": "Recent experimental results suggest that dendritic and back-propagating spikes can influence synaptic plasticity in different ways [1]. In this study we investigate how these signals could temporally interact at dendrites leading to changing plasticity properties at local synapse clusters. Similar to a previous study [2], we employ a differential Hebbian plasticity rule to emulate spike-timing dependent plasticity. We use dendritic (D-) and back-propagating (BP-) spikes as post-synaptic signals in the learning rule and investigate how their interaction will influence plasticity. We will analyze a situation where synapse plasticity characteristics change in the course of time, depending on the type of post-synaptic activity momentarily elicited. Starting with weak synapses, which only elicit local D-spikes, a slow, unspecific growth process is induced. As soon as the soma begins to spike this process is replaced by fast synaptic changes as the consequence of the much stronger and sharper BP-spike, which now dominates the plasticity rule. This way a winner-take-all-mechanism emerges in a two-stage process, enhancing the best-correlated inputs. These results suggest that synaptic plasticity is a temporal changing process by which the computational properties of dendrites or complete neurons can be substantially augmented.",
        "bibtex": "@inproceedings{NIPS2005_60243f9b,\n author = {Tamosiunaite, Minija and Porr, Bernd and W\\\"{o}rg\\\"{o}tter, Florentin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Temporally changing synaptic plasticity},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/60243f9b1ac2dba11ff8131c8f4431e0-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/60243f9b1ac2dba11ff8131c8f4431e0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/60243f9b1ac2dba11ff8131c8f4431e0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 357730,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12929467336740476574&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Psychology, University of Stirling + Department of Informatics, Vytautas Magnus University; Department of Electronics & Electrical Engineering, University of Glasgow; Department of Psychology, University of Stirling + Bernstein Centre for Computational Neuroscience, University of G \u00a8ottingen",
        "aff_domain": "cn.stir.ac.uk;cn.stir.ac.uk;elec.gla.ac.uk",
        "email": "cn.stir.ac.uk;cn.stir.ac.uk;elec.gla.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;0+3",
        "aff_unique_norm": "University of Stirling;Vytautas Magnus University;University of Glasgow;University of G\u00f6ttingen",
        "aff_unique_dep": "Department of Psychology;Department of Informatics;Department of Electronics & Electrical Engineering;Bernstein Centre for Computational Neuroscience",
        "aff_unique_url": "https://www.stirling.ac.uk;https://www.vdu.lt;https://www.gla.ac.uk;https://www.uni-goettingen.de",
        "aff_unique_abbr": "Stirling;VMU;UoG;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0;0+2",
        "aff_country_unique": "United Kingdom;Lithuania;Germany"
    },
    {
        "id": "00752b3cb3",
        "title": "Tensor Subspace Analysis",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/332647f433a1c10fa2e2ae04abfdf83e-Abstract.html",
        "author": "Xiaofei He; Deng Cai; Partha Niyogi",
        "abstract": "Previous work has demonstrated that the image variations of many ob- jects (human faces in particular) under variable lighting can be effec- tively modeled by low dimensional linear spaces. The typical linear sub- space learning algorithms include Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), and Locality Preserving Projec- tion (LPP). All of these methods consider an n1 \u00d7 n2 image as a high dimensional vector in Rn1\u00d7n2, while an image represented in the plane is intrinsically a matrix. In this paper, we propose a new algorithm called Tensor Subspace Analysis (TSA). TSA considers an image as the sec- ond order tensor in Rn1 \u2297 Rn2, where Rn1 and Rn2 are two vector spaces. The relationship between the column vectors of the image ma- trix and that between the row vectors can be naturally characterized by TSA. TSA detects the intrinsic local geometrical structure of the tensor space by learning a lower dimensional tensor subspace. We compare our proposed approach with PCA, LDA and LPP methods on two standard databases. Experimental results demonstrate that TSA achieves better recognition rate, while being much more ef\ufb01cient.",
        "bibtex": "@inproceedings{NIPS2005_332647f4,\n author = {He, Xiaofei and Cai, Deng and Niyogi, Partha},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Tensor Subspace Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/332647f433a1c10fa2e2ae04abfdf83e-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/332647f433a1c10fa2e2ae04abfdf83e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/332647f433a1c10fa2e2ae04abfdf83e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 87293,
        "gs_citation": 487,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=576860237967384826&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, University of Chicago; Department of Computer Science, University of Illinois at Urbana-Champaign; Department of Computer Science, University of Chicago",
        "aff_domain": "cs.uchicago.edu;uiuc.edu;cs.uchicago.edu",
        "email": "cs.uchicago.edu;uiuc.edu;cs.uchicago.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Chicago;University of Illinois Urbana-Champaign",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.uchicago.edu;https://illinois.edu",
        "aff_unique_abbr": "UChicago;UIUC",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Urbana-Champaign",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b6de1ef031",
        "title": "The Curse of Highly Variable Functions for Local Kernel Machines",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/663772ea088360f95bac3dc7ffb841be-Abstract.html",
        "author": "Yoshua Bengio; Olivier Delalleau; Nicolas L. Roux",
        "abstract": "We present a series of theoretical arguments supporting the claim that a large class of modern learning algorithms that rely solely on the smoothness prior  with similarity between examples expressed with a local kernel  are sensitive to the curse of dimensionality, or more precisely to the variability of the target. Our discussion covers supervised, semisupervised and unsupervised learning algorithms. These algorithms are found to be local in the sense that crucial properties of the learned function at x depend mostly on the neighbors of x in the training set. This makes them sensitive to the curse of dimensionality, well studied for classical non-parametric statistical learning. We show in the case of the Gaussian kernel that when the function to be learned has many variations, these algorithms require a number of training examples proportional to the number of variations, which could be large even though there may exist short descriptions of the target function, i.e. their Kolmogorov complexity may be low. This suggests that there exist non-local learning algorithms that at least have the potential to learn about such structured but apparently complex functions (because locally they have many variations), while not using very specific prior domain knowledge.",
        "bibtex": "@inproceedings{NIPS2005_663772ea,\n author = {Bengio, Yoshua and Delalleau, Olivier and Roux, Nicolas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {The Curse of Highly Variable Functions for Local Kernel Machines},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/663772ea088360f95bac3dc7ffb841be-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/663772ea088360f95bac3dc7ffb841be-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/663772ea088360f95bac3dc7ffb841be-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 74715,
        "gs_citation": 313,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5818301175910872211&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "119fb82d0c",
        "title": "The Forgetron: A Kernel-Based Perceptron on a Fixed Budget",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/c0f971d8cd24364f2029fcb9ac7b71f5-Abstract.html",
        "author": "Ofer Dekel; Shai Shalev-shwartz; Yoram Singer",
        "abstract": "The Perceptron algorithm, despite its simplicity, often performs well on online classification tasks. The Perceptron becomes especially effective when it is used in conjunction with kernels. However, a common difficulty encountered when implementing kernel-based online algorithms is the amount of memory required to store the online hypothesis, which may grow unboundedly. In this paper we present and analyze the Forgetron algorithm for kernel-based online learning on a fixed memory budget. To our knowledge, this is the first online learning algorithm which, on one hand, maintains a strict limit on the number of examples it stores while, on the other hand, entertains a relative mistake bound. In addition to the formal results, we also present experiments with real datasets which underscore the merits of our approach.",
        "bibtex": "@inproceedings{NIPS2005_c0f971d8,\n author = {Dekel, Ofer and Shalev-shwartz, Shai and Singer, Yoram},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {The Forgetron: A Kernel-Based Perceptron on a Fixed Budget},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/c0f971d8cd24364f2029fcb9ac7b71f5-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/c0f971d8cd24364f2029fcb9ac7b71f5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/c0f971d8cd24364f2029fcb9ac7b71f5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 80247,
        "gs_citation": 147,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7472873296889453697&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "School of Computer Science & Engineering, The Hebrew University, Jerusalem 91904, Israel; School of Computer Science & Engineering, The Hebrew University, Jerusalem 91904, Israel; School of Computer Science & Engineering, The Hebrew University, Jerusalem 91904, Israel",
        "aff_domain": "cs.huji.ac.il;cs.huji.ac.il;cs.huji.ac.il",
        "email": "cs.huji.ac.il;cs.huji.ac.il;cs.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Hebrew University",
        "aff_unique_dep": "School of Computer Science & Engineering",
        "aff_unique_url": "http://www.huji.ac.il",
        "aff_unique_abbr": "HUJI",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Jerusalem",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "083a424a42",
        "title": "The Information-Form Data Association Filter",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/98d8a23fd60826a2a474c5b4f5811707-Abstract.html",
        "author": "Brad Schumitsch; Sebastian Thrun; Gary Bradski; Kunle Olukotun",
        "abstract": "This paper presents a new \ufb01lter for online data association problems in high-dimensional spaces. The key innovation is a representation of the data association posterior in information form, in which the \u201cproxim- ity\u201d of objects and tracks are expressed by numerical links. Updating these links requires linear time, compared to exponential time required for computing the exact posterior probabilities. The paper derives the algorithm formally and provides comparative results using data obtained by a real-world camera array and by a large-scale sensor network simu- lation.",
        "bibtex": "@inproceedings{NIPS2005_98d8a23f,\n author = {Schumitsch, Brad and Thrun, Sebastian and Bradski, Gary and Olukotun, Kunle},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {The Information-Form Data Association Filter},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/98d8a23fd60826a2a474c5b4f5811707-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/98d8a23fd60826a2a474c5b4f5811707-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/98d8a23fd60826a2a474c5b4f5811707-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 124675,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17429287860468294830&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "d1ebb1597e",
        "title": "The Role of Top-down and Bottom-up Processes in Guiding Eye Movements during Visual Search",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/564645fbd0332f066cbd9d083ddd077c-Abstract.html",
        "author": "Gregory Zelinsky; Wei Zhang; Bing Yu; Xin Chen; Dimitris Samaras",
        "abstract": "To investigate how top-down (TD) and bottom-up (BU) information is weighted in the guidance of human search behavior, we manipulated the proportions of BU and TD components in a saliency-based model. The model is biologically plausible and implements an arti\ufb01cial retina and a neuronal population code. The BU component is based on feature- contrast. The TD component is de\ufb01ned by a feature-template match to a stored target representation. We compared the model\u2019s behavior at differ- ent mixtures of TD and BU components to the eye movement behavior of human observers performing the identical search task. We found that a purely TD model provides a much closer match to human behavior than any mixture model using BU information. Only when biological con- straints are removed (e.g., eliminating the retina) did a BU/TD mixture model begin to approximate human behavior.",
        "bibtex": "@inproceedings{NIPS2005_564645fb,\n author = {Zelinsky, Gregory and Zhang, Wei and Yu, Bing and Chen, Xin and Samaras, Dimitris},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {The Role of Top-down and Bottom-up Processes in Guiding Eye Movements during Visual Search},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/564645fbd0332f066cbd9d083ddd077c-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/564645fbd0332f066cbd9d083ddd077c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/564645fbd0332f066cbd9d083ddd077c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 190461,
        "gs_citation": 112,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3726573072083247676&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Dept. of Psychology\u2020; Dept. of Computer Science\u2021; Dept. of Computer Science\u2021; Dept. of Psychology\u2020; Dept. of Computer Science\u2021",
        "aff_domain": "stonybrook.edu\u2020;cs.sunysb.edu\u2021;cs.sunysb.edu\u2021;ic.sunysb.edu\u2020;cs.sunysb.edu\u2021",
        "email": "stonybrook.edu\u2020;cs.sunysb.edu\u2021;cs.sunysb.edu\u2021;ic.sunysb.edu\u2020;cs.sunysb.edu\u2021",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "1;1;1",
        "aff_unique_norm": ";University Affiliation Not Specified",
        "aff_unique_dep": ";Department of Computer Science",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "6695e87a29",
        "title": "Top-Down Control of Visual Attention: A Rational Account",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/4f1f29888cabf5d45f866fe457737a23-Abstract.html",
        "author": "Michael Shettel; Shaun Vecera; Michael Mozer",
        "abstract": "Theories of visual attention commonly posit that early parallel processes extract con- spicuous features such as color contrast and motion from the visual field. These features are then combined into a saliency map, and attention is directed to the most salient regions first. Top-down attentional control is achieved by modulating the contribution of different feature types to the saliency map. A key source of data concerning attentional control comes from behavioral studies in which the effect of recent experience is exam- ined as individuals repeatedly perform a perceptual discrimination task (e.g., \u201cwhat shape is the odd-colored object?\u201d). The robust finding is that repetition of features of recent trials (e.g., target color) facilitates performance. We view this facilitation as an adaptation to the statistical structure of the environment. We propose a probabilistic model of the environment that is updated after each trial. Under the assumption that attentional control operates so as to make performance more efficient for more likely environmental states, we obtain parsimonious explanations for data from four different experiments. Further, our model provides a rational explanation for why the influence of past experience on attentional control is short lived.",
        "bibtex": "@inproceedings{NIPS2005_4f1f2988,\n author = {Shettel, Michael and Vecera, Shaun and Mozer, Michael C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Top-Down Control of Visual Attention: A Rational Account},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4f1f29888cabf5d45f866fe457737a23-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/4f1f29888cabf5d45f866fe457737a23-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/4f1f29888cabf5d45f866fe457737a23-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 323894,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15015527754883459821&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "673458ac6e",
        "title": "Transfer learning for text classification",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/bf2fb7d1825a1df3ca308ad0bf48591e-Abstract.html",
        "author": "Chuong B. Do; Andrew Y. Ng",
        "abstract": "Linear text classi\ufb01cation algorithms work by computing an inner prod- uct between a test document vector and a parameter vector. In many such algorithms, including naive Bayes and most TFIDF variants, the parame- ters are determined by some simple, closed-form, function of training set statistics; we call this mapping mapping from statistics to parameters, the parameter function. Much research in text classi\ufb01cation over the last few decades has consisted of manual efforts to identify better parameter func- tions. In this paper, we propose an algorithm for automatically learning this function from related classi\ufb01cation problems. The parameter func- tion found by our algorithm then de\ufb01nes a new learning algorithm for text classi\ufb01cation, which we can apply to novel classi\ufb01cation tasks. We \ufb01nd that our learned classi\ufb01er outperforms existing methods on a variety of multiclass text classi\ufb01cation tasks.",
        "bibtex": "@inproceedings{NIPS2005_bf2fb7d1,\n author = {Do, Chuong B. and Ng, Andrew Y.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Transfer learning for text classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/bf2fb7d1825a1df3ca308ad0bf48591e-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/bf2fb7d1825a1df3ca308ad0bf48591e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/bf2fb7d1825a1df3ca308ad0bf48591e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 569225,
        "gs_citation": 329,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3122044596731576190&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Computer Science Department, Stanford University, Stanford, CA 94305; Computer Science Department, Stanford University, Stanford, CA 94305",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2a81920a57",
        "title": "Two view learning: SVM-2K, Theory and Practice",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/46b2644cbdf489fac0e2d192212d206d-Abstract.html",
        "author": "Jason Farquhar; David Hardoon; Hongying Meng; John S. Shawe-taylor; S\u00e1ndor Szedm\u00e1k",
        "abstract": "Kernel methods make it relatively easy to define complex highdimensional feature spaces. This raises the question of how we can identify the relevant subspaces for a particular learning task. When two views of the same phenomenon are available kernel Canonical Correlation Analysis (KCCA) has been shown to be an effective preprocessing step that can improve the performance of classification algorithms such as the Support Vector Machine (SVM). This paper takes this observation to its logical conclusion and proposes a method that combines this two stage learning (KCCA followed by SVM) into a single optimisation termed SVM-2K. We present both experimental and theoretical analysis of the approach showing encouraging results and insights.",
        "bibtex": "@inproceedings{NIPS2005_46b2644c,\n author = {Farquhar, Jason and Hardoon, David and Meng, Hongying and Shawe-taylor, John and Szedm\\'{a}k, S\\'{a}ndor},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Two view learning: SVM-2K, Theory and Practice},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/46b2644cbdf489fac0e2d192212d206d-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/46b2644cbdf489fac0e2d192212d206d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/46b2644cbdf489fac0e2d192212d206d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 89207,
        "gs_citation": 458,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3447514597310505792&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "School of Electronics and Computer Science, University of Southampton, Southampton, England; School of Electronics and Computer Science, University of Southampton, Southampton, England; School of Electronics and Computer Science, University of Southampton, Southampton, England; School of Electronics and Computer Science, University of Southampton, Southampton, England; School of Electronics and Computer Science, University of Southampton, Southampton, England",
        "aff_domain": "ecs.soton.ac.uk;ecs.soton.ac.uk;cs.york.ac.uk;ecs.soton.ac.uk;ecs.soton.ac.uk",
        "email": "ecs.soton.ac.uk;ecs.soton.ac.uk;cs.york.ac.uk;ecs.soton.ac.uk;ecs.soton.ac.uk",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Southampton",
        "aff_unique_dep": "School of Electronics and Computer Science",
        "aff_unique_url": "https://www.southampton.ac.uk",
        "aff_unique_abbr": "Southampton",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Southampton",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "af02f0a427",
        "title": "Unbiased Estimator of Shape Parameter for Spiking Irregularities under Changing Environments",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/6e3197aae95c2ff8fcab35cb730f6a86-Abstract.html",
        "author": "Keiji Miura; Masato Okada; Shun-ichi Amari",
        "abstract": "We considered a gamma distribution of interspike intervals as a statisti- cal model for neuronal spike generation. The model parameters consist of a time-dependent \ufb01ring rate and a shape parameter that characterizes spiking irregularities of individual neurons. Because the environment changes with time, observed data are generated from the time-dependent \ufb01ring rate, which is an unknown function. A statistical model with an unknown function is called a semiparametric model, which is one of the unsolved problem in statistics and is generally very dif\ufb01cult to solve. We used a novel method of estimating functions in information geometry to estimate the shape parameter without estimating the unknown function. We analytically obtained an optimal estimating function for the shape parameter independent of the functional form of the \ufb01ring rate. This estimation is ef\ufb01cient without Fisher information loss and better than maximum likelihood estimation.",
        "bibtex": "@inproceedings{NIPS2005_6e3197aa,\n author = {Miura, Keiji and Okada, Masato and Amari, Shun-ichi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Unbiased Estimator of Shape Parameter for Spiking Irregularities under Changing Environments},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6e3197aae95c2ff8fcab35cb730f6a86-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/6e3197aae95c2ff8fcab35cb730f6a86-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/6e3197aae95c2ff8fcab35cb730f6a86-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 70078,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16477950177109410641&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "838d49324e",
        "title": "Using ``epitomes'' to model genetic diversity: Rational design of HIV vaccine cocktails",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/6b8b8e3bd6ad94b985c1b1f1b7a94cb2-Abstract.html",
        "author": "Nebojsa Jojic; Vladimir Jojic; Christopher Meek; David Heckerman; Brendan J. Frey",
        "abstract": "We introduce a new model of genetic diversity which summarizes a large input dataset into an epitome, a short sequence or a small set of short sequences of probability distributions capturing many overlapping subsequences from the dataset. The epitome as a representation has already been used in modeling real-valued signals, such as images and audio. The discrete sequence model we introduce in this paper targets applications in genetics, from multiple alignment to recombination and mutation inference. In our experiments, we concentrate on modeling the diversity of HIV where the epitome emerges as a natural model for producing relatively small vaccines covering a large number of immune system targets known as epitopes. Our experiments show that the epitome includes more epitopes than other vaccine designs of similar length, including cocktails of consensus strains, phylogenetic tree centers, and observed strains. We also discuss epitome designs that take into account uncertainty about Tcell cross reactivity and epitope presentation. In our experiments, we find that vaccine optimization is fairly robust to these uncertainties.",
        "bibtex": "@inproceedings{NIPS2005_6b8b8e3b,\n author = {Jojic, Nebojsa and Jojic, Vladimir and Meek, Christopher and Heckerman, David and Frey, Brendan J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Using \\textasciigrave \\textasciigrave epitomes\\textquotesingle \\textquotesingle  to model genetic diversity: Rational design of HIV vaccine cocktails},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/6b8b8e3bd6ad94b985c1b1f1b7a94cb2-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/6b8b8e3bd6ad94b985c1b1f1b7a94cb2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/6b8b8e3bd6ad94b985c1b1f1b7a94cb2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 170946,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7649816782757654659&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Microsoft Research",
        "aff_unique_url": "https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ff1ae6b432",
        "title": "Value Function Approximation with Diffusion Wavelets and Laplacian Eigenfunctions",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/2650d6089a6d640c5e85b2b88265dc2b-Abstract.html",
        "author": "Sridhar Mahadevan; Mauro Maggioni",
        "abstract": "We investigate the problem of automatically constructing ef\ufb01cient rep- resentations or basis functions for approximating value functions based on analyzing the structure and topology of the state space. In particu- lar, two novel approaches to value function approximation are explored based on automatically constructing basis functions on state spaces that can be represented as graphs or manifolds: one approach uses the eigen- functions of the Laplacian, in effect performing a global Fourier analysis on the graph; the second approach is based on diffusion wavelets, which generalize classical wavelets to graphs using multiscale dilations induced by powers of a diffusion operator or random walk on the graph. Together, these approaches form the foundation of a new generation of methods for solving large Markov decision processes, in which the underlying repre- sentation and policies are simultaneously learned.",
        "bibtex": "@inproceedings{NIPS2005_2650d608,\n author = {Mahadevan, Sridhar and Maggioni, Mauro},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Value Function Approximation with Diffusion Wavelets and Laplacian Eigenfunctions},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/2650d6089a6d640c5e85b2b88265dc2b-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/2650d6089a6d640c5e85b2b88265dc2b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/2650d6089a6d640c5e85b2b88265dc2b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 395264,
        "gs_citation": 128,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16517008688148897600&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Computer Science, University of Massachusetts, Amherst, MA 01003; Program in Applied Mathematics, Department of Mathematics, Yale University, New Haven, CT 06511",
        "aff_domain": "cs.umass.edu;yale.edu",
        "email": "cs.umass.edu;yale.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Massachusetts Amherst;Yale University",
        "aff_unique_dep": "Department of Computer Science;Department of Mathematics",
        "aff_unique_url": "https://www.umass.edu;https://www.yale.edu",
        "aff_unique_abbr": "UMass Amherst;Yale",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Amherst;New Haven",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b466a847a1",
        "title": "Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/e924517087669cf201ea91bd737a4ff4-Abstract.html",
        "author": "Jeremy Kubica; Joseph Masiero; Robert Jedicke; Andrew Connolly; Andrew W. Moore",
        "abstract": "In this paper we consider the problem of finding sets of points that conform to a given underlying model from within a dense, noisy set of observations. This problem is motivated by the task of efficiently linking faint asteroid detections, but is applicable to a range of spatial queries. We survey current tree-based approaches, showing a trade-off exists between single tree and multiple tree algorithms. To this end, we present a new type of multiple tree algorithm that uses a variable number of trees to exploit the advantages of both approaches. We empirically show that this algorithm performs well using both simulated and astronomical data.",
        "bibtex": "@inproceedings{NIPS2005_e9245170,\n author = {Kubica, Jeremy and Masiero, Joseph and Jedicke, Robert and Connolly, Andrew and Moore, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Variable KD-Tree Algorithms for Spatial Pattern Search and Discovery},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/e924517087669cf201ea91bd737a4ff4-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/e924517087669cf201ea91bd737a4ff4-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/e924517087669cf201ea91bd737a4ff4-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 115619,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16803383491560009070&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213; Institute for Astronomy, University of Hawaii, Honolulu, HI 96822; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213; Institute for Astronomy, University of Hawaii, Honolulu, HI 96822; Physics & Astronomy Department, University of Pittsburgh, Pittsburgh, PA 15213",
        "aff_domain": "ri.cmu.edu;ifa.hawaii.edu;cs.cmu.edu;ifa.hawaii.edu;phyast.pitt.edu",
        "email": "ri.cmu.edu;ifa.hawaii.edu;cs.cmu.edu;ifa.hawaii.edu;phyast.pitt.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;1;2",
        "aff_unique_norm": "Carnegie Mellon University;University of Hawaii;University of Pittsburgh",
        "aff_unique_dep": "Robotics Institute;Institute for Astronomy;Physics & Astronomy Department",
        "aff_unique_url": "https://www.cmu.edu;https://www.hawaii.edu;https://www.pitt.edu",
        "aff_unique_abbr": "CMU;UH;Pitt",
        "aff_campus_unique_index": "0;1;0;1;0",
        "aff_campus_unique": "Pittsburgh;Honolulu",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3217154929",
        "title": "Variational Bayesian Stochastic Complexity of Mixture Models",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/b6617980ce90f637e68c3ebe8b9be745-Abstract.html",
        "author": "Kazuho Watanabe; Sumio Watanabe",
        "abstract": "The Variational Bayesian framework has been widely used to approximate the Bayesian learning. In various applications, it has provided computational tractability and good generalization performance. In this paper, we discuss the Variational Bayesian learning of the mixture of exponential families and provide some additional theoretical support by deriving the asymptotic form of the stochastic complexity. The stochastic complexity, which corresponds to the minimum free energy and a lower bound of the marginal likelihood, is a key quantity for model selection. It also enables us to discuss the effect of hyperparameters and the accuracy of the Variational Bayesian approach as an approximation of the true Bayesian learning.",
        "bibtex": "@inproceedings{NIPS2005_b6617980,\n author = {Watanabe, Kazuho and Watanabe, Sumio},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Variational Bayesian Stochastic Complexity of Mixture Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/b6617980ce90f637e68c3ebe8b9be745-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/b6617980ce90f637e68c3ebe8b9be745-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/b6617980ce90f637e68c3ebe8b9be745-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 124445,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5962283944507352198&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computational Intelligence and Systems Science, Tokyo Institute of Technology; P& I Lab., Tokyo Institute of Technology",
        "aff_domain": "pi.titech.ac.jp;pi.titech.ac.jp",
        "email": "pi.titech.ac.jp;pi.titech.ac.jp",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Tokyo Institute of Technology",
        "aff_unique_dep": "Department of Computational Intelligence and Systems Science",
        "aff_unique_url": "https://www.titech.ac.jp",
        "aff_unique_abbr": "Tokyo Tech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "b05ee1689a",
        "title": "Variational EM Algorithms for Non-Gaussian Latent Variable Models",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/25766f01628f3d34b93a36a2301dffc9-Abstract.html",
        "author": "Jason Palmer; Kenneth Kreutz-Delgado; Bhaskar D. Rao; David P. Wipf",
        "abstract": "We consider criteria for variational representations of non-Gaussian latent variables, and derive variational EM algorithms in general form. We establish a general equivalence among convex bounding methods, evidence based methods, and ensemble learning/Variational Bayes methods, which has previously been demonstrated only for particular cases.",
        "bibtex": "@inproceedings{NIPS2005_25766f01,\n author = {Palmer, Jason and Kreutz-Delgado, Kenneth and Rao, Bhaskar and Wipf, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Variational EM Algorithms for Non-Gaussian Latent Variable Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/25766f01628f3d34b93a36a2301dffc9-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/25766f01628f3d34b93a36a2301dffc9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/25766f01628f3d34b93a36a2301dffc9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 122989,
        "gs_citation": 250,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11066830738757676604&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "1298145136",
        "title": "Visual Encoding with Jittering Eyes",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/5b4130c9e891d39891289001cc97d86b-Abstract.html",
        "author": "Michele Rucci",
        "abstract": "Under natural viewing conditions, small movements of the eye and body prevent the maintenance of a steady direction of gaze. It is known that stimuli tend to fade when they are stabilized on the retina for several sec- onds. However, it is unclear whether the physiological self-motion of the retinal image serves a visual purpose during the brief periods of natural visual \ufb01xation. This study examines the impact of \ufb01xational instability on the statistics of visual input to the retina and on the structure of neural activity in the early visual system. Fixational instability introduces \ufb02uc- tuations in the retinal input signals that, in the presence of natural images, lack spatial correlations. These input \ufb02uctuations strongly in\ufb02uence neu- ral activity in a model of the LGN. They decorrelate cell responses, even if the contrast sensitivity functions of simulated cells are not perfectly tuned to counter-balance the power-law spectrum of natural images. A decorrelation of neural activity has been proposed to be bene\ufb01cial for discarding statistical redundancies in the input signals. Fixational insta- bility might, therefore, contribute to establishing ef\ufb01cient representations of natural stimuli.",
        "bibtex": "@inproceedings{NIPS2005_5b4130c9,\n author = {Rucci, Michele},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Visual Encoding with Jittering Eyes},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/5b4130c9e891d39891289001cc97d86b-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/5b4130c9e891d39891289001cc97d86b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/5b4130c9e891d39891289001cc97d86b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 85232,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=40189051642281469&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Cognitive and Neural Systems, Boston University",
        "aff_domain": "cns.bu.edu",
        "email": "cns.bu.edu",
        "github": "",
        "project": "www.cns.bu.edu/~rucci",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Boston University",
        "aff_unique_dep": "Department of Cognitive and Neural Systems",
        "aff_unique_url": "https://www.bu.edu",
        "aff_unique_abbr": "BU",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b0f68b10b3",
        "title": "Walk-Sum Interpretation and Analysis of Gaussian Belief Propagation",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/ade55409d1224074754035a5a937d2e0-Abstract.html",
        "author": "Dmitry Malioutov; Alan S. Willsky; Jason K. Johnson",
        "abstract": "This paper presents a new framework based on walks in a graph for analysis and inference in Gaussian graphical models. The key idea is to decompose correlations between variables as a sum over all walks between those variables in the graph. The weight of each walk is given by a product of edgewise partial correlations. We provide a walk-sum interpretation of Gaussian belief propagation in trees and of the approximate method of loopy belief propagation in graphs with cycles. This perspective leads to a better understanding of Gaussian belief propagation and of its convergence in loopy graphs.",
        "bibtex": "@inproceedings{NIPS2005_ade55409,\n author = {Malioutov, Dmitry and Willsky, Alan and Johnson, Jason},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Walk-Sum Interpretation and Analysis of Gaussian Belief Propagation},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/ade55409d1224074754035a5a937d2e0-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/ade55409d1224074754035a5a937d2e0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/ade55409d1224074754035a5a937d2e0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 91415,
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18283631833544945334&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "8092a90a0f",
        "title": "Worst-Case Bounds for Gaussian Process Models",
        "site": "https://papers.nips.cc/paper_files/paper/2005/hash/952c3ff98a6acdc36497d839e31aa57c-Abstract.html",
        "author": "Sham M. Kakade; Matthias W. Seeger; Dean P. Foster",
        "abstract": "We present a competitive analysis of some non-parametric Bayesian al- gorithms in a worst-case online learning setting, where no probabilistic assumptions about the generation of the data are made. We consider models which use a Gaussian process prior (over the space of all func- tions) and provide bounds on the regret (under the log loss) for com- monly used non-parametric Bayesian algorithms \u2014 including Gaussian regression and logistic regression \u2014 which show how these algorithms can perform favorably under rather general conditions. These bounds ex- plicitly handle the in\ufb01nite dimensionality of these non-parametric classes in a natural way. We also make formal connections to the minimax and minimum description length (MDL) framework. Here, we show precisely how Bayesian Gaussian regression is a minimax strategy.",
        "bibtex": "@inproceedings{NIPS2005_952c3ff9,\n author = {Kakade, Sham M. and Seeger, Matthias W. and Foster, Dean P.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Y. Weiss and B. Sch\\\"{o}lkopf and J. Platt},\n pages = {},\n publisher = {MIT Press},\n title = {Worst-Case Bounds for Gaussian Process Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/952c3ff98a6acdc36497d839e31aa57c-Paper.pdf},\n volume = {18},\n year = {2005}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/2005/file/952c3ff98a6acdc36497d839e31aa57c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/2005/file/952c3ff98a6acdc36497d839e31aa57c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 171047,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17076843760858539816&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    }
]