[
    {
        "id": "b62c887e94",
        "title": "A Generative Model for Attractor Dynamics",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/e721a54a8cf18c8543d44782d9ef681f-Abstract.html",
        "author": "Richard S. Zemel; Michael Mozer",
        "abstract": "Attractor networks, which map an input space to a discrete out(cid:173) put space, are useful for pattern completion. However, designing  a net to have a given set of attractors is notoriously tricky; training  procedures are CPU intensive and often produce spurious afuac(cid:173) tors and ill-conditioned attractor basins.  These difficulties  occur  because each connection in the network participates in the encod(cid:173) ing of multiple attractors.  We describe an alternative formulation  of attractor networks in which the encoding of knowledge is local,  not distributed. Although localist attractor networks have similar  dynamics to their distributed counterparts, they are much easier  to work with and interpret. We propose a statistical formulation of  localist attract or net dynamics, which yields a convergence proof  and a mathematical interpretation of model parameters.",
        "bibtex": "@inproceedings{NIPS1999_e721a54a,\n author = {Zemel, Richard and Mozer, Michael C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {A Generative Model for Attractor Dynamics},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e721a54a8cf18c8543d44782d9ef681f-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/e721a54a8cf18c8543d44782d9ef681f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/e721a54a8cf18c8543d44782d9ef681f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1880699,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10186094793671556617&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Department of Psychology, University of Arizona; Department of Computer Science, University of Colorado",
        "aff_domain": "u.arizona.edu;colorado.edu",
        "email": "u.arizona.edu;colorado.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Arizona;University of Colorado",
        "aff_unique_dep": "Department of Psychology;Department of Computer Science",
        "aff_unique_url": "https://www.arizona.edu;https://www.colorado.edu",
        "aff_unique_abbr": "UArizona;CU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9854d2d832",
        "title": "A Geometric Interpretation of v-SVM Classifiers",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/7fea637fd6d02b8f0adf6f7dc36aed93-Abstract.html",
        "author": "David J. Crisp; Christopher J. C. Burges",
        "abstract": "We show that the recently proposed variant of the Support Vector  machine  (SVM)  algorithm,  known  as  v-SVM,  can  be  interpreted  as a maximal separation between subsets of the convex hulls of the  data,  which  we  call  soft  convex  hulls.  The  soft  convex  hulls  are  controlled by  choice of the parameter v.  If the intersection of the  convex hulls is empty, the hyperplane is positioned halfway between  them such that the distance between convex hulls, measured along  the normal, is  maximized; and if it is not, the hyperplane's normal  is  similarly determined  by  the  soft  convex  hulls,  but  its  position  (perpendicular  distance  from  the  origin)  is  adjusted  to  minimize  the  error  sum.  The  proposed  geometric  interpretation of v-SVM  also leads to necessary and sufficient conditions for the existence of  a choice of v  for  which  the v-SVM solution is nontrivial.",
        "bibtex": "@inproceedings{NIPS1999_7fea637f,\n author = {Crisp, David and Burges, Christopher J. C.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {A Geometric Interpretation of v-SVM Classifiers},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7fea637fd6d02b8f0adf6f7dc36aed93-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/7fea637fd6d02b8f0adf6f7dc36aed93-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/7fea637fd6d02b8f0adf6f7dc36aed93-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1256217,
        "gs_citation": 265,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16518289591358655861&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Centre for Sensor Signal and Information Processing, Deptartment of Electrical Engineering, University of Adelaide, South Australia; Advanced Technologies, Bell Laboratories, Lucent Technologies Holmdel, New Jersey",
        "aff_domain": "eleceng.adelaide.edu.au;lucent.com",
        "email": "eleceng.adelaide.edu.au;lucent.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Adelaide;Bell Laboratories",
        "aff_unique_dep": "Deptartment of Electrical Engineering;Advanced Technologies",
        "aff_unique_url": "https://www.adelaide.edu.au;https://www.bell-labs.com",
        "aff_unique_abbr": "Adelaide;Bell Labs",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Adelaide;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Australia;United States"
    },
    {
        "id": "0b5aad996a",
        "title": "A MCMC Approach to Hierarchical Mixture Modelling",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/973a5f0ccbc4ee3524ccf035d35b284b-Abstract.html",
        "author": "Christopher K. I. Williams",
        "abstract": "There  are  many  hierarchical  clustering  algorithms  available,  but these  lack a firm  statistical basis.  Here we  set up a hierarchical probabilistic  mixture model, where data is  generated in  a hierarchical tree-structured  manner. Markov chain Monte Carlo (MCMC) methods are demonstrated  which can  be used to  sample from  the  posterior distribution  over trees  containing variable numbers of hidden units.",
        "bibtex": "@inproceedings{NIPS1999_973a5f0c,\n author = {Williams, Christopher},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {A MCMC Approach to Hierarchical Mixture Modelling},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/973a5f0ccbc4ee3524ccf035d35b284b-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/973a5f0ccbc4ee3524ccf035d35b284b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/973a5f0ccbc4ee3524ccf035d35b284b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1590301,
        "gs_citation": 91,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=781099849092453224&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "562791a501",
        "title": "A Multi-class Linear Learning Algorithm Related to Winnow",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/fc6709bfdf0572f183c1a84ce5276e96-Abstract.html",
        "author": "Chris Mesterharm",
        "abstract": "In  this  paper,  we  present  Committee,  a  new  multi-class  learning  algo(cid:173) rithm related  to the Winnow family  of algorithms.  Committee is  an  al(cid:173) gorithm for combining the predictions of a set of sub-experts in  the  on(cid:173) line mistake-bounded model oflearning. A sub-expert is a special type of  attribute that predicts with a distribution over a finite  number of classes.  Committee learns a linear function  of sub-experts and uses this function  to make class predictions.  We  provide bounds for Committee that show  it performs  well  when  the  target can  be represented  by  a  few  relevant  sub-experts.  We  also  show how Committee can  be  used  to solve more  traditional problems composed  of attributes.  This leads to a natural ex(cid:173) tension  that learns on  multi-class problems that contain both traditional  attributes and sub-experts.",
        "bibtex": "@inproceedings{NIPS1999_fc6709bf,\n author = {Mesterharm, Chris},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {A Multi-class Linear Learning Algorithm Related to Winnow},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/fc6709bfdf0572f183c1a84ce5276e96-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/fc6709bfdf0572f183c1a84ce5276e96-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/fc6709bfdf0572f183c1a84ce5276e96-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1509532,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4172417654695745530&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Rutgers Computer Science Department",
        "aff_domain": "paul.rutgers.edu",
        "email": "paul.rutgers.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Rutgers University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.rutgers.edu",
        "aff_unique_abbr": "Rutgers",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "84b0124913",
        "title": "A Neurodynamical Approach to Visual Attention",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/b3bbccd6c008e727785cb81b1aa08ac5-Abstract.html",
        "author": "Gustavo Deco; Josef Zihl",
        "abstract": "The psychophysical evidence for \"selective attention\" originates mainly  from  visual search experiments. In this work, we formulate a hierarchi(cid:173) cal  system of interconnected modules consisting in  populations of neu(cid:173) rons  for  modeling  the  underlying  mechanisms  involved  in  selective  visual  attention.  We  demonstrate  that  our  neural  system  for  visual  search  works  across the  visual  field  in  parallel  but due  to the  different  intrinsic dynamics can show the two experimentally observed modes of  visual  attention,  namely:  the  serial  and  the  parallel  search  mode.  In  other words, neither explicit model of a focus of attention nor saliencies  maps are  used.  The  focus of attention  appears as an  emergent property  of the dynamic behavior of the system. The neural population dynamics  are  handled in  the  framework  of the  mean-field  approximation.  Conse(cid:173) quently, the whole process can be expressed as a system of coupled dif(cid:173) ferential equations.",
        "bibtex": "@inproceedings{NIPS1999_b3bbccd6,\n author = {Deco, Gustavo and Zihl, Josef},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {A Neurodynamical Approach to Visual Attention},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b3bbccd6c008e727785cb81b1aa08ac5-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/b3bbccd6c008e727785cb81b1aa08ac5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/b3bbccd6c008e727785cb81b1aa08ac5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1458520,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:1HJo5XCvOmoJ:scholar.google.com/&scioq=A+Neurodynamical+Approach+to+Visual+Attention&hl=en&as_sdt=0,33",
        "gs_version_total": 6,
        "aff": "Siemens AG Corporate Technology Neural Computation, ZT IK 4 Otto-Hahn-Ring 6 81739 Munich, Germany; Institute of Psychology Neuropsychology Ludwig-Maximilians- University Munich Leopoldstr. 13 80802 Munich, Germany",
        "aff_domain": "mchp.siemens.de; ",
        "email": "mchp.siemens.de; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Siemens AG;Ludwig-Maximilians-University Munich",
        "aff_unique_dep": "Corporate Technology Neural Computation;Institute of Psychology Neuropsychology",
        "aff_unique_url": "https://www.siemens.com;https://www.lmu.de",
        "aff_unique_abbr": "Siemens;LMU Munich",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Munich",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "7e9ad10a3b",
        "title": "A Neuromorphic VLSI System for Modeling the Neural Control of Axial Locomotion",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/acab0116c354964a558e65bdd07ff047-Abstract.html",
        "author": "Girish N. Patel; Edgar A. Brown; Stephen P. DeWeerth",
        "abstract": "We have developed and tested an analog/digital VLSI system that mod(cid:173) els the coordination of biological segmental oscillators underlying axial  locomotion in animals such as leeches and lampreys. In its current form  the system consists of a chain of twelve pattern generating circuits that  are capable of arbitrary contralateral inhibitory synaptic coupling. Each  pattern generating circuit is implemented with two independent silicon  Morris-Lecar neurons with a total of 32 programmable (floating-gate  based) inhibitory synapses, and an asynchronous address-event inter(cid:173) connection element that provides synaptic connectivity and implements  axonal delay. We describe and analyze the data from a set of experi(cid:173) ments exploring the system behavior in terms of synaptic coupling.",
        "bibtex": "@inproceedings{NIPS1999_acab0116,\n author = {Patel, Girish and Brown, Edgar and DeWeerth, Stephen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {A Neuromorphic VLSI System for Modeling the Neural Control of Axial Locomotion},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/acab0116c354964a558e65bdd07ff047-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/acab0116c354964a558e65bdd07ff047-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/acab0116c354964a558e65bdd07ff047-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1589199,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7252848854699624669&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "School of Electrical and Computer Engineering, Georgia Institute of Technology; School of Electrical and Computer Engineering, Georgia Institute of Technology; School of Electrical and Computer Engineering, Georgia Institute of Technology",
        "aff_domain": "ece.gatech.edu;ece.gatech.edu;ece.gatech.edu",
        "email": "ece.gatech.edu;ece.gatech.edu;ece.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Georgia Institute of Technology",
        "aff_unique_dep": "School of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.gatech.edu",
        "aff_unique_abbr": "Georgia Tech",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Atlanta",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "eac9ea3daf",
        "title": "A Recurrent Model of the Interaction Between Prefrontal and Inferotemporal Cortex in Delay Tasks",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/2a27b8144ac02f67687f76782a3b5d8f-Abstract.html",
        "author": "Alfonso Renart; N\u00e9stor Parga; Edmund T. Rolls",
        "abstract": "A very simple model of two reciprocally connected attractor neural net(cid:173) works is studied analytically in situations similar to those encountered  in delay match-to-sample tasks with intervening stimuli and in tasks of  memory guided attention. The model qualitatively reproduces many of  the experimental data on these types of tasks and provides a framework  for the understanding of the experimental observations in the context of  the attractor neural network scenario.",
        "bibtex": "@inproceedings{NIPS1999_2a27b814,\n author = {Renart, Alfonso and Parga, N\\'{e}stor and Rolls, Edmund},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {A Recurrent Model of the Interaction Between Prefrontal and Inferotemporal Cortex in Delay Tasks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/2a27b8144ac02f67687f76782a3b5d8f-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/2a27b8144ac02f67687f76782a3b5d8f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/2a27b8144ac02f67687f76782a3b5d8f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1711337,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17242285265680141825&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "a5d1828f6c",
        "title": "A SNoW-Based Face Detector",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/3e15cc11f979ed25912dff5b0669f2cd-Abstract.html",
        "author": "Ming-Hsuan Yang; Dan Roth; Narendra Ahuja",
        "abstract": "A novel learning approach for human face detection using a network  of linear units is  presented.  The SNoW  learning architecture is  a  sparse network  of linear functions  over a  pre-defined or incremen(cid:173) tally learned feature  space  and  is  specifically tailored for  learning  in the presence of a  very large number of features.  A wide range of  face  images in different poses,  with different expressions and under  different  lighting conditions  are  used  as  a  training  set  to  capture  the  variations of human faces.  Experimental results on commonly  used benchmark data sets of a wide range of face images show that  the  SNoW-based  approach  outperforms  methods  that  use  neural  networks,  Bayesian  methods,  support  vector  machines  and  oth(cid:173) ers.  Furthermore, learning and  evaluation using  the  SNoW-based  method are significantly more efficient  than with other methods.",
        "bibtex": "@inproceedings{NIPS1999_3e15cc11,\n author = {Yang, Ming-Hsuan and Roth, Dan and Ahuja, Narendra},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {A SNoW-Based Face Detector},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/3e15cc11f979ed25912dff5b0669f2cd-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/3e15cc11f979ed25912dff5b0669f2cd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/3e15cc11f979ed25912dff5b0669f2cd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1967269,
        "gs_citation": 579,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16444862894496036565&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science and the Beckman Institute, University of Illinois at Urbana-Champaign; Department of Computer Science and the Beckman Institute, University of Illinois at Urbana-Champaign; Department of Computer Science and the Beckman Institute, University of Illinois at Urbana-Champaign",
        "aff_domain": "vision.ai.uiuc.edu;cs.uiuc.edu;vision.ai.uiuc.edu",
        "email": "vision.ai.uiuc.edu;cs.uiuc.edu;vision.ai.uiuc.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Urbana-Champaign",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "fc7871ea7f",
        "title": "A Variational Baysian Framework for Graphical Models",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/74563ba21a90da13dacf2a73e3ddefa7-Abstract.html",
        "author": "Hagai Attias",
        "abstract": "This paper presents a novel practical framework for Bayesian model  averaging  and  model  selection  in  probabilistic  graphical  models.  Our approach approximates full  posterior distributions over model  parameters and structures, as well as latent variables, in an analyt(cid:173) ical  manner.  These  posteriors fall  out of a  free-form  optimization  procedure,  which  naturally  incorporates  conjugate  priors.  Unlike  in  large sample  approximations,  the posteriors are  generally  non(cid:173) Gaussian and no Hessian needs to be computed.  Predictive quanti(cid:173) ties  are obtained analytically.  The resulting algorithm generalizes  the standard Expectation Maximization algorithm, and its conver(cid:173) gence  is  guaranteed.  We  demonstrate  that  this  approach  can  be  applied  to  a  large  class  of  models  in  several  domains,  including  mixture models and source separation.",
        "bibtex": "@inproceedings{NIPS1999_74563ba2,\n author = {Attias, Hagai},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {A Variational Baysian Framework for Graphical Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/74563ba21a90da13dacf2a73e3ddefa7-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/74563ba21a90da13dacf2a73e3ddefa7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/74563ba21a90da13dacf2a73e3ddefa7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1725962,
        "gs_citation": 1165,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16067200809130033758&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Gatsby Unit, University College London",
        "aff_domain": "gatsby.ucl.ac.uk",
        "email": "gatsby.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University College London",
        "aff_unique_dep": "Gatsby Unit",
        "aff_unique_url": "https://www.ucl.ac.uk",
        "aff_unique_abbr": "UCL",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "f52d6926aa",
        "title": "A Winner-Take-All Circuit with Controllable Soft Max Property",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/3e7e0224018ab3cf51abb96464d518cd-Abstract.html",
        "author": "Shih-Chii Liu",
        "abstract": "I describe a silicon network consisting of a group of excitatory neu(cid:173) rons  and a  global inhibitory neuron.  The output of the inhibitory  neuron is normalized with respect to the input strengths.  This out(cid:173) put models the normalization property of the wide-field direction(cid:173) selective cells in the fly  visual system.  This normalizing property is  also useful  in any system where we  wish  the output signal to code  only the strength of the inputs, and not be dependent on the num(cid:173) ber of inputs.  The circuitry in each neuron is equivalent to that in  Lazzaro's winner-take-all  (WTA)  circuit  with one additional tran(cid:173) sistor  and  a  voltage  reference.  Just  as  in  Lazzaro's  circuit,  the  outputs of the excitatory neurons code the neuron with the largest  input.  The difference here is  that multiple winners can be chosen.  By  varying  the  voltage  reference  of the  neuron,  the  network  can  transition  between  a  soft-max  behavior  and  a  hard  WTA  behav(cid:173) ior.  I show results from  a fabricated chip of 20  neurons in a  1.2J.Lm  CMOS technology.",
        "bibtex": "@inproceedings{NIPS1999_3e7e0224,\n author = {Liu, Shih-Chii},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {A Winner-Take-All Circuit with Controllable Soft Max Property},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/3e7e0224018ab3cf51abb96464d518cd-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/3e7e0224018ab3cf51abb96464d518cd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/3e7e0224018ab3cf51abb96464d518cd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1337097,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17956846004144816426&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Institute for Neuroinformatics, ETHjUNIZ",
        "aff_domain": "ini.phys.ethz.ch",
        "email": "ini.phys.ethz.ch",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "Institute for Neuroinformatics",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETH",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "04b071a2e7",
        "title": "Acquisition in Autoshaping",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/e9b73bccd1762555582b513ff9d02492-Abstract.html",
        "author": "Sham Kakade; Peter Dayan",
        "abstract": "Quantitative data on the speed with which animals acquire behav(cid:173) ioral responses during classical conditioning experiments should  provide strong constraints on models of learning.  However, most  models have simply ignored these data; the few that have attempt(cid:173) ed to address them have failed by at least an order of magnitude.  We discuss key data on the speed of acquisition, and show how to  account for them using a statistically sound model of learning, in  which differential reliabilities of stimuli playa crucial role.",
        "bibtex": "@inproceedings{NIPS1999_e9b73bcc,\n author = {Kakade, Sham and Dayan, Peter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Acquisition in Autoshaping},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e9b73bccd1762555582b513ff9d02492-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/e9b73bccd1762555582b513ff9d02492-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/e9b73bccd1762555582b513ff9d02492-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1785206,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10189106979413022005&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "5efe22c18e",
        "title": "Actor-Critic Algorithms",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html",
        "author": "Vijay R. Konda; John N. Tsitsiklis",
        "abstract": "We  propose  and  analyze  a  class  of  actor-critic  algorithms  for  simulation-based  optimization  of  a  Markov  decision  process  over  a  parameterized  family  of randomized  stationary  policies.  These  are two-time-scale  algorithms in  which  the critic uses TD learning  with  a  linear approximation architecture and the actor is  updated  in  an  approximate  gradient  direction  based  on  information  pro(cid:173) vided by the critic.  We  show that the features for  the critic should  span a subspace prescribed by the choice of parameterization of the  actor.  We  conclude by discussing convergence properties and some  open problems.",
        "bibtex": "@inproceedings{NIPS1999_6449f44a,\n author = {Konda, Vijay and Tsitsiklis, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Actor-Critic Algorithms},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/6449f44a102fde848669bdd9eb6b76fa-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1492973,
        "gs_citation": 4302,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4581310872837094866&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "12bf495a52",
        "title": "Agglomerative Information Bottleneck",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/be3e9d3f7d70537357c67bb3f4086846-Abstract.html",
        "author": "Noam Slonim; Naftali Tishby",
        "abstract": "We  introduce a novel distributional clustering algorithm that max(cid:173) imizes  the  mutual  information  per  cluster  between  data and  giv(cid:173) en  categories.  This  algorithm  can  be considered  as  a  bottom  up  hard  version  of  the  recently  introduced  \"Information  Bottleneck  Method\".  The algorithm is  compared with the top-down soft  ver(cid:173) sion  of the  information  bottleneck  method  and  a  relationship  be(cid:173) tween the hard and soft results is established.  We  demonstrate the  algorithm on the 20 Newsgroups data set.  For a subset of two news(cid:173) groups  we  achieve compression  by  3  orders  of magnitudes loosing  only  10%  of the original mutual information.",
        "bibtex": "@inproceedings{NIPS1999_be3e9d3f,\n author = {Slonim, Noam and Tishby, Naftali},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Agglomerative Information Bottleneck},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/be3e9d3f7d70537357c67bb3f4086846-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/be3e9d3f7d70537357c67bb3f4086846-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/be3e9d3f7d70537357c67bb3f4086846-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1563591,
        "gs_citation": 602,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16281829332503544688&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Institute of Computer Science and Center for Neural Computation, The Hebrew University, Jerusalem, 91904 Israel; Institute of Computer Science and Center for Neural Computation, The Hebrew University, Jerusalem, 91904 Israel",
        "aff_domain": "cs.huji.ac.il;cs.huji.ac.il",
        "email": "cs.huji.ac.il;cs.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hebrew University",
        "aff_unique_dep": "Institute of Computer Science and Center for Neural Computation",
        "aff_unique_url": "http://www.huji.ac.il",
        "aff_unique_abbr": "HUJI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Jerusalem",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "b04608ebf2",
        "title": "Algebraic Analysis for Non-regular Learning Machines",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/752d25a1f8dbfb2d656bac3094bfb81c-Abstract.html",
        "author": "Sumio Watanabe",
        "abstract": "Hierarchical learning machines are non-regular and non-identifiable  statistical models, whose true parameter sets are analytic sets with  singularities.  Using  algebraic  analysis,  we  rigorously  prove  that  the  stochastic  complexity  of  a  non-identifiable  learning  machine  (ml  - 1) log log n  + const.,  is  asymptotically  equal  to  >'1  log n  - where n is the number of training samples.  Moreover we show that  the rational  number >'1  and the integer ml  can be algorithmically  calculated  using  resolution  of singularities  in  algebraic  geometry.  Also we obtain inequalities 0 < >'1  ~ d/2 and 1  ~ ml  ~ d,  where d  is  the number of parameters.",
        "bibtex": "@inproceedings{NIPS1999_752d25a1,\n author = {Watanabe, Sumio},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Algebraic Analysis for Non-regular Learning Machines},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/752d25a1f8dbfb2d656bac3094bfb81c-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/752d25a1f8dbfb2d656bac3094bfb81c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/752d25a1f8dbfb2d656bac3094bfb81c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1227434,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=176862034434740017&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "613935de2e",
        "title": "Algorithms for Independent Components Analysis and Higher Order Statistics",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/3c1e4bd67169b8153e0047536c9f541e-Abstract.html",
        "author": "Daniel D. Lee; Uri Rokni; Haim Sompolinsky",
        "abstract": "A  latent  variable  generative  model  with  finite  noise  is  used  to  de(cid:173) scribe  several  different  algorithms  for  Independent Components  Anal(cid:173) ysis  (lCA).  In  particular,  the  Fixed  Point  ICA  algorithm  is  shown  to  be equivalent to  the Expectation-Maximization algorithm for maximum  likelihood  under certain  constraints,  allowing  the conditions for  global  convergence to  be  elucidated.  The algorithms can  also  be explained by  their generic  behavior  near  a  singular point where the  size  of the  opti(cid:173) mal generative bases vanishes.  An expansion of the likelihood about this  singular point indicates the role of higher order correlations in  determin(cid:173) ing the features discovered by  ICA. The application and convergence of  these algorithms are demonstrated on a simple illustrative example.",
        "bibtex": "@inproceedings{NIPS1999_3c1e4bd6,\n author = {Lee, Daniel and Rokni, Uri and Sompolinsky, Haim},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Algorithms for Independent Components Analysis and Higher Order Statistics},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/3c1e4bd67169b8153e0047536c9f541e-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/3c1e4bd67169b8153e0047536c9f541e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/3c1e4bd67169b8153e0047536c9f541e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1373805,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2650256690055458612&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "31765f225d",
        "title": "An Analog VLSI Model of Periodicity Extraction",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/8b6a80c3cf2cbd5f967063618dc54f39-Abstract.html",
        "author": "Andr\u00e9 van Schaik",
        "abstract": "that extracts",
        "bibtex": "@inproceedings{NIPS1999_8b6a80c3,\n author = {van Schaik, Andr\\'{e}},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {An Analog VLSI Model of Periodicity Extraction},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/8b6a80c3cf2cbd5f967063618dc54f39-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/8b6a80c3cf2cbd5f967063618dc54f39-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/8b6a80c3cf2cbd5f967063618dc54f39-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1797499,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17692026203547504432&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Computer Engineering Laboratory J03, University of Sydney, NSW 2006 Sydney, Australia",
        "aff_domain": "ee.usyd.edu.au",
        "email": "ee.usyd.edu.au",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Sydney",
        "aff_unique_dep": "Computer Engineering Laboratory",
        "aff_unique_url": "https://www.sydney.edu.au",
        "aff_unique_abbr": "USYD",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Sydney",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "b852904acb",
        "title": "An Analysis of Turbo Decoding with Gaussian Densities",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/a63fc8c5d915e1f1a40f40e6c7499863-Abstract.html",
        "author": "Paat Rusmevichientong; Benjamin Van Roy",
        "abstract": "We  provide  an  analysis  of  the  turbo  decoding  algorithm  (TDA)  in  a  setting involving  Gaussian  densities.  In  this  context,  we  are  able  to show  that  the  algorithm  converges  and  that  - somewhat  surprisingly - though the density generated by the TDA may differ  significantly from  the desired posterior density,  the means of these  two densities  coincide.",
        "bibtex": "@inproceedings{NIPS1999_a63fc8c5,\n author = {Rusmevichientong, Paat and Van Roy, Benjamin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {An Analysis of Turbo Decoding with Gaussian Densities},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/a63fc8c5d915e1f1a40f40e6c7499863-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/a63fc8c5d915e1f1a40f40e6c7499863-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/a63fc8c5d915e1f1a40f40e6c7499863-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1423939,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6377763079542041545&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "6add17dcd4",
        "title": "An Environment Model for Nonstationary Reinforcement Learning",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/e8d92f99edd25e2cef48eca48320a1a5-Abstract.html",
        "author": "Samuel P. M. Choi; Dit-Yan Yeung; Nevin Lianwen Zhang",
        "abstract": "Reinforcement learning in nonstationary environments is  generally  regarded  as  an  important  and  yet  difficult  problem.  This  paper  partially addresses the problem by formalizing a subclass of nonsta(cid:173) tionary environments.  The environment model, called hidden-mode  Markov  decision  process  (HM-MDP),  assumes that environmental  changes  are  always  confined  to  a  small  number  of hidden  modes.  A  mode  basically  indexes  a  Markov  decision  process  (MDP)  and  evolves  with  time according to a  Markov  chain.  While  HM-MDP  is  a  special  case  of partially  observable  Markov  decision  processes  (POMDP), modeling an HM-MDP environment via the more gen(cid:173) eral POMDP  model  unnecessarily increases the problem complex(cid:173) ity.  A variant of the Baum-Welch algorithm is  developed for model  learning requiring less  data and time.",
        "bibtex": "@inproceedings{NIPS1999_e8d92f99,\n author = {Choi, Samuel and Yeung, Dit-Yan and Zhang, Nevin},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {An Environment Model for Nonstationary Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e8d92f99edd25e2cef48eca48320a1a5-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/e8d92f99edd25e2cef48eca48320a1a5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/e8d92f99edd25e2cef48eca48320a1a5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1496439,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1006566732539362395&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Department of Computer Science, Hong Kong University of Science and Technology; Department of Computer Science, Hong Kong University of Science and Technology; Department of Computer Science, Hong Kong University of Science and Technology",
        "aff_domain": "cs.ust.hk;cs.ust.hk;cs.ust.hk",
        "email": "cs.ust.hk;cs.ust.hk;cs.ust.hk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ust.hk",
        "aff_unique_abbr": "HKUST",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "9872a12930",
        "title": "An Improved Decomposition Algorithm for Regression Support Vector Machines",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/8b5700012be65c9da25f49408d959ca0-Abstract.html",
        "author": "Pavel Laskov",
        "abstract": "A  new  decomposition  algorithm  for  training  regression  Support  Vector  Machines  (SVM)  is  presented.  The  algorithm  builds  on  the  basic  principles  of decomposition  proposed  by  Osuna et.  al.,  and addresses the issue of optimal working set  selection.  The new  criteria for  testing optimality  of a  working set are derived.  Based  on these  criteria, the principle of  \"maximal inconsistency\"  is  pro(cid:173) posed to form (approximately) optimal working sets.  Experimental  results show superior performance of the new algorithm in compar(cid:173) ison with traditional training of regression SVM without decompo(cid:173) sition.  Similar results have been previously reported on decomposi(cid:173) tion algorithms for  pattern recognition SVM. The new algorithm is  also applicable to advanced SVM formulations based on regression,  such as density estimation  and integral equation SVM.",
        "bibtex": "@inproceedings{NIPS1999_8b570001,\n author = {Laskov, Pavel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {An Improved Decomposition Algorithm for Regression Support Vector Machines},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/8b5700012be65c9da25f49408d959ca0-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/8b5700012be65c9da25f49408d959ca0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/8b5700012be65c9da25f49408d959ca0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1438919,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4335881948385125794&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer and Information Sciences, University of Delaware",
        "aff_domain": "asel.udel.edu",
        "email": "asel.udel.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Delaware",
        "aff_unique_dep": "Department of Computer and Information Sciences",
        "aff_unique_url": "https://www.udel.edu",
        "aff_unique_abbr": "UD",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "754b0c3a34",
        "title": "An Information-Theoretic Framework for Understanding Saccadic Eye Movements",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/8d420fa35754d1f1c19969c88780314d-Abstract.html",
        "author": "Tai Sing Lee; Stella X. Yu",
        "abstract": "In  this paper,  we  propose that information maximization can  pro(cid:173) vide  a  unified  framework  for  understanding  saccadic  eye  move(cid:173) ments.  In  this framework,  the mutual information among the cor(cid:173) tical  representations  of the  retinal  image,  the  priors  constructed  from  our  long  term  visual  experience,  and  a  dynamic  short-term  internal  representation  constructed  from  recent  saccades  provides  a  map for  guiding eye  navigation .  By  directing  the  eyes  to loca(cid:173) tions  of maximum complexity in  neuronal  ensemble  responses  at  each  step,  the  automatic saccadic  eye  movement system  greedily  collects information about the external world, while modifying the  neural  representations  in  the  process.  This  framework  attempts  to  connect  several  psychological  phenomena,  such  as  pop-out  and  inhibition of return,  to long term visual experience  and short term  working  memory.  It  also  provides  an  interesting  perspective  on  contextual computation and formation of neural  representation  in  the visual system.",
        "bibtex": "@inproceedings{NIPS1999_8d420fa3,\n author = {Lee, Tai Sing and Yu, Stella},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {An Information-Theoretic Framework for Understanding Saccadic Eye Movements},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/8d420fa35754d1f1c19969c88780314d-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/8d420fa35754d1f1c19969c88780314d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/8d420fa35754d1f1c19969c88780314d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1643041,
        "gs_citation": 85,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9118398645044518099&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Computer Science, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University",
        "aff_domain": "es.emu.edu;enbe.emu.edu",
        "email": "es.emu.edu;enbe.emu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ed28b319ee",
        "title": "An MEG Study of Response Latency and Variability in the Human Visual System During a Visual-Motor Integration Task",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/02f039058bd48307e6f653a2005c9dd2-Abstract.html",
        "author": "Akaysha C. Tang; Barak A. Pearlmutter; Tim A. Hely; Michael Zibulevsky; Michael P. Weisend",
        "abstract": "Human  reaction  times  during  sensory-motor  tasks  vary  consider(cid:173) ably.  To  begin to understand how  this variability arises, we  exam(cid:173) ined neuronal populational response time variability at early versus  late  visual  processing  stages.  The  conventional  view  is  that  pre(cid:173) cise temporal information is gradually lost as information is passed  through a  layered network of mean-rate  \"units.\"  We  tested in hu(cid:173) mans  whether  neuronal  populations  at different  processing  stages  behave like mean-rate \"units\".  A blind source separation algorithm  was  applied to MEG signals from  sensory-motor integration tasks.  Response  time  latency  and  variability  for  multiple  visual  sources  were estimated by  detecting single-trial stimulus-locked events for  each  source.  In  two  subjects  tested  on  four  visual  reaction  time  tasks,  we  reliably identified sources  belonging to early and late vi(cid:173) sual processing stages.  The standard deviation of response latency  was smaller for  early rather than late processing stages.  This sup(cid:173) ports the  hypothesis that human populational response  time vari(cid:173) ability increases from  early to late visual  processing stages.",
        "bibtex": "@inproceedings{NIPS1999_02f03905,\n author = {Tang, Akaysha and Pearlmutter, Barak and Hely, Tim and Zibulevsky, Michael and Weisend, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {An MEG Study of Response Latency and Variability in the Human Visual System During a Visual-Motor Integration Task},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/02f039058bd48307e6f653a2005c9dd2-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/02f039058bd48307e6f653a2005c9dd2-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/02f039058bd48307e6f653a2005c9dd2-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1786390,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12924234572008102485&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Dept. of Psychology, University of New Mexico; Dept. of Computer Science, University of New Mexico; Santa Fe Institute; Dept. of Computer Science, University of New Mexico; VA Medical Center",
        "aff_domain": "unm.edu;cs.unm.edu;santafe.edu;cs.unm.edu;unm.edu",
        "email": "unm.edu;cs.unm.edu;santafe.edu;cs.unm.edu;unm.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0;2",
        "aff_unique_norm": "University of New Mexico;Santa Fe Institute;VA Medical Center",
        "aff_unique_dep": "Dept. of Psychology;;",
        "aff_unique_url": "https://www.unm.edu;https://www.santafe.edu;https://www.va.gov",
        "aff_unique_abbr": "UNM;SFI;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e8fb7b874a",
        "title": "An Oculo-Motor System with Multi-Chip Neuromorphic Analog VLSI Control",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/7e230522657ecdc50e4249581b861f8e-Abstract.html",
        "author": "Oliver Landolt; Steve Gyger",
        "abstract": "A system emulating the functionality of a moving eye-hence the name  oculo-motor system-has been built and successfully tested.  It is  made  of an optical device for shifting the field of view of an image sensor by up  to 45 \u00b0 in  any direction, four neuromorphic analog VLSI circuits imple(cid:173) menting an oculo-motor control loop, and some off-the-shelf electronics.  The custom integrated circuits communicate with each other primarily by  non-arbitrated address-event buses.  The system  implements the behav(cid:173) iors of saliency-based saccadic exploration, and smooth pursuit of light  spots.  The duration of saccades ranges from  45 ms  to  100 ms,  which  is  comparable to human eye performance. Smooth pursuit operates on light  sources moving at up to 50 0 /s in the visual field.",
        "bibtex": "@inproceedings{NIPS1999_7e230522,\n author = {Landolt, Oliver and Gyger, Steve},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {An Oculo-Motor System with Multi-Chip Neuromorphic Analog VLSI Control},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7e230522657ecdc50e4249581b861f8e-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/7e230522657ecdc50e4249581b861f8e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/7e230522657ecdc50e4249581b861f8e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1678951,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8557832606611688110&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "CSEMSA 2007 Neuchatel / Switzerland + Koch Lab, Division of Biology 139-74, Caltech, Pasadena, CA 91125, USA; CSEMSA 2007 Neuchatel / Switzerland",
        "aff_domain": "caltech.edu;csem.ch",
        "email": "caltech.edu;csem.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0",
        "aff_unique_norm": "CSEMSA;California Institute of Technology",
        "aff_unique_dep": ";Division of Biology",
        "aff_unique_url": ";https://www.caltech.edu",
        "aff_unique_abbr": ";Caltech",
        "aff_campus_unique_index": "0+1;0",
        "aff_campus_unique": "Neuchatel;Pasadena",
        "aff_country_unique_index": "0+1;0",
        "aff_country_unique": "Switzerland;United States"
    },
    {
        "id": "00fdba6138",
        "title": "An Oscillatory Correlation Frame work for Computational Auditory Scene Analysis",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/cdf1035c34ec380218a8cc9a43d438f9-Abstract.html",
        "author": "Guy J. Brown; DeLiang L. Wang",
        "abstract": "A  neural  model  is  described  which  uses  oscillatory  correlation  to  segregate speech from  interfering sound sources. The core of the model  is  a two-layer neural  oscillator network.  A sound  stream  is  represented  by  a  synchronized  population  of oscillators,  and  different  streams  are  represented  by  desynchronized  oscillator  populations.  The  model  has  been evaluated using a corpus of speech mixed with interfering sounds,  and produces an  improvement in signal-to-noise ratio for every  mixture.",
        "bibtex": "@inproceedings{NIPS1999_cdf1035c,\n author = {Brown, Guy and Wang, DeLiang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {An Oscillatory Correlation Frame work for Computational Auditory Scene Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/cdf1035c34ec380218a8cc9a43d438f9-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/cdf1035c34ec380218a8cc9a43d438f9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/cdf1035c34ec380218a8cc9a43d438f9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1434290,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16698159402330800760&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, University of Sheffield; Department of Computer and Information Science and Centre for Cognitive Science, The Ohio State University",
        "aff_domain": "dcs.shef.ac.uk;cis.ohio-state.edu",
        "email": "dcs.shef.ac.uk;cis.ohio-state.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Sheffield;Ohio State University",
        "aff_unique_dep": "Department of Computer Science;Department of Computer and Information Science",
        "aff_unique_url": "https://www.sheffield.ac.uk;https://www.osu.edu",
        "aff_unique_abbr": "Sheffield;OSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "5d3134ed6d",
        "title": "Application of Blind Separation of Sources to Optical Recording of Brain Activity",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/b2531e7bb29bf22e1daae486fae3417a-Abstract.html",
        "author": "Holger Schoner; Martin Stetter; Ingo Schie\u00dfl; John E. W. Mayhew; Jennifer S. Lund; Niall McLoughlin; Klaus Obermayer",
        "abstract": "In the analysis of data recorded by optical imaging from intrinsic signals  (measurement of changes of light reflectance from cortical tissue) the re(cid:173) moval  of noise and  artifacts  such  as  blood  vessel  patterns  is  a  serious  problem. Often bandpass filtering is used, but the underlying assumption  that a spatial frequency  exists,  which separates  the mapping component  from  other components  (especially  the  global  signal),  is  questionable.  Here we propose alternative ways of processing optical imaging data, us(cid:173) ing blind source separation techniques based on the spatial decorre1ation  of the data.  We  first  perform  benchmarks  on  artificial  data in  order  to  select  the  way  of processing, which is  most robust with respect  to  sen(cid:173) sor noise.  We then apply it to recordings of optical imaging experiments  from  macaque primary visual cortex. We show that our BSS technique is  able  to  extract ocular dominance and orientation preference maps  from  single  condition  stacks,  for  data,  where  standard  post-processing pro(cid:173) cedures  fail.  Artifacts,  especially  blood  vessel  patterns,  can  often  be  completely removed  from  the maps.  In  summary,  our method for blind  source separation using extended spatial decorrelation is a superior tech(cid:173) nique for the analysis of optical recording data.",
        "bibtex": "@inproceedings{NIPS1999_b2531e7b,\n author = {Schoner, Holger and Stetter, Martin and Schie\\ss l, Ingo and Mayhew, John and Lund, Jennifer and McLoughlin, Niall and Obermayer, Klaus},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Application of Blind Separation of Sources to Optical Recording of Brain Activity},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b2531e7bb29bf22e1daae486fae3417a-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/b2531e7bb29bf22e1daae486fae3417a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/b2531e7bb29bf22e1daae486fae3417a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1578920,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6738343951512180563&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, Technical University of Berlin, Germany; Department of Computer Science, Technical University of Berlin, Germany; Department of Computer Science, Technical University of Berlin, Germany; University of Sheffield, UK; Institute of Ophthalmology, University College London, UK; Institute of Ophthalmology, University College London, UK; Department of Computer Science, Technical University of Berlin, Germany",
        "aff_domain": "cs.tu-berlin.de;cs.tu-berlin.de;cs.tu-berlin.de;sheffield.ac.uk;ucl.ac.uk;ucl.ac.uk;cs.tu-berlin.de",
        "email": "cs.tu-berlin.de;cs.tu-berlin.de;cs.tu-berlin.de;sheffield.ac.uk;ucl.ac.uk;ucl.ac.uk;cs.tu-berlin.de",
        "github": "",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;2;2;0",
        "aff_unique_norm": "Technical University of Berlin;University of Sheffield;University College London",
        "aff_unique_dep": "Department of Computer Science;;Institute of Ophthalmology",
        "aff_unique_url": "https://www.tu-berlin.de;https://www.sheffield.ac.uk;https://www.ucl.ac.uk",
        "aff_unique_abbr": "TU Berlin;Sheffield;UCL",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Berlin;",
        "aff_country_unique_index": "0;0;0;1;1;1;0",
        "aff_country_unique": "Germany;United Kingdom"
    },
    {
        "id": "0d5195fd5d",
        "title": "Approximate Inference A lgorithms for Two-Layer Bayesian Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/84f0f20482cde7e5eacaf7364a643d33-Abstract.html",
        "author": "Andrew Y. Ng; Michael I. Jordan",
        "abstract": "We  present  a  class  of approximate  inference  algorithms  for  graphical  models  of the  QMR-DT type.  We  give  convergence rates  for  these  al(cid:173) gorithms and  for  the  Jaakkola and  Jordan  (1999) algorithm,  and  verify  these  theoretical predictions empirically.  We  also present empirical re(cid:173) sults on the difficult QMR-DT network problem, obtaining performance  of the  new  algorithms roughly  comparable  to  the  Jaakkola and  Jordan  algorithm.",
        "bibtex": "@inproceedings{NIPS1999_84f0f204,\n author = {Ng, Andrew and Jordan, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Approximate Inference A lgorithms for Two-Layer Bayesian Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/84f0f20482cde7e5eacaf7364a643d33-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/84f0f20482cde7e5eacaf7364a643d33-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/84f0f20482cde7e5eacaf7364a643d33-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1462068,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12035539299271091064&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "Computer Science Division, UC Berkeley; Computer Science Division and Department of Statistics, UC Berkeley",
        "aff_domain": "cs.berkeley.edu;cs.berkeley.edu",
        "email": "cs.berkeley.edu;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Computer Science Division",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "306827df20",
        "title": "Approximate Planning in Large POMDPs via Reusable Trajectories",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/4f398cb9d6bc79ae567298335b51ba8a-Abstract.html",
        "author": "Michael J. Kearns; Yishay Mansour; Andrew Y. Ng",
        "abstract": "We consider the problem of reliably choosing a near-best strategy from  a restricted class  of strategies TI  in  a partially observable Markov deci(cid:173) sion process (POMDP). We assume we are given the ability to simulate  the POMDP,  and study  what might be called the sample complexity - that  is,  the  amount of data one must  generate in  the  POMDP  in  order  to choose a good strategy.  We  prove upper bounds on  the sample com(cid:173) plexity  showing  that,  even  for  infinitely  large  and arbitrarily  complex  POMDPs,  the  amount  of data needed  can  be finite,  and  depends  only  linearly  on  the complexity of the  restricted strategy class  TI,  and  expo(cid:173) nentially  on  the horizon time.  This latter dependence can be eased in  a  variety  of ways,  including the  application  of gradient and  local  search  algorithms.  Our measure of complexity generalizes the classical super(cid:173) vised  learning notion  of VC  dimension to the settings  of reinforcement  learning and planning.",
        "bibtex": "@inproceedings{NIPS1999_4f398cb9,\n author = {Kearns, Michael and Mansour, Yishay and Ng, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Approximate Planning in Large POMDPs via Reusable Trajectories},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/4f398cb9d6bc79ae567298335b51ba8a-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/4f398cb9d6bc79ae567298335b51ba8a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/4f398cb9d6bc79ae567298335b51ba8a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1786035,
        "gs_citation": 233,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11705434998299548052&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "AT&T Labs; Tel Aviv University; UC Berkeley",
        "aff_domain": "research.att.com;math.tau.ac.il;cs.berkeley.edu",
        "email": "research.att.com;math.tau.ac.il;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "AT&T Laboratories;Tel Aviv University;University of California, Berkeley",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.att.com/labs;https://www.tau.ac.il;https://www.berkeley.edu",
        "aff_unique_abbr": "AT&T Labs;TAU;UC Berkeley",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;Israel"
    },
    {
        "id": "428c9fca16",
        "title": "Audio Vision: Using Audio-Visual Synchrony to Locate Sounds",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/b618c3210e934362ac261db280128c22-Abstract.html",
        "author": "John R. Hershey; Javier R. Movellan",
        "abstract": "Psychophysical and physiological evidence shows that sound local(cid:173) ization of acoustic signals is strongly influenced by  their synchrony  with visual signals.  This effect,  known as ventriloquism, is at work  when  sound  coming  from  the  side  of a  TV  set  feels  as  if it  were  coming  from  the  mouth  of the  actors.  The  ventriloquism  effect  suggests that there is  important information about sound location  encoded in  the synchrony between the audio and video signals.  In  spite  of  this  evidence,  audiovisual  synchrony  is  rarely  used  as  a  source of information  in  computer  vision  tasks.  In  this  paper  we  explore the use  of audio  visual  synchrony to locate sound sources.  We developed a system that searches for regions of the visual land(cid:173) scape that correlate highly with the acoustic signals and tags them  as  likely  to contain an acoustic  source.  We  discuss  our experience  implementing the system,  present results on a  speaker localization  task and discuss potential applications of the approach.",
        "bibtex": "@inproceedings{NIPS1999_b618c321,\n author = {Hershey, John and Movellan, Javier},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Audio Vision: Using Audio-Visual Synchrony to Locate Sounds},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b618c3210e934362ac261db280128c22-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/b618c3210e934362ac261db280128c22-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/b618c3210e934362ac261db280128c22-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1429204,
        "gs_citation": 388,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15936829995906098275&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Cognitive Science, University of California, San Diego; Department of Cognitive Science, University of California, San Diego",
        "aff_domain": "cogsci.ucsd.edu;cogsci.ucsd.edu",
        "email": "cogsci.ucsd.edu;cogsci.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "Department of Cognitive Science",
        "aff_unique_url": "https://ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2517cda453",
        "title": "Bayesian Averaging is Well-Temperated",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/52d080a3e172c33fd6886a37e7288491-Abstract.html",
        "author": "Lars Kai Hansen",
        "abstract": "Bayesian predictions are stochastic just like predictions of any other  inference scheme that generalize from a finite sample.  While a sim(cid:173) ple variational argument shows  that Bayes averaging is  generaliza(cid:173) tion  optimal given  that  the  prior  matches  the  teacher  parameter  distribution the situation is  less  clear  if the  teacher  distribution is  unknown.  I define  a class of averaging procedures,  the temperated  likelihoods,  including  both  Bayes  averaging  with  a  uniform  prior  and  maximum likelihood estimation as  special  cases.  I  show  that  Bayes  is  generalization optimal in this family  for  any  teacher  dis(cid:173) tribution for  two  learning problems that are  analytically tractable:  learning the mean of a Gaussian and asymptotics of smooth learn(cid:173) ers.",
        "bibtex": "@inproceedings{NIPS1999_52d080a3,\n author = {Hansen, Lars},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Bayesian Averaging is Well-Temperated},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/52d080a3e172c33fd6886a37e7288491-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/52d080a3e172c33fd6886a37e7288491-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/52d080a3e172c33fd6886a37e7288491-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1378554,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10654599586128429443&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "Department of Mathematical Modelling, Technical University of Denmark B321, DK-2800 Lyngby, Denmark",
        "aff_domain": "imm.dtu.dk",
        "email": "imm.dtu.dk",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Technical University of Denmark",
        "aff_unique_dep": "Department of Mathematical Modelling",
        "aff_unique_url": "https://www.teknologisk.dk",
        "aff_unique_abbr": "DTU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Lyngby",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Denmark"
    },
    {
        "id": "93660f63bf",
        "title": "Bayesian Map Learning in Dynamic Environments",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/66be31e4c40d676991f2405aaecc6934-Abstract.html",
        "author": "Kevin P. Murphy",
        "abstract": "We consider the problem of learning a grid-based map using a robot  with noisy sensors and actuators. We compare two approaches:  online EM, where the map is treated as a fixed parameter, and  Bayesian inference, where the map is a (matrix-valued) random  variable. We show that even on a very simple example, online EM  can get stuck in local minima, which causes the robot to get \"lost\"  and the resulting map to be useless. By contrast, the Bayesian  approach, by maintaining multiple hypotheses, is much more ro(cid:173) bust. We then introduce a method for approximating the Bayesian  solution, called Rao-Blackwellised particle filtering. We show that  this approximation, when coupled with an active learning strategy,  is fast but accurate.",
        "bibtex": "@inproceedings{NIPS1999_66be31e4,\n author = {Murphy, Kevin P},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Bayesian Map Learning in Dynamic Environments},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/66be31e4c40d676991f2405aaecc6934-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/66be31e4c40d676991f2405aaecc6934-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/66be31e4c40d676991f2405aaecc6934-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1564636,
        "gs_citation": 788,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11799346627233723828&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Computer Science Division, University of California, Berkeley, CA 94720-1776",
        "aff_domain": "cs.berkeley.edu",
        "email": "cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Computer Science Division",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "90e0d56b93",
        "title": "Bayesian Model Selection for Support Vector Machines, Gaussian Processes and Other Kernel Classifiers",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/404dcc91b2aeaa7caa47487d1483e48a-Abstract.html",
        "author": "Matthias Seeger",
        "abstract": "We present a  variational Bayesian method for model selection over  families of kernels classifiers like Support Vector machines or Gaus(cid:173) sian processes.  The algorithm needs no user interaction and is  able  to adapt a large number of kernel parameters to given data without  having to sacrifice training cases for validation.  This opens the pos(cid:173) sibility to use  sophisticated families  of kernels  in  situations  where  the small  \"standard kernel\"  classes  are clearly  inappropriate.  We  relate  the  method  to other work  done  on  Gaussian  processes  and  clarify the  relation between  Support Vector  machines  and  certain  Gaussian process models.",
        "bibtex": "@inproceedings{NIPS1999_404dcc91,\n author = {Seeger, Matthias},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Bayesian Model Selection for Support Vector Machines, Gaussian Processes and Other Kernel Classifiers},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/404dcc91b2aeaa7caa47487d1483e48a-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/404dcc91b2aeaa7caa47487d1483e48a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/404dcc91b2aeaa7caa47487d1483e48a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1704652,
        "gs_citation": 184,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9662184777390811742&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Institute for Adaptive and Neural Computation, University of Edinburgh",
        "aff_domain": "dai.ed.ac.uk",
        "email": "dai.ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Edinburgh",
        "aff_unique_dep": "Institute for Adaptive and Neural Computation",
        "aff_unique_url": "https://www.ed.ac.uk",
        "aff_unique_abbr": "Edinburgh",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Edinburgh",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "c188c52f39",
        "title": "Bayesian Modelling of fMRI lime Series",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/52c5189391854c93e8a0e1326e56c14f-Abstract.html",
        "author": "Pedro A. d. F. R. H\u00f8jen-S\u00f8rensen; Lars Kai Hansen; Carl Edward Rasmussen",
        "abstract": "We present a Hidden Markov Model (HMM) for inferring the hidden  psychological state (or neural activity) during single trial tMRI activa(cid:173) tion experiments with blocked task paradigms. Inference is based on  Bayesian methodology, using a combination of analytical and a variety  of Markov Chain Monte Carlo (MCMC) sampling techniques. The ad(cid:173) vantage of this method is that detection of short time learning effects be(cid:173) tween repeated trials is possible since inference is based only on single  trial experiments.",
        "bibtex": "@inproceedings{NIPS1999_52c51893,\n author = {H\\o jen-S\\o rensen, Pedro and Hansen, Lars and Rasmussen, Carl},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Bayesian Modelling of fMRI lime Series},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/52c5189391854c93e8a0e1326e56c14f-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/52c5189391854c93e8a0e1326e56c14f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/52c5189391854c93e8a0e1326e56c14f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1483319,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=125836632595199653&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Mathematical Modelling, Building 321, Technical University of Denmark, DK-2800 Lyngby, Denmark; Department of Mathematical Modelling, Building 321, Technical University of Denmark, DK-2800 Lyngby, Denmark; Department of Mathematical Modelling, Building 321, Technical University of Denmark, DK-2800 Lyngby, Denmark",
        "aff_domain": "imrn.dtu.dk;imrn.dtu.dk;imrn.dtu.dk",
        "email": "imrn.dtu.dk;imrn.dtu.dk;imrn.dtu.dk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Technical University of Denmark",
        "aff_unique_dep": "Department of Mathematical Modelling",
        "aff_unique_url": "https://www.teknologisk.dk",
        "aff_unique_abbr": "DTU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Lyngby",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Denmark"
    },
    {
        "id": "5ea9f00821",
        "title": "Bayesian Network Induction via Local Neighborhoods",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/5d79099fcdf499f12b79770834c0164a-Abstract.html",
        "author": "Dimitris Margaritis; Sebastian Thrun",
        "abstract": "In  recent  years,  Bayesian  networks  have  become  highly  successful  tool  for  di(cid:173) agnosis,  analysis,  and  decision  making  in  real-world domains.  We  present  an  efficient algorithm for  learning Bayes  networks from  data.  Our approach  con(cid:173) structs Bayesian networks by first identifying each node's Markov blankets, then  connecting nodes  in  a maximally consistent way.  In  contrast to the majority of  work, which typically uses hill-climbing approaches that may produce dense and  causally incorrect nets, our approach yields much more compact causal networks  by heeding independencies in the data.  Compact causal networks facilitate fast in(cid:173) ference and are also easier to understand. We prove that under mild assumptions,  our approach requires time polynomial in the size of the data and the number of  nodes.  A randomized variant,  also presented here,  yields comparable results at  much higher speeds.",
        "bibtex": "@inproceedings{NIPS1999_5d79099f,\n author = {Margaritis, Dimitris and Thrun, Sebastian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Bayesian Network Induction via Local Neighborhoods},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/5d79099fcdf499f12b79770834c0164a-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/5d79099fcdf499f12b79770834c0164a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/5d79099fcdf499f12b79770834c0164a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1790817,
        "gs_citation": 553,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5219976018394736964&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Department of Computer Science, Carnegie Mellon University; Department of Computer Science, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "43d5c11258",
        "title": "Bayesian Reconstruction of 3D Human Motion from Single-Camera Video",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/9fe97fff97f089661135d0487843108e-Abstract.html",
        "author": "Nicholas R. Howe; Michael E. Leventon; William T. Freeman",
        "abstract": "The three-dimensional motion of humans is  underdetermined when  the  observation is  limited to a single camera, due to the inherent 3D ambi(cid:173) guity of 2D video.  We present a system that reconstructs the 3D motion  of human subjects from single-camera video, relying on prior knowledge  about  human motion,  learned  from  training  data,  to  resolve  those am(cid:173) biguities.  After initialization in  2D, the tracking and 3D reconstruction  is  automatic;  we show results  for several  video sequences.  The results  show the power of treating 3D body tracking as an inference problem.",
        "bibtex": "@inproceedings{NIPS1999_9fe97fff,\n author = {Howe, Nicholas and Leventon, Michael and Freeman, William},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Bayesian Reconstruction of 3D Human Motion from Single-Camera Video},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/9fe97fff97f089661135d0487843108e-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/9fe97fff97f089661135d0487843108e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/9fe97fff97f089661135d0487843108e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1302906,
        "gs_citation": 397,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17390495959762003680&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 22,
        "aff": "Department of Computer Science, Cornell University; Artificial Intelligence Lab, Massachusetts Institute of Technology; MERL - a Mitsubishi Electric Research Lab",
        "aff_domain": "cs.comell.edu;ai.mit.edu;merL.com",
        "email": "cs.comell.edu;ai.mit.edu;merL.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Cornell University;Massachusetts Institute of Technology;Mitsubishi Electric Research Laboratory",
        "aff_unique_dep": "Department of Computer Science;Artificial Intelligence Lab;",
        "aff_unique_url": "https://www.cornell.edu;https://www.mit.edu;https://www.merl.com",
        "aff_unique_abbr": "Cornell;MIT;MERL",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c74a017b07",
        "title": "Bayesian Transduction",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/a51c896c9cb81ecb5a199d51ac9fc3c5-Abstract.html",
        "author": "Thore Graepel; Ralf Herbrich; Klaus Obermayer",
        "abstract": "Transduction  is  an  inference  principle  that  takes  a  training sam(cid:173) ple  and aims at estimating the values of a function  at given points  contained in the so-called working sample as  opposed  to the whole  of input space  for  induction.  Transduction  provides  a  confidence  measure  on  single  predictions  rather  than  classifiers  - a  feature  particularly important for  risk-sensitive  applications.  The possibly  infinite number of functions  is  reduced  to a finite  number of equiv(cid:173) alence classes on the working sample.  A rigorous Bayesian analysis  reveals  that for  standard classification loss  we  cannot benefit  from  considering  more  than  one  test  point  at  a  time.  The  probability  of  the  label  of a  given  test  point  is  determined  as  the  posterior  measure of the corresponding subset  of hypothesis  space.  We  con(cid:173) sider the PAC setting of binary classification by linear discriminant  functions  (perceptrons)  in kernel space such that the probability of  labels  is  determined  by  the  volume  ratio  in  version  space.  We  suggest  to sample this  region  by  an  ergodic  billiard.  Experimen(cid:173) tal  results  on  real  world data indicate that Bayesian Transduction  compares  favourably  to  the  well-known  Support  Vector  Machine,  in  particular  if the  posterior  probability  of labellings  is  used  as  a  confidence  measure to exclude test  points of low  confidence.",
        "bibtex": "@inproceedings{NIPS1999_a51c896c,\n author = {Graepel, Thore and Herbrich, Ralf and Obermayer, Klaus},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Bayesian Transduction},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/a51c896c9cb81ecb5a199d51ac9fc3c5-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/a51c896c9cb81ecb5a199d51ac9fc3c5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/a51c896c9cb81ecb5a199d51ac9fc3c5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1489680,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7976094323982325156&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, Technical University of Berlin; Department of Computer Science, Technical University of Berlin; Department of Computer Science, Technical University of Berlin",
        "aff_domain": "cs.tu-berlin.de;cs.tu-berlin.de;cs.tu-berlin.de",
        "email": "cs.tu-berlin.de;cs.tu-berlin.de;cs.tu-berlin.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Technical University of Berlin",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.tu-berlin.de",
        "aff_unique_abbr": "TU Berlin",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berlin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "b2d4e5c81f",
        "title": "Better Generative Models for Sequential Data Problems: Bidirectional Recurrent Mixture Density Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/6709e8d64a5f47269ed5cea9f625f7ab-Abstract.html",
        "author": "Mike Schuster",
        "abstract": "This  paper  describes  bidirectional  recurrent  mixture  density  net(cid:173) works,  which  can  model  multi-modal  distributions  of  the  type  P(Xt Iyf)  and  P(Xt lXI, X2 , ... ,Xt-l, yf) without  any  explicit  as(cid:173) sumptions  about  the  use  of context .  These  expressions  occur  fre(cid:173) quently  in  pattern  recognition  problems  with  sequential  data,  for  example  in  speech  recognition.  Experiments  show  that  the  pro(cid:173) posed generative models give  a higher likelihood on test data com(cid:173) pared to a  traditional modeling approach, indicating that they can  summarize  the statistical  properties of the data better.",
        "bibtex": "@inproceedings{NIPS1999_6709e8d6,\n author = {Schuster, Mike},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Better Generative Models for Sequential Data Problems: Bidirectional Recurrent Mixture Density Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/6709e8d64a5f47269ed5cea9f625f7ab-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/6709e8d64a5f47269ed5cea9f625f7ab-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/6709e8d64a5f47269ed5cea9f625f7ab-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1403579,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17750397960217784226&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "ATR Interpreting Telecommunications Research Laboratories",
        "aff_domain": "itl.atr.co.jp",
        "email": "itl.atr.co.jp",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "ATR Interpreting Telecommunications Research Laboratories",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.atr.jp",
        "aff_unique_abbr": "ATR",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2e215109af",
        "title": "Bifurcation Analysis of a Silicon Neuron",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/d43ab110ab2489d6b9b2caa394bf920f-Abstract.html",
        "author": "Girish N. Patel; Gennady S. Cymbalyuk; Ronald L. Calabrese; Stephen P. DeWeerth",
        "abstract": "We  have developed a VLSI  silicon  neuron  and  a corresponding  mathe(cid:173) matical  model  that  is  a two  state-variable system. We  describe the  cir(cid:173) cuit implementation and compare the  behaviors  observed in  the  silicon  neuron and the mathematical model. We also perform bifurcation analy(cid:173) sis of the mathematical model by  varying the externally applied current  and show that the behaviors exhibited by the silicon neuron under corre(cid:173) sponding  conditions  are  in  good  agreement  to  those  predicted  by  the  bifurcation analysis.",
        "bibtex": "@inproceedings{NIPS1999_d43ab110,\n author = {Patel, Girish and Cymbalyuk, Gennady and Calabrese, Ronald and DeWeerth, Stephen},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Bifurcation Analysis of a Silicon Neuron},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/d43ab110ab2489d6b9b2caa394bf920f-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/d43ab110ab2489d6b9b2caa394bf920f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/d43ab110ab2489d6b9b2caa394bf920f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1270022,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14662081406092193901&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "School of Electrical and Computer Engineering, Georgia Institute of Technology; Department of Biology, Emory University + Institute of Mathematical Problems in Biology RAS; Department of Biology, Emory University; School of Electrical and Computer Engineering, Georgia Institute of Technology",
        "aff_domain": "ece.gatech.edu;ece.gatech.edu;biology.emory.edu;biology.emory.edu",
        "email": "ece.gatech.edu;ece.gatech.edu;biology.emory.edu;biology.emory.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2;1;0",
        "aff_unique_norm": "Georgia Institute of Technology;Emory University;Institute of Mathematical Problems in Biology",
        "aff_unique_dep": "School of Electrical and Computer Engineering;Department of Biology;RAS",
        "aff_unique_url": "https://www.gatech.edu;https://www.emory.edu;http://www.imppb.ru",
        "aff_unique_abbr": "Georgia Tech;Emory;IMPPB",
        "aff_campus_unique_index": "0;;0",
        "aff_campus_unique": "Atlanta;",
        "aff_country_unique_index": "0;0+1;0;0",
        "aff_country_unique": "United States;Russian Federation"
    },
    {
        "id": "fe1b70aaa8",
        "title": "Boosting Algorithms as Gradient Descent",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/96a93ba89a5b5c6c226e49b88973f46e-Abstract.html",
        "author": "Llew Mason; Jonathan Baxter; Peter L. Bartlett; Marcus R. Frean",
        "abstract": "We  provide an abstract characterization of boosting algorithms as  gradient  decsent  on  cost-functionals  in  an  inner-product  function  space.  We  prove convergence  of these functional-gradient-descent  algorithms under quite  weak conditions.  Following previous theo(cid:173) retical  results  bounding the generalization  performance of convex  combinations of classifiers in terms of general cost functions of the  margin,  we  present  a  new  algorithm  (DOOM  II)  for  performing a  gradient descent  optimization of such cost functions.  Experiments  on  several  data sets  from  the  UC  Irvine  repository  demonstrate  that DOOM II generally outperforms AdaBoost, especially in high  noise situations, and that the overfitting behaviour of AdaBoost is  predicted by our cost functions.",
        "bibtex": "@inproceedings{NIPS1999_96a93ba8,\n author = {Mason, Llew and Baxter, Jonathan and Bartlett, Peter and Frean, Marcus},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Boosting Algorithms as Gradient Descent},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/96a93ba89a5b5c6c226e49b88973f46e-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/96a93ba89a5b5c6c226e49b88973f46e-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/96a93ba89a5b5c6c226e49b88973f46e-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1493678,
        "gs_citation": 1647,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1306987912133278631&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Research School of Information Sciences and Engineering, Australian National University, Canberra, ACT, 0200, Australia; Research School of Information Sciences and Engineering, Australian National University, Canberra, ACT, 0200, Australia; Research School of Information Sciences and Engineering, Australian National University, Canberra, ACT, 0200, Australia; Department of Computer Science and Electrical Engineering, The University of Queensland, Brisbane, QLD, 4072, Australia",
        "aff_domain": "syseng.anu.edu.au;anu.edu.au;anu.edu.au;elec.uq.edu.au",
        "email": "syseng.anu.edu.au;anu.edu.au;anu.edu.au;elec.uq.edu.au",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Australian National University;University of Queensland",
        "aff_unique_dep": "Research School of Information Sciences and Engineering;Department of Computer Science and Electrical Engineering",
        "aff_unique_url": "https://www.anu.edu.au;https://www.uq.edu.au",
        "aff_unique_abbr": "ANU;UQ",
        "aff_campus_unique_index": "0;0;0;1",
        "aff_campus_unique": "Canberra;Brisbane",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "d3f285d782",
        "title": "Boosting with Multi-Way Branching in Decision Trees",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/e1696007be4eefb81b1a1d39ce48681b-Abstract.html",
        "author": "Yishay Mansour; David A. McAllester",
        "abstract": "It  is  known  that  decision  tree  learning  can  be  viewed  as  a  form  of boosting.  However,  existing boosting theorems for  decision tree  learning allow only binary-branching trees and the generalization to  multi-branching trees  is  not immediate.  Practical decision tree al(cid:173) gorithms, such  as CART and C4.5, implement a  trade-off between  the  number  of branches  and  the  improvement  in  tree  quality  as  measured  by an index function.  Here  we  give a  boosting justifica(cid:173) tion for a particular quantitative trade-off curve.  Our main theorem  states,  in  essence,  that if we  require  an  improvement proportional  to the  log  of the  number of branches  then  top-down  greedy  con(cid:173) struction of decision trees  remains an effective boosting algorithm.",
        "bibtex": "@inproceedings{NIPS1999_e1696007,\n author = {Mansour, Yishay and McAllester, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Boosting with Multi-Way Branching in Decision Trees},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e1696007be4eefb81b1a1d39ce48681b-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/e1696007be4eefb81b1a1d39ce48681b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/e1696007be4eefb81b1a1d39ce48681b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1570615,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15750651175741276577&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "AT&T Labs-Research; AT&T Labs-Research",
        "aff_domain": "research.att.com;research.att.com",
        "email": "research.att.com;research.att.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "AT&T Labs",
        "aff_unique_dep": "Research",
        "aff_unique_url": "https://www.att.com/labs/research",
        "aff_unique_abbr": "AT&T Labs",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "674921ea46",
        "title": "Broadband Direction-Of-Arrival Estimation Based on Second Order Statistics",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/11d0e6287202fced83f79975ec59a3a6-Abstract.html",
        "author": "Justinian P. Rosca; Joseph \u00d3 Ruanaidh; Alexander Jourjine; Scott Rickard",
        "abstract": "N  wideband  sources  recorded  using  N  closely  spaced  receivers  can  feasibly  be separated  based  only  on  second  order statistics when  using  a physical model  of the mixing process.  In  this case  we  show that  the  parameter estimation problem can  be essentially reduced to considering  directions of arrival and attenuations of each signal.  The paper presents  two demixing methods operating in  the time and frequency  domain and  experimentally shows that it is always possible to demix signals arriving at  different angles.  Moreover,  one can use spatial cues to solve the channel  selection  problem  and  a post-processing Wiener filter  to  ameliorate the  artifacts caused by demixing.",
        "bibtex": "@inproceedings{NIPS1999_11d0e628,\n author = {Rosca, Justinian and Ruanaidh, Joseph and Jourjine, Alexander and Rickard, Scott},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Broadband Direction-Of-Arrival Estimation Based on Second Order Statistics},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/11d0e6287202fced83f79975ec59a3a6-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/11d0e6287202fced83f79975ec59a3a6-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/11d0e6287202fced83f79975ec59a3a6-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1346177,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5339786234830640328&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "e93572d599",
        "title": "Building Predictive Models from Fractal Representations of Symbolic Sequences",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/17ed8abedc255908be746d245e50263a-Abstract.html",
        "author": "Peter Ti\u00f1o; Georg Dorffner",
        "abstract": "We propose a novel approach for building finite memory predictive mod(cid:173) els similar in spirit to variable memory length Markov models (VLMMs).  The models are constructed by first transforming the n-block structure of  the training sequence into a spatial structure of points in a unit hypercube,  such that the  longer is the common suffix shared by any  two n-blocks,  the closer lie their point representations. Such a transformation embodies  a Markov assumption - n-blocks with long common suffixes  are likely  to produce similar continuations.  Finding a set of prediction contexts is  formulated as a resource allocation problem solved by vector quantizing  the spatial n-block representation. We compare our model with both the  classical  and variable memory length Markov models on three data sets  with different memory  and stochastic components.  Our models have  a  superior performance, yet, their construction is fully automatic, which is  shown to be problematic in the case of VLMMs.",
        "bibtex": "@inproceedings{NIPS1999_17ed8abe,\n author = {Ti\\~{n}o, Peter and Dorffner, Georg},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Building Predictive Models from Fractal Representations of Symbolic Sequences},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/17ed8abedc255908be746d245e50263a-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/17ed8abedc255908be746d245e50263a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/17ed8abedc255908be746d245e50263a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1598245,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11374104109899816896&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "4678ab5615",
        "title": "Can VI Mechanisms Account for Figure-Ground and Medial Axis Effects?",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/442cde81694ca09a626eeddefd1b74ca-Abstract.html",
        "author": "Zhaoping Li",
        "abstract": "When a  visual image consists of a figure against a background, V1  cells are physiologically observed to give higher responses to image  regions  corresponding  to  the figure  relative  to  their  responses  to  the  background.  The  medial  axis  of the figure  also  induces  rela(cid:173) tively  higher  responses  compared  to responses  to  other  locations  in the figure  (except for  the boundary between  the figure  and the  background).  Since the receptive fields  of V1  cells  are very smal(cid:173) l  compared with  the global  scale  of the figure-ground  and  medial  axis effects, it has been suggested that these effects  may be caused  by feedback from  higher visual areas.  I show how these effects can  be  accounted  for  by  V1  mechanisms  when  the  size  of the  figure  is  small  or  is  of a  certain scale.  They are  a  manifestation  of the  processes of pre-attentive segmentation which detect and highlight  the boundaries between homogeneous image regions.",
        "bibtex": "@inproceedings{NIPS1999_442cde81,\n author = {Li, Zhaoping},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Can VI Mechanisms Account for Figure-Ground and Medial Axis Effects?},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/442cde81694ca09a626eeddefd1b74ca-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/442cde81694ca09a626eeddefd1b74ca-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/442cde81694ca09a626eeddefd1b74ca-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1646390,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4750624566772157058&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "184a3ba3da",
        "title": "Channel Noise in Excitable Neural Membranes",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/2612aa892d962d6f8056b195ca6e550d-Abstract.html",
        "author": "Amit Manwani; Peter N. Steinmetz; Christof Koch",
        "abstract": "Stochastic  fluctuations  of voltage-gated  ion  channels  generate  current  and  voltage  noise  in  neuronal  membranes.  This  noise  may  be  a  criti(cid:173) cal  determinant of the efficacy of information processing within  neural  systems.  Using Monte-Carlo simulations, we carry out a systematic in(cid:173) vestigation of the relationship  between channel  kinetics  and  the result(cid:173) ing membrane  voltage  noise  using  a  stochastic  Markov  version  of the  Mainen-Sejnowski  model  of dendritic  excitability  in  cortical  neurons.  Our simulations show that kinetic  parameters which  lead to an increase  in  membrane excitability (increasing channel densities, decreasing tem(cid:173) perature)  also  lead to  an  increase in  the magnitude of the sub-threshold  voltage noise. Noise also increases as the membrane is depolarized from  rest towards threshold.  This  suggests that channel fluctuations  may  in(cid:173) terfere with a neuron's ability to function as an integrator of its synaptic  inputs and may limit the reliability  and precision of neural  information  processing.",
        "bibtex": "@inproceedings{NIPS1999_2612aa89,\n author = {Manwani, Amit and Steinmetz, Peter and Koch, Christof},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Channel Noise in Excitable Neural Membranes},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/2612aa892d962d6f8056b195ca6e550d-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/2612aa892d962d6f8056b195ca6e550d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/2612aa892d962d6f8056b195ca6e550d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1553873,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10147159117404274848&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Computation and Neural Systems Program, M-S 139-74 California Institute of Technology Pasadena, CA 91125; Computation and Neural Systems Program, M-S 139-74 California Institute of Technology Pasadena, CA 91125; Computation and Neural Systems Program, M-S 139-74 California Institute of Technology Pasadena, CA 91125",
        "aff_domain": "klab.caltech.edu;klab.caltech.edu;klab.caltech.edu",
        "email": "klab.caltech.edu;klab.caltech.edu;klab.caltech.edu",
        "github": "",
        "project": "http://www.klab.caltech.edwquixote",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "California Institute of Technology",
        "aff_unique_dep": "Computation and Neural Systems Program",
        "aff_unique_url": "https://www.caltech.edu",
        "aff_unique_abbr": "Caltech",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Pasadena",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "bc07e87c2d",
        "title": "Churn Reduction in the Wireless Industry",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/fc9b003bb003a298c2ad0d05e4342bdc-Abstract.html",
        "author": "Michael Mozer; Richard H. Wolniewicz; David B. Grimes; Eric Johnson; Howard Kaushansky",
        "abstract": "Competition  in  the  wireless telecommunications industry  is  rampant.  To main(cid:173) tain  profitability,  wireless  carriers  must  control  chum,  the  loss  of subscribers  who  switch  from  one  carrier  to  another.  We  explore  statistical  techniques  for  chum prediction and, based on these predictions. an optimal policy for  identify(cid:173) ing  customers to  whom  incentives  should be offered to  increase  retention.  Our  experiments are  based on  a data base of nearly 47,000 U.S. domestic subscrib(cid:173) ers,  and  includes information about  their  usage,  billing,  credit, application,  and  complaint history. We show that under a wide variety of assumptions concerning  the cost of intervention and the retention  rate resulting from  intervention, chum  prediction  and  remediation  can  yield  significant  savings  to  a  carrier.  We  also  show the importance of a data representation crafted by domain experts. \n\nCompetition  in  the  wireless  telecommunications industry  is  rampant.  As many  as  seven  competing carriers operate in  each market.  The industry  is  extremely dynamic,  with  new  services,  technologies,  and  carriers  constantly  altering  the  landscape.  Carriers  announce  new rates and incentives weekly, hoping to entice new subscribers and to lure subscribers  away  from  the  competition.  The extent of rivalry  is  reflected  in  the  deluge  of advertise(cid:173) ments for wireless service in the daily newspaper and other mass media. \n\nThe United States had 69 million  wireless subscribers in  1998, roughly  25% of the \n\npopulation. Some markets are further developed; for example, the subscription rate in Fin(cid:173) land is 53%.  Industry  forecasts  are for a U.S.  penetration rate of 48%  by  2003. Although  there is significant room for growth in most markets, the industry growth rate is declining  and competition is rising. Consequently, it has become crucial for wireless carriers to con(cid:173) trol  chum-the  loss  of customers  who  switch  from  one  carrier  to  another.  At  present,  domestic monthly chum rates are 2-3%  of the  customer base.  At an average cost of $400  to acquire a subscriber, churn cost the industry nearly $6.3 bilIion in  1998; the total annual  loss rose to nearly $9.6 billion when lost monthly revenue from subscriber cancellations is  considered (Luna,  1998).  It  costs roughly five  times as much to  sign on a  new subscriber  as to retain an existing one. Consequently, for a carrier with 1.5 milIion subscribers, reduc(cid:173) ing the monthly churn' rate from 2% to 1 % would yield an increase in annual earnings of at  least  $54  milIion,  and  an  increase  in  shareholder  value  of approximately  $150  million.  (Estimates are even higher when lost monthly revenue is considered; see Fowlkes, Madan,  Andrew, &  Jensen,  1999; Luna,  1998.) \n\nThe goal  of our research is  to  evaluate  the  benefits  of predicting churn  using tech(cid:173)\n\nniques from statistical  machine learning. We designed models that predict the probability \n\n\n936 \n\nM.  C.  Mozer,  R.  Wolniewicz.  D.  B.  Grimes. E.  Johnson  and H.  Kaushansky \n\nof a subscriber churning within a short time window, and we evaluated how well these pre(cid:173) dictions  could  be  used  for  decision  making  by  estimating  potential  cost  savings  to  the  wireless carrier under a variety of assumptions concerning subscriber behavior.",
        "bibtex": "@inproceedings{NIPS1999_fc9b003b,\n author = {Mozer, Michael C and Wolniewicz, Richard and Grimes, David and Johnson, Eric and Kaushansky, Howard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Churn Reduction in the Wireless Industry},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/fc9b003bb003a298c2ad0d05e4342bdc-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/fc9b003bb003a298c2ad0d05e4342bdc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/fc9b003bb003a298c2ad0d05e4342bdc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1707295,
        "gs_citation": 81,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4823818340499591107&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Athene Software + Department of Computer Science; Athene Software; Athene Software + Department of Computer Science; Athene Software; Athene Software",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0+1;0;0",
        "aff_unique_norm": "Athene Software;Unknown Institution",
        "aff_unique_dep": ";Department of Computer Science",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";",
        "aff_country_unique": ""
    },
    {
        "id": "6a1d2ac90f",
        "title": "Coastal Navigation with Mobile Robots",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/df9028fcb6b065e000ffe8a4f03eeb38-Abstract.html",
        "author": "Nicholas Roy; Sebastian Thrun",
        "abstract": "The  problem  that  we  address  in  this paper  is  how  a mobile robot can  plan  in  order  to  arrive  at  its  goal  with  minimum  uncertainty.  Traditional  motion  planning algo(cid:173) rithms often assume that a mobile robot can track its position reliably, however, in real  world situations, reliable localization may not always be feasible.  Partially Observable  Markov Decision Processes (POMDPs) provide one way to maximize the certainty of  reaching  the goal  state,  but at the cost of computational  intractability for large state  spaces.  The  method  we propose  explicitly  models  the  uncertainty of the  robot's position as  a  state  variable,  and  generates  trajectories  through  the  augmented  pose-uncertainty  space.  By  minimizing  the  positional  uncertainty  at  the  goal,  the  robot reduces  the  likelihood  it  becomes  lost.  We  demonstrate  experimentally  that  coastal  navigation  reduces the uncertainty at the goal, especially with degraded localization.",
        "bibtex": "@inproceedings{NIPS1999_df9028fc,\n author = {Roy, Nicholas and Thrun, Sebastian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Coastal Navigation with Mobile Robots},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/df9028fcb6b065e000ffe8a4f03eeb38-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/df9028fcb6b065e000ffe8a4f03eeb38-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/df9028fcb6b065e000ffe8a4f03eeb38-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1588348,
        "gs_citation": 249,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7078614329247050351&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 27,
        "aff": "School of Computer Science, Carnegie Mellon University; School of Computer Science, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "fa50683ad9",
        "title": "Constrained Hidden Markov Models",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/84c6494d30851c63a55cdb8cb047fadd-Abstract.html",
        "author": "Sam T. Roweis",
        "abstract": "By  thinking  of each  state in  a hidden Markov  model  as  corresponding  to  some  spatial region of a fictitious topology space it is possible to naturally define neigh(cid:173) bouring states as  those which are connected in that space.  The transition matrix  can then be constrained to allow transitions only between neighbours; this means  that all valid state sequences correspond to connected paths in the topology space.  I show  how  such constrained HMMs  can learn to  discover underlying  structure  in complex sequences  of high dimensional data,  and apply  them  to  the problem  of recovering mouth movements from acoustics in continuous speech.",
        "bibtex": "@inproceedings{NIPS1999_84c6494d,\n author = {Roweis, Sam},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Constrained Hidden Markov Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/84c6494d30851c63a55cdb8cb047fadd-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/84c6494d30851c63a55cdb8cb047fadd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/84c6494d30851c63a55cdb8cb047fadd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1961905,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16461038351466736225&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Gatsby Unit, University College London",
        "aff_domain": "gatsby.ucl.ac.uk",
        "email": "gatsby.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University College London",
        "aff_unique_dep": "Gatsby Unit",
        "aff_unique_url": "https://www.ucl.ac.uk",
        "aff_unique_abbr": "UCL",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "1924f4d3b2",
        "title": "Constructing Heterogeneous Committees Using Input Feature Grouping: Application to Economic Forecasting",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/a588a6199feff5ba48402883d9b72700-Abstract.html",
        "author": "Yuansong Liao; John E. Moody",
        "abstract": "The  committee  approach  has  been  proposed  for  reducing  model  uncertainty  and  improving  generalization  performance.  The  ad(cid:173) vantage of committees depends  on  (1)  the performance of individ(cid:173) ual members  and  (2)  the correlational structure of errors between  members.  This paper presents an input grouping technique for  de(cid:173) signing a  heterogeneous  committee.  With  this  technique,  all input  variables are first grouped based on their mutual information.  Sta(cid:173) tistically  similar  variables  are  assigned  to  the  same  group.  Each  member's  input  set  is  then  formed  by  input  variables  extracted  from different groups.  Our designed  committees have less error cor(cid:173) relation between its members, since each member observes different  input variable combinations.  The individual member's feature sets  contain less redundant information, because highly correlated vari(cid:173) ables will not be combined together.  The member feature sets con(cid:173) tain almost complete information, since each set contains a feature  from  each  information group.  An  empirical study for  a  noisy  and  nonstationary  economic  forecasting  problem  shows  that  commit(cid:173) tees constructed by our proposed technique outperform committees  formed using several existing techniques.",
        "bibtex": "@inproceedings{NIPS1999_a588a619,\n author = {Liao, Yuansong and Moody, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Constructing Heterogeneous Committees Using Input Feature Grouping: Application to Economic Forecasting},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/a588a6199feff5ba48402883d9b72700-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/a588a6199feff5ba48402883d9b72700-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/a588a6199feff5ba48402883d9b72700-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1457128,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3284785579196894793&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, Oregon Graduate Institute; Department of Computer Science, Oregon Graduate Institute",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Oregon Graduate Institute",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ogi.edu",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "70df4fe074",
        "title": "Correctness of Belief Propagation in Gaussian Graphical Models of Arbitrary Topology",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/10c272d06794d3e5785d5e7c5356e9ff-Abstract.html",
        "author": "Yair Weiss; William T. Freeman",
        "abstract": "Local \"belief propagation\" rules of the  sort proposed by Pearl  [15]  are  guaranteed  to  converge to  the  correct  posterior probabilities  in  singly  connected graphical models. Recently, a number of researchers have em(cid:173) pirically demonstrated good performance of \"loopy belief propagation\"(cid:173) using these same rules on graphs with loops.  Perhaps the most dramatic  instance is the near Shannon-limit performance of \"Turbo codes\", whose  decoding algorithm is equivalent to loopy belief propagation.  Except for the case of graphs with a single loop, there has been little theo(cid:173) retical understanding of the performance of loopy propagation. Here we  analyze belief propagation in  networks  with  arbitrary  topologies  when  the nodes in  the graph describe jointly Gaussian random variables.  We  give an analytical formula relating the true  posterior probabilities with  those calculated using loopy propagation.  We give sufficient conditions  for convergence and show that when belief propagation converges it gives  the correct posterior means for all graph  topologies,  not just networks  with a single loop.  The related \"max-product\" belief propagation algorithm finds  the max(cid:173) imum posterior probability estimate for singly connected networks.  We  show  that,  even for non-Gaussian probability distributions,  the  conver(cid:173) gence points of the max-product algorithm in loopy  networks are  max(cid:173) ima  over a  particular large  local  neighborhood of the  posterior proba(cid:173) bility.  These results  help clarify  the  empirical performance results  and  motivate  using  the  powerful  belief propagation algorithm in  a  broader  class of networks.",
        "bibtex": "@inproceedings{NIPS1999_10c272d0,\n author = {Weiss, Yair and Freeman, William},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Correctness of Belief Propagation in Gaussian Graphical Models of Arbitrary Topology},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/10c272d06794d3e5785d5e7c5356e9ff-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/10c272d06794d3e5785d5e7c5356e9ff-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/10c272d06794d3e5785d5e7c5356e9ff-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1542819,
        "gs_citation": 816,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11640209110786264725&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 32,
        "aff": "Computer Science Division, UC Berkeley; Mitsubishi Electric Research Lab",
        "aff_domain": "cs.berkeley.edu;merl.com",
        "email": "cs.berkeley.edu;merl.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of California, Berkeley;Mitsubishi Electric Research Laboratories",
        "aff_unique_dep": "Computer Science Division;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.merl.com",
        "aff_unique_abbr": "UC Berkeley;MERL",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "552ef5e64b",
        "title": "Data Visualization and Feature Selection: New Algorithms for Nongaussian Data",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/8c01a75941549a705cf7275e41b21f0d-Abstract.html",
        "author": "Howard Hua Yang; John Moody",
        "abstract": "Data  visualization  and  feature  selection  methods  are  proposed  based on the )oint mutual information and ICA.  The visualization  methods can find  many good 2-D  projections for  high dimensional  data interpretation,  which  cannot be easily found by  the other ex(cid:173) isting methods.  The new  variable selection  method is found  to be  better in eliminating redundancy in the inputs than other methods  based  on  simple mutual information.  The efficacy  of the  methods  is illustrated on a radar signal analysis problem to find  2-D viewing  coordinates for  data visualization and to select  inputs for  a  neural  network  classifier.  Keywords:  feature  selection,  joint mutual information,  ICA,  vi(cid:173) sualization, classification.",
        "bibtex": "@inproceedings{NIPS1999_8c01a759,\n author = {Yang, Howard and Moody, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Data Visualization and Feature Selection: New Algorithms for Nongaussian Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/8c01a75941549a705cf7275e41b21f0d-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/8c01a75941549a705cf7275e41b21f0d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/8c01a75941549a705cf7275e41b21f0d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1442313,
        "gs_citation": 496,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7070864657105106284&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Oregon Graduate Institute of Science and Technology; Oregon Graduate Institute of Science and Technology",
        "aff_domain": "ece.ogi.edu;cse.ogi.edu",
        "email": "ece.ogi.edu;cse.ogi.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Oregon Health & Science University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ohsu.edu",
        "aff_unique_abbr": "OHSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a856d2518a",
        "title": "Differentiating Functions of the Jacobian with Respect to the Weights",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/b9f94c77652c9a76fc8a442748cd54bd-Abstract.html",
        "author": "Gary William Flake; Barak A. Pearlmutter",
        "abstract": "For many problems, the correct behavior of a model depends not only on  its input-output mapping but also on properties of its Jacobian matrix, the  matrix of partial derivatives of the model's outputs with respect to its in(cid:173) puts.  We introduce the J-prop algorithm, an efficient general method for  computing the exact partial derivatives of a variety of simple functions of  the Jacobian of a model with respect to its free parameters. The algorithm  applies to any parametrized feedforward model,  including nonlinear re(cid:173) gression, multilayer perceptrons, and radial basis function networks.",
        "bibtex": "@inproceedings{NIPS1999_b9f94c77,\n author = {Flake, Gary and Pearlmutter, Barak},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Differentiating Functions of the Jacobian with Respect to the Weights},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b9f94c77652c9a76fc8a442748cd54bd-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/b9f94c77652c9a76fc8a442748cd54bd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/b9f94c77652c9a76fc8a442748cd54bd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1332355,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5776100876789944017&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 23,
        "aff": "NEC Research Institute; Dept of Computer Science, FEC 313, University of New Mexico",
        "aff_domain": "research.nj.nec.com;cs.unm.edu",
        "email": "research.nj.nec.com;cs.unm.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "NEC Research Institute;University of New Mexico",
        "aff_unique_dep": ";Dept of Computer Science",
        "aff_unique_url": "https://www.neclab.eu;https://www.unm.edu",
        "aff_unique_abbr": "NEC RI;UNM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f540b598d6",
        "title": "Distributed Synchrony of Spiking Neurons in a Hebbian Cell Assembly",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/375c71349b295fbe2dcdca9206f20a06-Abstract.html",
        "author": "David Horn; Nir Levy; Isaac Meilijson; Eytan Ruppin",
        "abstract": "We  investigate the behavior of a  Hebbian  cell  assembly of spiking  neurons formed via a temporal synaptic learning curve.  This learn(cid:173) ing  function  is  based  on  recent  experimental findings .  It  includes  potentiation for  short time delays  between  pre- and  post-synaptic  neuronal spiking,  and depression for  spiking events occuring in the  reverse order.  The coupling  between the dynamics of the synaptic  learning and of the neuronal activation leads to interesting results.  We  find  that  the  cell  assembly  can  fire  asynchronously,  but  may  also  function  in  complete  synchrony,  or  in  distributed  synchrony.  The latter implies spontaneous division of the Hebbian cell  assem(cid:173) bly into groups of cells  that fire  in  a cyclic manner.  We  invetigate  the behavior of distributed synchrony both by  simulations and by  analytic calculations of the resulting synaptic distributions.",
        "bibtex": "@inproceedings{NIPS1999_375c7134,\n author = {Horn, David and Levy, Nir and Meilijson, Isaac and Ruppin, Eytan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Distributed Synchrony of Spiking Neurons in a Hebbian Cell Assembly},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/375c71349b295fbe2dcdca9206f20a06-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/375c71349b295fbe2dcdca9206f20a06-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/375c71349b295fbe2dcdca9206f20a06-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1497723,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7003704299018531521&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "School of Physics and Astronomy, Raymond and Beverly Sackler Faculty of Exact Sciences, Tel Aviv University, Tel Aviv 69978, Israel; School of Physics and Astronomy, Raymond and Beverly Sackler Faculty of Exact Sciences, Tel Aviv University, Tel Aviv 69978, Israel; School of Mathematical Sciences, Raymond and Beverly Sackler Faculty of Exact Sciences, Tel Aviv University, Tel Aviv 69978, Israel; School of Mathematical Sciences, Raymond and Beverly Sackler Faculty of Exact Sciences, Tel Aviv University, Tel Aviv 69978, Israel",
        "aff_domain": "neuron.tau.ac.il;post.tau.ac.il;math.tau.ac.il;math.tau.ac.il",
        "email": "neuron.tau.ac.il;post.tau.ac.il;math.tau.ac.il;math.tau.ac.il",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Tel Aviv University",
        "aff_unique_dep": "School of Physics and Astronomy",
        "aff_unique_url": "https://www.tau.ac.il",
        "aff_unique_abbr": "TAU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Tel Aviv",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "db04b5b34e",
        "title": "Dual Estimation and the Unscented Transformation",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/f50a6c02a3fc5a3a5d4d9391f05f3efc-Abstract.html",
        "author": "Eric A. Wan; Rudolph van der Merwe; Alex T. Nelson",
        "abstract": "Dual estimation  refers  to  the problem of simultaneously  estimating the  state of a dynamic system and the model which gives rise  to the dynam(cid:173) ics.  Algorithms  include  expectation-maximization (EM), dual  Kalman  filtering,  and joint Kalman methods.  These methods have recently  been  explored in  the context of nonlinear modeling,  where a  neural network  is  used  as  the functional form of the  unknown model.  Typically, an  ex(cid:173) tended  Kalman  filter  (EKF)  or smoother  is  used  for  the  part of the  al(cid:173) gorithm that estimates the clean state given the current estimated model.  An  EKF may  also be used to estimate the weights of the  network.  This  paper points  out the flaws  in  using  the  EKF,  and  proposes an  improve(cid:173) ment based on a new approach called the unscented transformation (UT)  [3].  A  substantial performance gain is  achieved  with  the same order of  computational complexity as  that of the standard EKF.  The approach is  illustrated on several dual estimation methods.",
        "bibtex": "@inproceedings{NIPS1999_f50a6c02,\n author = {Wan, Eric and van der Merwe, Rudolph and Nelson, Alex},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Dual Estimation and the Unscented Transformation},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/f50a6c02a3fc5a3a5d4d9391f05f3efc-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/f50a6c02a3fc5a3a5d4d9391f05f3efc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/f50a6c02a3fc5a3a5d4d9391f05f3efc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1453862,
        "gs_citation": 393,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14643384839538734713&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Oregon Graduate Institute of Science & Technology, Department of Electrical and Computer Engineering; Oregon Graduate Institute of Science & Technology, Department of Electrical and Computer Engineering; Oregon Graduate Institute of Science & Technology, Department of Electrical and Computer Engineering",
        "aff_domain": "ece.ogi.edu;ece.ogi.edu;ece.ogi.edu",
        "email": "ece.ogi.edu;ece.ogi.edu;ece.ogi.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Oregon Graduate Institute of Science & Technology",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.ogi.edu",
        "aff_unique_abbr": "OGI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0cbe974891",
        "title": "Dynamics of Supervised Learning with Restricted Training Sets and Noisy Teachers",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/b59a51a3c0bf9c5228fde841714f523a-Abstract.html",
        "author": "Anthony C. C. Coolen; C. W. H. Mace",
        "abstract": "We generalize a recent formalism to describe the dynamics of supervised  learning in layered neural networks, in the regime where data recycling  is  inevitable, to the case of noisy teachers.  Our theory generates reliable  predictions for the  evolution in  time of training- and generalization er(cid:173) rors, and extends the class of mathematically solvable learning processes  in large neural networks to those situations where overfitting can occur.",
        "bibtex": "@inproceedings{NIPS1999_b59a51a3,\n author = {Coolen, Anthony and Mace, C.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Dynamics of Supervised Learning with Restricted Training Sets and Noisy Teachers},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b59a51a3c0bf9c5228fde841714f523a-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/b59a51a3c0bf9c5228fde841714f523a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/b59a51a3c0bf9c5228fde841714f523a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1525678,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15757693177039297366&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Dept of Mathematics, King's College London; Dept of Mathematics, King's College London",
        "aff_domain": "mth.kc1.ac.uk;mth.kc1.ac.uk",
        "email": "mth.kc1.ac.uk;mth.kc1.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "King's College London",
        "aff_unique_dep": "Dept of Mathematics",
        "aff_unique_url": "https://www.kcl.ac.uk",
        "aff_unique_abbr": "KCL",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "f1eb87bcfa",
        "title": "Effective Learning Requires Neuronal Remodeling of Hebbian Synapses",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/f0bda020d2470f2e74990a07a607ebd9-Abstract.html",
        "author": "Gal Chechik; Isaac Meilijson; Eytan Ruppin",
        "abstract": "This paper revisits the classical neuroscience paradigm of Hebbian  learning.  We  find  that  a  necessary  requirement  for  effective  as(cid:173) sociative  memory  learning  is  that  the  efficacies  of  the  incoming  synapses  should  be  uncorrelated.  This  requirement  is  difficult  to  achieve in  a  robust  manner by  Hebbian synaptic learning,  since it  depends on network level information.  Effective learning can yet be  obtained by a neuronal process that maintains a zero sum of the in(cid:173) coming synaptic efficacies.  This normalization drastically improves  the  memory  capacity of associative networks,  from  an  essentially  bounded capacity to one that linearly scales with the network's size.  It also enables the effective storage of patterns with heterogeneous  coding levels in a single network.  Such neuronal normalization can  be successfully carried out by activity-dependent homeostasis of the  neuron's synaptic efficacies, which was recently observed in cortical  tissue.  Thus,  our findings  strongly suggest  that  effective  associa(cid:173) tive learning with Hebbian synapses alone is  biologically implausi(cid:173) ble and that Hebbian synapses must be continuously remodeled by  neuronally-driven regulatory processes in  the brain.",
        "bibtex": "@inproceedings{NIPS1999_f0bda020,\n author = {Chechik, Gal and Meilijson, Isaac and Ruppin, Eytan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Effective Learning Requires Neuronal Remodeling of Hebbian Synapses},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/f0bda020d2470f2e74990a07a607ebd9-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/f0bda020d2470f2e74990a07a607ebd9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/f0bda020d2470f2e74990a07a607ebd9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 204764,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:yrF4rHI-UHEJ:scholar.google.com/&scioq=Effective+Learning+Requires+Neuronal+Remodeling+of+Hebbian+Synapses&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Mathematical Sciences Tel-Aviv University Tel Aviv, Israel; School of Mathematical Sciences Tel-Aviv University Tel Aviv, Israel; School of Mathematical Sciences Tel-Aviv University Tel Aviv, Israel",
        "aff_domain": "math.tau.ac.il;math.tau.ac.il;math.tau.ac.il",
        "email": "math.tau.ac.il;math.tau.ac.il;math.tau.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Tel-Aviv University",
        "aff_unique_dep": "School of Mathematical Sciences",
        "aff_unique_url": "https://www.tau.ac.il",
        "aff_unique_abbr": "TAU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Tel Aviv",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "be21be4775",
        "title": "Effects of Spatial and Temporal Contiguity on the Acquisition of Spatial Information",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/ba1b3eba322eab5d895aa3023fe78b9c-Abstract.html",
        "author": "Thea B. Ghiselli-Crippa; Paul W. Munro",
        "abstract": "Spatial  information comes in two forms:  direct spatial information (for  example, retinal position) and indirect temporal contiguity information,  since objects encountered sequentially are in general spatially close. The  acquisition  of spatial  information  by  a  neural  network  is  investigated  here. Given a spatial layout of several objects, networks are trained on a  prediction task.  Networks using temporal sequences with no direct spa(cid:173) tial information are found to  develop internal representations that show  distances correlated with distances in the external layout.  The influence  of spatial information is analyzed by providing direct spatial information  to the system during training that is  either consistent with the layout or  inconsistent with  it.  This  approach  allows  examination of the relative  contributions of spatial and temporal contiguity.",
        "bibtex": "@inproceedings{NIPS1999_ba1b3eba,\n author = {Ghiselli-Crippa, Thea and Munro, Paul},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Effects of Spatial and Temporal Contiguity on the Acquisition of Spatial Information},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/ba1b3eba322eab5d895aa3023fe78b9c-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/ba1b3eba322eab5d895aa3023fe78b9c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/ba1b3eba322eab5d895aa3023fe78b9c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1594294,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13782829579417991811&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Information Science and Telecommunications, University of Pittsburgh; Department of Information Science and Telecommunications, University of Pittsburgh",
        "aff_domain": "sis.pitt.edu;sis.pitt.edu",
        "email": "sis.pitt.edu;sis.pitt.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Pittsburgh",
        "aff_unique_dep": "Department of Information Science and Telecommunications",
        "aff_unique_url": "https://www.pitt.edu",
        "aff_unique_abbr": "Pitt",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6b9399970c",
        "title": "Efficient Approaches to Gaussian Process Classification",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/26751be1181460baf78db8d5eb7aad39-Abstract.html",
        "author": "Lehel Csat\u00f3; Ernest Fokou\u00e9; Manfred Opper; Bernhard Schottky; Ole Winther",
        "abstract": "We  present  three  simple  approximations  for  the  calculation  of  the  posterior  mean  in  Gaussian  Process  classification.  The  first  two  methods  are  related  to mean field  ideas  known  in  Statistical  Physics.  The third approach is based on Bayesian online approach  which was  motivated by recent results in the Statistical Mechanics  of Neural Networks.  We present simulation results showing:  1. that  the mean field  Bayesian evidence may be used for  hyperparameter  tuning and 2.  that the online approach may achieve a low  training  error fast.",
        "bibtex": "@inproceedings{NIPS1999_26751be1,\n author = {Csat\\'{o}, Lehel and Fokou\\'{e}, Ernest and Opper, Manfred and Schottky, Bernhard and Winther, Ole},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Efficient Approaches to Gaussian Process Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/26751be1181460baf78db8d5eb7aad39-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/26751be1181460baf78db8d5eb7aad39-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/26751be1181460baf78db8d5eb7aad39-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1500122,
        "gs_citation": 83,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13030686079095220426&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Neural Computing Research Group, School of Engineering and Applied Sciences, Aston University Birmingham B4 7ET, UK; Neural Computing Research Group, School of Engineering and Applied Sciences, Aston University Birmingham B4 7ET, UK; Neural Computing Research Group, School of Engineering and Applied Sciences, Aston University Birmingham B4 7ET, UK; Neural Computing Research Group, School of Engineering and Applied Sciences, Aston University Birmingham B4 7ET, UK; Theoretical Physics II, Lund University, Solvegatan 14 A, S-223 62 Lund, Sweden",
        "aff_domain": "aston.ac.uk;aston.ac.uk; ; ;thep.lu.se",
        "email": "aston.ac.uk;aston.ac.uk; ; ;thep.lu.se",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "Aston University;Lund University",
        "aff_unique_dep": "School of Engineering and Applied Sciences;Theoretical Physics II",
        "aff_unique_url": "https://www.aston.ac.uk;https://www.lunduniversity.lu.se",
        "aff_unique_abbr": "Aston;LU",
        "aff_campus_unique_index": "0;0;0;0;1",
        "aff_campus_unique": "Birmingham;Lund",
        "aff_country_unique_index": "0;0;0;0;1",
        "aff_country_unique": "United Kingdom;Sweden"
    },
    {
        "id": "85c576bf17",
        "title": "Emergence of Topography and Complex Cell Properties from Natural Images using Extensions of ICA",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/148510031349642de5ca0c544f31b2ef-Abstract.html",
        "author": "Aapo Hyv\u00e4rinen; Patrik O. Hoyer",
        "abstract": "Independent  component  analysis of natural images  leads to emer(cid:173) gence  of  simple  cell  properties,  Le.  linear  filters  that  resemble  wavelets  or  Gabor  functions.  In  this  paper,  we  extend  ICA  to  explain further properties of VI cells.  First, we decompose natural  images  into  independent  subspaces  instead of scalar  components.  This  model  leads  to  emergence  of phase  and  shift  invariant  fea(cid:173) tures,  similar  to  those  in  VI  complex  cells.  Second,  we  define  a  topography between the linear components obtained by ICA.  The  topographic distance between  two components is  defined  by  their  higher-order correlations, so that two components are close to each  other  in  the  topography  if  they  are  strongly  dependent  on  each  other.  This  leads  to simultaneous emergence  of both  topography  and invariances similar to complex cell  properties.",
        "bibtex": "@inproceedings{NIPS1999_14851003,\n author = {Hyv\\\"{a}rinen, Aapo and Hoyer, Patrik},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Emergence of Topography and Complex Cell Properties from Natural Images using Extensions of ICA},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/148510031349642de5ca0c544f31b2ef-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/148510031349642de5ca0c544f31b2ef-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/148510031349642de5ca0c544f31b2ef-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1731801,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16822505020728769853&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Neural Networks Research Center, Helsinki University of Technology; Neural Networks Research Center, Helsinki University of Technology",
        "aff_domain": "hut.fi;hut.fi",
        "email": "hut.fi;hut.fi",
        "github": "",
        "project": "http://www.cis.hut.fi/projects/ica/",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Helsinki University of Technology",
        "aff_unique_dep": "Neural Networks Research Center",
        "aff_unique_url": "https://www.aalto.fi/en",
        "aff_unique_abbr": "HUT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Finland"
    },
    {
        "id": "41df2e79b7",
        "title": "Evolving Learnable Languages",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/4a2ddf148c5a9c42151a529e8cbdcc06-Abstract.html",
        "author": "Bradley Tonkes; Alan Blair; Janet Wiles",
        "abstract": "Recent theories suggest that language acquisition is assisted by the evolution of languages towards forms that are easily learnable. In this paper, we evolve combinatorial languages which can be learned by a recurrent neural network quickly and from relatively few ex(cid:173) amples. Additionally, we evolve languages for generalization in different \"worlds\", and for generalization from specific examples. We find that languages can be evolved to facilitate different forms of impressive generalization for a minimally biased, general pur(cid:173) pose learner. The results provide empirical support for the theory that the language itself, as well as the language environment of a learner, plays a substantial role in learning: that there is far more to language acquisition than the language acquisition device.",
        "bibtex": "@inproceedings{NIPS1999_4a2ddf14,\n author = {Tonkes, Bradley and Blair, Alan and Wiles, Janet},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Evolving Learnable Languages},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/4a2ddf148c5a9c42151a529e8cbdcc06-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/4a2ddf148c5a9c42151a529e8cbdcc06-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/4a2ddf148c5a9c42151a529e8cbdcc06-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1066694,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10943659476111462974&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Dept of Comp. Sci. and Elec. Engineering, University of Queensland, Queensland, 4072, Australia; Department of Computer Science, University of Melbourne, Parkville, Victoria, 3052, Australia; Dept of Comp. Sci. and Elec. Engineering + School of Psychology, University of Queensland, Queensland, 4072, Australia",
        "aff_domain": "csee.uq.edu.au;cs.mu.oz.au;csee.uq.edu.au",
        "email": "csee.uq.edu.au;cs.mu.oz.au;csee.uq.edu.au",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2+0",
        "aff_unique_norm": "University of Queensland;University of Melbourne;University of California, San Diego",
        "aff_unique_dep": "Dept of Comp. Sci. and Elec. Engineering;Department of Computer Science;Department of Computer Science and Electrical Engineering",
        "aff_unique_url": "https://www.uq.edu.au;https://www.unimelb.edu.au;https://www.ucsd.edu",
        "aff_unique_abbr": "UQ;UniMelb;UCSD",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Queensland;Parkville;",
        "aff_country_unique_index": "0;0;1+0",
        "aff_country_unique": "Australia;United States"
    },
    {
        "id": "bc4911d99e",
        "title": "From Coexpression to Coregulation: An Approach to Inferring Transcriptional Regulation among Gene Classes from Large-Scale Expression Data",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/8bb88f80d334b1869781beb89f7b73be-Abstract.html",
        "author": "Eric Mjolsness; Tobias Mann; Rebecca Casta\u00f1o; Barbara J. Wold",
        "abstract": "small-scale  gene",
        "bibtex": "@inproceedings{NIPS1999_8bb88f80,\n author = {Mjolsness, Eric and Mann, Tobias and Casta\\~{n}o, Rebecca and Wold, Barbara},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {From Coexpression to Coregulation: An Approach to Inferring Transcriptional Regulation among Gene Classes from Large-Scale Expression Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/8bb88f80d334b1869781beb89f7b73be-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/8bb88f80d334b1869781beb89f7b73be-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/8bb88f80d334b1869781beb89f7b73be-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1498734,
        "gs_citation": 73,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8507428921745657883&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Jet Propulsion Laboratory, California Institute of Technology, Pasadena CA 91109-8099; Jet Propulsion Laboratory, California Institute of Technology, Pasadena CA 91109-8099; Jet Propulsion Laboratory, California Institute of Technology, Pasadena CA 91109-8099; Division of Biology, California Institute of Technology, Pasadena CA 91125",
        "aff_domain": "jpl.nasa.gov;aigjpl.nasa.gov;aigjpl.nasa.gov;its.caltech.edu",
        "email": "jpl.nasa.gov;aigjpl.nasa.gov;aigjpl.nasa.gov;its.caltech.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "California Institute of Technology",
        "aff_unique_dep": "Jet Propulsion Laboratory",
        "aff_unique_url": "https://www.caltech.edu",
        "aff_unique_abbr": "Caltech",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Pasadena",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7fc7263907",
        "title": "Gaussian Fields for Approximate Inference in Layered Sigmoid Belief Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/f670ef5d2d6bdf8f29450a970494dd64-Abstract.html",
        "author": "David Barber; Peter Sollich",
        "abstract": "Local \"belief propagation\" rules of the  sort proposed by Pearl  [15]  are  guaranteed  to  converge to  the  correct  posterior probabilities  in  singly  connected graphical models. Recently, a number of researchers have em(cid:173) pirically demonstrated good performance of \"loopy belief propagation\"(cid:173) using these same rules on graphs with loops.  Perhaps the most dramatic  instance is the near Shannon-limit performance of \"Turbo codes\", whose  decoding algorithm is equivalent to loopy belief propagation.  Except for the case of graphs with a single loop, there has been little theo(cid:173) retical understanding of the performance of loopy propagation. Here we  analyze belief propagation in  networks  with  arbitrary  topologies  when  the nodes in  the graph describe jointly Gaussian random variables.  We  give an analytical formula relating the true  posterior probabilities with  those calculated using loopy propagation.  We give sufficient conditions  for convergence and show that when belief propagation converges it gives  the correct posterior means for all graph  topologies,  not just networks  with a single loop.  The related \"max-product\" belief propagation algorithm finds  the max(cid:173) imum posterior probability estimate for singly connected networks.  We  show  that,  even for non-Gaussian probability distributions,  the  conver(cid:173) gence points of the max-product algorithm in loopy  networks are  max(cid:173) ima  over a  particular large  local  neighborhood of the  posterior proba(cid:173) bility.  These results  help clarify  the  empirical performance results  and  motivate  using  the  powerful  belief propagation algorithm in  a  broader  class of networks.",
        "bibtex": "@inproceedings{NIPS1999_f670ef5d,\n author = {Barber, David and Sollich, Peter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Gaussian Fields for Approximate Inference in Layered Sigmoid Belief Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/f670ef5d2d6bdf8f29450a970494dd64-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/f670ef5d2d6bdf8f29450a970494dd64-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/f670ef5d2d6bdf8f29450a970494dd64-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 3024061,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4846584404263316023&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Computer Science Division, UC Berkeley; Mitsubishi Electric Research Lab",
        "aff_domain": "cs.berkeley.edu;merl.com",
        "email": "cs.berkeley.edu;merl.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of California, Berkeley;Mitsubishi Electric Research Laboratories",
        "aff_unique_dep": "Computer Science Division;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.merl.com",
        "aff_unique_abbr": "UC Berkeley;MERL",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ed03f0e529",
        "title": "Generalized Model Selection for Unsupervised Learning in High Dimensions",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/89f03f7d02720160f1b04cf5b27f5ccb-Abstract.html",
        "author": "Shivakumar Vaithyanathan; Byron Dom",
        "abstract": "We describe a Bayesian approach to  model  selection in unsupervised  learning  that  determines  both  the  feature  set  and  the  number  of  clusters. We then evaluate this scheme (based on marginal likelihood)  and  one  based  on  cross-validated  likelihood.  For  the  Bayesian  scheme  we  derive  a  closed-form  solution  of the  marginal  likelihood  by  assuming  appropriate  forms  of the  likelihood  function  and  prior.  Extensive  experiments  compare these  approaches  and  all  results  are  verified  by comparison against ground truth.  In  these experiments the  Bayesian scheme using our objective function gave better results than  cross-validation.",
        "bibtex": "@inproceedings{NIPS1999_89f03f7d,\n author = {Vaithyanathan, Shivakumar and Dom, Byron},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Generalized Model Selection for Unsupervised Learning in High Dimensions},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/89f03f7d02720160f1b04cf5b27f5ccb-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/89f03f7d02720160f1b04cf5b27f5ccb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/89f03f7d02720160f1b04cf5b27f5ccb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1439026,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4250544490707615790&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "IBM Almaden Research Center; IBM Almaden Research Center",
        "aff_domain": "almaden.ibm.com;almaden.ibm.com",
        "email": "almaden.ibm.com;almaden.ibm.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "IBM",
        "aff_unique_dep": "Research Center",
        "aff_unique_url": "https://www.ibm.com/research",
        "aff_unique_abbr": "IBM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Almaden",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f878a1a016",
        "title": "Graded Grammaticality in Prediction Fractal Machines",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/ae614c557843b1df326cb29c57225459-Abstract.html",
        "author": "Shan Parfitt; Peter Ti\u00f1o; Georg Dorffner",
        "abstract": "We  introduce  a  novel  method  of constructing  language  models,  which  avoids some of the problems associated  with recurrent  neu(cid:173) ral networks.  The method of creating a Prediction Fractal Machine  (PFM)  [1]  is  briefly described  and some experiments are presented  which  demonstrate the suitability of PFMs for  language modeling.  PFMs  distinguish  reliably  between  minimal  pairs,  and  their  be(cid:173) havior is  consistent  with the hypothesis  [4]  that wellformedness  is  'graded' not absolute.  A discussion of their potential to offer fresh  insights into language acquisition and processing follows.",
        "bibtex": "@inproceedings{NIPS1999_ae614c55,\n author = {Parfitt, Shan and Ti\\~{n}o, Peter and Dorffner, Georg},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Graded Grammaticality in Prediction Fractal Machines},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/ae614c557843b1df326cb29c57225459-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/ae614c557843b1df326cb29c57225459-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/ae614c557843b1df326cb29c57225459-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1715576,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8723667339929225188&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "29fd95af01",
        "title": "Greedy Importance Sampling",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/8303a79b1e19a194f1875981be5bdb6f-Abstract.html",
        "author": "Dale Schuurmans",
        "abstract": "I present a simple variation of importance sampling that explicitly search(cid:173) es for  important regions in the target distribution.  I prove that the tech(cid:173) nique yields unbiased estimates,  and show empirically it can reduce the  variance  of standard Monte Carlo estimators.  This is achieved  by con(cid:173) centrating samples in more significant regions of the sample space.",
        "bibtex": "@inproceedings{NIPS1999_8303a79b,\n author = {Schuurmans, Dale},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Greedy Importance Sampling},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/8303a79b1e19a194f1875981be5bdb6f-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/8303a79b1e19a194f1875981be5bdb6f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/8303a79b1e19a194f1875981be5bdb6f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1591042,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10760135396701840265&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, University of Waterloo",
        "aff_domain": "cs.uwaterloo.ca",
        "email": "cs.uwaterloo.ca",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Waterloo",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://uwaterloo.ca",
        "aff_unique_abbr": "UW",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "84809d0381",
        "title": "Hierarchical Image Probability (H1P) Models",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/365d17770080c807a0e47ae9118d8641-Abstract.html",
        "author": "Clay Spence; Lucas C. Parra",
        "abstract": "We formulate a model for probability distributions on image spaces.  We  show that any distribution of images can be factored exactly into condi(cid:173) tional  distributions of feature  vectors at  one resolution  (pyramid level)  conditioned on the  image information at  lower resolutions.  We  would  like to factor this over positions in the pyramid levels to make it tractable,  but such factoring may miss long-range dependencies. To fix this, we in(cid:173) troduce hidden class  labels  at  each  pixel  in  the pyramid.  The result  is  a  hierarchical mixture of conditional probabilities,  similar  to  a  hidden  Markov model on a tree.  The model parameters can be found with max(cid:173) imum likelihood estimation using the EM algorithm.  We  have obtained  encouraging preliminary results on the problems of detecting various ob(cid:173) jects in SAR images and target recognition in optical aerial images.",
        "bibtex": "@inproceedings{NIPS1999_365d1777,\n author = {Spence, Clay and Parra, Lucas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Hierarchical Image Probability (H1P) Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/365d17770080c807a0e47ae9118d8641-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/365d17770080c807a0e47ae9118d8641-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/365d17770080c807a0e47ae9118d8641-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1397439,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12797874529370890806&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "6e3a7bef7c",
        "title": "Image Recognition in Context: Application to Microscopic Urinalysis",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/64f1f27bf1b4ec22924fd0acb550c235-Abstract.html",
        "author": "Xubo B. Song; Joseph Sill; Yaser S. Abu-Mostafa; Harvey Kasdan",
        "abstract": "We  propose a  new  and efficient technique for  incorporating contextual  information into object classification. Most of the current techniques face  the problem of exponential computation cost.  In this paper, we propose a  new general framework that incorporates partial context at a linear cost.  This technique  is  applied  to  microscopic  urinalysis  image  recognition,  resulting in a significant improvement of recognition rate over the context  free approach. This gain would have been impossible using conventional  context incorporation techniques.",
        "bibtex": "@inproceedings{NIPS1999_64f1f27b,\n author = {Song, Xubo and Sill, Joseph and Abu-Mostafa, Yaser and Kasdan, Harvey},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Image Recognition in Context: Application to Microscopic Urinalysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/64f1f27bf1b4ec22924fd0acb550c235-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/64f1f27bf1b4ec22924fd0acb550c235-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/64f1f27bf1b4ec22924fd0acb550c235-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1373138,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6566020581837190013&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Electrical and Computer Engineering, Oregon Graduate Institute of Science and Technology; Department of Computation and Neural Systems, California Institute of Technology; Department of Electrical Engineering, California Institute of Technology; International Remote Imaging Systems, Inc.",
        "aff_domain": "ece.ogi.edu;busy.work.caltech.edu;work.caltech.edu; ",
        "email": "ece.ogi.edu;busy.work.caltech.edu;work.caltech.edu; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;2",
        "aff_unique_norm": "Oregon Graduate Institute of Science and Technology;California Institute of Technology;International Remote Imaging Systems",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;Department of Computation and Neural Systems;Inc.",
        "aff_unique_url": "https://www.ogi.edu;https://www.caltech.edu;",
        "aff_unique_abbr": "OGI;Caltech;IRIS",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Pasadena",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d494424d45",
        "title": "Image Representations for Facial Expression Coding",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/393c55aea738548df743a186d15f3bef-Abstract.html",
        "author": "Marian Stewart Bartlett; Gianluca Donato; Javier R. Movellan; Joseph C. Hager; Paul Ekman; Terrence J. Sejnowski",
        "abstract": "The  Facial  Action  Coding  System  (FACS)  (9)  is  an  objective  method  for  quantifying  facial  movement  in  terms  of  component  actions.  This  system  is  widely  used  in  behavioral  investigations  of emotion,  cognitive  processes,  and  social  interaction.  The  cod(cid:173) ing is  presently  performed by  highly trained human experts.  This  paper  explores  and  compares  techniques  for  automatically  recog(cid:173) nizing facial actions in sequences of images.  These methods include  unsupervised  learning techniques  for  finding  basis  images  such  as  principal component analysis, independent component analysis and  local  feature  analysis,  and supervised learning  techniques  such  as  Fisher's  linear  discriminants.  These  data-driven  bases  are  com(cid:173) pared to Gabor wavelets, in which the basis images are predefined.  Best  performances  were  obtained  using  the  Gabor  wavelet  repre(cid:173) sentation and the  independent  component  representation,  both of  which achieved 96%  accuracy for  classifying 12 facial  actions.  The  ICA  representation employs  2 orders of magnitude fewer  basis im(cid:173) ages than the  Gabor representation and takes 90%  less  CPU  time  to compute for new images.  The results provide converging support  for using local basis images, high spatial frequencies, and statistical  independence for  classifying facial  actions.",
        "bibtex": "@inproceedings{NIPS1999_393c55ae,\n author = {Bartlett, Marian and Donato, Gianluca and Movellan, Javier and Hager, Joseph and Ekman, Paul and Sejnowski, Terrence J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Image Representations for Facial Expression Coding},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/393c55aea738548df743a186d15f3bef-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/393c55aea738548df743a186d15f3bef-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/393c55aea738548df743a186d15f3bef-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1749660,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=496007501820712684&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "11f7dd1b1d",
        "title": "Independent Factor Analysis with Temporally Structured Sources",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/6a81681a7af700c6385d36577ebec359-Abstract.html",
        "author": "Hagai Attias",
        "abstract": "We  present  a  new  technique  for  time  series  analysis  based on  dy(cid:173) namic probabilistic networks.  In this approach,  the observed data  are modeled in terms of unobserved, mutually independent factors,  as in the recently introduced technique of Independent Factor Anal(cid:173) ysis  (IFA).  However,  unlike  in  IFA,  the  factors  are not  Li.d.;  each  factor has its own temporal statistical characteristics.  We  derive a  family of EM  algorithms that learn the structure of the underlying  factors  and  their  relation  to  the  data.  These  algorithms  perform  source separation and noise reduction in an integrated manner, and  demonstrate superior performance compared to IFA.",
        "bibtex": "@inproceedings{NIPS1999_6a81681a,\n author = {Attias, Hagai},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Independent Factor Analysis with Temporally Structured Sources},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/6a81681a7af700c6385d36577ebec359-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/6a81681a7af700c6385d36577ebec359-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/6a81681a7af700c6385d36577ebec359-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1708488,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4739405910613043059&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Gatsby Unit, University College London",
        "aff_domain": "gatsby.ucl.ac.uk",
        "email": "gatsby.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University College London",
        "aff_unique_dep": "Gatsby Unit",
        "aff_unique_url": "https://www.ucl.ac.uk",
        "aff_unique_abbr": "UCL",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "29b1320fa4",
        "title": "Inference for the Generalization Error",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/7d12b66d3df6af8d429c1a357d8b9e1a-Abstract.html",
        "author": "Claude Nadeau; Yoshua Bengio",
        "abstract": "In order to to compare learning algorithms, experimental results reported  in  the  machine  learning  litterature often  use  statistical  tests  of signifi(cid:173) cance.  Unfortunately,  most of these  tests  do  not take  into account the  variability  due  to  the  choice  of training  set.  We  perform  a  theoretical  investigation of the variance of the cross-validation estimate of the gen(cid:173) eralization error that takes into account the  variability due to  the choice  of training  sets.  This  allows  us  to  propose  two  new  ways  to  estimate  this variance. We show, via simulations, that these new statistics perform  well relative to the statistics considered by Dietterich (Dietterich, 1998).",
        "bibtex": "@inproceedings{NIPS1999_7d12b66d,\n author = {Nadeau, Claude and Bengio, Yoshua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Inference for the Generalization Error},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7d12b66d3df6af8d429c1a357d8b9e1a-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/7d12b66d3df6af8d429c1a357d8b9e1a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/7d12b66d3df6af8d429c1a357d8b9e1a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1644008,
        "gs_citation": 1446,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1700004940304636288&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 26,
        "aff": "CIRANO; CIRANO and Dept. IRO Universite de Montreal",
        "aff_domain": "altavista.net;iro.umontreal.ca",
        "email": "altavista.net;iro.umontreal.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "CIRANO;Universite de Montreal",
        "aff_unique_dep": ";Dept. IRO",
        "aff_unique_url": "https://www.cirano.org;https://www.umontreal.ca",
        "aff_unique_abbr": "CIRANO;UdeM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "df4bb328dd",
        "title": "Information Capacity and Robustness of Stochastic Neuron Models",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/a40511cad8383e5ae8ddd8b855d135da-Abstract.html",
        "author": "Elad Schneidman; Idan Segev; Naftali Tishby",
        "abstract": "The  reliability  and  accuracy  of  spike  trains  have  been  shown  to  depend  on  the  nature  of  the  stimulus  that  the  neuron  encodes.  Adding  ion  channel  stochasticity  to  neuronal  models  results  in  a  macroscopic behavior that replicates the input-dependent reliabili(cid:173) ty and precision of real neurons.  We  calculate the amount of infor(cid:173) mation that an ion channel based stochastic Hodgkin-Huxley (HH)  neuron model can encode about a wide set of stimuli.  We show that  both the information  rate and the information  per  spike  of the s(cid:173) tochastic model are similar to the values  reported experimentally.  Moreover,  the  amount  of information  that  the  neuron  encodes  is  correlated with the amplitude of fluctuations in the input, and less  so with the average firing rate of the neuron.  We also show that for  the HH  ion  channel density,  the information capacity is  robust  to  changes  in  the density  of ion  channels  in  the  membrane,  whereas  changing  the  ratio  between  the  Na+  and  K+  ion  channels  has  a  considerable effect on the information that the neuron can encode.  Finally,  we  suggest  that neurons  may maximize  their information  capacity by appropriately balancing the density of the different ion  channels that underlie neuronal excitability.",
        "bibtex": "@inproceedings{NIPS1999_a40511ca,\n author = {Schneidman, Elad and Segev, Idan and Tishby, Naftali},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Information Capacity and Robustness of Stochastic Neuron Models},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/a40511cad8383e5ae8ddd8b855d135da-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/a40511cad8383e5ae8ddd8b855d135da-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/a40511cad8383e5ae8ddd8b855d135da-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1694832,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10351274268088447001&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Institute of Computer Science, Department of Neurobiology and Center for Neural Computation, Hebrew University; Institute of Computer Science, Department of Neurobiology and Center for Neural Computation, Hebrew University; Institute of Computer Science, Department of Neurobiology and Center for Neural Computation, Hebrew University",
        "aff_domain": "cs.huji.ac.il;lobster.ls.huji.ac.il;cs.huji.ac.il",
        "email": "cs.huji.ac.il;lobster.ls.huji.ac.il;cs.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Hebrew University",
        "aff_unique_dep": "Institute of Computer Science",
        "aff_unique_url": "http://www.huji.ac.il",
        "aff_unique_abbr": "HUJI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "9a8cab92a0",
        "title": "Information Factorization in Connectionist Models of Perception",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/2cb6b10338a7fc4117a80da24b582060-Abstract.html",
        "author": "Javier R. Movellan; James L. McClelland",
        "abstract": "We  examine  a  psychophysical  law  that  describes  the  influence  of  stimulus and context  on  perception.  According to this law  choice  probability  ratios  factorize  into  components  independently  con(cid:173) trolled by stimulus and context.  It has been argued that this  pat(cid:173) tern of results is  incompatible with feedback models of perception.  In this  paper we  examine this  claim  using neural  network models  defined via stochastic differential equations.  We show that the law  is  related to a  condition  named channel separability and has little  to do with the existence of feedback connections.  In essence, chan(cid:173) nels are separable if they converge into the response units without  direct lateral connections to other channels and if their sensors are  not  directly  contaminated  by  external  inputs  to  the  other  chan(cid:173) nels.  Implications of the analysis for  cognitive and computational  neurosicence are discussed.",
        "bibtex": "@inproceedings{NIPS1999_2cb6b103,\n author = {Movellan, Javier and McClelland, James},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Information Factorization in Connectionist Models of Perception},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/2cb6b10338a7fc4117a80da24b582060-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/2cb6b10338a7fc4117a80da24b582060-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/2cb6b10338a7fc4117a80da24b582060-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1483677,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:nbs7UtLN1KcJ:scholar.google.com/&scioq=Information+Factorization+in+Connectionist+Models+of+Perception&hl=en&as_sdt=0,5",
        "gs_version_total": 5,
        "aff": "Department of Cognitive Science, Institute for Neural Computation, University of California San Diego; Center for the Neural Bases of Cognition, Department of Psychology, Carnegie Mellon University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of California, San Diego;Carnegie Mellon University",
        "aff_unique_dep": "Department of Cognitive Science, Institute for Neural Computation;Department of Psychology",
        "aff_unique_url": "https://ucsd.edu;https://www.cmu.edu",
        "aff_unique_abbr": "UCSD;CMU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "San Diego;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8bd1d43ea7",
        "title": "Invariant Feature Extraction and Classification in Kernel Spaces",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/0efbe98067c6c73dba1250d2beaa81f9-Abstract.html",
        "author": "Sebastian Mika; Gunnar R\u00e4tsch; Jason Weston; Bernhard Sch\u00f6lkopf; Alex J. Smola; Klaus-Robert M\u00fcller",
        "abstract": "In hyperspectral imagery one pixel  typically  consists of a  mixture  of the  reflectance  spectra of several  materials,  where  the  mixture  coefficients  correspond  to  the  abundances of the  constituting  ma(cid:173) terials.  We  assume linear combinations of reflectance spectra with  some additive normal sensor noise and derive a  probabilistic MAP  framework  for  analyzing  hyperspectral  data.  As  the  material re(cid:173) flectance characteristics are not know  a priori, we face  the problem  of  unsupervised  linear  unmixing.  The  incorporation  of  different  prior  information  (e.g.  positivity  and  normalization  of the  abun(cid:173) dances)  naturally  leads  to  a  family  of  interesting  algorithms,  for  example  in  the  noise-free  case  yielding  an  algorithm  that  can  be  understood as constrained independent component analysis (ICA).  Simulations underline the usefulness of our theory.",
        "bibtex": "@inproceedings{NIPS1999_0efbe980,\n author = {Mika, Sebastian and R\\\"{a}tsch, Gunnar and Weston, Jason and Sch\\\"{o}lkopf, Bernhard and Smola, Alex and M\\\"{u}ller, Klaus-Robert},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Invariant Feature Extraction and Classification in Kernel Spaces},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/0efbe98067c6c73dba1250d2beaa81f9-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/0efbe98067c6c73dba1250d2beaa81f9-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/0efbe98067c6c73dba1250d2beaa81f9-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 3422452,
        "gs_citation": 258,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15519962779773768640&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "33e01a22eb",
        "title": "Kirchoff Law Markov Fields for Analog Circuit Design",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/418ef6127e44214882c61e372e866691-Abstract.html",
        "author": "Richard M. Golden",
        "abstract": "Three contributions to developing an algorithm for  assisting engi(cid:173) neers in designing analog circuits are provided in this paper.  First,  a  method  for  representing  highly  nonlinear  and  non-continuous  analog circuits using Kirchoff current law potential functions within  the context of a Markov field  is described.  Second, a relatively effi(cid:173) cient  algorithm for  optimizing the Markov field  objective function  is  briefly  described  and the  convergence proof is  briefly  sketched.  And  third,  empirical  results  illustrating  the strengths and limita(cid:173) tions  of the approach  are provided within the context  of a  JFET  transistor design problem.  The proposed algorithm generated a set  of circuit components for  the JFET circuit model  that accurately  generated the desired characteristic curves.",
        "bibtex": "@inproceedings{NIPS1999_418ef612,\n author = {Golden, Richard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Kirchoff Law Markov Fields for Analog Circuit Design},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/418ef6127e44214882c61e372e866691-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/418ef6127e44214882c61e372e866691-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/418ef6127e44214882c61e372e866691-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1623248,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5960878070175584712&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "RMG Consulting Inc. + Associate Professor at University of Texas at Dallas",
        "aff_domain": "AOL.COM",
        "email": "AOL.COM",
        "github": "",
        "project": "www.neural-network.com",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1",
        "aff_unique_norm": "RMG Consulting Inc.;University of Texas at Dallas",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.utdallas.edu",
        "aff_unique_abbr": ";UT Dallas",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Dallas",
        "aff_country_unique_index": "0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "30feee6218",
        "title": "LTD Facilitates Learning in a Noisy Environment",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/7dd0240cd412efde8bc165e864d3644f-Abstract.html",
        "author": "Paul W. Munro; Gerardina Hern\u00e1ndez",
        "abstract": "Long-term potentiation (LTP) has long been held as a biological  substrate for associative learning. Recently, evidence has emerged  that long-term depression (LTD) results when the presynaptic cell  fires after the postsynaptic cell. The computational utility of LTD  is explored here. Synaptic modification kernels for both LTP and  LTD have been proposed by other laboratories based studies of one  postsynaptic unit. Here, the interaction between time-dependent  LTP and LTD is studied in small networks.",
        "bibtex": "@inproceedings{NIPS1999_7dd0240c,\n author = {Munro, Paul and Hern\\'{a}ndez, Gerardina},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {LTD Facilitates Learning in a Noisy Environment},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7dd0240cd412efde8bc165e864d3644f-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/7dd0240cd412efde8bc165e864d3644f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/7dd0240cd412efde8bc165e864d3644f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1434360,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13126657051037055515&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "School of Information Sciences, University of Pittsburgh; Intelligent Systems Program, University of Pittsburgh",
        "aff_domain": "pitt.edu;pitt.edu",
        "email": "pitt.edu;pitt.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Pittsburgh",
        "aff_unique_dep": "School of Information Sciences",
        "aff_unique_url": "https://www.pitt.edu",
        "aff_unique_abbr": "Pitt",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Pittsburgh;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "85dd092b5b",
        "title": "Large Margin DAGs for Multiclass Classification",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/4abe17a1c80cbdd2aa241b70840879de-Abstract.html",
        "author": "John C. Platt; Nello Cristianini; John Shawe-Taylor",
        "abstract": "We  present a  new  learning  architecture:  the  Decision  Directed  Acyclic  Graph  (DDAG),  which  is  used  to  combine  many  two-class  classifiers  into  a  multiclass  classifier.  For  an  N -class  problem,  the  DDAG  con(cid:173) tains N(N - 1)/2 classifiers, one for each pair of classes.  We present a  VC analysis of the case when the node classifiers are hyperplanes; the re(cid:173) sulting bound on the test error depends on N  and on the margin achieved  at  the  nodes,  but not  on  the  dimension  of the  space.  This motivates an  algorithm, DAGSVM, which operates in  a kernel-induced feature  space  and  uses  two-class maximal  margin  hyperplanes at each  decision-node  of the  DDAG.  The DAGSVM  is  substantially faster  to  train  and  evalu(cid:173) ate  than  either the  standard  algorithm  or Max Wins,  while maintaining  comparable accuracy to both of these algorithms.",
        "bibtex": "@inproceedings{NIPS1999_4abe17a1,\n author = {Platt, John and Cristianini, Nello and Shawe-Taylor, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Large Margin DAGs for Multiclass Classification},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/4abe17a1c80cbdd2aa241b70840879de-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/4abe17a1c80cbdd2aa241b70840879de-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/4abe17a1c80cbdd2aa241b70840879de-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1654858,
        "gs_citation": 2941,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15119385572164381165&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 21,
        "aff": "Microsoft Research; Dept. of Engineering Mathematics, University of Bristol; Department of Computer Science, Royal Holloway College - University of London",
        "aff_domain": "microsojt.com;bristol.ac.uk;dcs.rhbnc.ac.uk",
        "email": "microsojt.com;bristol.ac.uk;dcs.rhbnc.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Microsoft;University of Bristol;University of London",
        "aff_unique_dep": "Microsoft Research;Dept. of Engineering Mathematics;Department of Computer Science",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://www.bristol.ac.uk;https://www.royalholloway.ac.uk",
        "aff_unique_abbr": "MSR;UoB;RHUL",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Royal Holloway College",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "e7bd2b2520",
        "title": "Learning Factored Representations for Partially Observable Markov Decision Processes",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/231141b34c82aa95e48810a9d1b33a79-Abstract.html",
        "author": "Brian Sallans",
        "abstract": "The problem of reinforcement learning in a non-Markov environment is  explored using a dynamic Bayesian network, where conditional indepen(cid:173) dence assumptions between random variables are compactly represented  by network parameters.  The parameters are learned on-line, and approx(cid:173) imations are used to perform inference and to compute the optimal value  function.  The  relative  effects of inference  and  value  function  approxi(cid:173) mations on the quality of the final  policy are investigated, by learning to  solve a moderately difficult driving task. The two value function approx(cid:173) imations,  linear and quadratic, were found to  perform similarly, but the  quadratic model was more sensitive to initialization.  Both performed be(cid:173) low the level of human performance on the task.  The dynamic Bayesian  network performed comparably to  a model  using a localist hidden state  representation, while requiring exponentially fewer parameters.",
        "bibtex": "@inproceedings{NIPS1999_231141b3,\n author = {Sallans, Brian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Learning Factored Representations for Partially Observable Markov Decision Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/231141b34c82aa95e48810a9d1b33a79-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/231141b34c82aa95e48810a9d1b33a79-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/231141b34c82aa95e48810a9d1b33a79-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1470140,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14037392429958307878&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "5b227e5855",
        "title": "Learning Informative Statistics: A Nonparametnic Approach",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/8698ff92115213ab187d31d4ee5da8ea-Abstract.html",
        "author": "John W. Fisher III; Alexander T. Ihler; Paul A. Viola",
        "abstract": "We discuss an information theoretic approach for categorizing and mod(cid:173) eling dynamic processes. The approach can learn a compact and informa(cid:173) tive statistic which summarizes past states to predict future observations.  Furthermore, the  uncertainty of the prediction is characterized nonpara(cid:173) metrically by a joint density over the learned statistic and present obser(cid:173) vation.  We  discuss the  application of the technique to both noise driven  dynamical systems and random processes sampled from a density which  is conditioned on the past. In the first case we show results in which both  the  dynamics of random walk and the  statistics of the  driving noise  are  captured.  In the second case we  present results in  which a summarizing  statistic  is  learned  on  noisy  random telegraph  waves  with  differing de(cid:173) pendencies on past states.  In both cases the algorithm yields a principled  approach for discriminating processes with differing dynamics and/or de(cid:173) pendencies.  The method is  grounded in  ideas  from  information theory  and nonparametric statistics.",
        "bibtex": "@inproceedings{NIPS1999_8698ff92,\n author = {Fisher III, John W and Ihler, Alexander and Viola, Paul},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Learning Informative Statistics: A Nonparametnic Approach},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/8698ff92115213ab187d31d4ee5da8ea-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/8698ff92115213ab187d31d4ee5da8ea-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/8698ff92115213ab187d31d4ee5da8ea-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1526691,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15759252384156833819&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "908ea191d9",
        "title": "Learning Sparse Codes with a Mixture-of-Gaussians Prior",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/b0f2ad44d26e1a6f244201fe0fd864d1-Abstract.html",
        "author": "Bruno A. Olshausen; K. Jarrod Millman",
        "abstract": "We  describe  a  method  for  learning  an  overcomplete  set  of  basis  functions  for  the  purpose of modeling  sparse structure in images.  The  sparsity  of the  basis  function  coefficients  is  modeled  with  a  mixture-of-Gaussians  distribution.  One  Gaussian  captures  non(cid:173) active  coefficients  with  a  small-variance  distribution  centered  at  zero,  while one or more other Gaussians capture active coefficients  with a large-variance distribution.  We show that when the prior is  in such  a form,  there exist efficient  methods for  learning the basis  functions  as  well  as  the parameters of the prior.  The performance  of the  algorithm  is  demonstrated  on  a  number  of test  cases  and  also  on  natural  images.  The  basis  functions  learned  on  natural  images  are similar to those  obtained with other methods,  but the  sparse form of the coefficient distribution is much better described.  Also, since the parameters of the prior are adapted to the data, no  assumption about sparse  structure in the  images  need  be  made  a  priori,  rather it is  learned from  the data.",
        "bibtex": "@inproceedings{NIPS1999_b0f2ad44,\n author = {Olshausen, Bruno and Millman, K.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Learning Sparse Codes with a Mixture-of-Gaussians Prior},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b0f2ad44d26e1a6f244201fe0fd864d1-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/b0f2ad44d26e1a6f244201fe0fd864d1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/b0f2ad44d26e1a6f244201fe0fd864d1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1308763,
        "gs_citation": 132,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16915127447386538899&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Psychology and Center for Neuroscience, UC Davis; Center for Neuroscience, UC Davis",
        "aff_domain": "ucdavis.edu;ucdavis.edu",
        "email": "ucdavis.edu;ucdavis.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Davis",
        "aff_unique_dep": "Department of Psychology and Center for Neuroscience",
        "aff_unique_url": "https://www.ucdavis.edu",
        "aff_unique_abbr": "UC Davis",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Davis",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0c77a85c0f",
        "title": "Learning Statistically Neutral Tasks without Expert Guidance",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/f63f65b503e22cb970527f23c9ad7db1-Abstract.html",
        "author": "Ton Weijters; Antal van den Bosch; Eric O. Postma",
        "abstract": "Eric  Postma",
        "bibtex": "@inproceedings{NIPS1999_f63f65b5,\n author = {Weijters, Ton and van den Bosch, Antal and Postma, Eric},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Learning Statistically Neutral Tasks without Expert Guidance},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/f63f65b503e22cb970527f23c9ad7db1-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/f63f65b503e22cb970527f23c9ad7db1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/f63f65b503e22cb970527f23c9ad7db1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1708264,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3763228441855910545&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "94d4010769",
        "title": "Learning from User Feedback in Image Retrieval Systems",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/7283518d47a05a09d33779a17adf1707-Abstract.html",
        "author": "Nuno Vasconcelos; Andrew Lippman",
        "abstract": "We  formulate  the  problem  of retrieving  images  from  visual  databases  as  a problem of Bayesian inference.  This leads to  natural and effective  solutions for two of the most challenging issues in the design of a retrieval  system:  providing  support  for  region-based  queries  without  requiring  prior  image  segmentation,  and  accounting  for  user-feedback  during  a  retrieval  session.  We  present  a  new  learning  algorithm  that  relies  on  belief propagation to account for both positive and negative examples of  the user's interests.",
        "bibtex": "@inproceedings{NIPS1999_7283518d,\n author = {Vasconcelos, Nuno and Lippman, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Learning from User Feedback in Image Retrieval Systems},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7283518d47a05a09d33779a17adf1707-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/7283518d47a05a09d33779a17adf1707-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/7283518d47a05a09d33779a17adf1707-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1675708,
        "gs_citation": 186,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=27812585998796996&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "http://www.media.mit.edwnuno",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "304b8b1a2c",
        "title": "Learning the Similarity of Documents: An Information-Geometric Approach to Document Retrieval and Categorization",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html",
        "author": "Thomas Hofmann",
        "abstract": "The  project  pursued  in  this  paper  is  to  develop  from  first  information-geometric  principles  a  general  method  for  learning  the  similarity  between  text  documents.  Each  individual  docu(cid:173) ment  is  modeled  as  a  memoryless  information source.  Based  on  a  latent  class  decomposition of the  term-document  matrix, a  low(cid:173) dimensional  (curved)  multinomial subfamily is  learned.  From this  model a  canonical similarity function - known as the Fisher  kernel  - is  derived.  Our  approach  can  be  applied  for  unsupervised  and  supervised  learning problems alike.  This in  particular covers inter(cid:173) esting  cases  where  both,  labeled and  unlabeled  data are  available.  Experiments in  automated indexing and text categorization verify  the advantages of the proposed  method.",
        "bibtex": "@inproceedings{NIPS1999_9d268236,\n author = {Hofmann, Thomas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Learning the Similarity of Documents: An Information-Geometric Approach to Document Retrieval and Categorization},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/9d2682367c3935defcb1f9e247a97c0d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1716950,
        "gs_citation": 230,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12523340768439426283&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, Brown University, Providence, RI",
        "aff_domain": "cs.brown.edu",
        "email": "cs.brown.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Brown University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.brown.edu",
        "aff_unique_abbr": "Brown",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Providence",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "35c03a4c78",
        "title": "Learning to Parse Images",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/5a142a55461d5fef016acfb927fee0bd-Abstract.html",
        "author": "Geoffrey E. Hinton; Zoubin Ghahramani; Yee Whye Teh",
        "abstract": "We  describe a class of probabilistic models  that we  call credibility  networks.  Using  parse trees as  internal representations of images,  credibility  networks  are able  to  perform  segmentation  and  recog(cid:173) nition simultaneously,  removing the need for  ad hoc  segmentation  heuristics.  Promising results  in  the  problem of segmenting  hand(cid:173) written digits  were obtained.",
        "bibtex": "@inproceedings{NIPS1999_5a142a55,\n author = {Hinton, Geoffrey E and Ghahramani, Zoubin and Teh, Yee Whye},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Learning to Parse Images},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/5a142a55461d5fef016acfb927fee0bd-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/5a142a55461d5fef016acfb927fee0bd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/5a142a55461d5fef016acfb927fee0bd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1623814,
        "gs_citation": 91,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4139990718027630451&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Gatsby Computational Neuroscience Unit, University College London, London, United Kingdom WC1N 3AR; Gatsby Computational Neuroscience Unit, University College London, London, United Kingdom WC1N 3AR; Department of Computer Science, University of Toronto, Toronto, Ontario, Canada M5S 3G4",
        "aff_domain": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk;cs.utoronto.ca",
        "email": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk;cs.utoronto.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University College London;University of Toronto",
        "aff_unique_dep": "Gatsby Computational Neuroscience Unit;Department of Computer Science",
        "aff_unique_url": "https://www.ucl.ac.uk;https://www.utoronto.ca",
        "aff_unique_abbr": "UCL;U of T",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "London;Toronto",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United Kingdom;Canada"
    },
    {
        "id": "4fc55c6827",
        "title": "Leveraged Vector Machines",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/21be9a4bd4f81549a9d1d241981cec3c-Abstract.html",
        "author": "Yoram Singer",
        "abstract": "We describe an iterative algorithm for building vector machines used in  classification tasks.  The algorithm builds  on  ideas from  support vector  machines, boosting, and generalized additive models. The algorithm can  be  used  with  various continuously differential functions  that bound the  discrete (0-1) classification loss and is very simple to implement. We test  the proposed algorithm with two different loss functions on synthetic and  natural data. We also describe a norm-penalized version of the algorithm  for the exponential loss function used in AdaBoost.  The performance of  the algorithm on  natural data is  comparable to  support vector machines  while typically its running time is shorter than of SVM.",
        "bibtex": "@inproceedings{NIPS1999_21be9a4b,\n author = {Singer, Yoram},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Leveraged Vector Machines},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/21be9a4bd4f81549a9d1d241981cec3c-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/21be9a4bd4f81549a9d1d241981cec3c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/21be9a4bd4f81549a9d1d241981cec3c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1709986,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5384425480399850408&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Hebrew University",
        "aff_domain": "cs.huji.ac.il",
        "email": "cs.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Hebrew University of Jerusalem",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.huji.ac.il",
        "aff_unique_abbr": "HUJI",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "9b318306ab",
        "title": "Local Probability Propagation for Factor Analysis",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/96de2547f44254c97f5f4f1f402711c1-Abstract.html",
        "author": "Brendan J. Frey",
        "abstract": "Ever since Pearl's probability propagation algorithm in graphs with  cycles  was  shown  to  produce excellent  results  for  error-correcting  decoding  a  few  years  ago,  we  have  been  curious  about  whether  local  probability  propagation  could  be  used  successfully  for  ma(cid:173) chine  learning.  One  of the simplest  adaptive models  is  the factor  analyzer,  which  is  a  two-layer  network  that  models  bottom  layer  sensory inputs as a  linear combination of top layer factors  plus in(cid:173) dependent  Gaussian  sensor noise.  We  show  that local probability  propagation in the factor analyzer network usually takes just a few  iterations to perform accurate inference, even in networks with 320  sensors and 80 factors.  We derive an expression for  the algorithm's  fixed  point and show that this fixed  point matches the exact solu(cid:173) tion in a variety of networks, even when the fixed point is  unstable.  We also show that this method can be used successfully to perform  inference for approximate EM and we give results on an online face  recognition task.  1  Factor analysis  A  simple way  to encode  input patterns is  to suppose that  each input  can  be  well(cid:173) approximated by a linear combination of component vectors,  where the amplitudes  of the vectors are modulated to match the input.  For a given training set,  the most  appropriate set  of component vectors  will  depend  on  how  we  expect  the  modula(cid:173) tion levels  to  behave  and how  we  measure the distance  between  the input  and its  approximation.  These  effects  can  be  captured  by  a  generative  probabilit~ model  that  specifies  a  distribution  p(z)  over  modulation  levels  z  =  (Zl, ... ,ZK)  and  a  distribution  p(xlz)  over  sensors  x  =  (Xl, ... ,XN)T  given  the  modulation  levels.  Principal component analysis,  independent component analysis and factor analysis  can be viewed as maximum likelihood learning in a model of this type, where we as(cid:173) sume that over the training set, the appropriate modulation levels  are independent  and the overall distortion is  given  by  the sum of the individual sensor distortions.",
        "bibtex": "@inproceedings{NIPS1999_96de2547,\n author = {Frey, Brendan J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Local Probability Propagation for Factor Analysis},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/96de2547f44254c97f5f4f1f402711c1-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/96de2547f44254c97f5f4f1f402711c1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/96de2547f44254c97f5f4f1f402711c1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1655456,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18374011637565226378&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Computer Science, University of Waterloo, Waterloo, Ontario, Canada",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Waterloo",
        "aff_unique_dep": "Computer Science",
        "aff_unique_url": "https://uwaterloo.ca",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Waterloo",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "c797fee223",
        "title": "Low Power Wireless Communication via Reinforcement Learning",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/54f5f4071faca32ad5285fef87b78646-Abstract.html",
        "author": "Timothy X. Brown",
        "abstract": "This paper examines the application of reinforcement learning to a wire(cid:173) less  communication problem.  The problem requires  that channel  util(cid:173) ity  be  maximized while simultaneously minimizing battery  usage.  We  present a  solution  to  this  multi-criteria problem  that  is  able  to  signifi(cid:173) cantly reduce power consumption.  The solution uses a variable discount  factor to capture the effects of battery usage.",
        "bibtex": "@inproceedings{NIPS1999_54f5f407,\n author = {Brown, Timothy},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Low Power Wireless Communication via Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/54f5f4071faca32ad5285fef87b78646-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/54f5f4071faca32ad5285fef87b78646-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/54f5f4071faca32ad5285fef87b78646-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1457778,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12279636881406358753&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Electrical and Computer Engineering, University of Colorado",
        "aff_domain": "colorado.edu",
        "email": "colorado.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Colorado",
        "aff_unique_dep": "Electrical and Computer Engineering",
        "aff_unique_url": "https://www.colorado.edu",
        "aff_unique_abbr": "CU",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "274e8c65a3",
        "title": "Lower Bounds on the Complexity of Approximating Continuous Functions by Sigmoidal Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/4921f95baf824205e1b13f22d60357a1-Abstract.html",
        "author": "Michael Schmitt",
        "abstract": "We calculate lower bounds on the size of sigmoidal neural networks  that  approximate  continuous  functions.  In  particular,  we  show  that  for  the  approximation  of  polynomials  the  network  size  has  to grow as  O((logk)1/4)  where  k  is  the degree of the polynomials.  This bound is  valid for  any input dimension, i.e.  independently of  the  number of variables.  The result  is  obtained  by  introducing a  new method employing upper bounds on the Vapnik-Chervonenkis  dimension  for  proving lower  bounds  on  the  size  of networks  that  approximate continuous functions.",
        "bibtex": "@inproceedings{NIPS1999_4921f95b,\n author = {Schmitt, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Lower Bounds on the Complexity of Approximating Continuous Functions by Sigmoidal Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/4921f95baf824205e1b13f22d60357a1-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/4921f95baf824205e1b13f22d60357a1-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/4921f95baf824205e1b13f22d60357a1-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1733371,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4330544771542248396&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Lehrstuhl Mathematik und Informatik FakuWit ftir Mathematik Ruhr-Universitat Bochum D-44780 Bochum, Germany",
        "aff_domain": "lmi.ruhr-uni-bochum.de",
        "email": "lmi.ruhr-uni-bochum.de",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Ruhr-Universit\u00e4t Bochum",
        "aff_unique_dep": "Lehrstuhl Mathematik und Informatik",
        "aff_unique_url": "https://www.ruhr-uni-bochum.de",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Bochum",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "4167858dd8",
        "title": "Managing Uncertainty in Cue Combination",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/6e62a992c676f611616097dbea8ea030-Abstract.html",
        "author": "Zhiyong Yang; Richard S. Zemel",
        "abstract": "We develop a hierarchical generative model to study cue combi(cid:173) nation.  The model maps a global shape parameter to local cue(cid:173) specific  parameters,  which in tum generate  an intensity image.  Inferring shape from images is achieved by inverting this model.  Inference produces a probability distribution at each level; using  distributions rather than a single value of underlying variables at  each stage preserves information about the validity  of each local  cue for the given image.  This allows the model, unlike standard  combination models, to adaptively weight each cue based on gen(cid:173) eral  cue  reliability  and  specific  image context.  We  describe  the  results  of a cue combination psychophysics experiment we con(cid:173) ducted that allows a direct comparison with the model. The model  provides a good fit to our data and a natural account for some in(cid:173) teresting aspects of cue combination.",
        "bibtex": "@inproceedings{NIPS1999_6e62a992,\n author = {Yang, Zhiyong and Zemel, Richard},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Managing Uncertainty in Cue Combination},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/6e62a992c676f611616097dbea8ea030-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/6e62a992c676f611616097dbea8ea030-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/6e62a992c676f611616097dbea8ea030-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1715375,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17178111247277030729&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Department of Neurobiology, Box 3209, Duke University Medical Center, Durham, NC 27710; Department of Psychology, University of Arizona, Tucson, AZ 85721",
        "aff_domain": "duke.edu;u.arizona.edu",
        "email": "duke.edu;u.arizona.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Duke University;University of Arizona",
        "aff_unique_dep": "Department of Neurobiology;Department of Psychology",
        "aff_unique_url": "https://www.duke.edu;https://www.arizona.edu",
        "aff_unique_abbr": "Duke;UArizona",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Durham;Tucson",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7601daf4a5",
        "title": "Manifold Stochastic Dynamics for Bayesian Learning",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/d2cdf047a6674cef251d56544a3cf029-Abstract.html",
        "author": "Mark Zlochin; Yoram Baram",
        "abstract": "We propose a new Markov Chain Monte Carlo algorithm which is a gen(cid:173) eralization of the stochastic dynamics method.  The algorithm performs  exploration of the state space using its intrinsic geometric structure, facil(cid:173) itating efficient sampling of complex distributions. Applied to Bayesian  learning in neural networks, our algorithm was found to perform at least  as well as the best state-of-the-art method while consuming considerably  less time.",
        "bibtex": "@inproceedings{NIPS1999_d2cdf047,\n author = {Zlochin, Mark and Baram, Yoram},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Manifold Stochastic Dynamics for Bayesian Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/d2cdf047a6674cef251d56544a3cf029-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/d2cdf047a6674cef251d56544a3cf029-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/d2cdf047a6674cef251d56544a3cf029-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1287784,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=839177506419908683&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computer Science Technion -Israel Institute of Technology; Department of Computer Science Technion -Israel Institute of Technology",
        "aff_domain": "cs.technion.ac.il;cs.technion.ac.il",
        "email": "cs.technion.ac.il;cs.technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Technion -Israel Institute of Technology",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.technion.ac.il",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "752dd04e1b",
        "title": "Maximum Entropy Discrimination",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/4fa53be91b4933d536748a60458b9797-Abstract.html",
        "author": "Tommi Jaakkola; Marina Meila; Tony Jebara",
        "abstract": "We present a general framework for discriminative estimation based  on the maximum entropy principle and its extensions.  All  calcula(cid:173) tions involve distributions over structures and/or parameters rather  than  specific  settings  and  reduce  to  relative  entropy  projections.  This holds  even  when  the data is  not  separable within  the chosen  parametric class,  in  the context of anomaly  detection  rather than  classification, or when the labels in the training set are uncertain or  incomplete.  Support  vector  machines  are  naturally subsumed  un(cid:173) der  this  class  and we  provide several extensions.  We  are also  able  to estimate exactly and efficiently discriminative distributions over  tree structures of class-conditional  models  within  this framework.  Preliminary experimental results  are  indicative of the potential in  these techniques.",
        "bibtex": "@inproceedings{NIPS1999_4fa53be9,\n author = {Jaakkola, Tommi and Meila, Marina and Jebara, Tony},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Maximum Entropy Discrimination},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/4fa53be91b4933d536748a60458b9797-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/4fa53be91b4933d536748a60458b9797-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/4fa53be91b4933d536748a60458b9797-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1515229,
        "gs_citation": 354,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4818152492456458732&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "MIT AI Lab; MIT AI Lab; MIT Media Lab",
        "aff_domain": "ai.mit.edu;ai.mit.edu;media.mit.edu",
        "email": "ai.mit.edu;ai.mit.edu;media.mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Artificial Intelligence Laboratory",
        "aff_unique_url": "http://www.ai.mit.edu",
        "aff_unique_abbr": "MIT AI Lab",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b9f1dc7dc4",
        "title": "Memory Capacity of Linear vs. Nonlinear Models of Dendritic Integration",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/e4873aa9a05cc5ed839561d121516766-Abstract.html",
        "author": "Panayiota Poirazi; Bartlett W. Mel",
        "abstract": "Previous biophysical modeling work showed that nonlinear interac(cid:173) tions among nearby synapses located on active dendritic trees can  provide a large boost in the memory capacity of a cell  (Mel,  1992a,  1992b).  The aim of our present work is  to quantify  this  boost by  estimating  the  capacity  of  (1)  a  neuron  model  with  passive  den(cid:173) dritic  integration  where  inputs  are  combined  linearly  across  the  entire cell  followed  by a  single  global  threshold,  and  (2)  an active  dendrite  model  in  which  a  threshold  is  applied  separately  to  the  output of each branch, and the branch subtotals are combined lin(cid:173) early.  We focus  here on the limiting case of binary-valued synaptic  weights,  and derive expressions which  measure model  capacity by  estimating the number of distinct input-output functions  available  to both neuron types.  We  show that  (1)  the application of a fixed  nonlinearity to each dendritic compartment substantially increases  the model's flexibility,  (2) for a neuron of realistic size, the capacity  of the nonlinear cell can exceed that of the same-sized linear cell by  more than an order of magnitude, and (3)  the largest capacity boost  occurs for cells with a relatively large number of dendritic subunits  of relatively  small  size.  We  validated  the analysis  by  empirically  measuring memory  capacity with  randomized two-class  classifica(cid:173) tion problems, where a stochastic delta rule was used to train both  linear and nonlinear models.  We  found  that large capacity boosts  predicted for  the nonlinear  dendritic  model  were  readily  achieved  in practice.",
        "bibtex": "@inproceedings{NIPS1999_e4873aa9,\n author = {Poirazi, Panayiota and Mel, Bartlett},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Memory Capacity of Linear vs. Nonlinear Models of Dendritic Integration},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e4873aa9a05cc5ed839561d121516766-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/e4873aa9a05cc5ed839561d121516766-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/e4873aa9a05cc5ed839561d121516766-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1644322,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16181011925963195679&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Biomedical Engineering Department, University of Southern California; Biomedical Engineering Department, University of Southern California",
        "aff_domain": "sc.usc.edu;lnc.usc.edu",
        "email": "sc.usc.edu;lnc.usc.edu",
        "github": "",
        "project": "http://lnc.usc.edu",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Southern California",
        "aff_unique_dep": "Biomedical Engineering Department",
        "aff_unique_url": "https://www.usc.edu",
        "aff_unique_abbr": "USC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1cbc173eb1",
        "title": "Mixture Density Estimation",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/a0f3601dc682036423013a5d965db9aa-Abstract.html",
        "author": "Jonathan Q. Li; Andrew R. Barron",
        "abstract": "Gaussian mixtures (or so-called radial basis function networks)  for  density estimation provide a natural counterpart to sigmoidal neu(cid:173) ral networks for function fitting and approximation.  In both cases,  it is  possible to give  simple expressions for  the iterative improve(cid:173) ment of performance as components of the network are introduced  one at a time.  In particular, for mixture density estimation we show  that a k-component mixture estimated by maximum likelihood  (or  by an iterative likelihood improvement that we introduce) achieves  log-likelihood  within order  1/k of the log-likelihood  achievable by  any convex combination.  Consequences for  approximation and es(cid:173) timation  using  Kullback-Leibler  risk  are  also  given.  A  Minimum  Description Length principle selects the optimal number of compo(cid:173) nents k that minimizes the risk bound.",
        "bibtex": "@inproceedings{NIPS1999_a0f3601d,\n author = {Li, Jonathan and Barron, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Mixture Density Estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/a0f3601dc682036423013a5d965db9aa-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/a0f3601dc682036423013a5d965db9aa-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/a0f3601dc682036423013a5d965db9aa-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1426571,
        "gs_citation": 392,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1959410733262921665&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Statistics, Yale University; Department of Statistics, Yale University",
        "aff_domain": "aya.yale.edu;yale.edu",
        "email": "aya.yale.edu;yale.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Yale University",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.yale.edu",
        "aff_unique_abbr": "Yale",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1f3830e8b1",
        "title": "Model Selection for Support Vector Machines",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/e449b9317dad920c0dd5ad0a2a2d5e49-Abstract.html",
        "author": "Olivier Chapelle; Vladimir Vapnik",
        "abstract": "New functionals for parameter (model) selection of Support Vector Ma(cid:173) chines are introduced based on the concepts of the span of support vec(cid:173) tors and rescaling of the feature space.  It is shown that using these func(cid:173) tionals, one can both predict the best choice of parameters of the model  and the relative quality of performance for any value of parameter.",
        "bibtex": "@inproceedings{NIPS1999_e449b931,\n author = {Chapelle, Olivier and Vapnik, Vladimir},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Model Selection for Support Vector Machines},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e449b9317dad920c0dd5ad0a2a2d5e49-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/e449b9317dad920c0dd5ad0a2a2d5e49-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/e449b9317dad920c0dd5ad0a2a2d5e49-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1363335,
        "gs_citation": 514,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13489326809474286323&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "AT&T Research Labs, Red Bank, NJ; AT&T Research Labs, Red Bank, NJ + LIP6, Paris, France",
        "aff_domain": "research.au.com;research.au.com",
        "email": "research.au.com;research.au.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "AT&T Research Labs;Laboratoire d'Informatique de Paris 6",
        "aff_unique_dep": ";LIP6",
        "aff_unique_url": "https://www.att.com/research;https://www.lip6.fr",
        "aff_unique_abbr": "AT&T;LIP6",
        "aff_campus_unique_index": "0;0+1",
        "aff_campus_unique": "Red Bank;Paris",
        "aff_country_unique_index": "0;0+1",
        "aff_country_unique": "United States;France"
    },
    {
        "id": "83b0eff535",
        "title": "Model Selection in Clustering by Uniform Convergence Bounds",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/757f843a169cc678064d9530d12a1881-Abstract.html",
        "author": "Joachim M. Buhmann; Marcus Held",
        "abstract": "Unsupervised  learning  algorithms  are  designed  to  extract  struc(cid:173) ture from  data samples.  Reliable  and robust  inference  requires  a  guarantee that extracted structures are typical for the data source,  Le.,  similar  structures  have  to be  inferred  from  a  second  sample  set of the same data source.  The overfitting phenomenon in max(cid:173) imum  entropy  based  annealing  algorithms  is  exemplarily  studied  for  a  class of histogram clustering  models.  Bernstein's  inequality  for  large deviations is used to determine the maximally achievable  approximation  quality  parameterized  by  a  minimal  temperature.  Monte Carlo simulations support the proposed model selection cri(cid:173) terion by finite  temperature annealing.",
        "bibtex": "@inproceedings{NIPS1999_757f843a,\n author = {Buhmann, Joachim and Held, Marcus},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Model Selection in Clustering by Uniform Convergence Bounds},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/757f843a169cc678064d9530d12a1881-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/757f843a169cc678064d9530d12a1881-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/757f843a169cc678064d9530d12a1881-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1696188,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8145104689071339339&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Institut flir Informatik III, RomerstraBe 164, D-53117 Bonn, Germany; Institut flir Informatik III, RomerstraBe 164, D-53117 Bonn, Germany",
        "aff_domain": "cs.uni-bonn.de;cs.uni-bonn.de",
        "email": "cs.uni-bonn.de;cs.uni-bonn.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Institut f\u00fcr Informatik III",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "72f2900b47",
        "title": "Modeling High-Dimensional Discrete Data with Multi-Layer Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/e6384711491713d29bc63fc5eeb5ba4f-Abstract.html",
        "author": "Yoshua Bengio; Samy Bengio",
        "abstract": "The curse of dimensionality is  severe when modeling high-dimensional  discrete data:  the number of possible combinations of the variables ex(cid:173) plodes exponentially.  In  this  paper we propose a  new  architecture for  modeling high-dimensional data that requires resources (parameters and  computations) that grow only at most as the square of the number of vari(cid:173) ables,  using a multi-layer neural  network to represent the joint distribu(cid:173) tion of the variables as the product of conditional distributions. The neu(cid:173) ral  network can be interpreted as  a graphical model  without hidden ran(cid:173) dom variables, but in which the conditional distributions are tied through  the hidden units. The connectivity of the neural network can be pruned by  using dependency tests between the variables. Experiments on modeling  the distribution of several discrete data sets show statistically significant  improvements over other methods such as  naive Bayes and comparable  Bayesian networks,  and show  that significant improvements can be ob(cid:173) tained by pruning the network.",
        "bibtex": "@inproceedings{NIPS1999_e6384711,\n author = {Bengio, Yoshua and Bengio, Samy},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Modeling High-Dimensional Discrete Data with Multi-Layer Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e6384711491713d29bc63fc5eeb5ba4f-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/e6384711491713d29bc63fc5eeb5ba4f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/e6384711491713d29bc63fc5eeb5ba4f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1673690,
        "gs_citation": 215,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5662192127574743121&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff": "Dept.IRO, Universite de Montreal, Montreal, Qc, Canada, H3C 317; IDIAP, CP 592, rue du Simplon 4, 1920 Martigny, Switzerland + CIRANO, Montreal, Qc, Canada",
        "aff_domain": "iro.umontreal.ca;idiap.ch",
        "email": "iro.umontreal.ca;idiap.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2",
        "aff_unique_norm": "Universite de Montreal;IDIAP;CIRANO",
        "aff_unique_dep": "Dept.IRO;;",
        "aff_unique_url": "https://www.umontreal.ca;https://www.idiap.ch;https://www.cirano.org",
        "aff_unique_abbr": "UdeM;;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Montreal;",
        "aff_country_unique_index": "0;1+0",
        "aff_country_unique": "Canada;Switzerland"
    },
    {
        "id": "5405f524c0",
        "title": "Monte Carlo POMDPs",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/299570476c6f0309545110c592b6a63b-Abstract.html",
        "author": "Sebastian Thrun",
        "abstract": "We  present  a Monte Carlo algorithm for  learning to  act  in  partially observable  Markov decision processes (POMDPs) with real-valued state and action spaces.  Our approach uses importance sampling for representing beliefs, and Monte Carlo  approximation for belief propagation.  A reinforcement learning algorithm, value  iteration, is employed to learn value functions over belief states. Finally, a sample(cid:173) based  version  of nearest  neighbor  is  used  to  generalize  across  states.  Initial  empirical results suggest that our approach works well in practical applications.",
        "bibtex": "@inproceedings{NIPS1999_29957047,\n author = {Thrun, Sebastian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Monte Carlo POMDPs},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/299570476c6f0309545110c592b6a63b-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/299570476c6f0309545110c592b6a63b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/299570476c6f0309545110c592b6a63b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1509059,
        "gs_citation": 411,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4241898648866726497&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 22,
        "aff": "School of Computer Science, Carnegie Mellon University",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b9099f3566",
        "title": "Neural Computation with Winner-Take-All as the Only Nonlinear Operation",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/1c54985e4f95b7819ca0357c0cb9a09f-Abstract.html",
        "author": "Wolfgang Maass",
        "abstract": "Everybody \"knows\" that neural networks need more than a single layer  of nonlinear units to compute interesting functions.  We show that this is  false if one employs winner-take-all as nonlinear unit:",
        "bibtex": "@inproceedings{NIPS1999_1c54985e,\n author = {Maass, Wolfgang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Neural Computation with Winner-Take-All as the Only Nonlinear Operation},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/1c54985e4f95b7819ca0357c0cb9a09f-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/1c54985e4f95b7819ca0357c0cb9a09f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/1c54985e4f95b7819ca0357c0cb9a09f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1145094,
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8036691329768982145&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "http://www.cis.tu-graz.ac.atiigi/maass",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "1f98134b48",
        "title": "Neural Network Based Model Predictive Control",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/db957c626a8cd7a27231adfbf51e20eb-Abstract.html",
        "author": "Stephen Piche; James D. Keeler; Greg Martin; Gene Boe; Doug Johnson; Mark Gerules",
        "abstract": "Model  Predictive  Control  (MPC),  a  control  algorithm which  uses  an  optimizer  to solve  for  the optimal  control  moves  over  a  future  time horizon based upon a model of the process, has become a stan(cid:173) dard control technique  in  the process industries over  the  past  two  decades.  In  most  industrial  applications,  a  linear  dynamic  model  developed using empirical data is  used even though the process it(cid:173) self is often nonlinear.  Linear models have been used because of the  difficulty  in  developing  a  generic  nonlinear  model  from  empirical  data and  the  computational expense  often involved  in  using  non(cid:173) linear models.  In  this  paper,  we  present  a  generic neural  network  based technique for developing nonlinear dynamic models from em(cid:173) pirical  data and show that these  models  can be efficiently  used  in  a  model predictive control framework.  This nonlinear MPC based  approach has been successfully implemented in a  number of indus(cid:173) trial  applications  in  the  refining,  petrochemical,  paper  and  food  industries.  Performance of the controller on  a  nonlinear industrial  process, a  polyethylene reactor, is  presented.",
        "bibtex": "@inproceedings{NIPS1999_db957c62,\n author = {Piche, Stephen and Keeler, James and Martin, Greg and Boe, Gene and Johnson, Doug and Gerules, Mark},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Neural Network Based Model Predictive Control},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/db957c626a8cd7a27231adfbf51e20eb-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/db957c626a8cd7a27231adfbf51e20eb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/db957c626a8cd7a27231adfbf51e20eb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1513936,
        "gs_citation": 264,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5377439060038532159&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Pavilion Technologies; Pavilion Technologies; Pavilion Technologies; Pavilion Technologies; Pavilion Technologies; Pavilion Technologies",
        "aff_domain": "pav.com;pav.com;pav.com;pav.com;pav.com;pav.com",
        "email": "pav.com;pav.com;pav.com;pav.com;pav.com;pav.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Pavilion Technologies",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.paviliontech.com",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "20fa6c6e76",
        "title": "Neural Representation of Multi-Dimensional Stimuli",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/e22dd5dabde45eda5a1a67772c8e25dd-Abstract.html",
        "author": "Christian W. Eurich; Stefan D. Wilke; Helmut Schwegler",
        "abstract": "Spatial  information comes in two forms:  direct spatial information (for  example, retinal position) and indirect temporal contiguity information,  since objects encountered sequentially are in general spatially close. The  acquisition  of spatial  information  by  a  neural  network  is  investigated  here. Given a spatial layout of several objects, networks are trained on a  prediction task.  Networks using temporal sequences with no direct spa(cid:173) tial information are found to  develop internal representations that show  distances correlated with distances in the external layout.  The influence  of spatial information is analyzed by providing direct spatial information  to the system during training that is  either consistent with the layout or  inconsistent with  it.  This  approach  allows  examination of the relative  contributions of spatial and temporal contiguity.",
        "bibtex": "@inproceedings{NIPS1999_e22dd5da,\n author = {Eurich, Christian and Wilke, Stefan and Schwegler, Helmut},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Neural Representation of Multi-Dimensional Stimuli},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e22dd5dabde45eda5a1a67772c8e25dd-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/e22dd5dabde45eda5a1a67772c8e25dd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/e22dd5dabde45eda5a1a67772c8e25dd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 3067786,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16216239891698655964&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "9bce93dd22",
        "title": "Neural System Model of Human Sound Localization",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/ab2b41c63853f0a651ba9fbf502b0cd8-Abstract.html",
        "author": "Craig T. Jin; Simon Carlile",
        "abstract": "This paper examines the role of biological constraints in the human audi(cid:173) tory localization process.  A psychophysical and neural system modeling  approach  was  undertaken  in which  performance  comparisons between  competing  models  and  a  human  subject  explore  the  relevant  biologi(cid:173) cally plausible  \"realism  constraints\".  The  directional  acoustical  cues,  upon which sound localization is  based,  were  derived  from  the  human  subject's  head-related transfer functions  (HRTFs).  Sound stimuli  were  generated by convolving bandpass noise with the HRTFs and were pre(cid:173) sented to both the subject and the model.  The input stimuli to the model  was processed using the Auditory Image Model of cochlear processing.  The  cochlear  data  was  then  analyzed  by  a  time-delay  neural  network  which integrated temporal and spectral information to determine the spa(cid:173) tial  location  of the  sound  source.  The  combined  cochlear  model  and  neural network provided a system model of the sound localization pro(cid:173) cess.  Human-like  localization performance  was  qualitatively  achieved  for broadband and bandpass stimuli when the model architecture incor(cid:173) porated frequency division (or tonotopicity), and was trained using vari(cid:173) able bandwidth and center-frequency sounds.",
        "bibtex": "@inproceedings{NIPS1999_ab2b41c6,\n author = {Jin, Craig and Carlile, Simon},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Neural System Model of Human Sound Localization},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/ab2b41c63853f0a651ba9fbf502b0cd8-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/ab2b41c63853f0a651ba9fbf502b0cd8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/ab2b41c63853f0a651ba9fbf502b0cd8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1688163,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8985050793721929574&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Physiology and Department of Electrical Engineering, Univ. of Sydney, NSW 2006, Australia; Department of Physiology and Institute of Biomedical Research, Univ. of Sydney, NSW 2006, Australia",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Sydney",
        "aff_unique_dep": "Department of Physiology",
        "aff_unique_url": "https://www.sydney.edu.au",
        "aff_unique_abbr": "USYD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Sydney",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "c53542c33d",
        "title": "Noisy Neural Networks and Generalizations",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/94e4451ad23909020c28b26ca3a13cb8-Abstract.html",
        "author": "Hava T. Siegelmann; Alexander Roitershtein; Asa Ben-Hur",
        "abstract": "In this paper we  define  a  probabilistic computational model which  generalizes many noisy neural network models, including the recent  work  of Maass and Sontag [5].  We identify weak  ergodicjty  as  the  mechanism responsible for  restriction  of the computational power  of  probabilistic  models  to  definite  languages,  independent  of the  characteristics  of the noise:  whether  it is  discrete  or analog,  or if  it  depends  on  the  input  or  not,  and  independent  of whether  the  variables  are  discrete  or continuous.  We  give examples of weakly  ergodic  models including  noisy  computational systems  with noise  depending on the current state and inputs, aggregate models,  and  computational systems which update in continuous time.",
        "bibtex": "@inproceedings{NIPS1999_94e4451a,\n author = {Siegelmann, Hava and Roitershtein, Alexander and Ben-Hur, Asa},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Noisy Neural Networks and Generalizations},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/94e4451ad23909020c28b26ca3a13cb8-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/94e4451ad23909020c28b26ca3a13cb8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/94e4451ad23909020c28b26ca3a13cb8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1580969,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5758716151140602169&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Industrial Eng. and Management, Mathematics, Technion -lIT, Haifa 32000, Israel; Mathematics, Technion -lIT, Haifa 32000, Israel; Industrial Eng. and Management, Technion -lIT, Haifa 32000, Israel",
        "aff_domain": "ie.technion.ac.il;math.technion.ac.il;tx.technion.ac. il",
        "email": "ie.technion.ac.il;math.technion.ac.il;tx.technion.ac. il",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Technion - Israel Institute of Technology",
        "aff_unique_dep": "Industrial Engineering and Management, Mathematics",
        "aff_unique_url": "https://www.technion.ac.il",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Haifa",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "742693f3d9",
        "title": "Nonlinear Discriminant Analysis Using Kernel Functions",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/c0d0e461de8d0024aebcb0a7c68836df-Abstract.html",
        "author": "Volker Roth; Volker Steinhage",
        "abstract": "Fishers linear discriminant analysis  (LDA)  is  a  classical multivari(cid:173) ate technique both for  dimension reduction and classification.  The  data vectors are transformed into a low dimensional subspace such  that  the  class  centroids  are  spread  out  as  much  as  possible.  In  this subspace LDA  works  as  a  simple  prototype classifier  with lin(cid:173) ear decision  boundaries.  However,  in many applications the linear  boundaries  do  not  adequately separate the  classes.  We  present  a  nonlinear generalization of discriminant analysis that uses the ker(cid:173) nel trick of representing dot products by kernel functions.  The pre(cid:173) sented algorithm allows a simple formulation of the EM-algorithm  in terms of kernel functions which leads to a unique concept for  un(cid:173) supervised  mixture analysis,  supervised discriminant  analysis  and  semi-supervised discriminant analysis with partially unlabelled ob(cid:173) servations in feature spaces.",
        "bibtex": "@inproceedings{NIPS1999_c0d0e461,\n author = {Roth, Volker and Steinhage, Volker},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Nonlinear Discriminant Analysis Using Kernel Functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/c0d0e461de8d0024aebcb0a7c68836df-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/c0d0e461de8d0024aebcb0a7c68836df-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/c0d0e461de8d0024aebcb0a7c68836df-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1651567,
        "gs_citation": 391,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4930785488407977904&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "University of Bonn, Institut of Computer Science III; University of Bonn, Institut of Computer Science III",
        "aff_domain": "cs.uni-bonn.de;cs.uni-bonn.de",
        "email": "cs.uni-bonn.de;cs.uni-bonn.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Bonn",
        "aff_unique_dep": "Institut of Computer Science III",
        "aff_unique_url": "https://www.uni-bonn.de",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "822a2f7847",
        "title": "On Input Selection with Reversible Jump Markov Chain Monte Carlo Sampling",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/c1fea270c48e8079d8ddf7d06d26ab52-Abstract.html",
        "author": "Peter Sykacek",
        "abstract": "In this paper we will treat input selection for a radial basis function  (RBF) like classifier within a Bayesian framework.  We approximate  the a-posteriori distribution over both model coefficients  and input  subsets  by samples drawn with Gibbs updates and reversible jump  moves.  Using some  public  datasets,  we  compare the  classification  accuracy  of the  method with  a  conventional  ARD  scheme.  These  datasets  are  also used  to  infer the  a-posteriori  probabilities of dif(cid:173) ferent  input subsets.",
        "bibtex": "@inproceedings{NIPS1999_c1fea270,\n author = {Sykacek, Peter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {On Input Selection with Reversible Jump Markov Chain Monte Carlo Sampling},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/c1fea270c48e8079d8ddf7d06d26ab52-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/c1fea270c48e8079d8ddf7d06d26ab52-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/c1fea270c48e8079d8ddf7d06d26ab52-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1462430,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=629311971840951589&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Austrian Research Institute for Artificial Intelligence (OFAI)",
        "aff_domain": "ai.univie.ac.at",
        "email": "ai.univie.ac.at",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Austrian Research Institute for Artificial Intelligence",
        "aff_unique_dep": "Artificial Intelligence",
        "aff_unique_url": "https://www.ofai.at",
        "aff_unique_abbr": "OFAI",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Austria"
    },
    {
        "id": "7f08dce4ff",
        "title": "Online Independent Component Analysis with Local Learning Rate Adaptation",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/7437d136770f5b35194cb46c1653efaa-Abstract.html",
        "author": "Nicol N. Schraudolph; Xavier Giannakopoulos",
        "abstract": "Stochastic meta-descent (SMD) is a new technique for online adap(cid:173) tation  of local  learning  rates  in  arbitrary twice-differentiable  sys(cid:173) tems.  Like matrix momentum it uses full  second-order information  while  retaining  O(n)  computational  complexity  by  exploiting  the  efficient  computation  of Hessian-vector  products.  Here  we  apply  SMD  to independent  component  analysis,  and  employ  the  result(cid:173) ing  algorithm  for  the  blind  separation  of time-varying  mixtures.  By matching individual learning rates to the rate of change in each  source signal's  mixture coefficients,  our technique is  capable of si(cid:173) multaneously tracking sources that move at very different,  a priori  unknown speeds.",
        "bibtex": "@inproceedings{NIPS1999_7437d136,\n author = {Schraudolph, Nicol and Giannakopoulos, Xavier},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Online Independent Component Analysis with Local Learning Rate Adaptation},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7437d136770f5b35194cb46c1653efaa-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/7437d136770f5b35194cb46c1653efaa-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/7437d136770f5b35194cb46c1653efaa-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1371787,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2259083848045220276&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "IDSIA, Corso Elvezia 36 6900 Lugano, Switzerland; IDSIA, Corso Elvezia 36 6900 Lugano, Switzerland",
        "aff_domain": "idsia.ch;idsia.ch",
        "email": "idsia.ch;idsia.ch",
        "github": "",
        "project": "http://www.idsia.ch/",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "IDSIA",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.idsia.ch",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "5ecaea16be",
        "title": "Optimal Kernel Shapes for Local Linear Regression",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/d8d31bd778da8bdd536187c36e48892b-Abstract.html",
        "author": "Dirk Ormoneit; Trevor Hastie",
        "abstract": "Local linear regression performs very well in many low-dimensional  forecasting problems.  In high-dimensional spaces, its performance  typically  decays  due  to the  well-known  \"curse-of-dimensionality\".  A possible way to approach this problem is  by varying the  \"shape\"  of the weighting kernel.  In this work we suggest a new, data-driven  method to estimating the optimal  kernel  shape.  Experiments  us(cid:173) ing an artificially generated data set and data from  the UC  Irvine  repository show the benefits of kernel shaping.",
        "bibtex": "@inproceedings{NIPS1999_d8d31bd7,\n author = {Ormoneit, Dirk and Hastie, Trevor},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Optimal Kernel Shapes for Local Linear Regression},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/d8d31bd778da8bdd536187c36e48892b-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/d8d31bd778da8bdd536187c36e48892b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/d8d31bd778da8bdd536187c36e48892b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1596776,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15940085539426799629&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Statistics, Stanford University; Department of Statistics, Stanford University",
        "aff_domain": "stat.stanford.edu; ",
        "email": "stat.stanford.edu; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f55226c3f5",
        "title": "Optimal Sizes of Dendritic and Axonal Arbors",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/270edd69788dce200a3b395a6da6fdb7-Abstract.html",
        "author": "Dmitri B. Chklovskii",
        "abstract": "I consider a topographic projection between two neuronal layers with dif(cid:173) ferent densities of neurons.  Given  the  number of output  neurons con(cid:173) nected to each input neuron (divergence or fan-out) and the number of  input neurons synapsing on each output neuron (convergence or fan-in) I  determine the widths of axonal and dendritic arbors which minimize the  total  volume ofaxons and dendrites.  My analytical results can be sum(cid:173) marized qualitatively in the following rule:  neurons of the sparser layer  should have arbors wider than those of the denser layer.  This agrees with  the anatomical data from retinal and cerebellar neurons whose morphol(cid:173) ogy and connectivity are known.  The rule may be used to infer connec(cid:173) tivity of neurons from their morphology.",
        "bibtex": "@inproceedings{NIPS1999_270edd69,\n author = {Chklovskii, Dmitri},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Optimal Sizes of Dendritic and Axonal Arbors},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/270edd69788dce200a3b395a6da6fdb7-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/270edd69788dce200a3b395a6da6fdb7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/270edd69788dce200a3b395a6da6fdb7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1452043,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:8TnK-UzILQwJ:scholar.google.com/&scioq=Optimal+Sizes+of+Dendritic+and+Axonal+Arbors&hl=en&as_sdt=0,5",
        "gs_version_total": 9,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "406cb5daec",
        "title": "Perceptual Organization Based on Temporal Dynamics",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/851300ee84c2b80ed40f51ed26d866fc-Abstract.html",
        "author": "Xiuwen Liu; DeLiang L. Wang",
        "abstract": "A figure-ground  segregation  network is  proposed  based on a  novel  boundary  pair  representation.  Nodes  in  the  network  are  bound(cid:173) ary  segments  obtained  through  local  grouping.  Each  node  is  ex(cid:173) citatorily  coupled  with  the  neighboring  nodes  that  belong  to  the  same region, and inhibitorily coupled with the corresponding paired  node.  Gestalt grouping rules are incorporated by  modulating con(cid:173) nections.  The  status  of  a  node  represents  its  probability  being  figural  and  is  updated  according  to  a  differential  equation.  The  system solves the figure-ground segregation problem through tem(cid:173) poral  evolution.  Different  perceptual  phenomena,  such  as  modal  and  amodal completion,  virtual contours,  grouping and  shape de(cid:173) composition are then explained through local diffusion.  The system  eliminates combinatorial optimization and accounts for  many psy(cid:173) chophysical results with a  fixed  set of parameters.",
        "bibtex": "@inproceedings{NIPS1999_851300ee,\n author = {Liu, Xiuwen and Wang, DeLiang},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Perceptual Organization Based on Temporal Dynamics},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/851300ee84c2b80ed40f51ed26d866fc-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/851300ee84c2b80ed40f51ed26d866fc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/851300ee84c2b80ed40f51ed26d866fc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1568686,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9745011830117296056&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer and Information Science+Center for Cognitive Science, The Ohio State University; Department of Computer and Information Science+Center for Cognitive Science, The Ohio State University",
        "aff_domain": "cis.ohio-state.edu;cis.ohio-state.edu",
        "email": "cis.ohio-state.edu;cis.ohio-state.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "1;1",
        "aff_unique_norm": ";Ohio State University",
        "aff_unique_dep": ";Center for Cognitive Science",
        "aff_unique_url": ";https://www.osu.edu",
        "aff_unique_abbr": ";OSU",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "470c4fe076",
        "title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/464d828b85b0bed98e80ade0a5c43b0f-Abstract.html",
        "author": "Richard S. Sutton; David A. McAllester; Satinder P. Singh; Yishay Mansour",
        "abstract": "Function  approximation  is  essential  to  reinforcement  learning,  but  the standard approach of approximating a  value function and deter(cid:173) mining  a  policy  from  it  has so  far  proven theoretically  intractable.  In this paper we explore an alternative approach in which the policy  is explicitly represented by its own function approximator,  indepen(cid:173) dent of the value function,  and is  updated according to the gradient  of expected reward with respect to the policy parameters.  Williams's  REINFORCE method and actor-critic methods are examples of this  approach.  Our  main  new  result  is  to  show  that  the  gradient  can  be  written  in  a  form  suitable  for  estimation  from  experience  aided  by  an  approximate  action-value  or  advantage  function.  Using  this  result,  we  prove for  the first  time that a  version  of policy  iteration  with arbitrary differentiable function approximation is convergent to  a  locally optimal policy.",
        "bibtex": "@inproceedings{NIPS1999_464d828b,\n author = {Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1541309,
        "gs_citation": 9674,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5062229662770158011&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 35,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "8a54ff081f",
        "title": "Policy Search via Density Estimation",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/54e36c5ff5f6a1802925ca009f3ebb68-Abstract.html",
        "author": "Andrew Y. Ng; Ronald Parr; Daphne Koller",
        "abstract": "We  propose  a  new  approach  to  the  problem  of searching  a  space  of  stochastic controllers for a Markov decision process (MDP) or a partially  observable Markov decision process (POMDP). Following several other  authors,  our approach  is  based  on  searching  in  parameterized  families  of policies (for example, via gradient descent) to optimize solution qual(cid:173) ity.  However,  rather than  trying  to  estimate  the  values  and  derivatives  of a policy  directly,  we do  so  indirectly  using  estimates  for  the  proba(cid:173) bility  densities  that  the  policy  induces  on  states  at  the  different points  in  time.  This enables our algorithms to  exploit the many techniques for  efficient  and  robust approximate density  propagation  in  stochastic  sys(cid:173) tems.  We show how our techniques can  be applied both to deterministic  propagation schemes (where the MDP's dynamics are given explicitly in  compact form,)  and  to  stochastic  propagation schemes (where we  have  access only to a generative model, or simulator, of the MDP). We present  empirical results for both of these variants on complex problems.",
        "bibtex": "@inproceedings{NIPS1999_54e36c5f,\n author = {Ng, Andrew and Parr, Ronald and Koller, Daphne},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Policy Search via Density Estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/54e36c5ff5f6a1802925ca009f3ebb68-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/54e36c5ff5f6a1802925ca009f3ebb68-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/54e36c5ff5f6a1802925ca009f3ebb68-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1776227,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12134689340021673590&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Computer Science Division, u.c. Berkeley; Computer Science Dept., Stanford University; Computer Science Dept., Stanford University",
        "aff_domain": "cs.berkeley.edu;cs.stanjord.edu;cs.stanjord.edu",
        "email": "cs.berkeley.edu;cs.stanjord.edu;cs.stanjord.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of California, Berkeley;Stanford University",
        "aff_unique_dep": "Computer Science Division;Computer Science Dept.",
        "aff_unique_url": "https://www.berkeley.edu;https://www.stanford.edu",
        "aff_unique_abbr": "UC Berkeley;Stanford",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Berkeley;Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "212caf103a",
        "title": "Population Decoding Based on an Unfaithful Model",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/11c484ea9305ea4c7bb6b2e6d570d466-Abstract.html",
        "author": "Si Wu; Hiroyuki Nakahara; Noboru Murata; Shun-ichi Amari",
        "abstract": "We study a population decoding paradigm in which the maximum likeli(cid:173) hood inference is based on an  unfaithful decoding model (UMLI). This  is usually the case for neural population decoding because the encoding  process  of the  brain  is  not exactly  known,  or because a  simplified de(cid:173) coding model  is  preferred for  saving computational cost.  We  consider  an  unfaithful  decoding model  which  neglects the  pair-wise  correlation  between neuronal activities, and prove that UMLI is asymptotically effi(cid:173) cient when the neuronal correlation is uniform or of limited-range.  The  performance of UMLI is compared with that of the maximum likelihood  inference based on  a faithful  model  and  that of the  center of mass de(cid:173) coding  method.  It turns  out  that  UMLI  has  advantages  of decreasing  the computational complexity remarkablely and maintaining a high-level  decoding  accuracy  at  the  same  time.  The  effect of correlation  on  the  decoding accuracy is also discussed.",
        "bibtex": "@inproceedings{NIPS1999_11c484ea,\n author = {Wu, Si and Nakahara, Hiroyuki and Murata, Noboru and Amari, Shun-ichi},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Population Decoding Based on an Unfaithful Model},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/11c484ea9305ea4c7bb6b2e6d570d466-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/11c484ea9305ea4c7bb6b2e6d570d466-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/11c484ea9305ea4c7bb6b2e6d570d466-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1354181,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12972666167784724579&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "b9a0ac21b3",
        "title": "Potential Boosters?",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/70ece1e1e0931919438fcfc6bd5f199c-Abstract.html",
        "author": "Nigel Duffy; David P. Helmbold",
        "abstract": "Recent  interpretations  of the  Adaboost  algorithm  view  it  as  per(cid:173) forming a gradient descent on a  potential function.  Simply chang(cid:173) ing the potential function  allows  one to create new  algorithms re(cid:173) lated  to  AdaBoost.  However,  these  new  algorithms are generally  not  known  to have  the formal  boosting property.  This  paper ex(cid:173) amines  the question  of which  potential  functions  lead  to  new  al(cid:173) gorithms that are boosters.  The two main results are general sets  of  conditions  on  the  potential;  one  set  implies  that  the  resulting  algorithm is  a  booster,  while the other implies  that the algorithm  is  not.  These conditions are applied to previously studied potential  functions , such as those used by LogitBoost and  Doom II.",
        "bibtex": "@inproceedings{NIPS1999_70ece1e1,\n author = {Duffy, Nigel and Helmbold, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Potential Boosters?},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/70ece1e1e0931919438fcfc6bd5f199c-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/70ece1e1e0931919438fcfc6bd5f199c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/70ece1e1e0931919438fcfc6bd5f199c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1566137,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9788937942528373615&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, University of California, Santa Cruz, CA 95064; Department of Computer Science, University of California, Santa Cruz, CA 95064",
        "aff_domain": "cse.ucsc.edu;~se.ucsc.edu",
        "email": "cse.ucsc.edu;~se.ucsc.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Santa Cruz",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ucsc.edu",
        "aff_unique_abbr": "UCSC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Santa Cruz",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3a3357e3df",
        "title": "Predictive App roaches for Choosing Hyperparameters in Gaussian Processes",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/e8fd4a8a5bab2b3785d794ab51fef55c-Abstract.html",
        "author": "S. Sundararajan; S. Sathiya Keerthi",
        "abstract": "Gaussian  Processes  are  powerful  regression  models  specified  by  parametrized mean and covariance functions.  Standard approaches  to  estimate  these  parameters  (known  by  the  name  Hyperparam(cid:173) eters)  are  Maximum  Likelihood  (ML)  and  Maximum  APosterior  (MAP)  approaches.  In this paper, we  propose and investigate pre(cid:173) dictive  approaches,  namely,  maximization  of  Geisser's  Surrogate  Predictive Probability (GPP) and minimization of mean square er(cid:173) ror with  respect to GPP  (referred to as  Geisser's  Predictive mean  square  Error  (GPE))  to  estimate  the  hyperparameters.  We  also  derive  results  for  the  standard  Cross-Validation  (CV)  error  and  make  a  comparison.  These approaches are tested on  a  number of  problems and experimental results show that these approaches are  strongly competitive to existing approaches.",
        "bibtex": "@inproceedings{NIPS1999_e8fd4a8a,\n author = {Sundararajan, S. and Keerthi, S.},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Predictive App roaches for Choosing Hyperparameters in Gaussian Processes},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e8fd4a8a5bab2b3785d794ab51fef55c-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/e8fd4a8a5bab2b3785d794ab51fef55c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/e8fd4a8a5bab2b3785d794ab51fef55c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1565101,
        "gs_citation": 201,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15913315601674996123&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Mechanical and Production Engg., National University of Singapore; Computer Science and Automation, Indian Institute of Science",
        "aff_domain": "guppy.mpe.nus.edu.sg;csa.iisc.ernet.in",
        "email": "guppy.mpe.nus.edu.sg;csa.iisc.ernet.in",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "National University of Singapore;Indian Institute of Science",
        "aff_unique_dep": "Mechanical and Production Engineering;Computer Science and Automation",
        "aff_unique_url": "https://www.nus.edu.sg;https://www.iisc.ac.in",
        "aff_unique_abbr": "NUS;IISc",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Singapore;India"
    },
    {
        "id": "d440697003",
        "title": "Predictive Sequence Learning in Recurrent Neocortical Circuits",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/b865367fc4c0845c0682bd466e6ebf4c-Abstract.html",
        "author": "Rajesh P. N. Rao; Terrence J. Sejnowski",
        "abstract": "Neocortical circuits are dominated by massive excitatory feedback:  more  than eighty percent of the synapses made by excitatory cortical neurons  are onto other excitatory cortical neurons.  Why is there such massive re(cid:173) current excitation in the neocortex and what is its role in cortical compu(cid:173) tation? Recent neurophysiological experiments have shown that the plas(cid:173) ticity of recurrent neocortical synapses is governed by a temporally asym(cid:173) metric Hebbian learning rule.  We  describe how such a rule may  allow  the cortex to modify recurrent synapses for prediction of input sequences.  The goal is to predict the next cortical input from the recent past based on  previous experience of similar input sequences. We show that a temporal  difference learning rule for prediction used in conjunction with dendritic  back-propagating action potentials reproduces the temporally asymmet(cid:173) ric Hebbian plasticity observed physiologically. Biophysical simulations  demonstrate that a network of cortical neurons can learn to predict mov(cid:173) ing stimuli and develop direction selective responses as a consequence of  learning. The space-time response properties of model neurons are shown  to be similar to those of direction selective cells in alert monkey VI.",
        "bibtex": "@inproceedings{NIPS1999_b865367f,\n author = {Rao, Rajesh and Sejnowski, Terrence J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Predictive Sequence Learning in Recurrent Neocortical Circuits},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b865367fc4c0845c0682bd466e6ebf4c-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/b865367fc4c0845c0682bd466e6ebf4c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/b865367fc4c0845c0682bd466e6ebf4c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1755688,
        "gs_citation": 128,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16171200905381071790&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Computational Neurobiology Lab and Sloan Center for Theoretical Neurobiology, The Salk Institute, La Jolla, CA 92037; Computational Neurobiology Lab and Howard Hughes Medical Institute, The Salk Institute, La Jolla, CA 92037",
        "aff_domain": "salk.edu;salk.edu",
        "email": "salk.edu;salk.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Salk Institute",
        "aff_unique_dep": "Computational Neurobiology Lab and Sloan Center for Theoretical Neurobiology",
        "aff_unique_url": "https://www.salk.edu",
        "aff_unique_abbr": "Salk Institute",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "La Jolla",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "72cabe07ca",
        "title": "Probabilistic Methods for Support Vector Machines",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/a941493eeea57ede8214fd77d41806bc-Abstract.html",
        "author": "Peter Sollich",
        "abstract": "I  describe  a  framework  for  interpreting Support  Vector  Machines  (SVMs)  as  maximum  a  posteriori  (MAP)  solutions  to  inference  problems with Gaussian Process priors.  This can provide intuitive  guidelines  for  choosing  a  'good'  SVM  kernel.  It  can  also  assign  (by  evidence  maximization)  optimal values  to parameters such  as  the noise level C which cannot be determined unambiguously from  properties of the MAP  solution alone  (such as  cross-validation er(cid:173) ror) . I illustrate this using a simple approximate expression for the  SVM  evidence.  Once  C  has  been determined,  error bars on SVM  predictions can also  be obtained.",
        "bibtex": "@inproceedings{NIPS1999_a941493e,\n author = {Sollich, Peter},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Probabilistic Methods for Support Vector Machines},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/a941493eeea57ede8214fd77d41806bc-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/a941493eeea57ede8214fd77d41806bc-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/a941493eeea57ede8214fd77d41806bc-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1673704,
        "gs_citation": 123,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1354420473979815991&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Mathematics, King's College London",
        "aff_domain": "kcl.ac.uk",
        "email": "kcl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "King's College London",
        "aff_unique_dep": "Department of Mathematics",
        "aff_unique_url": "https://www.kcl.ac.uk",
        "aff_unique_abbr": "KCL",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "415f146e28",
        "title": "Recognizing Evoked Potentials in a Virtual Environment",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/fddd7938a71db5f81fcc621673ab67b7-Abstract.html",
        "author": "Jessica D. Bayliss; Dana H. Ballard",
        "abstract": "Virtual  reality  (VR)  provides  immersive  and  controllable  experimen(cid:173) tal  environments.  It expands  the  bounds  of possible  evoked  potential  (EP)  experiments by  providing complex,  dynamic  environments in  or(cid:173) der  to  study  cognition  without  sacrificing  environmental  control.  VR  also serves as a safe dynamic testbed for brain-computer .interface (BCl)  research.  However, there has been some concern about detecting EP sig(cid:173) nals in a complex VR environment.  This paper shows that EPs exist at  red,  green, and yellow stop lights in  a virtual driving environment.  Ex(cid:173) perimental results show  the existence of the  P3  EP at  \"go\" and  \"stop\"  lights and  the contingent negative variation (CNY)  EP  at \"slow down\"  lights.  In order to  test the  feasibility  of on-line recognition in  VR,  we  looked at recognizing the P3 EP at red stop tights and the absence of this  signal at yellow slow down lights.  Recognition results show that the P3  may successfully be used to control the brakes of a VR car at stop lights.",
        "bibtex": "@inproceedings{NIPS1999_fddd7938,\n author = {Bayliss, Jessica and Ballard, Dana},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Recognizing Evoked Potentials in a Virtual Environment},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/fddd7938a71db5f81fcc621673ab67b7-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/fddd7938a71db5f81fcc621673ab67b7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/fddd7938a71db5f81fcc621673ab67b7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1540234,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18165662561291512486&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, University of Rochester; Department of Computer Science, University of Rochester",
        "aff_domain": "cs.rochester.edu;cs.rochester.edu",
        "email": "cs.rochester.edu;cs.rochester.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Rochester",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.rochester.edu",
        "aff_unique_abbr": "U of R",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2d9a7054a8",
        "title": "Reconstruction of Sequential Data with Probabilistic Models and Continuity Constraints",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/9a4400501febb2a95e79248486a5f6d3-Abstract.html",
        "author": "Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n",
        "abstract": "We consider the problem of reconstructing a temporal discrete sequence  of multidimensional real vectors when part of the data is  missing, under  the  assumption  that  the  sequence  was  generated  by  a  continuous  pro(cid:173) cess.  A particular case of this problem is  multivariate regression, which  is  very difficult when  the  underlying mapping is one-to-many.  We  pro(cid:173) pose  an  algorithm  based  on  a joint probability  model  of the  variables  of interest,  implemented using a  nonlinear latent  variable  model.  Each  point  in  the  sequence  is  potentially reconstructed  as  any  of the  modes  of the conditional distribution of the missing  variables given the present  variables (computed using an exhaustive mode search in a Gaussian mix(cid:173) ture).  Mode selection is determined by  a dynamic programming search  that  minimises a geometric measure of the reconstructed sequence,  de(cid:173) rived from continuity constraints.  We illustrate the algorithm with a toy  example  and  apply  it  to  a real-world  inverse problem,  the  acoustic-to(cid:173) articulatory mapping.  The results  show  that the  algorithm outperforms  conditional mean imputation and multilayer perceptrons.",
        "bibtex": "@inproceedings{NIPS1999_9a440050,\n author = {Carreira-Perpi\\~{n}\\'{a}n, Miguel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Reconstruction of Sequential Data with Probabilistic Models and Continuity Constraints},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/9a4400501febb2a95e79248486a5f6d3-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/9a4400501febb2a95e79248486a5f6d3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/9a4400501febb2a95e79248486a5f6d3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1709901,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12175994176126380313&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Dept. of Computer Science, University of Sheffield, UK",
        "aff_domain": "dcs.shef.ac.uk",
        "email": "dcs.shef.ac.uk",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Sheffield",
        "aff_unique_dep": "Dept. of Computer Science",
        "aff_unique_url": "https://www.sheffield.ac.uk",
        "aff_unique_abbr": "Sheffield",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "628d3e23f5",
        "title": "Recurrent Cortical Competition: Strengthen or Weaken?",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/831c2f88a604a07ca94314b56a4921b8-Abstract.html",
        "author": "P\u00e9ter Adorj\u00e1n; Lars Schwabe; Christian Piepenbrock; Klaus Obermayer",
        "abstract": "We investigate the short term .dynamics of the recurrent competition and  neural  activity in the primary visual cortex in terms of information pro(cid:173) cessing and in the context of orientation selectivity.  We propose that af(cid:173) ter stimulus onset, the strength of the recurrent excitation decreases due  to  fast  synaptic depression.  As  a consequence,  the network shifts from  an  initially highly  nonlinear to a  more  linear operating  regime.  Sharp  orientation tuning is established in the first highly competitive phase.  In  the second and less competitive phase, precise signaling of multiple ori(cid:173) entations and long range modulation, e.g.,  by intra- and inter-areal con(cid:173) nections becomes possible (surround effects).  Thus the network first ex(cid:173) tracts  the  salient  features  from  the  stimulus,  and  then  starts  to  process  the details.  We  show  that  this  signal  processing  strategy  is  optimal  if  the neurons have limited bandwidth and their objective is to transmit the  maximum amount of information in any time interval beginning with the  stimulus onset.",
        "bibtex": "@inproceedings{NIPS1999_831c2f88,\n author = {Adorj\\'{a}n, P\\'{e}ter and Schwabe, Lars and Piepenbrock, Christian and Obermayer, Klaus},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Recurrent Cortical Competition: Strengthen or Weaken?},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/831c2f88a604a07ca94314b56a4921b8-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/831c2f88a604a07ca94314b56a4921b8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/831c2f88a604a07ca94314b56a4921b8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1764823,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14812391103176566948&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Dept. of Compo Sci., FR2-I, Technical University Berlin; Dept. of Compo Sci., FR2-I, Technical University Berlin; Dept. of Compo Sci., FR2-I, Technical University Berlin + Epigenomics GmbH; Dept. of Compo Sci., FR2-I, Technical University Berlin",
        "aff_domain": "epigenomics.com;cs.tu-berlin.de;epigenomics.com;cs.tu-berlin.de",
        "email": "epigenomics.com;cs.tu-berlin.de;epigenomics.com;cs.tu-berlin.de",
        "github": "",
        "project": "http://www.ni.cs.tu-berlin.de",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+1;0",
        "aff_unique_norm": "Technical University Berlin;Epigenomics GmbH",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.tu-berlin.de;https://www.epigenomics.com",
        "aff_unique_abbr": "TU Berlin;",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Berlin;",
        "aff_country_unique_index": "0;0;0+0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "a750fbcec2",
        "title": "Regular and Irregular Gallager-zype Error-Correcting Codes",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/01e00f2f4bfcbb7505cb641066f2859b-Abstract.html",
        "author": "Yoshiyuki Kabashima; Tatsuto Murayama; David Saad; Renato Vicente",
        "abstract": "The  performance  of  regular  and  irregular  Gallager-type  error(cid:173) correcting code  is  investigated  via  methods  of statistical  physics.  The transmitted codeword comprises products of the original mes(cid:173) sage  bits  selected  by  two  randomly-constructed  sparse  matrices;  the  number  of  non-zero  row/column  elements  in  these  matrices  constitutes  a  family  of codes.  We  show  that  Shannon's  channel  capacity may  be saturated in equilibrium for  many  of the regular  codes while slightly lower performance is obtained for others which  may  be  of higher  practical  relevance.  Decoding  aspects  are  con(cid:173) sidered  by employing the TAP  approach  which  is  identical  to the  commonly used  belief-propagation-based decoding.  We  show that  irregular codes may saturate Shannon's capacity but with improved  dynamical properties.",
        "bibtex": "@inproceedings{NIPS1999_01e00f2f,\n author = {Kabashima, Yoshiyuki and Murayama, Tatsuto and Saad, David and Vicente, Renato},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Regular and Irregular Gallager-zype Error-Correcting Codes},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/01e00f2f4bfcbb7505cb641066f2859b-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/01e00f2f4bfcbb7505cb641066f2859b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/01e00f2f4bfcbb7505cb641066f2859b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1785537,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9997772188209825000&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "02ff567aea",
        "title": "Reinforcement Learning Using Approximate Belief States",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/158fc2ddd52ec2cf54d3c161f2dd6517-Abstract.html",
        "author": "Andres C. Rodriguez; Ronald Parr; Daphne Koller",
        "abstract": "The problem of developing good policies for partially observable Markov  decision  problems (POMDPs) remains one of the  most challenging ar(cid:173) eas of research  in  stochastic planning.  One line of research  in  this  area  involves  the use  of reinforcement learning  with  belief states,  probabil(cid:173) ity  distributions  over  the  underlying  model  states.  This  is  a  promis(cid:173) ing method for  small problems,  but its  application is  limited  by  the  in(cid:173) tractability of computing or representing a full belief state for large prob(cid:173) lems.  Recent  work  shows  that,  in  many  settings,  we  can  maintain  an  approximate belief state, which is fairly close to the true belief state.  In  particular,  great success has  been shown  with  approximate belief states  that marginalize out correlations between  state  variables.  In  this  paper,  we investigate two methods of full belief state reinforcement learning and  one novel method for reinforcement learning using factored approximate  belief states. We compare the performance of these algorithms on several  well-known problem from the literature. Our results demonstrate the im(cid:173) portance of approximate belief state representations for large problems.",
        "bibtex": "@inproceedings{NIPS1999_158fc2dd,\n author = {Rodriguez, Andres and Parr, Ronald and Koller, Daphne},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Reinforcement Learning Using Approximate Belief States},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/158fc2ddd52ec2cf54d3c161f2dd6517-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/158fc2ddd52ec2cf54d3c161f2dd6517-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/158fc2ddd52ec2cf54d3c161f2dd6517-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1605040,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14264718714248142528&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Artificial Intelligence Center, SRI International; Computer Science Department, Stanford University; Computer Science Department, Stanford University",
        "aff_domain": "ai.sri.com;cs.stanford.edu;cs.stanford.edu",
        "email": "ai.sri.com;cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "SRI International;Stanford University",
        "aff_unique_dep": "Artificial Intelligence Center;Computer Science Department",
        "aff_unique_url": "https://www.sri.com;https://www.stanford.edu",
        "aff_unique_abbr": "SRI;Stanford",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c87a7c22ac",
        "title": "Reinforcement Learning for Spoken Dialogue Systems",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/36d7534290610d9b7e9abed244dd2f28-Abstract.html",
        "author": "Satinder P. Singh; Michael J. Kearns; Diane J. Litman; Marilyn A. Walker",
        "abstract": "Recently, a number of authors have proposed treating dialogue systems as Markov  decision processes (MDPs). However, the practical application ofMDP algorithms  to dialogue systems faces a number of severe technical challenges. We have built a  general software tool (RLDS, for Reinforcement Learning for Dialogue Systems)  based on the MDP framework, and have applied it to dialogue corpora gathered  from two dialogue systems built at AT&T Labs. Our experiments demonstrate that  RLDS holds promise as a tool for \"browsing\" and understanding correlations in  complex, temporally dependent dialogue corpora.",
        "bibtex": "@inproceedings{NIPS1999_36d75342,\n author = {Singh, Satinder and Kearns, Michael and Litman, Diane and Walker, Marilyn},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Reinforcement Learning for Spoken Dialogue Systems},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/36d7534290610d9b7e9abed244dd2f28-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/36d7534290610d9b7e9abed244dd2f28-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/36d7534290610d9b7e9abed244dd2f28-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1793853,
        "gs_citation": 299,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2761830672892411182&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 22,
        "aff": "AT&T Labs; AT&T Labs; AT&T Labs; AT&T Labs",
        "aff_domain": "research.att.com;research.att.com;research.att.com;research.att.com",
        "email": "research.att.com;research.att.com;research.att.com;research.att.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "AT&T Laboratories",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.att.com/labs",
        "aff_unique_abbr": "AT&T Labs",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "df6ea91dcb",
        "title": "Resonance in a Stochastic Neuron Model with Delayed Interaction",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/712a3c9878efeae8ff06d57432016ceb-Abstract.html",
        "author": "Toru Ohira; Yuzuru Sato; Jack D. Cowan",
        "abstract": "We study here a simple stochastic single neuron model with delayed  self-feedback capable of generating spike  trains.  Simulations show  that its spike trains exhibit resonant behavior between \"noise\"  and  \"delay\".  In  order to gain  insight  into this  resonance,  we  simplify  the model and study a  stochastic binary element whose  transition  probability  depends  on  its  state  at  a  fixed  interval  in  the  past.  With this simplified model we  can analytically compute interspike  interval histograms, and show how the resonance between noise and  delay  arises.  The resonance  is  also  observed  when  such  elements  are coupled through delayed interaction.",
        "bibtex": "@inproceedings{NIPS1999_712a3c98,\n author = {Ohira, Toru and Sato, Yuzuru and Cowan, Jack},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Resonance in a Stochastic Neuron Model with Delayed Interaction},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/712a3c9878efeae8ff06d57432016ceb-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/712a3c9878efeae8ff06d57432016ceb-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/712a3c9878efeae8ff06d57432016ceb-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1287029,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=484682508215452777&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Sony Computer Science Laboratory, Shinagawa, Tokyo 141, Japan + Laboratory for Information Synthesis, RIKEN Brain Science Institute, Wako, Saitama, Japan; Institute of Physics, Graduate School of Arts and Science, University of Tokyo, Meguro, Tokyo 153 Japan; Department of Mathematics, University of Chicago, Chicago, IL 60637, U.S.A",
        "aff_domain": "csl.sony.co.jp;sacral.c.u-tokyo.ac.jp;math.uchicago.edu",
        "email": "csl.sony.co.jp;sacral.c.u-tokyo.ac.jp;math.uchicago.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;3",
        "aff_unique_norm": "Sony Computer Science Laboratory;RIKEN Brain Science Institute;University of Tokyo;University of Chicago",
        "aff_unique_dep": ";Laboratory for Information Synthesis;Institute of Physics, Graduate School of Arts and Science;Department of Mathematics",
        "aff_unique_url": "https://www.sony.net/;https://bri.riken.jp;https://www.u-tokyo.ac.jp;https://www.uchicago.edu",
        "aff_unique_abbr": "Sony CSL;RIKEN BSI;UTokyo;UChicago",
        "aff_campus_unique_index": "0+1;2;3",
        "aff_campus_unique": "Shinagawa;Wako;Meguro, Tokyo;Chicago",
        "aff_country_unique_index": "0+0;0;1",
        "aff_country_unique": "Japan;United States"
    },
    {
        "id": "c03adf01a8",
        "title": "Robust Full Bayesian Methods for Neural Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/b3b43aeeacb258365cc69cdaf42a68af-Abstract.html",
        "author": "Christophe Andrieu; Jo\u00e3o F. G. de Freitas; Arnaud Doucet",
        "abstract": "In this paper, we propose a full Bayesian model for neural networks.  This model treats the model dimension (number of neurons), model  parameters, regularisation parameters and noise parameters as ran(cid:173) dom  variables  that  need  to be estimated.  We  then  propose  a  re(cid:173) versible jump Markov chain Monte Carlo (MCMC)  method to per(cid:173) form  the necessary computations.  We  find  that the results are not  only  better than the previously  reported ones,  but  also  appear to  be  robust  with  respect  to  the  prior  specification.  Moreover,  we  present a  geometric convergence theorem for  the algorithm.",
        "bibtex": "@inproceedings{NIPS1999_b3b43aee,\n author = {Andrieu, Christophe and de Freitas, Jo\\~{a}o and Doucet, Arnaud},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Robust Full Bayesian Methods for Neural Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b3b43aeeacb258365cc69cdaf42a68af-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/b3b43aeeacb258365cc69cdaf42a68af-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/b3b43aeeacb258365cc69cdaf42a68af-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1518407,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1244443517452354923&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Cambridge University Engineering Department; UC Berkeley Computer Science; Cambridge University Engineering Department",
        "aff_domain": "eng.cam.ac.uk;cs.berkeley.edu;eng.cam.ac.uk",
        "email": "eng.cam.ac.uk;cs.berkeley.edu;eng.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Cambridge University;University of California, Berkeley",
        "aff_unique_dep": "Engineering Department;Department of Computer Science",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.berkeley.edu",
        "aff_unique_abbr": "Cambridge;UC Berkeley",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Cambridge;Berkeley",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "fa7937fd1d",
        "title": "Robust Learning of Chaotic Attractors",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/81c650caac28cdefce4de5ddc18befa0-Abstract.html",
        "author": "Rembrandt Bakker; Jaap C. Schouten; Marc-Olivier Coppens; Floris Takens; C. Lee Giles; Cor M. van den Bleek",
        "abstract": "A fundamental problem with the modeling of chaotic time series data is that  minimizing short-term prediction errors does not guarantee a match  between the reconstructed attractors of model and experiments. We  introduce a modeling paradigm that simultaneously learns to short-tenn  predict and to locate the outlines of the attractor by a new way of nonlinear  principal component analysis. Closed-loop predictions are constrained to  stay within these outlines, to prevent divergence from the attractor. Learning  is exceptionally fast: parameter estimation for the 1000 sample laser data  from the 1991 Santa Fe time series competition took less than a minute on  a 166 MHz Pentium PC.",
        "bibtex": "@inproceedings{NIPS1999_81c650ca,\n author = {Bakker, Rembrandt and Schouten, Jaap and Coppens, Marc-Olivier and Takens, Floris and Giles, C. and van den Bleek, Cor},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Robust Learning of Chaotic Attractors},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/81c650caac28cdefce4de5ddc18befa0-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/81c650caac28cdefce4de5ddc18befa0-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/81c650caac28cdefce4de5ddc18befa0-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1656881,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13898892140189600097&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 13,
        "aff": "Chemical Reactor Engineering, Delft Univ. of Technology; Dept. Mathematics, University of Groningen; Chemical Reactor Engineering, Eindhoven Univ. of Technology; NEC Research Institute, Princeton Nl; Chemical Reactor Engineering, Delft Univ. of Technology; Chemical Reactor Engineering, Delft Univ. of Technology",
        "aff_domain": "stm.tudelft\u00b7nl;math.rug.nl;tue.nl;research.nj.nec.com;stm.tudelft\u00b7nl;stm.tudelft\u00b7nl",
        "email": "stm.tudelft\u00b7nl;math.rug.nl;tue.nl;research.nj.nec.com;stm.tudelft\u00b7nl;stm.tudelft\u00b7nl",
        "github": "",
        "project": "http://www.cpt.stm.tudelft.nllcptlcre!researchlbakker/",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3;0;0",
        "aff_unique_norm": "Delft University of Technology;University of Groningen;Eindhoven University of Technology;NEC Research Institute",
        "aff_unique_dep": "Chemical Reactor Engineering;Dept. Mathematics;Chemical Reactor Engineering;",
        "aff_unique_url": "https://www.tudelft.nl;https://www.rug.nl;https://www.tue.nl;https://www.neci.nec.com",
        "aff_unique_abbr": "TUDelft;RUG;TUE;NECRI",
        "aff_campus_unique_index": "0;2;3;0;0",
        "aff_campus_unique": "Delft;;Eindhoven;Princeton",
        "aff_country_unique_index": "0;0;0;1;0;0",
        "aff_country_unique": "Netherlands;United States"
    },
    {
        "id": "f8da1350b8",
        "title": "Robust Neural Network Regression for Offline and Online Learning",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/db29450c3f5e97f97846693611f98c15-Abstract.html",
        "author": "Thomas Briegel; Volker Tresp",
        "abstract": "We  replace  the  commonly  used  Gaussian  noise  model  in  nonlinear  regression  by  a  more  flexible  noise  model  based  on  the  Student-t(cid:173) distribution.  The degrees of freedom of the t-distribution can be  chosen  such that as  special cases either the Gaussian distribution or the Cauchy  distribution are  realized.  The  latter is  commonly used in  robust regres(cid:173) sion.  Since the t-distribution can be  interpreted as being an infinite mix(cid:173) ture of Gaussians,  parameters and  hyperparameters such  as  the  degrees  of freedom of the t-distribution can be learned from the data based on an  EM-learning algorithm.  We  show that modeling using the  t-distribution  leads  to  improved  predictors  on  real  world  data  sets.  In  particular,  if  outliers are present,  the  t-distribution  is  superior to  the  Gaussian  noise  model.  In  effect, by  adapting  the  degrees of freedom,  the  system  can  \"learn\" to distinguish  between outliers  and  non-outliers.  Especially for  online learning tasks, one is  interested in  avoiding inappropriate weight  changes  due  to  measurement  outliers  to  maintain  stable  online  learn(cid:173) ing  capability.  We  show experimentally that using the  t-distribution  as  a noise model leads to stable online learning algorithms and outperforms  state-of-the art online learning methods like  the  extended Kalman filter  algorithm.",
        "bibtex": "@inproceedings{NIPS1999_db29450c,\n author = {Briegel, Thomas and Tresp, Volker},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Robust Neural Network Regression for Offline and Online Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/db29450c3f5e97f97846693611f98c15-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/db29450c3f5e97f97846693611f98c15-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/db29450c3f5e97f97846693611f98c15-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1646861,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6363917143419747695&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Siemens AG, Corporate Technology + McKinsey & Company, Inc.; Siemens AG, Corporate Technology",
        "aff_domain": "mchp.siemens.de;mchp.siemens.de",
        "email": "mchp.siemens.de;mchp.siemens.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0",
        "aff_unique_norm": "Siemens AG;McKinsey & Company",
        "aff_unique_dep": "Corporate Technology;",
        "aff_unique_url": "https://www.siemens.com;https://www.mckinsey.com",
        "aff_unique_abbr": "Siemens;McKinsey",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0",
        "aff_country_unique": "Germany;United States"
    },
    {
        "id": "66fef16377",
        "title": "Robust Recognition of Noisy and Superimposed Patterns via Selective Attention",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/5cf21ce30208cfffaa832c6e44bb567d-Abstract.html",
        "author": "Soo-Young Lee; Michael Mozer",
        "abstract": "In many classification tasks, recognition accuracy is low because input  patterns  are  corrupted  by  noise  or  are  spatially  or  temporally  overlapping. We propose an approach to overcoming these  limitations  based  on  a  model  of human  selective  attention.  The  model,  an  early  selection filter guided by top-down attentional control, entertains each  candidate  output  class  in  sequence  and  adjusts  attentional  gain  coefficients  in  order to produce a strong response  for that class.  The  chosen class is then the one that  obtains the strongest response with the  least  modulation  of  attention.  We  present  simulation  results  on  classification of corrupted and superimposed handwritten digit patterns,  showing a significant improvement in recognition rates.  The algorithm  has  also  been  applied  in  the  domain  of  speech  recognition,  with  comparable results.",
        "bibtex": "@inproceedings{NIPS1999_5cf21ce3,\n author = {Lee, Soo-Young and Mozer, Michael C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Robust Recognition of Noisy and Superimposed Patterns via Selective Attention},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/5cf21ce30208cfffaa832c6e44bb567d-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/5cf21ce30208cfffaa832c6e44bb567d-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/5cf21ce30208cfffaa832c6e44bb567d-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1462734,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9388465520361026981&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Brain Science Research Center, Korea Advanced Institute of Science & Technology; Department of Computer Science, University of Colorado at Boulder",
        "aff_domain": "ee.kaist.ac.kr;cs.colorado.edu",
        "email": "ee.kaist.ac.kr;cs.colorado.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology;University of Colorado at Boulder",
        "aff_unique_dep": "Brain Science Research Center;Department of Computer Science",
        "aff_unique_url": "https://www.kaist.ac.kr;https://www.colorado.edu",
        "aff_unique_abbr": "KAIST;CU Boulder",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Boulder",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "South Korea;United States"
    },
    {
        "id": "42b2070992",
        "title": "Rules and Similarity in Concept Learning",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/86d7c8a08b4aaa1bc7c599473f5dddda-Abstract.html",
        "author": "Joshua B. Tenenbaum",
        "abstract": "This paper argues that two apparently distinct modes of generalizing con(cid:173) cepts - abstracting rules and computing similarity to exemplars - should  both be seen as special cases of a more general Bayesian learning frame(cid:173) work.  Bayes explains the specific workings of these two modes - which  rules are abstracted,  how similarity is measured - as  well  as  why gener(cid:173) alization should appear rule- or similarity-based in different situations.  This  analysis also suggests why  the rules/similarity distinction, even  if  not computationally fundamental,  may  still be useful at the algorithmic  level as part of a principled approximation to fully Bayesian learning.",
        "bibtex": "@inproceedings{NIPS1999_86d7c8a0,\n author = {Tenenbaum, Joshua},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Rules and Similarity in Concept Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/86d7c8a08b4aaa1bc7c599473f5dddda-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/86d7c8a08b4aaa1bc7c599473f5dddda-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/86d7c8a08b4aaa1bc7c599473f5dddda-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1757298,
        "gs_citation": 121,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10968021160883668417&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Psychology, Stanford University, Stanford, CA 94305",
        "aff_domain": "psych.stanford.edu",
        "email": "psych.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Psychology",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "26c56c87ae",
        "title": "Scale Mixtures of Gaussians and the Statistics of Natural Images",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/6a5dfac4be1502501489fc0f5a24b667-Abstract.html",
        "author": "Martin J. Wainwright; Eero P. Simoncelli",
        "abstract": "The statistics of photographic images, when represented using  multiscale (wavelet) bases, exhibit two striking types of non(cid:173) Gaussian behavior. First, the marginal densities of the coefficients  have extended heavy tails. Second, the joint densities exhibit vari(cid:173) ance dependencies not captured by second-order models. We ex(cid:173) amine properties of the class of Gaussian scale mixtures, and show  that these densities can accurately characterize both the marginal  and joint distributions of natural image wavelet coefficients. This  class of model suggests a Markov structure, in which wavelet coeffi(cid:173) cients are linked by hidden scaling variables corresponding to local  image structure. We derive an estimator for these hidden variables,  and show that a nonlinear \"normalization\" procedure can be used  to Gaussianize the coefficients.",
        "bibtex": "@inproceedings{NIPS1999_6a5dfac4,\n author = {Wainwright, Martin J and Simoncelli, Eero},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Scale Mixtures of Gaussians and the Statistics of Natural Images},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/6a5dfac4be1502501489fc0f5a24b667-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/6a5dfac4be1502501489fc0f5a24b667-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/6a5dfac4be1502501489fc0f5a24b667-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1586915,
        "gs_citation": 694,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4131433237494782042&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Stochastic Systems Group, Electrical Engineering & CS, MIT; Ctr. for Neural Science, and Courant Inst. of Mathematical Sciences, New York University",
        "aff_domain": "mit.edu;nyu.edu",
        "email": "mit.edu;nyu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;New York University",
        "aff_unique_dep": "Electrical Engineering & Computer Science;Center for Neural Science",
        "aff_unique_url": "https://web.mit.edu;https://www.nyu.edu",
        "aff_unique_abbr": "MIT;NYU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";New York",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6f0be41534",
        "title": "Search for Information Bearing Components in Speech",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/cc42acc8ce334185e0193753adb6cb77-Abstract.html",
        "author": "Howard Hua Yang; Hynek Hermansky",
        "abstract": "In  this  paper,  we  use  mutual information to characterize  the  dis(cid:173) tributions of phonetic  and speaker/channel  information in  a  time(cid:173) frequency  space.  The  mutual information  (MI)  between  the  pho(cid:173) netic label and one feature,  and the joint mutual information (JMI)  between the phonetic label and two or three features are estimated .  The Miller's bias formulas for  entropy and mutual information es(cid:173) timates  are  extended  to include  higher  order  terms.  The  MI  and  the  JMI for  speaker/channel  recognition  are  also  estimated.  The  results  are complementary to those for phonetic classification.  Our  results  show  how  the  phonetic  information is  locally  spread  and  how  the  speaker/channel  information  is  globally  spread  in  time  and frequency.",
        "bibtex": "@inproceedings{NIPS1999_cc42acc8,\n author = {Yang, Howard and Hermansky, Hynek},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Search for Information Bearing Components in Speech},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/cc42acc8ce334185e0193753adb6cb77-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/cc42acc8ce334185e0193753adb6cb77-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/cc42acc8ce334185e0193753adb6cb77-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1460653,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7472690725243500715&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "5d2836b923",
        "title": "Semiparametric Approach to Multichannel Blind Deconvolution of Nonminimum Phase Systems",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/b147a61c1d07c1c999560f62add6dbc7-Abstract.html",
        "author": "Liqing Zhang; Shun-ichi Amari; Andrzej Cichocki",
        "abstract": "In  this  paper we discuss  the  semi parametric statistical  model  for  blind  deconvolution.  First we  introduce a Lie Group to  the  manifold of non(cid:173) causal  FIR  filters.  Then  blind deconvolution problem is  formulated  in  the  framework of a  semiparametric  model,  and  a  family  of estimating  functions  is  derived for  blind  deconvolution.  A  natural gradient  learn(cid:173) ing algorithm is developed for training noncausal filters.  Stability of the  natural gradient algorithm is also analyzed in this framework.",
        "bibtex": "@inproceedings{NIPS1999_b147a61c,\n author = {Zhang, Liqing and Amari, Shun-ichi and Cichocki, Andrzej},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Semiparametric Approach to Multichannel Blind Deconvolution of Nonminimum Phase Systems},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b147a61c1d07c1c999560f62add6dbc7-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/b147a61c1d07c1c999560f62add6dbc7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/b147a61c1d07c1c999560f62add6dbc7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1431323,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3166541602474905572&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Brain-style Information Systems Research Group, BSI The Institute of Physical and Chemical Research Wako shi, Saitama 351-0198, JAPAN; Brain-style Information Systems Research Group, BSI The Institute of Physical and Chemical Research Wako shi, Saitama 351-0198, JAPAN; Brain-style Information Systems Research Group, BSI The Institute of Physical and Chemical Research Wako shi, Saitama 351-0198, JAPAN",
        "aff_domain": "open.brain.riken.go.jp;brain.riken.go.jp;brain.riken.go.jp",
        "email": "open.brain.riken.go.jp;brain.riken.go.jp;brain.riken.go.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Institute of Physical and Chemical Research",
        "aff_unique_dep": "Brain-style Information Systems Research Group",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Wako",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "9cf6065adc",
        "title": "Some Theoretical Results Concerning the Convergence of Compositions of Regularized Linear Functions",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/7c4ede33a62160a19586f6e26eaefacf-Abstract.html",
        "author": "Tong Zhang",
        "abstract": "Recently,  sample complexity bounds have been derived for problems in(cid:173) volving linear functions such as neural networks and support vector ma(cid:173) chines.  In this paper,  we extend some theoretical results in this area by  deriving dimensional independent covering number bounds for regular(cid:173) ized  linear functions under certain regularization conditions.  We  show  that such bounds lead to a class of new methods for training linear clas(cid:173) sifiers with similar theoretical advantages of the support vector machine.  Furthermore,  we also present a theoretical analysis for these new meth(cid:173) ods from the asymptotic statistical point of view.  This technique provides  better description for large sample behaviors of these algorithms.",
        "bibtex": "@inproceedings{NIPS1999_7c4ede33,\n author = {Zhang, Tong},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Some Theoretical Results Concerning the Convergence of Compositions of Regularized Linear Functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7c4ede33a62160a19586f6e26eaefacf-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/7c4ede33a62160a19586f6e26eaefacf-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/7c4ede33a62160a19586f6e26eaefacf-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1506815,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8122514087372925757&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Mathematical Sciences Department, IBM T.1. Watson Research Center",
        "aff_domain": "watson.ibm.com",
        "email": "watson.ibm.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "IBM",
        "aff_unique_dep": "Mathematical Sciences Department",
        "aff_unique_url": "https://www.ibm.com",
        "aff_unique_abbr": "IBM",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "T.J. Watson Research Center",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b4abba844d",
        "title": "Spectral Cues in Human Sound Localization",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/b29eed44276144e4e8103a661f9a78b7-Abstract.html",
        "author": "Craig T. Jin; Anna Corderoy; Simon Carlile; Andr\u00e9 van Schaik",
        "abstract": "The  differential  contribution  of the  monaural  and  interaural  spectral  cues to human sound localization was examined using a combined psy(cid:173) chophysical  and  analytical  approach.  The  cues  to  a  sound's  location  were  correlated  on an individual basis  with  the  human  localization re(cid:173) sponses to a variety of spectrally manipulated sounds.  The spectral cues  derive from the acoustical filtering of an individual's auditory periphery  which  is  characterized by the measured head-related transfer functions  (HRTFs).  Auditory localization performance was determined in  virtual  auditory space (VAS).  Psychoacoustical experiments were conducted in  which the amplitude spectra of the sound stimulus was  varied indepen(cid:173) dentlyat each ear while preserving the normal timing cues, an impossibil(cid:173) ity in the free-field environment. Virtual auditory noise stimuli were gen(cid:173) erated over earphones for a specified target direction such that there was  a \"false\" flat  spectrum at the left eardrum.  Using the subject's HRTFs,  the sound spectrum at the right eardrum was then adjusted so that either  the  true right  monaural  spectral  cue  or the  true  interaural  spectral  cue  was preserved.  All  subjects showed systematic mislocalizations in both  the true right and true interaural spectral conditions which was absent in  their control localization performance. The analysis of the different cues  along with the subjects' localization responses suggests there are signif(cid:173) icant differences in the use of the monaural and interaural spectral cues  and that the auditory system's reliance on the spectral  cues varies with  the sound condition.",
        "bibtex": "@inproceedings{NIPS1999_b29eed44,\n author = {Jin, Craig and Corderoy, Anna and Carlile, Simon and van Schaik, Andr\\'{e}},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Spectral Cues in Human Sound Localization},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/b29eed44276144e4e8103a661f9a78b7-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/b29eed44276144e4e8103a661f9a78b7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/b29eed44276144e4e8103a661f9a78b7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1676159,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13502361433881089390&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "c25af111e9",
        "title": "Speech Modelling Using Subspace and EM Techniques",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/d860bd12ce9c026814bbdfc1c573f0f5-Abstract.html",
        "author": "Gavin Smith; Jo\u00e3o F. G. de Freitas; Tony Robinson; Mahesan Niranjan",
        "abstract": "The speech waveform can be modelled as  a piecewise-stationary linear  stochastic state space system, and its parameters can be estimated using  an  expectation-maximisation (EM)  algorithm.  One problem is  the  ini(cid:173) tialisation of the EM algorithm. Standard initialisation schemes can lead  to poor formant trajectories.  But these  trajectories however are impor(cid:173) tant for  vowel  intelligibility.  The aim of this paper is to investigate the  suitability of subspace identification methods to initialise EM.  The  paper  compares  the  subspace  state  space  system  identification  (4SID) method with the EM algorithm.  The 4SID and EM methods are  similar in that they both estimate a state sequence (but using Kalman fil(cid:173) ters and Kalman smoothers respectively),  and then estimate parameters  (but using least-squares and maximum likelihood respectively). The sim(cid:173) ilarity of 4SID and EM motivates the use of 4SID to initialise EM. Also,  4SID is non-iterative and requires no initialisation, whereas EM is itera(cid:173) tive and requires initialisation.  However 4SID is sub-optimal compared  to EM in a probabilistic sense. During experiments on real speech, 4SID  methods compare favourably with conventional initialisation techniques.  They produce smoother formant trajectories, have greater frequency res(cid:173) olution, and produce higher likelihoods.",
        "bibtex": "@inproceedings{NIPS1999_d860bd12,\n author = {Smith, Gavin and de Freitas, Jo\\~{a}o and Robinson, Tony and Niranjan, Mahesan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Speech Modelling Using Subspace and EM Techniques},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/d860bd12ce9c026814bbdfc1c573f0f5-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/d860bd12ce9c026814bbdfc1c573f0f5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/d860bd12ce9c026814bbdfc1c573f0f5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1399719,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12205017942918506415&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Cambridge University Engineering Department; UC Berkeley Computer Science Division; Sheffield University Computer Science; Cambridge University Engineering Department",
        "aff_domain": "eng.cam.ac.uk;cs.berkeley.edu;dcs.shef.ac.uk;eng.cam.ac.uk",
        "email": "eng.cam.ac.uk;cs.berkeley.edu;dcs.shef.ac.uk;eng.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Cambridge University;University of California, Berkeley;University of Sheffield",
        "aff_unique_dep": "Engineering Department;Computer Science Division;Department of Computer Science",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.berkeley.edu;https://www.sheffield.ac.uk",
        "aff_unique_abbr": "Cambridge;UC Berkeley;Sheffield",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Cambridge;Berkeley;",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "506f5853b7",
        "title": "Spike-based Learning Rules and Stabilization of Persistent Neural Activity",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/c0560792e4a3c79e62f76cbf9fb277dd-Abstract.html",
        "author": "Xiaohui Xie; H. Sebastian Seung",
        "abstract": "We  analyze  the  conditions  under  which  synaptic  learning  rules  based  on  action  potential timing can  be approximated by  learning rules  based  on  firing  rates.  In  particular, we  consider a form  of plasticity in  which  synapses depress when a presynaptic spike is followed by a postsynaptic  spike, and potentiate with the opposite temporal ordering.  Such differen(cid:173) tial anti-Hebbian plasticity can be approximated under certain conditions  by a learning rule that depends on the time derivative of the postsynaptic  firing rate.  Such a learning rule acts to stabilize persistent neural activity  patterns in recurrent neural networks.",
        "bibtex": "@inproceedings{NIPS1999_c0560792,\n author = {Xie, Xiaohui and Seung, H. Sebastian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Spike-based Learning Rules and Stabilization of Persistent Neural Activity},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/c0560792e4a3c79e62f76cbf9fb277dd-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/c0560792e4a3c79e62f76cbf9fb277dd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/c0560792e4a3c79e62f76cbf9fb277dd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1639437,
        "gs_citation": 79,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7595793273302321355&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Dept. of Brain & Cog. Sci., MIT, Cambridge, MA 02139; Dept. of Brain & Cog. Sci., MIT, Cambridge, MA 02139",
        "aff_domain": "mit.edu;mit.edu",
        "email": "mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Department of Brain and Cognitive Sciences",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "742626bbfa",
        "title": "Spiking Boltzmann Machines",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/62889e73828c756c961c5a6d6c01a463-Abstract.html",
        "author": "Geoffrey E. Hinton; Andrew D. Brown",
        "abstract": "We first show how to represent sharp posterior probability distribu(cid:173) tions using real valued coefficients on broadly-tuned basis functions.  Then we  show how  the precise times of spikes can be used to con(cid:173) vey the real-valued  coefficients on  the basis functions  quickly and  accurately.  Finally we  describe a  simple  simulation in which spik(cid:173) ing neurons learn to model an image sequence by fitting a dynamic  generative model.",
        "bibtex": "@inproceedings{NIPS1999_62889e73,\n author = {Hinton, Geoffrey E and Brown, Andrew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Spiking Boltzmann Machines},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/62889e73828c756c961c5a6d6c01a463-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/62889e73828c756c961c5a6d6c01a463-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/62889e73828c756c961c5a6d6c01a463-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1646614,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5794719817488395538&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Gatsby Computational Neuroscience Unit, University College London; Department of Computer Science, University of Toronto",
        "aff_domain": "gatsby.ucl.ac.uk;cs.utoronto.ca",
        "email": "gatsby.ucl.ac.uk;cs.utoronto.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University College London;University of Toronto",
        "aff_unique_dep": "Gatsby Computational Neuroscience Unit;Department of Computer Science",
        "aff_unique_url": "https://www.ucl.ac.uk;https://www.utoronto.ca",
        "aff_unique_abbr": "UCL;U of T",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "London;Toronto",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United Kingdom;Canada"
    },
    {
        "id": "b5f6b81375",
        "title": "State Abstraction in MAXQ Hierarchical Reinforcement Learning",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/e5a4d6bf330f23a8707bb0d6001dfbe8-Abstract.html",
        "author": "Thomas G. Dietterich",
        "abstract": "Part of",
        "bibtex": "@inproceedings{NIPS1999_e5a4d6bf,\n author = {Dietterich, Thomas},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {State Abstraction in MAXQ Hierarchical Reinforcement Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/e5a4d6bf330f23a8707bb0d6001dfbe8-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/e5a4d6bf330f23a8707bb0d6001dfbe8-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/e5a4d6bf330f23a8707bb0d6001dfbe8-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1706848,
        "gs_citation": 122,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16258802425033211078&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, Oregon State University",
        "aff_domain": "cs.orst.edu",
        "email": "cs.orst.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Oregon State University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://oregonstate.edu",
        "aff_unique_abbr": "OSU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Corvallis",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "89ba908898",
        "title": "Statistical Dynamics of Batch Learning",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/673271cc47c1a4e77f57e239ed4d28a7-Abstract.html",
        "author": "Song Li; K. Y. Michael Wong",
        "abstract": "An important issue in neural computing concerns the description of  learning dynamics  with  macroscopic  dynamical  variables.  Recen(cid:173) t  progress on  on-line learning only  addresses the often unrealistic  case of an infinite  training set.  We  introduce a  new  framework  to  model batch learning of restricted sets of examples, widely applica(cid:173) ble to  any learning cost function,  and fully  taking into account the  temporal correlations introduced by the recycling of the examples.  For  illustration  we  analyze  the  effects  of weight  decay  and  early  stopping during the learning of teacher-generated examples.",
        "bibtex": "@inproceedings{NIPS1999_673271cc,\n author = {Li, Song and Wong, K. Y. Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Statistical Dynamics of Batch Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/673271cc47c1a4e77f57e239ed4d28a7-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/673271cc47c1a4e77f57e239ed4d28a7-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/673271cc47c1a4e77f57e239ed4d28a7-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1544142,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2783856773240229773&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "dc348f1890",
        "title": "Support Vector Method for Multivariate Density Estimation",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/207f88018f72237565570f8a9e5ca240-Abstract.html",
        "author": "Vladimir Vapnik; Sayan Mukherjee",
        "abstract": "A  new  method  for  multivariate  density  estimation  is  developed  based  on  the  Support  Vector  Method  (SVM)  solution  of inverse  ill-posed problems.  The solution has  the form  of a  mixture of den(cid:173) sities.  This  method  with  Gaussian  kernels  compared favorably  to  both  Parzen's  method  and  the  Gaussian  Mixture  Model  method.  For synthetic data we achieve more accurate estimates for densities  of 2,  6,  12, and 40  dimensions.",
        "bibtex": "@inproceedings{NIPS1999_207f8801,\n author = {Vapnik, Vladimir and Mukherjee, Sayan},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Support Vector Method for Multivariate Density Estimation},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/207f88018f72237565570f8a9e5ca240-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/207f88018f72237565570f8a9e5ca240-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/207f88018f72237565570f8a9e5ca240-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1203095,
        "gs_citation": 289,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13033143910970399933&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Royal Halloway College + AT &T Labs; CBCL, MIT",
        "aff_domain": "research.att.com;ai.mit.edu",
        "email": "research.att.com;ai.mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "Royal Holloway, University of London;AT&T Laboratories;Massachusetts Institute of Technology",
        "aff_unique_dep": ";;Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.royalholloway.ac.uk;https://www.att.com/labs;https://www.csail.mit.edu",
        "aff_unique_abbr": "RHUL;AT&T Labs;MIT",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Egham;;Cambridge",
        "aff_country_unique_index": "0+1;1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "b07735e727",
        "title": "Support Vector Method for Novelty Detection",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/8725fb777f25776ffa9076e44fcfd776-Abstract.html",
        "author": "Bernhard Sch\u00f6lkopf; Robert C. Williamson; Alex J. Smola; John Shawe-Taylor; John C. Platt",
        "abstract": "Suppose you are given some dataset drawn from an underlying probabil(cid:173) ity distribution P  and you want to estimate a \"simple\" subset S  of input  space such that the probability that a test point drawn from P lies outside  of S equals some a priori specified l/ between 0 and 1.  We  propose a method to  approach this  problem by trying to estimate a  function f  which is positive on S  and negative on the complement.  The  functional form of f  is given by a kernel expansion in terms of a poten(cid:173) tially small subset of the training data; it is regularized by controlling the  length of the weight vector in  an associated feature space.  We provide a  theoretical analysis of the statistical performance of our algorithm.  The algorithm is  a natural extension of the  support vector algorithm to  the case of unlabelled data.",
        "bibtex": "@inproceedings{NIPS1999_8725fb77,\n author = {Sch\\\"{o}lkopf, Bernhard and Williamson, Robert C and Smola, Alex and Shawe-Taylor, John and Platt, John},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Support Vector Method for Novelty Detection},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/8725fb777f25776ffa9076e44fcfd776-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/8725fb777f25776ffa9076e44fcfd776-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/8725fb777f25776ffa9076e44fcfd776-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1702647,
        "gs_citation": 3405,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8432347189419440642&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "Microsoft Research Ltd., 1 Guildhall Street, Cambridge, UK + Microsoft, 1 Microsoft Way, Redmond, WA, USA; Department of Engineering, Australian National University, Canberra 0200; Department of Engineering, Australian National University, Canberra 0200; Royal Holloway, University of London, Egham, UK; Microsoft Research Ltd., 1 Guildhall Street, Cambridge, UK + Microsoft, 1 Microsoft Way, Redmond, WA, USA",
        "aff_domain": "microsoft.com;microsoft.com;anu.edu.au;anu.edu.au;dcs.rhbnc.ac.uk",
        "email": "microsoft.com;microsoft.com;anu.edu.au;anu.edu.au;dcs.rhbnc.ac.uk",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+0;1;1;2;0+0",
        "aff_unique_norm": "Microsoft;Australian National University;University of London",
        "aff_unique_dep": "Microsoft Research;Department of Engineering;Royal Holloway",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://www.anu.edu.au;https://www.royalholloway.ac.uk",
        "aff_unique_abbr": "MSR;ANU;RHUL",
        "aff_campus_unique_index": "0+1;2;2;3;0+1",
        "aff_campus_unique": "Cambridge;Redmond;Canberra;Egham",
        "aff_country_unique_index": "0+1;2;2;0;0+1",
        "aff_country_unique": "United Kingdom;United States;Australia"
    },
    {
        "id": "8f59e62a09",
        "title": "The Entropy Regularization Information Criterion",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/08e6bea8e90ba87af3c9554d94db6579-Abstract.html",
        "author": "Alex J. Smola; John Shawe-Taylor; Bernhard Sch\u00f6lkopf; Robert C. Williamson",
        "abstract": "Effective methods of capacity control via uniform convergence bounds  for function expansions have been largely limited to Support Vector ma(cid:173) chines,  where  good  bounds  are  obtainable  by  the  entropy  number ap(cid:173) proach. We extend these methods to systems with expansions in terms of  arbitrary (parametrized) basis functions and a wide range of regulariza(cid:173) tion methods covering the whole range of general linear additive models.  This is  achieved by  a data dependent analysis of the eigenvalues of the  corresponding design matrix.",
        "bibtex": "@inproceedings{NIPS1999_08e6bea8,\n author = {Smola, Alex and Shawe-Taylor, John and Sch\\\"{o}lkopf, Bernhard and Williamson, Robert C},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {The Entropy Regularization Information Criterion},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/08e6bea8e90ba87af3c9554d94db6579-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/08e6bea8e90ba87af3c9554d94db6579-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/08e6bea8e90ba87af3c9554d94db6579-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1418268,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7915915528811895241&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "Dept. of Engineering and RSISE, Australian National University; Microsoft Research Limited; Royal Holloway College, University of London; Dept. of Engineering, Australian National University",
        "aff_domain": "anu.edu.au;microsoft.com;dcs.rhbnc.ac.uk;anu.edu.au",
        "email": "anu.edu.au;microsoft.com;dcs.rhbnc.ac.uk;anu.edu.au",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Australian National University;Microsoft;University of London",
        "aff_unique_dep": "Dept. of Engineering and RSISE;Microsoft Research;",
        "aff_unique_url": "https://www.anu.edu.au;https://www.microsoft.com/en-us/research;https://www.royalholloway.ac.uk",
        "aff_unique_abbr": "ANU;MSR;RHUL",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Royal Holloway",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "Australia;United Kingdom"
    },
    {
        "id": "ab1012380d",
        "title": "The Infinite Gaussian Mixture Model",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/97d98119037c5b8a9663cb21fb8ebf47-Abstract.html",
        "author": "Carl Edward Rasmussen",
        "abstract": "In a Bayesian mixture model it is not necessary a priori to limit the num(cid:173) ber of components to be finite.  In this paper an infinite Gaussian mixture  model is  presented which neatly sidesteps the difficult problem of find(cid:173) ing the \"right\" number of mixture components. Inference in the model is  done using an efficient parameter-free Markov Chain that relies entirely  on Gibbs sampling.",
        "bibtex": "@inproceedings{NIPS1999_97d98119,\n author = {Rasmussen, Carl},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {The Infinite Gaussian Mixture Model},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/97d98119037c5b8a9663cb21fb8ebf47-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/97d98119037c5b8a9663cb21fb8ebf47-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/97d98119037c5b8a9663cb21fb8ebf47-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1479548,
        "gs_citation": 1928,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2750673739925303536&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 23,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "http://bayes.imm.dtu.dk",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "5d5f62f796",
        "title": "The Nonnegative Boltzmann Machine",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/955a1584af63a546588caae4d23840b3-Abstract.html",
        "author": "Oliver B. Downs; David J. C. MacKay; Daniel D. Lee",
        "abstract": "The nonnegative Boltzmann machine (NNBM) is a recurrent neural net(cid:173) work model that can describe multimodal nonnegative data.  Application  of maximum likelihood estimation to this model gives a learning rule that  is analogous to the binary Boltzmann machine. We examine the utility of  the mean field  approximation for the  NNBM,  and describe  how Monte  Carlo sampling techniques can be  used to  learn its parameters.  Reflec(cid:173) tive  slice  sampling  is  particularly well-suited  for  this  distribution,  and  can efficiently be implemented to sample the  distribution.  We  illustrate  learning of the NNBM on a transiationally invariant distribution, as well  as on a generative model for images of human faces.",
        "bibtex": "@inproceedings{NIPS1999_955a1584,\n author = {Downs, Oliver and MacKay, David and Lee, Daniel},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {The Nonnegative Boltzmann Machine},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/955a1584af63a546588caae4d23840b3-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/955a1584af63a546588caae4d23840b3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/955a1584af63a546588caae4d23840b3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1343113,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15329625037697615533&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Princeton University; Cavendish Laboratory, United Kingdom; Bell Laboratories, Lucent Technologies",
        "aff_domain": "princeton.edu;mrao.cam.ac.uk;bell-labs.com",
        "email": "princeton.edu;mrao.cam.ac.uk;bell-labs.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Princeton University;University of Cambridge;Bell Laboratories",
        "aff_unique_dep": ";Cavendish Laboratory;",
        "aff_unique_url": "https://www.princeton.edu;https://www.cavendishlaboratory.cam.ac.uk;https://www.bell-labs.com",
        "aff_unique_abbr": "Princeton;Cavendish Lab;Bell Labs",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "e98f6f5818",
        "title": "The Parallel Problems Server: an Interactive Tool for Large Scale Machine Learning",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/c59b469d724f7919b7d35514184fdc0f-Abstract.html",
        "author": "Charles Lee Isbell Jr.; Parry Husbands",
        "abstract": "Imagine that you wish to classify data consisting of tens of thousands of ex(cid:173) amples residing in a twenty thousand dimensional space.  How can one ap(cid:173) ply standard machine learning algorithms? We describe the Parallel Prob(cid:173) lems Server (PPServer) and MATLAB*P.  In tandem they  allow users  of  networked computers to work transparently on large data sets from within  Matlab.  This  work is motivated by the desire to  bring the many  benefits  of scientific computing algorithms and computational power to  machine  learning researchers.  We  demonstrate the usefulness  of the system on  a number of tasks.  For  example,  we perform independent components analysis on very large text  corpora consisting  of tens  of thousands of documents,  making  minimal  changes  to  the original Bell  and  Sejnowski Matlab source  (Bell  and  Se(cid:173) jnowski,  1995).  Applying ML techniques to data previously beyond their  reach leads to interesting analyses of both data and algorithms.",
        "bibtex": "@inproceedings{NIPS1999_c59b469d,\n author = {Isbell, Charles and Husbands, Parry},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {The Parallel Problems Server: an Interactive Tool for Large Scale Machine Learning},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/c59b469d724f7919b7d35514184fdc0f-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/c59b469d724f7919b7d35514184fdc0f-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/c59b469d724f7919b7d35514184fdc0f-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1510880,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4467524046371549166&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "AT&T Labs; Lawrence Berkeley National LaboratorylNERSC",
        "aff_domain": "research.att.com;lbl.gov",
        "email": "research.att.com;lbl.gov",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "AT&T Laboratories;Lawrence Berkeley National Laboratory",
        "aff_unique_dep": ";NERSC",
        "aff_unique_url": "https://www.att.com/labs;https://www.lbl.gov",
        "aff_unique_abbr": "AT&T Labs;LBL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "985dd83c3d",
        "title": "The Relaxed Online Maximum Margin Algorithm",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/515ab26c135e92ed8bf3a594d67e4ade-Abstract.html",
        "author": "Yi Li; Philip M. Long",
        "abstract": "We  describe  a  new  incremental  algorithm  for  training  linear  thresh(cid:173) old  functions:  the  Relaxed  Online  Maximum  Margin  Algorithm,  or  ROMMA. ROMMA can be viewed as  an approximation to the algorithm  that repeatedly chooses the hyperplane that classifies previously seen ex(cid:173) amples  correctly  with  the  maximum  margin.  It  is  known  that  such  a  maximum-margin hypothesis can be computed by minimizing the length  of the weight vector subject to a number of linear constraints.  ROMMA  works  by  maintaining a relatively simple relaxation of these constraints  that can be efficiently updated.  We prove a mistake bound for ROMMA  that is the same as that proved for the perceptron algorithm.  Our analysis  implies that the more computationally intensive maximum-margin algo(cid:173) rithm also satisfies this mistake bound; this is the first worst-case perfor(cid:173) mance guarantee for this algorithm.  We describe some experiments us(cid:173) ing ROMMA and  a variant that updates its hypothesis more aggressively  as  batch  algorithms to recognize handwritten digits.  The computational  complexity  and  simplicity of these algorithms is  similar to  that of per(cid:173) ceptron algorithm , but their generalization is much better.  We describe a  sense  in  which the performance of ROMMA converges  to that of SVM  in the limit if bias isn't considered.",
        "bibtex": "@inproceedings{NIPS1999_515ab26c,\n author = {Li, Yi and Long, Philip},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {The Relaxed Online Maximum Margin Algorithm},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/515ab26c135e92ed8bf3a594d67e4ade-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/515ab26c135e92ed8bf3a594d67e4ade-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/515ab26c135e92ed8bf3a594d67e4ade-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1564126,
        "gs_citation": 311,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=974136374417187204&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "Department of Computer Science, National University of Singapore; Department of Computer Science, National University of Singapore",
        "aff_domain": "comp.nus.edu.sg;comp.nus.edu.sg",
        "email": "comp.nus.edu.sg;comp.nus.edu.sg",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "National University of Singapore",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.nus.edu.sg",
        "aff_unique_abbr": "NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "c69e74439f",
        "title": "The Relevance Vector Machine",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/f3144cefe89a60d6a1afaf7859c5076b-Abstract.html",
        "author": "Michael E. Tipping",
        "abstract": "The support vector machine  (SVM)  is  a state-of-the-art technique  for regression and classification, combining excellent generalisation  properties  with  a  sparse  kernel  representation.  However,  it  does  suffer from a number of disadvantages, notably the absence of prob(cid:173) abilistic outputs, the requirement to estimate a trade-off parameter  and the need to utilise  'Mercer' kernel functions.  In this paper we  introduce the Relevance  Vector Machine  (RVM),  a Bayesian treat(cid:173) ment  of a  generalised  linear  model  of identical  functional  form  to  the SVM.  The RVM  suffers  from  none of the above disadvantages,  and examples demonstrate that for  comparable generalisation per(cid:173) formance,  the RVM  requires dramatically fewer  kernel functions.",
        "bibtex": "@inproceedings{NIPS1999_f3144cef,\n author = {Tipping, Michael},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {The Relevance Vector Machine},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/f3144cefe89a60d6a1afaf7859c5076b-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/f3144cefe89a60d6a1afaf7859c5076b-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/f3144cefe89a60d6a1afaf7859c5076b-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1551903,
        "gs_citation": 1446,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1798808834793179733&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Microsoft Research",
        "aff_domain": "microsoft.com",
        "email": "microsoft.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Microsoft Research",
        "aff_unique_url": "https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "MSR",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b8623a56f7",
        "title": "Topographic Transformation as a Discrete Latent Variable",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/25e2a30f44898b9f3e978b1786dcd85c-Abstract.html",
        "author": "Nebojsa Jojic; Brendan J. Frey",
        "abstract": "Invariance to topographic transformations such as translation and  shearing in  an image has  been successfully incorporated into feed(cid:173) forward  mechanisms,  e.g.,  \"convolutional neural  networks\",  \"tan(cid:173) gent propagation\".  We describe a way to add transformation invari(cid:173) ance to a generative density model by approximating the nonlinear  transformation manifold  by  a  discrete  set of transformations.  An  EM  algorithm for  the original model  can  be extended to the  new  model  by  computing expectations over the set of transformations.  We show how to add a discrete transformation variable to Gaussian  mixture modeling,  factor  analysis and mixtures of factor  analysis.  We  give results on filtering microscopy images, face  and facial  pose  clustering, and handwritten digit  modeling and recognition.",
        "bibtex": "@inproceedings{NIPS1999_25e2a30f,\n author = {Jojic, Nebojsa and Frey, Brendan J},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Topographic Transformation as a Discrete Latent Variable},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/25e2a30f44898b9f3e978b1786dcd85c-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/25e2a30f44898b9f3e978b1786dcd85c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/25e2a30f44898b9f3e978b1786dcd85c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1614762,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15297068743388236253&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Beckman Institute, University of Illinois at Urbana; Computer Science, University of Waterloo",
        "aff_domain": "ifp.uiuc.edu;cs.uwaterloo.ca",
        "email": "ifp.uiuc.edu;cs.uwaterloo.ca",
        "github": "",
        "project": "www.ifp.uiuc.edu/",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;University of Waterloo",
        "aff_unique_dep": "Beckman Institute;Computer Science",
        "aff_unique_url": "https://www.illinois.edu;https://www.uwaterloo.ca",
        "aff_unique_abbr": "UIUC;UW",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Urbana;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "267c44e297",
        "title": "Training Data Selection for Optimal Generalization in Trigonometric Polynomial Networks",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/647c722bf90a49140184672e0d3723e3-Abstract.html",
        "author": "Masashi Sugiyama; Hidemitsu Ogawa",
        "abstract": "In this  paper,  we  consider the problem of active learning  in trigonomet(cid:173) ric  polynomial  networks  and  give  a  necessary  and sufficient  condition of  sample  points to  provide the  optimal generalization capability.  By ana(cid:173) lyzing the condition from  the functional analytic point of view,  we  clarify  the  mechanism  of  achieving  the  optimal  generalization  capability.  We  also  show  that  a  set  of  training  examples  satisfying  the  condition  does  not only provide the optimal generalization  but also  reduces the compu(cid:173) tational  complexity and memory required  for  the  calculation of learning  results.  Finally, examples  of sample  points  satisfying  the  condition  are  given and computer simulations are performed to demonstrate the effec(cid:173) tiveness of the proposed  active learning method.",
        "bibtex": "@inproceedings{NIPS1999_647c722b,\n author = {Sugiyama, Masashi and Ogawa, Hidemitsu},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Training Data Selection for Optimal Generalization in Trigonometric Polynomial Networks},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/647c722bf90a49140184672e0d3723e3-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/647c722bf90a49140184672e0d3723e3-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/647c722bf90a49140184672e0d3723e3-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1389489,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15486631500010833390&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Science, Tokyo Institute of Technology; Department of Computer Science, Tokyo Institute of Technology",
        "aff_domain": "cs.titeck.ac.jp; ",
        "email": "cs.titeck.ac.jp; ",
        "github": "",
        "project": "http://ogawa-www.cs.titech.ac",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Tokyo Institute of Technology",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.titech.ac.jp",
        "aff_unique_abbr": "Titech",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Tokyo",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "f4e34bace3",
        "title": "Transductive Inference for Estimating Values of Functions",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/ef2a4be5473ab0b3cc286e67b1f59f44-Abstract.html",
        "author": "Olivier Chapelle; Vladimir Vapnik; Jason Weston",
        "abstract": "We  introduce an algorithm for  estimating the values of a  function  at a set of test points  Xe+!, ... , xl+m  given a set of training points  (XI,YI), ... ,(xe,Ye)  without  estimating  (as  an  intermediate  step)  the regression function .  We demonstrate that this direct (transduc(cid:173) ti ve)  way  for  estimating  values  of the  regression  (or  classification  in  pattern  recognition)  can  be  more  accurate  than  the  tradition(cid:173) alone  based  on  two  steps,  first  estimating  the  function  and  then  calculating the values of this function  at the  points of interest.",
        "bibtex": "@inproceedings{NIPS1999_ef2a4be5,\n author = {Chapelle, Olivier and Vapnik, Vladimir and Weston, Jason},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Transductive Inference for Estimating Values of Functions},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/ef2a4be5473ab0b3cc286e67b1f59f44-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/ef2a4be5473ab0b3cc286e67b1f59f44-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/ef2a4be5473ab0b3cc286e67b1f59f44-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1297513,
        "gs_citation": 100,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3220547927039393026&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "AT&T Research Laboratories, Red Bank, USA; AT&T Research Laboratories, Red Bank, USA + Royal Holloway, University of London, Egham, Surrey, UK; Royal Holloway, University of London, Egham, Surrey, UK + Barnhill BioInformatics.com, Savannah, Georgia, USA",
        "aff_domain": "research.att.com;research.att.com;research.att.com",
        "email": "research.att.com;research.att.com;research.att.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;1+2",
        "aff_unique_norm": "AT&T Research Laboratories;University of London;Barnhill Bioinformatics",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.att.com/research;https://www.royalholloway.ac.uk;",
        "aff_unique_abbr": "AT&T;RHUL;",
        "aff_campus_unique_index": "0;0+1;1",
        "aff_campus_unique": "Red Bank;Egham;",
        "aff_country_unique_index": "0;0+1;1+0",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "165063e8ac",
        "title": "Understanding Stepwise Generalization of Support Vector Machines: a Toy Model",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/7eb7eabbe9bd03c2fc99881d04da9cbd-Abstract.html",
        "author": "Sebastian Risau-Gusman; Mirta B. Gordon",
        "abstract": "In this  article  we  study the effects  of introducing structure in the  input distribution of the data to be learnt by a  simple perceptron.  We  determine the learning curves within the framework of Statis(cid:173) tical  Mechanics.  Stepwise  generalization  occurs  as  a  function  of  the number of examples when the distribution of patterns is highly  anisotropic.  Although  extremely simple,  the model  seems  to cap(cid:173) ture  the  relevant  features  of  a  class  of  Support  Vector  Machines  which was  recently shown to present this behavior.",
        "bibtex": "@inproceedings{NIPS1999_7eb7eabb,\n author = {Risau-Gusman, Sebastian and Gordon, Mirta},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Understanding Stepwise Generalization of Support Vector Machines: a Toy Model},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7eb7eabbe9bd03c2fc99881d04da9cbd-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/7eb7eabbe9bd03c2fc99881d04da9cbd-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/7eb7eabbe9bd03c2fc99881d04da9cbd-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1380942,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12986127617157540062&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "DRFMCjSPSMS CEA Grenoble, 17 avo des Martyrs 38054 Grenoble cedex 09, France; DRFMCjSPSMS CEA Grenoble, 17 avo des Martyrs 38054 Grenoble cedex 09, France",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "CEA Grenoble",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cea.fr",
        "aff_unique_abbr": "CEA",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Grenoble",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "a5c7d29188",
        "title": "Uniqueness of the SVM Solution",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/c4492cbe90fbdbf88a5aec486aa81ed5-Abstract.html",
        "author": "Christopher J. C. Burges; David J. Crisp",
        "abstract": "We  give  necessary  and  sufficient  conditions  for  uniqueness  of the  support vector solution for the problems of pattern recognition and  regression estimation, for a general class of cost functions.  We show  that if the solution is not unique, all support vectors are necessarily  at  bound,  and  we  give  some  simple examples of non-unique solu(cid:173) tions.  We  note that uniqueness of the primal (dual)  solution does  not necessarily imply uniqueness of the dual (primal) solution.  We  show  how to compute the threshold b when the solution is  unique,  but when  all support vectors are at bound, in which case the usual  method for  determining b does  not work.",
        "bibtex": "@inproceedings{NIPS1999_c4492cbe,\n author = {Burges, Christopher J. C. and Crisp, David},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Uniqueness of the SVM Solution},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/c4492cbe90fbdbf88a5aec486aa81ed5-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/c4492cbe90fbdbf88a5aec486aa81ed5-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/c4492cbe90fbdbf88a5aec486aa81ed5-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1715531,
        "gs_citation": 207,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4891191334507911648&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Advanced Technologies, Bell Laboratories, Lucent Technologies, Holmdel, New Jersey; Centre for Sensor Signal and Information Processing, Deptartment of Electrical Engineering, University of Adelaide, South Australia",
        "aff_domain": "iucent.com;eleceng.adelaide.edu.au",
        "email": "iucent.com;eleceng.adelaide.edu.au",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Bell Laboratories;University of Adelaide",
        "aff_unique_dep": "Advanced Technologies;Deptartment of Electrical Engineering",
        "aff_unique_url": "https://www.bell-labs.com;https://www.adelaide.edu.au",
        "aff_unique_abbr": "Bell Labs;Adelaide",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Adelaide",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;Australia"
    },
    {
        "id": "89cfbca6f1",
        "title": "Unmixing Hyperspectral Data",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/798ed7d4ee7138d49b8828958048130a-Abstract.html",
        "author": "Lucas C. Parra; Clay Spence; Paul Sajda; Andreas Ziehe; Klaus-Robert M\u00fcller",
        "abstract": "In hyperspectral imagery one pixel  typically  consists of a  mixture  of the  reflectance  spectra of several  materials,  where  the  mixture  coefficients  correspond  to  the  abundances of the  constituting  ma(cid:173) terials.  We  assume linear combinations of reflectance spectra with  some additive normal sensor noise and derive a  probabilistic MAP  framework  for  analyzing  hyperspectral  data.  As  the  material re(cid:173) flectance characteristics are not know  a priori, we face  the problem  of  unsupervised  linear  unmixing.  The  incorporation  of  different  prior  information  (e.g.  positivity  and  normalization  of the  abun(cid:173) dances)  naturally  leads  to  a  family  of  interesting  algorithms,  for  example  in  the  noise-free  case  yielding  an  algorithm  that  can  be  understood as constrained independent component analysis (ICA).  Simulations underline the usefulness of our theory.",
        "bibtex": "@inproceedings{NIPS1999_798ed7d4,\n author = {Parra, Lucas and Spence, Clay and Sajda, Paul and Ziehe, Andreas and M\\\"{u}ller, Klaus-Robert},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Unmixing Hyperspectral Data},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/798ed7d4ee7138d49b8828958048130a-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/798ed7d4ee7138d49b8828958048130a-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/798ed7d4ee7138d49b8828958048130a-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1632407,
        "gs_citation": 214,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2918005261282025596&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "Sarnoff Corporation, CN-5300, Princeton, NJ 08543, USA; Sarnoff Corporation, CN-5300, Princeton, NJ 08543, USA; Sarnoff Corporation, CN-5300, Princeton, NJ 08543, USA; GMD FIRST.lDA, Kekulestr. 7, 12489 Berlin, Germany; GMD FIRST.lDA, Kekulestr. 7, 12489 Berlin, Germany",
        "aff_domain": "sarnoff.com;sarnoff.com;sarnoff.com;first.gmd.de;first.gmd.de",
        "email": "sarnoff.com;sarnoff.com;sarnoff.com;first.gmd.de;first.gmd.de",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;1",
        "aff_unique_norm": "Sarnoff Corporation;GMD FIRST.lDA",
        "aff_unique_dep": ";",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;1",
        "aff_country_unique": "United States;Germany"
    },
    {
        "id": "36adc268f7",
        "title": "Variational Inference for Bayesian Mixtures of Factor Analysers",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/2451041557a22145b3701b0184109cab-Abstract.html",
        "author": "Zoubin Ghahramani; Matthew J. Beal",
        "abstract": "We  present an algorithm that infers the model structure of a  mix(cid:173) ture of factor  analysers using  an efficient  and  deterministic  varia(cid:173) tional  approximation to  full  Bayesian  integration  over  model  pa(cid:173) rameters.  This  procedure  can  automatically  determine  the  opti(cid:173) mal  number  of components  and  the  local  dimensionality  of each  component  (Le.  the  number  of  factors  in  each  factor  analyser) .  Alternatively  it  can  be  used  to  infer  posterior  distributions  over  number of components and dimensionalities.  Since  all  parameters  are integrated out the method is  not prone to overfitting.  Using a  stochastic  procedure for  adding  components  it is  possible  to  per(cid:173) form  the variational optimisation incrementally and to avoid  local  maxima.  Results show that the method works very well in practice  and  correctly  infers  the  number  and  dimensionality  of  nontrivial  synthetic examples.  By  importance  sampling  from  the  variational  approximation  we  show  how  to  obtain  unbiased  estimates  of the  true  evidence,  the  exact predictive density,  and the KL divergence between the varia(cid:173) tional posterior and the true posterior, not only in this  model  but  for  variational approximations in general.",
        "bibtex": "@inproceedings{NIPS1999_24510415,\n author = {Ghahramani, Zoubin and Beal, Matthew},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Variational Inference for Bayesian Mixtures of Factor Analysers},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/2451041557a22145b3701b0184109cab-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/2451041557a22145b3701b0184109cab-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/2451041557a22145b3701b0184109cab-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1745328,
        "gs_citation": 597,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17867290318946747378&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "b8f56e439c",
        "title": "Wiring Optimization in the Brain",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/7137debd45ae4d0ab9aa953017286b20-Abstract.html",
        "author": "Dmitri B. Chklovskii; Charles F. Stevens",
        "abstract": "The complexity of cortical circuits may be characterized by the number  of synapses per neuron.  We study the dependence of complexity on the  fraction of the cortical volume that is made up of \"wire\" (that is, ofaxons  and dendrites),  and find  that complexity is  maximized when wire takes  up about 60% of the cortical volume.  This prediction is  in  good agree(cid:173) ment with experimental observations.  A consequence of our arguments  is that any rearrangement of neurons that takes more wire would sacrifice  computational power.",
        "bibtex": "@inproceedings{NIPS1999_7137debd,\n author = {Chklovskii, Dmitri and Stevens, Charles},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {Wiring Optimization in the Brain},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/7137debd45ae4d0ab9aa953017286b20-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/7137debd45ae4d0ab9aa953017286b20-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/7137debd45ae4d0ab9aa953017286b20-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1012096,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5086078679914983127&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Sloan Center for Theoretical Neurobiology, The Salk Institute, La Jolla, CA 92037; Howard Hughes Medical Institute and Molecular Neurobiology Lab, The Salk Institute, La Jolla, CA 92037",
        "aff_domain": "salk.edu;salk.edu",
        "email": "salk.edu;salk.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Salk Institute",
        "aff_unique_dep": "Sloan Center for Theoretical Neurobiology",
        "aff_unique_url": "https://www.salk.edu",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "La Jolla",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "272fc26301",
        "title": "v-Arc: Ensemble Learning in the Presence of Outliers",
        "site": "https://papers.nips.cc/paper_files/paper/1999/hash/28dc6b0e1b33769b4b94685e4f4d1e5c-Abstract.html",
        "author": "Gunnar R\u00e4tsch; Bernhard Sch\u00f6lkopf; Alex J. Smola; Klaus-Robert M\u00fcller; Takashi Onoda; Sebastian Mika",
        "abstract": "AdaBoost and other ensemble methods have successfully  been ap(cid:173) plied  to a  number  of classification  tasks,  seemingly  defying  prob(cid:173) lems of overfitting.  AdaBoost performs gradient descent in an error  function  with  respect  to the margin,  asymptotically concentrating  on  the  patterns which  are  hardest to learn.  For  very  noisy  prob(cid:173) lems,  however,  this  can  be  disadvantageous.  Indeed,  theoretical  analysis has shown that the margin distribution,  as opposed to just  the minimal margin, plays a crucial role in understanding this phe(cid:173) nomenon.  Loosely  speaking,  some  outliers  should  be  tolerated  if  this  has  the  benefit  of substantially  increasing  the  margin  on  the  remaining points.  We  propose a  new  boosting algorithm which  al(cid:173) lows for  the possibility of a  pre-specified fraction of points to lie  in  the margin area Or even on the wrong side of the decision boundary.",
        "bibtex": "@inproceedings{NIPS1999_28dc6b0e,\n author = {R\\\"{a}tsch, Gunnar and Sch\\\"{o}lkopf, Bernhard and Smola, Alex and M\\\"{u}ller, Klaus-Robert and Onoda, Takashi and Mika, Sebastian},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {S. Solla and T. Leen and K. M\\\"{u}ller},\n pages = {},\n publisher = {MIT Press},\n title = {v-Arc: Ensemble Learning in the Presence of Outliers},\n url = {https://proceedings.neurips.cc/paper_files/paper/1999/file/28dc6b0e1b33769b4b94685e4f4d1e5c-Paper.pdf},\n volume = {12},\n year = {1999}\n}",
        "pdf": "https://papers.nips.cc/paper_files/paper/1999/file/28dc6b0e1b33769b4b94685e4f4d1e5c-Paper.pdf",
        "supp": "",
        "metadata": "https://papers.nips.cc/paper_files/paper/1999/file/28dc6b0e1b33769b4b94685e4f4d1e5c-Metadata.json",
        "review": "",
        "metareview": "",
        "pdf_size": 1684632,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9193277018084151340&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    }
]