[
    {
        "id": "64b2a3f26c",
        "title": "A Bayesian nonparametric procedure for comparing algorithms",
        "site": "https://proceedings.mlr.press/v37/benavoli15.html",
        "author": "Alessio Benavoli; Giorgio Corani; Francesca Mangili; Marco Zaffalon",
        "abstract": "A fundamental task in machine learning is to compare the performance of multiple algorithms. This is typically performed by frequentist tests (usually the Friedman test followed by a series of multiple pairwise comparisons). This implies dealing with null hypothesis significance tests and p-values, although the shortcomings of such methods are well known. First, we propose a nonparametric Bayesian version of the Friedman test using a Dirichlet process (DP) based prior. Our derivations show that, from a Bayesian perspective, the Friedman test is an inference for a multivariate mean based on an ellipsoid inclusion test. Second, we derive a joint procedure for the analysis of the multiple comparisons which accounts for their dependencies and which is based on the posterior probability computed through the DP. The proposed approach allows verifying the null hypothesis, not only rejecting it. Third, we apply our test to perform algorithms racing, i.e., the problem of identifying the best algorithm among a large set of candidates. We show by simulation that our approach is competitive both in terms of accuracy and speed in identifying the best algorithm.",
        "bibtex": "@InProceedings{pmlr-v37-benavoli15,\n  title = \t {A Bayesian nonparametric procedure for comparing algorithms},\n  author = \t {Benavoli, Alessio and Corani, Giorgio and Mangili, Francesca and Zaffalon, Marco},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1264--1272},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/benavoli15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/benavoli15.html},\n  abstract = \t {A fundamental task in machine learning is to compare the performance of multiple algorithms. This is typically performed by frequentist tests (usually the Friedman test followed by a series of multiple pairwise comparisons). This implies dealing with null hypothesis significance tests and p-values, although the shortcomings of such methods are well known. First, we propose a nonparametric Bayesian version of the Friedman test using a Dirichlet process (DP) based prior. Our derivations show that, from a Bayesian perspective, the Friedman test is an inference for a multivariate mean based on an ellipsoid inclusion test. Second, we derive a joint procedure for the analysis of the multiple comparisons which accounts for their dependencies and which is based on the posterior probability computed through the DP. The proposed approach allows verifying the null hypothesis, not only rejecting it. Third, we apply our test to perform algorithms racing, i.e., the problem of identifying the best algorithm among a large set of candidates. We show by simulation that our approach is competitive both in terms of accuracy and speed in identifying the best algorithm.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/benavoli15.pdf",
        "supp": "",
        "pdf_size": 177860,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14621318846451880471&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "IDSIA, Manno, Switzerland; IDSIA, Manno, Switzerland; IDSIA, Manno, Switzerland; IDSIA, Manno, Switzerland",
        "aff_domain": "IDSIA.CH;IDSIA.CH;IDSIA.CH;IDSIA.CH",
        "email": "IDSIA.CH;IDSIA.CH;IDSIA.CH;IDSIA.CH",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "IDSIA",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.idsia.ch",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Manno",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "28a51497e5",
        "title": "A Convex Exemplar-based Approach to MAD-Bayes Dirichlet Process Mixture Models",
        "site": "https://proceedings.mlr.press/v37/yen15.html",
        "author": "En-Hsu Yen; Xin Lin; Kai Zhong; Pradeep Ravikumar; Inderjit Dhillon",
        "abstract": "MAD-Bayes (MAP-based Asymptotic Derivations) has been recently proposed as a general technique to derive scalable algorithm for Bayesian Nonparametric models. However, the combinatorial nature of objective functions derived from MAD-Bayes results in hard optimization problem, for which current practice employs heuristic algorithms analogous to k-means to find local minimum. In this paper, we consider the exemplar-based version of MAD-Bayes formulation for DP and Hierarchical DP (HDP) mixture model. We show that an exemplar-based MAD-Bayes formulation can be relaxed to a convex structural-regularized program that, under cluster-separation conditions, shares the same optimal solution to its combinatorial counterpart. An algorithm based on Alternating Direction Method of Multiplier (ADMM) is then proposed to solve such program. In our experiments on several benchmark data sets, the proposed method finds optimal solution of the combinatorial problem and significantly improves existing methods in terms of the exemplar-based objective.",
        "bibtex": "@InProceedings{pmlr-v37-yen15,\n  title = \t {A Convex Exemplar-based Approach to MAD-Bayes Dirichlet Process Mixture Models},\n  author = \t {Yen, En-Hsu and Lin, Xin and Zhong, Kai and Ravikumar, Pradeep and Dhillon, Inderjit},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2418--2426},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/yen15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/yen15.html},\n  abstract = \t {MAD-Bayes (MAP-based Asymptotic Derivations) has been recently proposed as a general technique to derive scalable algorithm for Bayesian Nonparametric models. However, the combinatorial nature of objective functions derived from MAD-Bayes results in hard optimization problem, for which current practice employs heuristic algorithms analogous to k-means to find local minimum. In this paper, we consider the exemplar-based version of MAD-Bayes formulation for DP and Hierarchical DP (HDP) mixture model. We show that an exemplar-based MAD-Bayes formulation can be relaxed to a convex structural-regularized program that, under cluster-separation conditions, shares the same optimal solution to its combinatorial counterpart. An algorithm based on Alternating Direction Method of Multiplier (ADMM) is then proposed to solve such program. In our experiments on several benchmark data sets, the proposed method finds optimal solution of the combinatorial problem and significantly improves existing methods in terms of the exemplar-based objective.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/yen15.pdf",
        "supp": "",
        "pdf_size": 875971,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2570597215026622306&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, University of Texas at Austin; Department of Computer Science, University of Texas at Austin; Institute for Computational Engineering and Sciences, University of Texas at Austin + Department of Statistics and Data Sciences, University of Texas at Austin; Department of Computer Science, University of Texas at Austin + Institute for Computational Engineering and Sciences, University of Texas at Austin + Department of Statistics and Data Sciences, University of Texas at Austin; Department of Computer Science, University of Texas at Austin + Institute for Computational Engineering and Sciences, University of Texas at Austin",
        "aff_domain": "cs.utexas.edu;utexas.edu;ices.utexas.edu;cs.utexas.edu;cs.utexas.edu",
        "email": "cs.utexas.edu;utexas.edu;ices.utexas.edu;cs.utexas.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+0;0+0+0;0+0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0;0+0;0+0+0;0+0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0;0+0;0+0+0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "67d698d93b",
        "title": "A Convex Optimization Framework for Bi-Clustering",
        "site": "https://proceedings.mlr.press/v37/limb15.html",
        "author": "Shiau Hong Lim; Yudong Chen; Huan Xu",
        "abstract": "We present a framework for biclustering and clustering where the observations are general labels. Our approach is based on the maximum likelihood estimator and its convex relaxation, and generalizes recent works in graph clustering to the biclustering setting. In addition to standard biclustering setting where one seeks to discover clustering structure simultaneously in two domain sets, we show that the same algorithm can be as effective when clustering structure only occurs in one domain. This allows for an alternative approach to clustering that is more natural in some scenarios. We present theoretical results that provide sufficient conditions for the recovery of the true underlying clusters under a generalized stochastic block model. These are further validated by our empirical results on both synthetic and real data.",
        "bibtex": "@InProceedings{pmlr-v37-limb15,\n  title = \t {A Convex Optimization Framework for Bi-Clustering},\n  author = \t {Lim, Shiau Hong and Chen, Yudong and Xu, Huan},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1679--1688},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/limb15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/limb15.html},\n  abstract = \t {We present a framework for biclustering and clustering where the observations are general labels. Our approach is based on the maximum likelihood estimator and its convex relaxation, and generalizes recent works in graph clustering to the biclustering setting. In addition to standard biclustering setting where one seeks to discover clustering structure simultaneously in two domain sets, we show that the same algorithm can be as effective when clustering structure only occurs in one domain. This allows for an alternative approach to clustering that is more natural in some scenarios. We present theoretical results that provide sufficient conditions for the recovery of the true underlying clusters under a generalized stochastic block model. These are further validated by our empirical results on both synthetic and real data.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/limb15.pdf",
        "supp": "",
        "pdf_size": 456063,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=97287685190145758&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "National University of Singapore; University of California, Berkeley; National University of Singapore",
        "aff_domain": "gmail.com;eecs.berkeley.edu;nus.edu.sg",
        "email": "gmail.com;eecs.berkeley.edu;nus.edu.sg",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "National University of Singapore;University of California, Berkeley",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.nus.edu.sg;https://www.berkeley.edu",
        "aff_unique_abbr": "NUS;UC Berkeley",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Singapore;United States"
    },
    {
        "id": "233675223d",
        "title": "A Deeper Look at Planning as Learning from Replay",
        "site": "https://proceedings.mlr.press/v37/vanseijen15.html",
        "author": "Harm Vanseijen; Rich Sutton",
        "abstract": "In reinforcement learning, the notions of experience replay, and of planning as learning from replayed experience, have long been used to find good policies with minimal training data. Replay can be seen either as model-based reinforcement learning, where the store of past experiences serves as the model, or as a way to avoid a conventional model of the environment altogether. In this paper, we look more deeply at how replay blurs the line between model-based and model-free methods. First, we show for the first time an exact equivalence between the sequence of value functions found by a model-based policy-evaluation method and by a model-free method with replay. Second, we present a general replay method that can mimic a spectrum of methods ranging from the explicitly model-free (TD(0)) to the explicitly model-based (linear Dyna). Finally, we use insights gained from these relationships to design a new model-based reinforcement learning algorithm for linear function approximation. This method, which we call forgetful LSTD(lambda), improves upon regular LSTD(lambda) because it extends more naturally to online control, and improves upon linear Dyna because it is a multi-step method, enabling it to perform well even in non-Markov problems or, equivalently, in problems with significant function approximation.",
        "bibtex": "@InProceedings{pmlr-v37-vanseijen15,\n  title = \t {A Deeper Look at Planning as Learning from Replay},\n  author = \t {Vanseijen, Harm and Sutton, Rich},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2314--2322},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/vanseijen15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/vanseijen15.html},\n  abstract = \t {In reinforcement learning, the notions of experience replay, and of planning as learning from replayed experience, have long been used to find good policies with minimal training data. Replay can be seen either as model-based reinforcement learning, where the store of past experiences serves as the model, or as a way to avoid a conventional model of the environment altogether. In this paper, we look more deeply at how replay blurs the line between model-based and model-free methods. First, we show for the first time an exact equivalence between the sequence of value functions found by a model-based policy-evaluation method and by a model-free method with replay. Second, we present a general replay method that can mimic a spectrum of methods ranging from the explicitly model-free (TD(0)) to the explicitly model-based (linear Dyna). Finally, we use insights gained from these relationships to design a new model-based reinforcement learning algorithm for linear function approximation. This method, which we call forgetful LSTD(lambda), improves upon regular LSTD(lambda) because it extends more naturally to online control, and improves upon linear Dyna because it is a multi-step method, enabling it to perform well even in non-Markov problems or, equivalently, in problems with significant function approximation.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/vanseijen15.pdf",
        "supp": "",
        "pdf_size": 271288,
        "gs_citation": 81,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15591551080616246516&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computing Science, University of Alberta, Edmonton, Alberta, T6G 2E8, Canada; Department of Computing Science, University of Alberta, Edmonton, Alberta, T6G 2E8, Canada",
        "aff_domain": "UALBERTA.CA;CS.UALBERTA.CA",
        "email": "UALBERTA.CA;CS.UALBERTA.CA",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "Department of Computing Science",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Edmonton",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "cb9b00df3c",
        "title": "A Deterministic Analysis of Noisy Sparse Subspace Clustering for Dimensionality-reduced Data",
        "site": "https://proceedings.mlr.press/v37/wange15.html",
        "author": "Yining Wang; Yu-Xiang Wang; Aarti Singh",
        "abstract": "Subspace clustering groups data into several lowrank subspaces. In this paper, we propose a theoretical framework to analyze a popular optimization-based algorithm, Sparse Subspace Clustering (SSC), when the data dimension is compressed via some random projection algorithms. We show SSC provably succeeds if the random projection is a subspace embedding, which includes random Gaussian projection, uniform row sampling, FJLT, sketching, etc. Our analysis applies to the most general deterministic setting and is able to handle both adversarial and stochastic noise. It also results in the first algorithm for privacy-preserved subspace clustering.",
        "bibtex": "@InProceedings{pmlr-v37-wange15,\n  title = \t {A Deterministic Analysis of Noisy Sparse Subspace Clustering for Dimensionality-reduced Data},\n  author = \t {Wang, Yining and Wang, Yu-Xiang and Singh, Aarti},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1422--1431},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/wange15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/wange15.html},\n  abstract = \t {Subspace clustering groups data into several lowrank subspaces. In this paper, we propose a theoretical framework to analyze a popular optimization-based algorithm, Sparse Subspace Clustering (SSC), when the data dimension is compressed via some random projection algorithms. We show SSC provably succeeds if the random projection is a subspace embedding, which includes random Gaussian projection, uniform row sampling, FJLT, sketching, etc. Our analysis applies to the most general deterministic setting and is able to handle both adversarial and stochastic noise. It also results in the first algorithm for privacy-preserved subspace clustering.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/wange15.pdf",
        "supp": "",
        "pdf_size": 413222,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6803456311745765436&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15213, USA; Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15213, USA; Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15213, USA",
        "aff_domain": "CS.CMU.EDU;CS.CMU.EDU;CS.CMU.EDU",
        "email": "CS.CMU.EDU;CS.CMU.EDU;CS.CMU.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Machine Learning Department",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3c51f78890",
        "title": "A Divide and Conquer Framework for Distributed Graph Clustering",
        "site": "https://proceedings.mlr.press/v37/yange15.html",
        "author": "Wenzhuo Yang; Huan Xu",
        "abstract": "Graph clustering is about identifying clusters of closely connected nodes, and is a fundamental technique of data analysis with many applications including community detection, VLSI network partitioning, collaborative filtering, and many others. In order to improve the scalability of existing graph clustering algorithms, we propose a novel divide and conquer framework for graph clustering, and establish theoretical guarantees of exact recovery of the clusters. One additional advantage of the proposed framework is that it can identify small clusters \u2013 the size of the smallest cluster can be of size o(\\sqrtn), in contrast to \u03a9(\\sqrtn) required by standard methods. Extensive experiments on synthetic and real-world datasets demonstrate the efficiency and effectiveness of our framework.",
        "bibtex": "@InProceedings{pmlr-v37-yange15,\n  title = \t {A Divide and Conquer Framework for Distributed Graph Clustering},\n  author = \t {Yang, Wenzhuo and Xu, Huan},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {504--513},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/yange15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/yange15.html},\n  abstract = \t {Graph clustering is about identifying clusters of closely connected nodes, and is a fundamental technique of data analysis with many applications including community detection, VLSI network partitioning, collaborative filtering, and many others. In order to improve the scalability of existing graph clustering algorithms, we propose a novel divide and conquer framework for graph clustering, and establish theoretical guarantees of exact recovery of the clusters. One additional advantage of the proposed framework is that it can identify small clusters \u2013 the size of the smallest cluster can be of size o(\\sqrtn), in contrast to \u03a9(\\sqrtn) required by standard methods. Extensive experiments on synthetic and real-world datasets demonstrate the efficiency and effectiveness of our framework.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/yange15.pdf",
        "supp": "",
        "pdf_size": 188229,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2242991430500964320&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Mechanical Engineering, National University of Singapore, Singapore 117576; Department of Mechanical Engineering, National University of Singapore, Singapore 117576",
        "aff_domain": "NUS.EDU.SG;NUS.EDU.SG",
        "email": "NUS.EDU.SG;NUS.EDU.SG",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "National University of Singapore",
        "aff_unique_dep": "Department of Mechanical Engineering",
        "aff_unique_url": "https://www.nus.edu.sg",
        "aff_unique_abbr": "NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "e5c618e99e",
        "title": "A Fast Variational Approach for Learning Markov Random Field Language Models",
        "site": "https://proceedings.mlr.press/v37/jernite15.html",
        "author": "Yacine Jernite; Alexander Rush; David Sontag",
        "abstract": "Language modelling is a fundamental building block of natural language processing. However, in practice the size of the vocabulary limits the distributions applicable for this task: specifically, one has to either resort to local optimization methods, such as those used in neural language models, or work with heavily constrained distributions. In this work, we take a step towards overcoming these difficulties. We present a method for global-likelihood optimization of a Markov random field language model exploiting long-range contexts in time independent of the corpus size. We take a variational approach to optimizing the likelihood and exploit underlying symmetries to greatly simplify learning. We demonstrate the efficiency of this method both for language modelling and for part-of-speech tagging.",
        "bibtex": "@InProceedings{pmlr-v37-jernite15,\n  title = \t {A Fast Variational Approach for Learning Markov Random Field Language Models},\n  author = \t {Jernite, Yacine and Rush, Alexander and Sontag, David},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2209--2217},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/jernite15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/jernite15.html},\n  abstract = \t {Language modelling is a fundamental building block of natural language processing. However, in practice the size of the vocabulary limits the distributions applicable for this task: specifically, one has to either resort to local optimization methods, such as those used in neural language models, or work with heavily constrained distributions. In this work, we take a step towards overcoming these difficulties. We present a method for global-likelihood optimization of a Markov random field language model exploiting long-range contexts in time independent of the corpus size. We take a variational approach to optimizing the likelihood and exploit underlying symmetries to greatly simplify learning. We demonstrate the efficiency of this method both for language modelling and for part-of-speech tagging.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/jernite15.pdf",
        "supp": "",
        "pdf_size": 461631,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5981358633733162385&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "CIMS, New York University; Facebook AI Research; CIMS, New York University",
        "aff_domain": "cs.nyu.edu;seas.harvard.edu;cs.nyu.edu",
        "email": "cs.nyu.edu;seas.harvard.edu;cs.nyu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "New York University;Meta",
        "aff_unique_dep": "Courant Institute of Mathematical Sciences;Facebook AI Research",
        "aff_unique_url": "https://www.nyu.edu;https://research.facebook.com",
        "aff_unique_abbr": "NYU;FAIR",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "New York;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "25c67baf5e",
        "title": "A General Analysis of the Convergence of ADMM",
        "site": "https://proceedings.mlr.press/v37/nishihara15.html",
        "author": "Robert Nishihara; Laurent Lessard; Ben Recht; Andrew Packard; Michael Jordan",
        "abstract": "We provide a new proof of the linear convergence of the alternating direction method of multipliers (ADMM) when one of the objective terms is strongly convex. Our proof is based on a framework for analyzing optimization algorithms introduced in Lessard et al. (2014), reducing algorithm convergence to verifying the stability of a dynamical system. This approach generalizes a number of existing results and obviates any assumptions about specific choices of algorithm parameters. On a numerical example, we demonstrate that minimizing the derived bound on the convergence rate provides a practical approach to selecting algorithm parameters for particular ADMM instances. We complement our upper bound by constructing a nearly-matching lower bound on the worst-case rate of convergence.",
        "bibtex": "@InProceedings{pmlr-v37-nishihara15,\n  title = \t {A General Analysis of the Convergence of ADMM},\n  author = \t {Nishihara, Robert and Lessard, Laurent and Recht, Ben and Packard, Andrew and Jordan, Michael},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {343--352},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/nishihara15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/nishihara15.html},\n  abstract = \t {We provide a new proof of the linear convergence of the alternating direction method of multipliers (ADMM) when one of the objective terms is strongly convex. Our proof is based on a framework for analyzing optimization algorithms introduced in Lessard et al. (2014), reducing algorithm convergence to verifying the stability of a dynamical system. This approach generalizes a number of existing results and obviates any assumptions about specific choices of algorithm parameters. On a numerical example, we demonstrate that minimizing the derived bound on the convergence rate provides a practical approach to selecting algorithm parameters for particular ADMM instances. We complement our upper bound by constructing a nearly-matching lower bound on the worst-case rate of convergence.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/nishihara15.pdf",
        "supp": "",
        "pdf_size": 342480,
        "gs_citation": 404,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9935674567319083530&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "University of California, Berkeley, CA 94720 USA; University of California, Berkeley, CA 94720 USA; University of California, Berkeley, CA 94720 USA; University of California, Berkeley, CA 94720 USA; University of California, Berkeley, CA 94720 USA",
        "aff_domain": "EECS.BERKELEY.EDU;BERKELEY.EDU;EECS.BERKELEY.EDU;BERKELEY.EDU;EECS.BERKELEY.EDU",
        "email": "EECS.BERKELEY.EDU;BERKELEY.EDU;EECS.BERKELEY.EDU;BERKELEY.EDU;EECS.BERKELEY.EDU",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b65aab3416",
        "title": "A Hybrid Approach for Probabilistic Inference using Random Projections",
        "site": "https://proceedings.mlr.press/v37/zhuc15.html",
        "author": "Michael Zhu; Stefano Ermon",
        "abstract": "We introduce a new meta-algorithm for probabilistic inference in graphical models based on random projections. The key idea is to use approximate inference algorithms for an (exponentially) large number of samples, obtained by randomly projecting the original statistical model using universal hash functions. In the case where the approximate inference algorithm is a variational approximation, this approach can be viewed as interpolating between sampling-based and variational techniques. The number of samples used controls the trade-off between the accuracy of the approximate inference algorithm and the variance of the estimator. We show empirically that by using random projections, we can improve the accuracy of common approximate inference algorithms.",
        "bibtex": "@InProceedings{pmlr-v37-zhuc15,\n  title = \t {A Hybrid Approach for Probabilistic Inference using Random Projections},\n  author = \t {Zhu, Michael and Ermon, Stefano},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2039--2047},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/zhuc15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/zhuc15.html},\n  abstract = \t {We introduce a new meta-algorithm for probabilistic inference in graphical models based on random projections. The key idea is to use approximate inference algorithms for an (exponentially) large number of samples, obtained by randomly projecting the original statistical model using universal hash functions. In the case where the approximate inference algorithm is a variational approximation, this approach can be viewed as interpolating between sampling-based and variational techniques. The number of samples used controls the trade-off between the accuracy of the approximate inference algorithm and the variance of the estimator. We show empirically that by using random projections, we can improve the accuracy of common approximate inference algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/zhuc15.pdf",
        "supp": "",
        "pdf_size": 400853,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15270194410042315730&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Stanford University; Stanford University",
        "aff_domain": "CS.STANFORD.EDU;CS.STANFORD.EDU",
        "email": "CS.STANFORD.EDU;CS.STANFORD.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "587160b3cc",
        "title": "A Linear Dynamical System Model for Text",
        "site": "https://proceedings.mlr.press/v37/belanger15.html",
        "author": "David Belanger; Sham Kakade",
        "abstract": "Low dimensional representations of words allow accurate NLP models to be trained on limited annotated data. While most representations ignore words\u2019 local context, a natural way to induce context-dependent representations is to perform inference in a probabilistic latent-variable sequence model. Given the recent success of continuous vector space word representations, we provide such an inference procedure for continuous states, where words\u2019 representations are given by the posterior mean of a linear dynamical system. Here, efficient inference can be performed using Kalman filtering. Our learning algorithm is extremely scalable, operating on simple co-occurrence counts for both parameter initialization using the method of moments and subsequent iterations of EM. In our experiments, we employ our inferred word embeddings as features in standard tagging tasks, obtaining significant accuracy improvements. Finally, the Kalman filter updates can be seen as a linear recurrent neural network. We demonstrate that using the parameters of our model to initialize a non-linear recurrent neural network language model reduces its training time by a day and yields lower perplexity.",
        "bibtex": "@InProceedings{pmlr-v37-belanger15,\n  title = \t {A Linear Dynamical System Model for Text},\n  author = \t {Belanger, David and Kakade, Sham},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {833--842},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/belanger15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/belanger15.html},\n  abstract = \t {Low dimensional representations of words allow accurate NLP models to be trained on limited annotated data. While most representations ignore words\u2019 local context, a natural way to induce context-dependent representations is to perform inference in a probabilistic latent-variable sequence model. Given the recent success of continuous vector space word representations, we provide such an inference procedure for continuous states, where words\u2019 representations are given by the posterior mean of a linear dynamical system. Here, efficient inference can be performed using Kalman filtering. Our learning algorithm is extremely scalable, operating on simple co-occurrence counts for both parameter initialization using the method of moments and subsequent iterations of EM. In our experiments, we employ our inferred word embeddings as features in standard tagging tasks, obtaining significant accuracy improvements. Finally, the Kalman filter updates can be seen as a linear recurrent neural network. We demonstrate that using the parameters of our model to initialize a non-linear recurrent neural network language model reduces its training time by a day and yields lower perplexity.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/belanger15.pdf",
        "supp": "",
        "pdf_size": 342185,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2467213344437448012&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "College of Information and Computer Sciences, University of Massachusetts Amherst; Microsoft Research",
        "aff_domain": "CS.UMASS.EDU;MICROSOFT.COM",
        "email": "CS.UMASS.EDU;MICROSOFT.COM",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Massachusetts Amherst;Microsoft",
        "aff_unique_dep": "College of Information and Computer Sciences;Microsoft Research",
        "aff_unique_url": "https://www.umass.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "UMass Amherst;MSR",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Amherst;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "cb970d5d8b",
        "title": "A Lower Bound for the Optimization of Finite Sums",
        "site": "https://proceedings.mlr.press/v37/agarwal15.html",
        "author": "Alekh Agarwal; Leon Bottou",
        "abstract": "This paper presents a lower bound for optimizing a finite sum of n functions, where each function is L-smooth and the sum is \u03bc-strongly convex. We show that no algorithm can reach an error \u03b5in minimizing all functions from this class in fewer than \u03a9(n + \\sqrtn(\u03ba-1)\\log(1/\u03b5)) iterations, where \u03ba=L/\u03bcis a surrogate condition number. We then compare this lower bound to upper bounds for recently developed methods specializing to this setting. When the functions involved in this sum are not arbitrary, but based on i.i.d. random data, then we further contrast these complexity results with those for optimal first-order methods to directly optimize the sum. The conclusion we draw is that a lot of caution is necessary for an accurate comparison, and identify machine learning scenarios where the new methods help computationally.",
        "bibtex": "@InProceedings{pmlr-v37-agarwal15,\n  title = \t {A Lower Bound for the Optimization of Finite Sums},\n  author = \t {Agarwal, Alekh and Bottou, Leon},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {78--86},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/agarwal15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/agarwal15.html},\n  abstract = \t {This paper presents a lower bound for optimizing a finite sum of n functions, where each function is L-smooth and the sum is \u03bc-strongly convex. We show that no algorithm can reach an error \u03b5in minimizing all functions from this class in fewer than \u03a9(n + \\sqrtn(\u03ba-1)\\log(1/\u03b5)) iterations, where \u03ba=L/\u03bcis a surrogate condition number. We then compare this lower bound to upper bounds for recently developed methods specializing to this setting. When the functions involved in this sum are not arbitrary, but based on i.i.d. random data, then we further contrast these complexity results with those for optimal first-order methods to directly optimize the sum. The conclusion we draw is that a lot of caution is necessary for an accurate comparison, and identify machine learning scenarios where the new methods help computationally.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/agarwal15.pdf",
        "supp": "",
        "pdf_size": 236815,
        "gs_citation": 164,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8035458299584535551&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Microsoft Research NYC, New York, NY; Facebook AI Research, New York, NY",
        "aff_domain": "microsoft.com;botou.org",
        "email": "microsoft.com;botou.org",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Microsoft;Meta",
        "aff_unique_dep": "Microsoft Research;Facebook AI Research",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/group/microsoft-research-new-york-city;https://research.facebook.com",
        "aff_unique_abbr": "MSR NYC;FAIR",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "New York",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "41eb597139",
        "title": "A Modified Orthant-Wise Limited Memory Quasi-Newton Method with Convergence Analysis",
        "site": "https://proceedings.mlr.press/v37/gonga15.html",
        "author": "Pinghua Gong; Jieping Ye",
        "abstract": "The Orthant-Wise Limited memory Quasi-Newton (OWL-QN) method has been demonstrated to be very effective in solving the \\ell_1-regularized sparse learning problem. OWL-QN extends the L-BFGS from solving unconstrained smooth optimization problems to \\ell_1-regularized (non-smooth) sparse learning problems. At each iteration, OWL-QN does not involve any \\ell_1-regularized quadratic optimization subproblem and only requires matrix-vector multiplications without an explicit use of the (inverse) Hessian matrix, which enables OWL-QN to tackle large-scale problems efficiently. Although many empirical studies have demonstrated that OWL-QN works quite well in practice, several recent papers point out that the existing convergence proof of OWL-QN is flawed and a rigorous convergence analysis for OWL-QN still remains to be established. In this paper, we propose a modified Orthant-Wise Limited memory Quasi-Newton (mOWL-QN) algorithm by slightly modifying the OWL-QN algorithm. As the main technical contribution of this paper, we establish a rigorous convergence proof for the mOWL-QN algorithm. To the best of our knowledge, our work fills the theoretical gap by providing the first rigorous convergence proof for the OWL-QN-type algorithm on solving \\ell_1-regularized sparse learning problems. We also provide empirical studies to show that mOWL-QN works well and is as efficient as OWL-QN.",
        "bibtex": "@InProceedings{pmlr-v37-gonga15,\n  title = \t {A Modified Orthant-Wise Limited Memory Quasi-Newton Method with Convergence Analysis},\n  author = \t {Gong, Pinghua and Ye, Jieping},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {276--284},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/gonga15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/gonga15.html},\n  abstract = \t {The Orthant-Wise Limited memory Quasi-Newton (OWL-QN) method has been demonstrated to be very effective in solving the \\ell_1-regularized sparse learning problem. OWL-QN extends the L-BFGS from solving unconstrained smooth optimization problems to \\ell_1-regularized (non-smooth) sparse learning problems. At each iteration, OWL-QN does not involve any \\ell_1-regularized quadratic optimization subproblem and only requires matrix-vector multiplications without an explicit use of the (inverse) Hessian matrix, which enables OWL-QN to tackle large-scale problems efficiently. Although many empirical studies have demonstrated that OWL-QN works quite well in practice, several recent papers point out that the existing convergence proof of OWL-QN is flawed and a rigorous convergence analysis for OWL-QN still remains to be established. In this paper, we propose a modified Orthant-Wise Limited memory Quasi-Newton (mOWL-QN) algorithm by slightly modifying the OWL-QN algorithm. As the main technical contribution of this paper, we establish a rigorous convergence proof for the mOWL-QN algorithm. To the best of our knowledge, our work fills the theoretical gap by providing the first rigorous convergence proof for the OWL-QN-type algorithm on solving \\ell_1-regularized sparse learning problems. We also provide empirical studies to show that mOWL-QN works well and is as efficient as OWL-QN.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/gonga15.pdf",
        "supp": "",
        "pdf_size": 148478,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2337096958333356509&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Michigan, Ann Arbor, MI 48109; University of Michigan, Ann Arbor, MI 48109",
        "aff_domain": "umich.edu;umich.edu",
        "email": "umich.edu;umich.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "827a2df5e7",
        "title": "A Multitask Point Process Predictive Model",
        "site": "https://proceedings.mlr.press/v37/lian15.html",
        "author": "Wenzhao Lian; Ricardo Henao; Vinayak Rao; Joseph Lucas; Lawrence Carin",
        "abstract": "Point process data are commonly observed in fields like healthcare and social science. Designing predictive models for such event streams is an under-explored problem, due to often scarce training data. In this work we propose a multitask point process model, leveraging information from all tasks via a hierarchical Gaussian process (GP). Nonparametric learning functions implemented by a GP, which map from past events to future rates, allow analysis of flexible arrival patterns. To facilitate efficient inference, we propose a sparse construction for this hierarchical model, and derive a variational Bayes method for learning and inference. Experimental results are shown on both synthetic data and an application on real electronic health records.",
        "bibtex": "@InProceedings{pmlr-v37-lian15,\n  title = \t {A Multitask Point Process Predictive Model},\n  author = \t {Lian, Wenzhao and Henao, Ricardo and Rao, Vinayak and Lucas, Joseph and Carin, Lawrence},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2030--2038},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/lian15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/lian15.html},\n  abstract = \t {Point process data are commonly observed in fields like healthcare and social science. Designing predictive models for such event streams is an under-explored problem, due to often scarce training data. In this work we propose a multitask point process model, leveraging information from all tasks via a hierarchical Gaussian process (GP). Nonparametric learning functions implemented by a GP, which map from past events to future rates, allow analysis of flexible arrival patterns. To facilitate efficient inference, we propose a sparse construction for this hierarchical model, and derive a variational Bayes method for learning and inference. Experimental results are shown on both synthetic data and an application on real electronic health records.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/lian15.pdf",
        "supp": "",
        "pdf_size": 690871,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8633841299034810011&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Electrical and Computer Engineering, Duke University; Department of Electrical and Computer Engineering, Duke University; Department of Statistics, Purdue University; Center for Predictive Medicine, Duke Clinical Research Institute; Department of Electrical and Computer Engineering, Duke University",
        "aff_domain": "DUKE.EDU;DUKE.EDU;PURDUE.EDU;STAT.DUKE.EDU;DUKE.EDU",
        "email": "DUKE.EDU;DUKE.EDU;PURDUE.EDU;STAT.DUKE.EDU;DUKE.EDU",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "Duke University;Purdue University;Duke Clinical Research Institute",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;Department of Statistics;Center for Predictive Medicine",
        "aff_unique_url": "https://www.duke.edu;https://www.purdue.edu;https://www.duke.edu",
        "aff_unique_abbr": "Duke;Purdue;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c9cb4a6859",
        "title": "A Nearly-Linear Time Framework for Graph-Structured Sparsity",
        "site": "https://proceedings.mlr.press/v37/hegde15.html",
        "author": "Chinmay Hegde; Piotr Indyk; Ludwig Schmidt",
        "abstract": "We introduce a framework for sparsity structures defined via graphs. Our approach is flexible and generalizes several previously studied sparsity models. Moreover, we provide efficient projection algorithms for our sparsity model that run in nearly-linear time. In the context of sparse recovery, we show that our framework achieves an information-theoretically optimal sample complexity for a wide range of parameters. We complement our theoretical analysis with experiments demonstrating that our algorithms improve on prior work also in practice.",
        "bibtex": "@InProceedings{pmlr-v37-hegde15,\n  title = \t {A Nearly-Linear Time Framework for Graph-Structured Sparsity},\n  author = \t {Hegde, Chinmay and Indyk, Piotr and Schmidt, Ludwig},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {928--937},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/hegde15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/hegde15.html},\n  abstract = \t {We introduce a framework for sparsity structures defined via graphs. Our approach is flexible and generalizes several previously studied sparsity models. Moreover, we provide efficient projection algorithms for our sparsity model that run in nearly-linear time. In the context of sparse recovery, we show that our framework achieves an information-theoretically optimal sample complexity for a wide range of parameters. We complement our theoretical analysis with experiments demonstrating that our algorithms improve on prior work also in practice.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/hegde15.pdf",
        "supp": "",
        "pdf_size": 427971,
        "gs_citation": 121,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11302772927507469406&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Massachusetts Institute of Technology; Massachusetts Institute of Technology; Massachusetts Institute of Technology",
        "aff_domain": "MIT.EDU;MIT.EDU;MIT.EDU",
        "email": "MIT.EDU;MIT.EDU;MIT.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b761802644",
        "title": "A New Generalized Error Path Algorithm for Model Selection",
        "site": "https://proceedings.mlr.press/v37/gu15.html",
        "author": "Bin Gu; Charles Ling",
        "abstract": "Model selection with cross validation (CV) is very popular in machine learning. However, CV with grid and other common search strategies cannot guarantee to find the model with minimum CV error, which is often the ultimate goal of model selection. Recently, various solution path algorithms have been proposed for several important learning algorithms including support vector classification, Lasso, and so on. However, they still do not guarantee to find the model with minimum CV error.In this paper, we first show that the solution paths produced by various algorithms have the property of piecewise linearity. Then, we prove that a large class of error (or loss) functions are piecewise constant, linear, or quadratic w.r.t. the regularization parameter, based on the solution path. Finally, we propose a new generalized error path algorithm (GEP), and prove that it will find the model with minimum CV error for the entire range of the regularization parameter. The experimental results on a variety of datasets not only confirm our theoretical findings, but also show that the best model with our GEP has better generalization error on the test data, compared to the grid search, manual search, and random search.",
        "bibtex": "@InProceedings{pmlr-v37-gu15,\n  title = \t {A New Generalized Error Path Algorithm for Model Selection},\n  author = \t {Gu, Bin and Ling, Charles},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2549--2558},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/gu15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/gu15.html},\n  abstract = \t {Model selection with cross validation (CV) is very popular in machine learning. However, CV with grid and other common search strategies cannot guarantee to find the model with minimum CV error, which is often the ultimate goal of model selection. Recently, various solution path algorithms have been proposed for several important learning algorithms including support vector classification, Lasso, and so on. However, they still do not guarantee to find the model with minimum CV error.In this paper, we first show that the solution paths produced by various algorithms have the property of piecewise linearity. Then, we prove that a large class of error (or loss) functions are piecewise constant, linear, or quadratic w.r.t. the regularization parameter, based on the solution path. Finally, we propose a new generalized error path algorithm (GEP), and prove that it will find the model with minimum CV error for the entire range of the regularization parameter. The experimental results on a variety of datasets not only confirm our theoretical findings, but also show that the best model with our GEP has better generalization error on the test data, compared to the grid search, manual search, and random search.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/gu15.pdf",
        "supp": "",
        "pdf_size": 365602,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14622796350744847808&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, University of Western Ontario, Canada + School of Computer & Software, Nanjing University of Information Science & Technology, China; Department of Computer Science, University of Western Ontario, Canada",
        "aff_domain": "NUIST.EDU.CN;CSD.UWO.CA",
        "email": "NUIST.EDU.CN;CSD.UWO.CA",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0",
        "aff_unique_norm": "University of Western Ontario;Nanjing University of Information Science & Technology",
        "aff_unique_dep": "Department of Computer Science;School of Computer & Software",
        "aff_unique_url": "https://www.uwo.ca;",
        "aff_unique_abbr": "UWO;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0",
        "aff_country_unique": "Canada;China"
    },
    {
        "id": "43f6168205",
        "title": "A Probabilistic Model for Dirty Multi-task Feature Selection",
        "site": "https://proceedings.mlr.press/v37/hernandez-lobatoa15.html",
        "author": "Daniel Hernandez-Lobato; Jose Miguel Hernandez-Lobato; Zoubin Ghahramani",
        "abstract": "Multi-task feature selection methods often make the hypothesis that learning tasks share relevant and irrelevant features. However, this hypothesis may be too restrictive in practice. For example, there may be a few tasks with specific relevant and irrelevant features (outlier tasks). Similarly, a few of the features may be relevant for only some of the tasks (outlier features). To account for this, we propose a model for multi-task feature selection based on a robust prior distribution that introduces a set of binary latent variables to identify outlier tasks and outlier features. Expectation propagation can be used for efficient approximate inference under the proposed prior. Several experiments show that a model based on the new robust prior provides better predictive performance than other benchmark methods.",
        "bibtex": "@InProceedings{pmlr-v37-hernandez-lobatoa15,\n  title = \t {A Probabilistic Model for Dirty Multi-task Feature Selection},\n  author = \t {Hernandez-Lobato, Daniel and Hernandez-Lobato, Jose Miguel and Ghahramani, Zoubin},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1073--1082},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/hernandez-lobatoa15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/hernandez-lobatoa15.html},\n  abstract = \t {Multi-task feature selection methods often make the hypothesis that learning tasks share relevant and irrelevant features. However, this hypothesis may be too restrictive in practice. For example, there may be a few tasks with specific relevant and irrelevant features (outlier tasks). Similarly, a few of the features may be relevant for only some of the tasks (outlier features). To account for this, we propose a model for multi-task feature selection based on a robust prior distribution that introduces a set of binary latent variables to identify outlier tasks and outlier features. Expectation propagation can be used for efficient approximate inference under the proposed prior. Several experiments show that a model based on the new robust prior provides better predictive performance than other benchmark methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/hernandez-lobatoa15.pdf",
        "supp": "",
        "pdf_size": 985154,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11020577161174306814&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 12,
        "aff": "Universidad Aut\u00f3noma de Madrid, Computer Science Department, Madrid, 28049, Spain; Harvard University, School of Engineering and Applied Sciences, Cambridge, MA 02138, USA; University of Cambridge, Department of Engineering, Cambridge CB2 1PZ, UK",
        "aff_domain": "UAM.ES;SEAS.HARVARD.EDU;ENG.CAM.AC.UK",
        "email": "UAM.ES;SEAS.HARVARD.EDU;ENG.CAM.AC.UK",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Universidad Aut\u00f3noma de Madrid;Harvard University;University of Cambridge",
        "aff_unique_dep": "Computer Science Department;School of Engineering and Applied Sciences;Department of Engineering",
        "aff_unique_url": "https://www.uam.es;https://www.harvard.edu;https://www.cam.ac.uk",
        "aff_unique_abbr": "UAM;Harvard;Cambridge",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Madrid;Cambridge",
        "aff_country_unique_index": "0;1;2",
        "aff_country_unique": "Spain;United States;United Kingdom"
    },
    {
        "id": "fe4876f835",
        "title": "A Provable Generalized Tensor Spectral Method for Uniform Hypergraph Partitioning",
        "site": "https://proceedings.mlr.press/v37/ghoshdastidar15.html",
        "author": "Debarghya Ghoshdastidar; Ambedkar Dukkipati",
        "abstract": "Matrix spectral methods play an important role in statistics and machine learning, and most often the word \u2018matrix\u2019 is dropped as, by default, one assumes that similarities or affinities are measured between two points, thereby resulting in similarity matrices. However, recent challenges in computer vision and text mining have necessitated the use of multi-way affinities in the learning methods, and this has led to a considerable interest in hypergraph partitioning methods in machine learning community. A plethora of \u201chigher-order\u201d algorithms have been proposed in the past decade, but their theoretical guarantees are not well-studied. In this paper, we develop a unified approach for partitioning uniform hypergraphs by means of a tensor trace optimization problem involving the affinity tensor, and a number of existing higher-order methods turn out to be special cases of the proposed formulation. We further propose an algorithm to solve the proposed trace optimization problem, and prove that it is consistent under a planted hypergraph model. We also provide experimental results to validate our theoretical findings.",
        "bibtex": "@InProceedings{pmlr-v37-ghoshdastidar15,\n  title = \t {A Provable Generalized Tensor Spectral Method for Uniform Hypergraph Partitioning},\n  author = \t {Ghoshdastidar, Debarghya and Dukkipati, Ambedkar},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {400--409},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/ghoshdastidar15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/ghoshdastidar15.html},\n  abstract = \t {Matrix spectral methods play an important role in statistics and machine learning, and most often the word \u2018matrix\u2019 is dropped as, by default, one assumes that similarities or affinities are measured between two points, thereby resulting in similarity matrices. However, recent challenges in computer vision and text mining have necessitated the use of multi-way affinities in the learning methods, and this has led to a considerable interest in hypergraph partitioning methods in machine learning community. A plethora of \u201chigher-order\u201d algorithms have been proposed in the past decade, but their theoretical guarantees are not well-studied. In this paper, we develop a unified approach for partitioning uniform hypergraphs by means of a tensor trace optimization problem involving the affinity tensor, and a number of existing higher-order methods turn out to be special cases of the proposed formulation. We further propose an algorithm to solve the proposed trace optimization problem, and prove that it is consistent under a planted hypergraph model. We also provide experimental results to validate our theoretical findings.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/ghoshdastidar15.pdf",
        "supp": "",
        "pdf_size": 486355,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4842269195160953480&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Departiment of Computer Science & Automation, Indian Institute of Science, Bangalore \u2013 560012, India; Departiment of Computer Science & Automation, Indian Institute of Science, Bangalore \u2013 560012, India",
        "aff_domain": "CSA.IISC.ERNET.IN;CSA.IISC.ERNET.IN",
        "email": "CSA.IISC.ERNET.IN;CSA.IISC.ERNET.IN",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indian Institute of Science",
        "aff_unique_dep": "Department of Computer Science & Automation",
        "aff_unique_url": "https://www.iisc.ac.in",
        "aff_unique_abbr": "IISc",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Bangalore",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "f393a5056f",
        "title": "A Relative Exponential Weighing Algorithm for Adversarial Utility-based Dueling Bandits",
        "site": "https://proceedings.mlr.press/v37/gajane15.html",
        "author": "Pratik Gajane; Tanguy Urvoy; Fabrice Cl\u00e9rot",
        "abstract": "We study the K-armed dueling bandit problem which is a variation of the classical Multi-Armed Bandit (MAB) problem in which the learner receives only relative feedback about the selected pairs of arms. We propose a new algorithm called Relative Exponential-weight algorithm for Exploration and Exploitation (REX3) to handle the adversarial utility-based formulation of this problem. This algorithm is a non-trivial extension of the Exponential-weight algorithm for Exploration and Exploitation (EXP3) algorithm. We prove a finite time expected regret upper bound of order O(sqrt(K ln(K)T)) for this algorithm and a general lower bound of order omega(sqrt(KT)). At the end, we provide experimental results using real data from information retrieval applications.",
        "bibtex": "@InProceedings{pmlr-v37-gajane15,\n  title = \t {A Relative Exponential Weighing Algorithm for Adversarial Utility-based Dueling Bandits},\n  author = \t {Gajane, Pratik and Urvoy, Tanguy and Cl\u00e9rot, Fabrice},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {218--227},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/gajane15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/gajane15.html},\n  abstract = \t {We study the K-armed dueling bandit problem which is a variation of the classical Multi-Armed Bandit (MAB) problem in which the learner receives only relative feedback about the selected pairs of arms. We propose a new algorithm called Relative Exponential-weight algorithm for Exploration and Exploitation (REX3) to handle the adversarial utility-based formulation of this problem. This algorithm is a non-trivial extension of the Exponential-weight algorithm for Exploration and Exploitation (EXP3) algorithm. We prove a finite time expected regret upper bound of order O(sqrt(K ln(K)T)) for this algorithm and a general lower bound of order omega(sqrt(KT)). At the end, we provide experimental results using real data from information retrieval applications.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/gajane15.pdf",
        "supp": "",
        "pdf_size": 701652,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8340627090620116739&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Orange-labs, Lannion, France; Orange-labs, Lannion, France; Orange-labs, Lannion, France",
        "aff_domain": "orange.com;orange.com;orange.com",
        "email": "orange.com;orange.com;orange.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Orange Labs",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.orangelabs.fr",
        "aff_unique_abbr": "Orange Labs",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Lannion",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "b46924ab07",
        "title": "A Stochastic PCA and SVD Algorithm with an Exponential Convergence Rate",
        "site": "https://proceedings.mlr.press/v37/shamir15.html",
        "author": "Ohad Shamir",
        "abstract": "We describe and analyze a simple algorithm for principal component analysis and singular value decomposition, VR-PCA, which uses computationally cheap stochastic iterations, yet converges exponentially fast to the optimal solution. In contrast, existing algorithms suffer either from slow convergence, or computationally intensive iterations whose runtime scales with the data size. The algorithm builds on a recent variance-reduced stochastic gradient technique, which was previously analyzed for strongly convex optimization, whereas here we apply it to an inherently non-convex problem, using a very different analysis.",
        "bibtex": "@InProceedings{pmlr-v37-shamir15,\n  title = \t {A Stochastic PCA and SVD Algorithm with an Exponential Convergence Rate},\n  author = \t {Shamir, Ohad},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {144--152},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/shamir15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/shamir15.html},\n  abstract = \t {We describe and analyze a simple algorithm for principal component analysis and singular value decomposition, VR-PCA, which uses computationally cheap stochastic iterations, yet converges exponentially fast to the optimal solution. In contrast, existing algorithms suffer either from slow convergence, or computationally intensive iterations whose runtime scales with the data size. The algorithm builds on a recent variance-reduced stochastic gradient technique, which was previously analyzed for strongly convex optimization, whereas here we apply it to an inherently non-convex problem, using a very different analysis.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/shamir15.pdf",
        "supp": "",
        "pdf_size": 398682,
        "gs_citation": 182,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10371937360009328238&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Weizmann Institute of Science, Rehovot, Israel",
        "aff_domain": "weizmann.ac.il",
        "email": "weizmann.ac.il",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Weizmann Institute of Science",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.weizmann.org.il",
        "aff_unique_abbr": "Weizmann",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Rehovot",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "64213d25d4",
        "title": "A Theoretical Analysis of Metric Hypothesis Transfer Learning",
        "site": "https://proceedings.mlr.press/v37/perrot15.html",
        "author": "Micha\u00ebl Perrot; Amaury Habrard",
        "abstract": "We consider the problem of transferring some a priori knowledge in the context of supervised metric learning approaches. While this setting has been successfully applied in some empirical contexts, no theoretical evidence exists to justify this approach. In this paper, we provide a theoretical justification based on the notion of algorithmic stability adapted to the regularized metric learning setting. We propose an on-average-replace-two-stability model allowing us to prove fast generalization rates when an auxiliary source metric is used to bias the regularizer. Moreover, we prove a consistency result from which we show the interest of considering biased weighted regularized formulations and we provide a solution to estimate the associated weight. We also present some experiments illustrating the interest of the approach in standard metric learning tasks and in a transfer learning problem where few labelled data are available.",
        "bibtex": "@InProceedings{pmlr-v37-perrot15,\n  title = \t {A Theoretical Analysis of Metric Hypothesis Transfer Learning},\n  author = \t {Perrot, Micha\u00ebl and Habrard, Amaury},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1708--1717},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/perrot15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/perrot15.html},\n  abstract = \t {We consider the problem of transferring some a priori knowledge in the context of supervised metric learning approaches. While this setting has been successfully applied in some empirical contexts, no theoretical evidence exists to justify this approach. In this paper, we provide a theoretical justification based on the notion of algorithmic stability adapted to the regularized metric learning setting. We propose an on-average-replace-two-stability model allowing us to prove fast generalization rates when an auxiliary source metric is used to bias the regularizer. Moreover, we prove a consistency result from which we show the interest of considering biased weighted regularized formulations and we provide a solution to estimate the associated weight. We also present some experiments illustrating the interest of the approach in standard metric learning tasks and in a transfer learning problem where few labelled data are available.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/perrot15.pdf",
        "supp": "",
        "pdf_size": 404341,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7299371476045727035&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Universit\u00e9 de Lyon, Universit\u00e9 Jean Monnet de Saint-Etienne, Laboratoire Hubert Curien, CNRS, UMR5516, F-42000, Saint-Etienne, France; Universit\u00e9 de Lyon, Universit\u00e9 Jean Monnet de Saint-Etienne, Laboratoire Hubert Curien, CNRS, UMR5516, F-42000, Saint-Etienne, France",
        "aff_domain": "univ-st-etienne.fr;univ-st-etienne.fr",
        "email": "univ-st-etienne.fr;univ-st-etienne.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Universit\u00e9 de Lyon",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.universitedelyon.fr",
        "aff_unique_abbr": "UDL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "8b7aab5786",
        "title": "A Unified Framework for Outlier-Robust PCA-like Algorithms",
        "site": "https://proceedings.mlr.press/v37/yangc15.html",
        "author": "Wenzhuo Yang; Huan Xu",
        "abstract": "We propose a unified framework for making a wide range of PCA-like algorithms \u2013 including the standard PCA, sparse PCA and non-negative sparse PCA, etc. \u2013 robust when facing a constant fraction of arbitrarily corrupted outliers. Our theoretic analysis establishes solid performance guarantees of the proposed framework: its estimation error is upper bounded by a term depending on the intrinsic parameters of the data model, the selected PCA-like algorithm and the fraction of outliers. Comprehensive experiments on synthetic and real-world datasets demonstrate that the outlier-robust PCA-like algorithms derived from our framework have outstanding performance.",
        "bibtex": "@InProceedings{pmlr-v37-yangc15,\n  title = \t {A Unified Framework for Outlier-Robust PCA-like Algorithms},\n  author = \t {Yang, Wenzhuo and Xu, Huan},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {484--493},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/yangc15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/yangc15.html},\n  abstract = \t {We propose a unified framework for making a wide range of PCA-like algorithms \u2013 including the standard PCA, sparse PCA and non-negative sparse PCA, etc. \u2013 robust when facing a constant fraction of arbitrarily corrupted outliers. Our theoretic analysis establishes solid performance guarantees of the proposed framework: its estimation error is upper bounded by a term depending on the intrinsic parameters of the data model, the selected PCA-like algorithm and the fraction of outliers. Comprehensive experiments on synthetic and real-world datasets demonstrate that the outlier-robust PCA-like algorithms derived from our framework have outstanding performance.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/yangc15.pdf",
        "supp": "",
        "pdf_size": 221324,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5196462864461794100&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Mechanical Engineering, National University of Singapore, Singapore 117576; Department of Mechanical Engineering, National University of Singapore, Singapore 117576",
        "aff_domain": "NUS.EDU.SG;NUS.EDU.SG",
        "email": "NUS.EDU.SG;NUS.EDU.SG",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "National University of Singapore",
        "aff_unique_dep": "Department of Mechanical Engineering",
        "aff_unique_url": "https://www.nus.edu.sg",
        "aff_unique_abbr": "NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "41c7097c3a",
        "title": "A Unifying Framework of Anytime Sparse Gaussian Process Regression Models with Stochastic Variational Inference for Big Data",
        "site": "https://proceedings.mlr.press/v37/hoang15.html",
        "author": "Trong Nghia Hoang; Quang Minh Hoang; Bryan Kian Hsiang Low",
        "abstract": "This paper presents a novel unifying framework of anytime sparse Gaussian process regression (SGPR) models that can produce good predictive performance fast and improve their predictive performance over time. Our proposed unifying framework reverses the variational inference procedure to theoretically construct a non-trivial, concave functional that is maximized at the predictive distribution of any SGPR model of our choice. As a result, a stochastic natural gradient ascent method can be derived that involves iteratively following the stochastic natural gradient of the functional to improve its estimate of the predictive distribution of the chosen SGPR model and is guaranteed to achieve asymptotic convergence to it. Interestingly, we show that if the predictive distribution of the chosen SGPR model satisfies certain decomposability conditions, then the stochastic natural gradient is an unbiased estimator of the exact natural gradient and can be computed in constant time (i.e., independent of data size) at each iteration. We empirically evaluate the trade-off between the predictive performance vs. time efficiency of the anytime SGPR models on two real-world million-sized datasets.",
        "bibtex": "@InProceedings{pmlr-v37-hoang15,\n  title = \t {A Unifying Framework of Anytime Sparse Gaussian Process Regression Models with Stochastic Variational Inference for Big Data},\n  author = \t {Hoang, Trong Nghia and Hoang, Quang Minh and Low, Bryan Kian Hsiang},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {569--578},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/hoang15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/hoang15.html},\n  abstract = \t {This paper presents a novel unifying framework of anytime sparse Gaussian process regression (SGPR) models that can produce good predictive performance fast and improve their predictive performance over time. Our proposed unifying framework reverses the variational inference procedure to theoretically construct a non-trivial, concave functional that is maximized at the predictive distribution of any SGPR model of our choice. As a result, a stochastic natural gradient ascent method can be derived that involves iteratively following the stochastic natural gradient of the functional to improve its estimate of the predictive distribution of the chosen SGPR model and is guaranteed to achieve asymptotic convergence to it. Interestingly, we show that if the predictive distribution of the chosen SGPR model satisfies certain decomposability conditions, then the stochastic natural gradient is an unbiased estimator of the exact natural gradient and can be computed in constant time (i.e., independent of data size) at each iteration. We empirically evaluate the trade-off between the predictive performance vs. time efficiency of the anytime SGPR models on two real-world million-sized datasets.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/hoang15.pdf",
        "supp": "",
        "pdf_size": 2096890,
        "gs_citation": 109,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10142656837354526124&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, National University of Singapore, Republic of Singapore; Department of Computer Science, National University of Singapore, Republic of Singapore; Department of Computer Science, National University of Singapore, Republic of Singapore",
        "aff_domain": "COMP.NUS.EDU.SG;COMP.NUS.EDU.SG;COMP.NUS.EDU.SG",
        "email": "COMP.NUS.EDU.SG;COMP.NUS.EDU.SG;COMP.NUS.EDU.SG",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "National University of Singapore",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.nus.edu.sg",
        "aff_unique_abbr": "NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "a457fba924",
        "title": "A low variance consistent test of relative dependency",
        "site": "https://proceedings.mlr.press/v37/bounliphone15.html",
        "author": "Wacha Bounliphone; Arthur Gretton; Arthur Tenenhaus; Matthew Blaschko",
        "abstract": "We describe a novel non-parametric statistical hypothesis test of relative dependence between a source variable and two candidate target variables. Such a test enables us to determine whether one source variable is significantly more dependent on a first target variable or a second. Dependence is measured via the Hilbert-Schmidt Independence Criterion (HSIC), resulting in a pair of empirical dependence measures (source-target 1, source-target 2). We test whether the first dependence measure is significantly larger than the second. Modeling the covariance between these HSIC statistics leads to a provably more powerful test than the construction of independent HSIC statistics by sub-sampling. The resulting test is consistent and unbiased, and (being based on U-statistics) has favorable convergence properties. The test can be computed in quadratic time, matching the computational complexity of standard empirical HSIC estimators. The effectiveness of the test is demonstrated on several real-world problems: we identify language groups from a multilingual corpus, and we prove that tumor location is more dependent on gene expression than chromosomal imbalances. Source code is available for download at https://github.com/wbounliphone/reldep/.",
        "bibtex": "@InProceedings{pmlr-v37-bounliphone15,\n  title = \t {A low variance consistent test of relative dependency},\n  author = \t {Bounliphone, Wacha and Gretton, Arthur and Tenenhaus, Arthur and Blaschko, Matthew},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {20--29},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/bounliphone15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/bounliphone15.html},\n  abstract = \t {We describe a novel non-parametric statistical hypothesis test of relative dependence between a source variable and two candidate target variables. Such a test enables us to determine whether one source variable is significantly more dependent on a first target variable or a second. Dependence is measured via the Hilbert-Schmidt Independence Criterion (HSIC), resulting in a pair of empirical dependence measures (source-target 1, source-target 2). We test whether the first dependence measure is significantly larger than the second. Modeling the covariance between these HSIC statistics leads to a provably more powerful test than the construction of independent HSIC statistics by sub-sampling. The resulting test is consistent and unbiased, and (being based on U-statistics) has favorable convergence properties. The test can be computed in quadratic time, matching the computational complexity of standard empirical HSIC estimators. The effectiveness of the test is demonstrated on several real-world problems: we identify language groups from a multilingual corpus, and we prove that tumor location is more dependent on gene expression than chromosomal imbalances. Source code is available for download at https://github.com/wbounliphone/reldep/.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/bounliphone15.pdf",
        "supp": "",
        "pdf_size": 1297617,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1251239976382586051&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "CentraleSup\u00e9lec & Inria; Gatsby Computational Neuroscience Unit, University College London; CentraleSup\u00e9lec; Inria & CentraleSup\u00e9lec",
        "aff_domain": "centralesupelec.fr;gmail.com;centralesupelec.fr;inria.fr",
        "email": "centralesupelec.fr;gmail.com;centralesupelec.fr;inria.fr",
        "github": "https://github.com/wbounliphone/reldep",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "CentraleSup\u00e9lec;University College London;INRIA",
        "aff_unique_dep": ";Gatsby Computational Neuroscience Unit;",
        "aff_unique_url": "https://www.centralesupelec.fr;https://www.ucl.ac.uk;https://www.inria.fr",
        "aff_unique_abbr": "CentraleSup\u00e9lec;UCL;Inria",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";London",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "France;United Kingdom"
    },
    {
        "id": "9a6f0ace66",
        "title": "A trust-region method for stochastic variational inference with applications to streaming data",
        "site": "https://proceedings.mlr.press/v37/theis15.html",
        "author": "Lucas Theis; Matt Hoffman",
        "abstract": "Stochastic variational inference allows for fast posterior inference in complex Bayesian models. However, the algorithm is prone to local optima which can make the quality of the posterior approximation sensitive to the choice of hyperparameters and initialization. We address this problem by replacing the natural gradient step of stochastic varitional inference with a trust-region update. We show that this leads to generally better results and reduced sensitivity to hyperparameters. We also describe a new strategy for variational inference on streaming data and show that here our trust-region method is crucial for getting good performance.",
        "bibtex": "@InProceedings{pmlr-v37-theis15,\n  title = \t {A trust-region method for stochastic variational inference with applications to streaming data},\n  author = \t {Theis, Lucas and Hoffman, Matt},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2503--2511},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/theis15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/theis15.html},\n  abstract = \t {Stochastic variational inference allows for fast posterior inference in complex Bayesian models. However, the algorithm is prone to local optima which can make the quality of the posterior approximation sensitive to the choice of hyperparameters and initialization. We address this problem by replacing the natural gradient step of stochastic varitional inference with a trust-region update. We show that this leads to generally better results and reduced sensitivity to hyperparameters. We also describe a new strategy for variational inference on streaming data and show that here our trust-region method is crucial for getting good performance.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/theis15.pdf",
        "supp": "",
        "pdf_size": 572044,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15886904722484176033&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Centre for Integrative Neuroscience, Otfried-M \u00a8uller-Str. 25, BW 72076 Germany; Adobe Research, 601 Townsend St., San Francisco, CA 94103 USA",
        "aff_domain": "BETHGELAB.ORG;ADOBE.COM",
        "email": "BETHGELAB.ORG;ADOBE.COM",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Centre for Integrative Neuroscience;Adobe",
        "aff_unique_dep": "Neuroscience;Adobe Research",
        "aff_unique_url": ";https://research.adobe.com",
        "aff_unique_abbr": ";Adobe",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";San Francisco",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Germany;United States"
    },
    {
        "id": "4178d5fc06",
        "title": "Abstraction Selection in Model-based Reinforcement Learning",
        "site": "https://proceedings.mlr.press/v37/jiang15.html",
        "author": "Nan Jiang; Alex Kulesza; Satinder Singh",
        "abstract": "State abstractions are often used to reduce the complexity of model-based reinforcement learning when only limited quantities of data are available. However, choosing the appropriate level of abstraction is an important problem in practice. Existing approaches have theoretical guarantees only under strong assumptions on the domain or asymptotically large amounts of data, but in this paper we propose a simple algorithm based on statistical hypothesis testing that comes with a finite-sample guarantee under assumptions on candidate abstractions. Our algorithm trades off the low approximation error of finer abstractions against the low estimation error of coarser abstractions, resulting in a loss bound that depends only on the quality of the best available abstraction and is polynomial in planning horizon.",
        "bibtex": "@InProceedings{pmlr-v37-jiang15,\n  title = \t {Abstraction Selection in Model-based Reinforcement Learning},\n  author = \t {Jiang, Nan and Kulesza, Alex and Singh, Satinder},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {179--188},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/jiang15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/jiang15.html},\n  abstract = \t {State abstractions are often used to reduce the complexity of model-based reinforcement learning when only limited quantities of data are available. However, choosing the appropriate level of abstraction is an important problem in practice. Existing approaches have theoretical guarantees only under strong assumptions on the domain or asymptotically large amounts of data, but in this paper we propose a simple algorithm based on statistical hypothesis testing that comes with a finite-sample guarantee under assumptions on candidate abstractions. Our algorithm trades off the low approximation error of finer abstractions against the low estimation error of coarser abstractions, resulting in a loss bound that depends only on the quality of the best available abstraction and is polynomial in planning horizon.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/jiang15.pdf",
        "supp": "",
        "pdf_size": 350671,
        "gs_citation": 93,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3067609878713033778&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Computer Science & Engineering, University of Michigan; Computer Science & Engineering, University of Michigan; Computer Science & Engineering, University of Michigan",
        "aff_domain": "umich.edu;umich.edu;umich.edu",
        "email": "umich.edu;umich.edu;umich.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Computer Science & Engineering",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "dae95b6ba3",
        "title": "Accelerated Online Low Rank Tensor Learning for Multivariate Spatiotemporal Streams",
        "site": "https://proceedings.mlr.press/v37/yua15.html",
        "author": "Rose Yu; Dehua Cheng; Yan Liu",
        "abstract": "Low-rank tensor learning has many applications in machine learning. A series of batch learning algorithms have achieved great successes. However, in many emerging applications, such as climate data analysis, we are confronted with large-scale tensor streams, which poses significant challenges to existing solution in terms of computational costs and limited response time. In this paper, we propose an online accelerated low-rank tensor learning algorithm (ALTO) to solve the problem. At each iteration, we project the current tensor to the subspace of low-rank tensors in order to perform efficient tensor decomposition, then recover the decomposition of the new tensor. By randomly glancing at additional subspaces, we successfully avoid local optima at negligible extra computational cost. We evaluate our method on two tasks in streaming multivariate spatio-temporal analysis: online forecasting and multi-model ensemble, which shows that our method achieves comparable predictive accuracy with significant boost in run time.",
        "bibtex": "@InProceedings{pmlr-v37-yua15,\n  title = \t {Accelerated Online Low Rank Tensor Learning for Multivariate Spatiotemporal Streams},\n  author = \t {Yu, Rose and Cheng, Dehua and Liu, Yan},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {238--247},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/yua15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/yua15.html},\n  abstract = \t {Low-rank tensor learning has many applications in machine learning. A series of batch learning algorithms have achieved great successes. However, in many emerging applications, such as climate data analysis, we are confronted with large-scale tensor streams, which poses significant challenges to existing solution in terms of computational costs and limited response time. In this paper, we propose an online accelerated low-rank tensor learning algorithm (ALTO) to solve the problem. At each iteration, we project the current tensor to the subspace of low-rank tensors in order to perform efficient tensor decomposition, then recover the decomposition of the new tensor. By randomly glancing at additional subspaces, we successfully avoid local optima at negligible extra computational cost. We evaluate our method on two tasks in streaming multivariate spatio-temporal analysis: online forecasting and multi-model ensemble, which shows that our method achieves comparable predictive accuracy with significant boost in run time.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/yua15.pdf",
        "supp": "",
        "pdf_size": 543488,
        "gs_citation": 79,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=75133325030504279&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, University of Southern California; Department of Computer Science, University of Southern California; Department of Computer Science, University of Southern California",
        "aff_domain": "usc.edu;usc.edu;usc.edu",
        "email": "usc.edu;usc.edu;usc.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Southern California",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.usc.edu",
        "aff_unique_abbr": "USC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b23fd83f3f",
        "title": "Active Nearest Neighbors in Changing Environments",
        "site": "https://proceedings.mlr.press/v37/berlind15.html",
        "author": "Christopher Berlind; Ruth Urner",
        "abstract": "While classic machine learning paradigms assume training and test data are generated from the same process, domain adaptation addresses the more realistic setting in which the learner has large quantities of labeled data from some source task but limited or no labeled data from the target task it is attempting to learn. In this work, we give the first formal analysis showing that using active learning for domain adaptation yields a way to address the statistical challenges inherent in this setting. We propose a novel nonparametric algorithm, ANDA, that combines an active nearest neighbor querying strategy with nearest neighbor prediction. We provide analyses of its querying behavior and of finite sample convergence rates of the resulting classifier under covariate shift. Our experiments show that ANDA successfully corrects for dataset bias in multi-class image categorization.",
        "bibtex": "@InProceedings{pmlr-v37-berlind15,\n  title = \t {Active Nearest Neighbors in Changing Environments},\n  author = \t {Berlind, Christopher and Urner, Ruth},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1870--1879},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/berlind15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/berlind15.html},\n  abstract = \t {While classic machine learning paradigms assume training and test data are generated from the same process, domain adaptation addresses the more realistic setting in which the learner has large quantities of labeled data from some source task but limited or no labeled data from the target task it is attempting to learn. In this work, we give the first formal analysis showing that using active learning for domain adaptation yields a way to address the statistical challenges inherent in this setting. We propose a novel nonparametric algorithm, ANDA, that combines an active nearest neighbor querying strategy with nearest neighbor prediction. We provide analyses of its querying behavior and of finite sample convergence rates of the resulting classifier under covariate shift. Our experiments show that ANDA successfully corrects for dataset bias in multi-class image categorization.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/berlind15.pdf",
        "supp": "",
        "pdf_size": 490952,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5772813534569256107&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Georgia Institute of Technology, Atlanta, GA, USA; Max Planck Institute for Intelligent Systems, T\u00fcbingen, Germany",
        "aff_domain": "GATECH.EDU;TUEBINGEN.MPG.DE",
        "email": "GATECH.EDU;TUEBINGEN.MPG.DE",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Georgia Institute of Technology;Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.gatech.edu;https://www.mpi-is.mpg.de",
        "aff_unique_abbr": "Georgia Tech;MPI-IS",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Atlanta;T\u00fcbingen",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;Germany"
    },
    {
        "id": "8a1c492009",
        "title": "Adaptive Belief Propagation",
        "site": "https://proceedings.mlr.press/v37/papachristoudis15.html",
        "author": "Georgios Papachristoudis; John Fisher",
        "abstract": "Graphical models are widely used in inference problems. In practice, one may construct a single large-scale model to explain a phenomenon of interest, which may be utilized in a variety of settings. The latent variables of interest, which can differ in each setting, may only represent a small subset of all variables. The marginals of variables of interest may change after the addition of measurements at different time points. In such adaptive settings, naive algorithms, such as standard belief propagation (BP), may utilize many unnecessary computations by propagating messages over the entire graph. Here, we formulate an efficient inference procedure, termed adaptive BP (AdaBP), suitable for adaptive inference settings. We show that it gives exact results for trees in discrete and Gaussian Markov Random Fields (MRFs), and provide an extension to Gaussian loopy graphs. We also provide extensions on finding the most likely sequence of the entire latent graph. Lastly, we compare the proposed method to standard BP and to that of (Sumer et al., 2011), which tackles the same problem. We show in synthetic and real experiments that it outperforms standard BP by orders of magnitude and explore the settings that it is advantageous over (Sumer et al., 2011).",
        "bibtex": "@InProceedings{pmlr-v37-papachristoudis15,\n  title = \t {Adaptive Belief Propagation},\n  author = \t {Papachristoudis, Georgios and Fisher, John},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {899--907},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/papachristoudis15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/papachristoudis15.html},\n  abstract = \t {Graphical models are widely used in inference problems. In practice, one may construct a single large-scale model to explain a phenomenon of interest, which may be utilized in a variety of settings. The latent variables of interest, which can differ in each setting, may only represent a small subset of all variables. The marginals of variables of interest may change after the addition of measurements at different time points. In such adaptive settings, naive algorithms, such as standard belief propagation (BP), may utilize many unnecessary computations by propagating messages over the entire graph. Here, we formulate an efficient inference procedure, termed adaptive BP (AdaBP), suitable for adaptive inference settings. We show that it gives exact results for trees in discrete and Gaussian Markov Random Fields (MRFs), and provide an extension to Gaussian loopy graphs. We also provide extensions on finding the most likely sequence of the entire latent graph. Lastly, we compare the proposed method to standard BP and to that of (Sumer et al., 2011), which tackles the same problem. We show in synthetic and real experiments that it outperforms standard BP by orders of magnitude and explore the settings that it is advantageous over (Sumer et al., 2011).}\n}",
        "pdf": "http://proceedings.mlr.press/v37/papachristoudis15.pdf",
        "supp": "",
        "pdf_size": 1691968,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10023188335275856029&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "CSAIL, MIT, Cambridge, MA 02139, USA; CSAIL, MIT, Cambridge, MA 02139, USA",
        "aff_domain": "mit.edu;csail.mit.edu",
        "email": "mit.edu;csail.mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.csail.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e6e77d237e",
        "title": "Adaptive Stochastic Alternating Direction Method of Multipliers",
        "site": "https://proceedings.mlr.press/v37/zhaob15.html",
        "author": "Peilin Zhao; Jinwei Yang; Tong Zhang; Ping Li",
        "abstract": "The Alternating Direction Method of Multipliers (ADMM) has been studied for years. Traditional ADMM algorithms need to compute, at each iteration, an (empirical) expected loss function on all training examples, resulting in a computational complexity proportional to the number of training examples. To reduce the complexity, stochastic ADMM algorithms were proposed to replace the expected loss function with a random loss function associated with one uniformly drawn example plus a Bregman divergence term. The Bregman divergence, however, is derived from a simple 2nd-order proximal function, i.e., the half squared norm, which could be a suboptimal choice. In this paper, we present a new family of stochastic ADMM algorithms with optimal 2nd-order proximal functions, which produce a new family of adaptive stochastic ADMM methods. We theoretically prove that the regret bounds are as good as the bounds which could be achieved by the best proximal function that can be chosen in hindsight. Encouraging empirical results on a variety of real-world datasets confirm the effectiveness and efficiency of the proposed algorithms.",
        "bibtex": "@InProceedings{pmlr-v37-zhaob15,\n  title = \t {Adaptive Stochastic Alternating Direction Method of Multipliers},\n  author = \t {Zhao, Peilin and Yang, Jinwei and Zhang, Tong and Li, Ping},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {69--77},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/zhaob15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/zhaob15.html},\n  abstract = \t {The Alternating Direction Method of Multipliers (ADMM) has been studied for years. Traditional ADMM algorithms need to compute, at each iteration, an (empirical) expected loss function on all training examples, resulting in a computational complexity proportional to the number of training examples. To reduce the complexity, stochastic ADMM algorithms were proposed to replace the expected loss function with a random loss function associated with one uniformly drawn example plus a Bregman divergence term. The Bregman divergence, however, is derived from a simple 2nd-order proximal function, i.e., the half squared norm, which could be a suboptimal choice. In this paper, we present a new family of stochastic ADMM algorithms with optimal 2nd-order proximal functions, which produce a new family of adaptive stochastic ADMM methods. We theoretically prove that the regret bounds are as good as the bounds which could be achieved by the best proximal function that can be chosen in hindsight. Encouraging empirical results on a variety of real-world datasets confirm the effectiveness and efficiency of the proposed algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/zhaob15.pdf",
        "supp": "",
        "pdf_size": 230263,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14742274725764926136&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "Data Analytics Department, Institute for Infocomm Research, A*STAR, Singapore+Big Data Lab, Baidu Research, China; Department of Mathematics, Rutgers University, USA+Department of Mathematics, University of Notre Dame, USA; Department of Statistics & Biostatistics, Rutgers University, USA+Big Data Lab, Baidu Research, China; Department of Statistics & Biostatistics, Department of Computer Science, Rutgers University, USA+Baidu Research, USA",
        "aff_domain": "I2R.A-STAR.EDU.SG;ND.EDU;STAT.RUTGERS.EDU;STAT.RUTGERS.EDU",
        "email": "I2R.A-STAR.EDU.SG;ND.EDU;STAT.RUTGERS.EDU;STAT.RUTGERS.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2+3;2+1;2+1",
        "aff_unique_norm": "Institute for Infocomm Research;Baidu;Rutgers University;University of Notre Dame",
        "aff_unique_dep": "Data Analytics Department;Big Data Lab;Department of Mathematics;Department of Mathematics",
        "aff_unique_url": "https://www.i2r.a-star.edu.sg;https://baidu.com;https://www.rutgers.edu;https://www.nd.edu",
        "aff_unique_abbr": "I2R;Baidu;Rutgers;Notre Dame",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;2+2;2+1;2+2",
        "aff_country_unique": "Singapore;China;United States"
    },
    {
        "id": "af4638d101",
        "title": "Adding vs. Averaging in Distributed Primal-Dual Optimization",
        "site": "https://proceedings.mlr.press/v37/mab15.html",
        "author": "Chenxin Ma; Virginia Smith; Martin Jaggi; Michael Jordan; Peter Richtarik; Martin Takac",
        "abstract": "Distributed optimization methods for large-scale machine learning suffer from a communication bottleneck. It is difficult to reduce this bottleneck while still efficiently and accurately aggregating partial work from different machines. In this paper, we present a novel generalization of the recent communication-efficient primal-dual framework (COCOA) for distributed optimization. Our framework, COCOA+, allows for additive combination of local updates to the global parameters at each iteration, whereas previous schemes only allow conservative averaging. We give stronger (primal-dual) convergence rate guarantees for both COCOA as well as our new variants, and generalize the theory for both methods to cover non-smooth convex loss functions. We provide an extensive experimental comparison that shows the markedly improved performance of COCOA+ on several real-world distributed datasets, especially when scaling up the number of machines.",
        "bibtex": "@InProceedings{pmlr-v37-mab15,\n  title = \t {Adding vs. Averaging in Distributed Primal-Dual Optimization},\n  author = \t {Ma, Chenxin and Smith, Virginia and Jaggi, Martin and Jordan, Michael and Richtarik, Peter and Takac, Martin},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1973--1982},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/mab15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/mab15.html},\n  abstract = \t {Distributed optimization methods for large-scale machine learning suffer from a communication bottleneck. It is difficult to reduce this bottleneck while still efficiently and accurately aggregating partial work from different machines. In this paper, we present a novel generalization of the recent communication-efficient primal-dual framework (COCOA) for distributed optimization. Our framework, COCOA+, allows for additive combination of local updates to the global parameters at each iteration, whereas previous schemes only allow conservative averaging. We give stronger (primal-dual) convergence rate guarantees for both COCOA as well as our new variants, and generalize the theory for both methods to cover non-smooth convex loss functions. We provide an extensive experimental comparison that shows the markedly improved performance of COCOA+ on several real-world distributed datasets, especially when scaling up the number of machines.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/mab15.pdf",
        "supp": "",
        "pdf_size": 839993,
        "gs_citation": 214,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3160224954035075803&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "Industrial and Systems Engineering, Lehigh University, USA; University of California, Berkeley, USA; ETH Zurich, Switzerland; University of California, Berkeley, USA; School of Mathematics, University of Edinburgh, UK; Industrial and Systems Engineering, Lehigh University, USA",
        "aff_domain": "LEHIGH.EDU;BERKELEY.EDU;INF.ETHZ.CH;CS.BERKELEY.EDU;ED.AC.UK;GMAIL.COM",
        "email": "LEHIGH.EDU;BERKELEY.EDU;INF.ETHZ.CH;CS.BERKELEY.EDU;ED.AC.UK;GMAIL.COM",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;1;3;0",
        "aff_unique_norm": "Lehigh University;University of California, Berkeley;ETH Zurich;University of Edinburgh",
        "aff_unique_dep": "Industrial and Systems Engineering;;;School of Mathematics",
        "aff_unique_url": "https://www.lehigh.edu;https://www.berkeley.edu;https://www.ethz.ch;https://www.ed.ac.uk",
        "aff_unique_abbr": "Lehigh;UC Berkeley;ETHZ;Edinburgh",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0;0;1;0;2;0",
        "aff_country_unique": "United States;Switzerland;United Kingdom"
    },
    {
        "id": "8db3bb10fe",
        "title": "Algorithms for the Hard Pre-Image Problem of String Kernels and the General Problem of String Prediction",
        "site": "https://proceedings.mlr.press/v37/giguere15.html",
        "author": "S\u00e9bastien Gigu\u00e8re; Am\u00e9lie Rolland; Francois Laviolette; Mario Marchand",
        "abstract": "We address the pre-image problem encountered in structured output prediction and the one of finding a string maximizing the prediction function of various kernel-based classifiers and regressors. We demonstrate that these problems reduce to a common combinatorial problem valid for many string kernels. For this problem, we propose an upper bound on the prediction function which has low computational complexity and which can be used in a branch and bound search algorithm to obtain optimal solutions. We also show that for many string kernels, the complexity of the problem increases significantly when the kernel is normalized. On the optical word recognition task, the exact solution of the pre-image problem is shown to significantly improve the prediction accuracy in comparison with an approximation found by the best known heuristic. On the task of finding a string maximizing the prediction function of kernel-based classifiers and regressors, we highlight that existing methods can be biased toward long strings that contain many repeated symbols. We demonstrate that this bias is removed when using normalized kernels. Finally, we present results for the discovery of lead compounds in drug discovery. The source code can be found at https://github.com/a-ro/preimage",
        "bibtex": "@InProceedings{pmlr-v37-giguere15,\n  title = \t {Algorithms for the Hard Pre-Image Problem of String Kernels and the General Problem of String Prediction},\n  author = \t {Gigu\u00e8re, S\u00e9bastien and Rolland, Am\u00e9lie and Laviolette, Francois and Marchand, Mario},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2021--2029},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/giguere15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/giguere15.html},\n  abstract = \t {We address the pre-image problem encountered in structured output prediction and the one of finding a string maximizing the prediction function of various kernel-based classifiers and regressors. We demonstrate that these problems reduce to a common combinatorial problem valid for many string kernels. For this problem, we propose an upper bound on the prediction function which has low computational complexity and which can be used in a branch and bound search algorithm to obtain optimal solutions. We also show that for many string kernels, the complexity of the problem increases significantly when the kernel is normalized. On the optical word recognition task, the exact solution of the pre-image problem is shown to significantly improve the prediction accuracy in comparison with an approximation found by the best known heuristic. On the task of finding a string maximizing the prediction function of kernel-based classifiers and regressors, we highlight that existing methods can be biased toward long strings that contain many repeated symbols. We demonstrate that this bias is removed when using normalized kernels. Finally, we present results for the discovery of lead compounds in drug discovery. The source code can be found at https://github.com/a-ro/preimage}\n}",
        "pdf": "http://proceedings.mlr.press/v37/giguere15.pdf",
        "supp": "",
        "pdf_size": 403519,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5319956633845185281&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Institute for Research in Immunology and Cancer, University of Montreal, Montreal, Canada + Department of Computer Science and Software Engineering, Laval University, Quebec, Canada; Department of Computer Science and Software Engineering, Laval University, Quebec, Canada; Department of Computer Science and Software Engineering, Laval University, Quebec, Canada; Department of Computer Science and Software Engineering, Laval University, Quebec, Canada",
        "aff_domain": "GMAIL.COM;ULAVAL.CA;IFT.ULAVAL.CA;IFT.ULAVAL.CA",
        "email": "GMAIL.COM;ULAVAL.CA;IFT.ULAVAL.CA;IFT.ULAVAL.CA",
        "github": "https://github.com/a-ro/preimage",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;1;1",
        "aff_unique_norm": "University of Montreal;Laval University",
        "aff_unique_dep": "Institute for Research in Immunology and Cancer;Department of Computer Science and Software Engineering",
        "aff_unique_url": "https://www.mcgill.ca;https://www.laval.ca",
        "aff_unique_abbr": "UM;Laval",
        "aff_campus_unique_index": "0+1;1;1;1",
        "aff_campus_unique": "Montreal;Quebec",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "9c6ec4821d",
        "title": "Alpha-Beta Divergences Discover Micro and Macro Structures in Data",
        "site": "https://proceedings.mlr.press/v37/narayan15.html",
        "author": "Karthik Narayan; Ali Punjani; Pieter Abbeel",
        "abstract": "Although recent work in non-linear dimensionality reduction investigates multiple choices of divergence measure during optimization\u00a0\\citeyang2013icml,bunte2012neuro, little work discusses the direct effects that divergence measures have on visualization. We study this relationship, theoretically and through an empirical analysis over 10 datasets. Our works shows how the \u03b1and \u03b2parameters of the generalized alpha-beta divergence can be chosen to discover hidden macro-structures (categories, e.g. birds) or micro-structures (fine-grained classes, e.g. toucans). Our method, which generalizes t-SNE\u00a0\\citetsne, allows us to discover such structure without extensive grid searches over (\u03b1, \u03b2) due to our theoretical analysis: such structure is apparent with particular choices of (\u03b1, \u03b2) that generalize across datasets. We also discuss efficient parallel CPU and GPU schemes which are non-trivial due to the tree-structures employed in optimization and the large datasets that do not fully fit into GPU memory. Our method runs 20x faster than the fastest published code\u00a0\\citefmm. We conclude with detailed case studies on the following very large datasets: ILSVRC 2012, a standard computer vision dataset with 1.2M images; SUSY, a particle physics dataset with 5M instances; and HIGGS, another particle physics dataset with 11M instances. This represents the largest published visualization attained by SNE methods. We have open-sourced our visualization code: \\texttthttp://rll.berkeley.edu/absne/.",
        "bibtex": "@InProceedings{pmlr-v37-narayan15,\n  title = \t {Alpha-Beta Divergences Discover Micro and Macro Structures in Data},\n  author = \t {Narayan, Karthik and Punjani, Ali and Abbeel, Pieter},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {796--804},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/narayan15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/narayan15.html},\n  abstract = \t {Although recent work in non-linear dimensionality reduction investigates multiple choices of divergence measure during optimization\u00a0\\citeyang2013icml,bunte2012neuro, little work discusses the direct effects that divergence measures have on visualization. We study this relationship, theoretically and through an empirical analysis over 10 datasets. Our works shows how the \u03b1and \u03b2parameters of the generalized alpha-beta divergence can be chosen to discover hidden macro-structures (categories, e.g. birds) or micro-structures (fine-grained classes, e.g. toucans). Our method, which generalizes t-SNE\u00a0\\citetsne, allows us to discover such structure without extensive grid searches over (\u03b1, \u03b2) due to our theoretical analysis: such structure is apparent with particular choices of (\u03b1, \u03b2) that generalize across datasets. We also discuss efficient parallel CPU and GPU schemes which are non-trivial due to the tree-structures employed in optimization and the large datasets that do not fully fit into GPU memory. Our method runs 20x faster than the fastest published code\u00a0\\citefmm. We conclude with detailed case studies on the following very large datasets: ILSVRC 2012, a standard computer vision dataset with 1.2M images; SUSY, a particle physics dataset with 5M instances; and HIGGS, another particle physics dataset with 11M instances. This represents the largest published visualization attained by SNE methods. We have open-sourced our visualization code: \\texttthttp://rll.berkeley.edu/absne/.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/narayan15.pdf",
        "supp": "",
        "pdf_size": 9879819,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7150950058084998461&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of California, Berkeley, CA, 94720, USA; University of Toronto, ON M5S, CANADA; University of California, Berkeley, CA, 94720, USA",
        "aff_domain": "berkeley.edu;cs.toronto.edu;cs.berkeley.edu",
        "email": "berkeley.edu;cs.toronto.edu;cs.berkeley.edu",
        "github": "",
        "project": "http://rll.berkeley.edu/absne/",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of California, Berkeley;University of Toronto",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.berkeley.edu;https://www.utoronto.ca",
        "aff_unique_abbr": "UC Berkeley;U of T",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Berkeley;Toronto",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "d60f347dbe",
        "title": "An Aligned Subtree Kernel for Weighted Graphs",
        "site": "https://proceedings.mlr.press/v37/bai15.html",
        "author": "Lu Bai; Luca Rossi; Zhihong Zhang; Edwin Hancock",
        "abstract": "In this paper, we develop a new entropic matching kernel for weighted graphs by aligning depth-based representations. We demonstrate that this kernel can be seen as an \\textbfaligned subtree kernel that incorporates explicit subtree correspondences, and thus addresses the drawback of neglecting the relative locations between substructures that arises in the R-convolution kernels. Experiments on standard datasets demonstrate that our kernel can easily outperform state-of-the-art graph kernels in terms of classification accuracy.",
        "bibtex": "@InProceedings{pmlr-v37-bai15,\n  title = \t {An Aligned Subtree Kernel for Weighted Graphs},\n  author = \t {Bai, Lu and Rossi, Luca and Zhang, Zhihong and Hancock, Edwin},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {30--39},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/bai15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/bai15.html},\n  abstract = \t {In this paper, we develop a new entropic matching kernel for weighted graphs by aligning depth-based representations. We demonstrate that this kernel can be seen as an \\textbfaligned subtree kernel that incorporates explicit subtree correspondences, and thus addresses the drawback of neglecting the relative locations between substructures that arises in the R-convolution kernels. Experiments on standard datasets demonstrate that our kernel can easily outperform state-of-the-art graph kernels in terms of classification accuracy.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/bai15.pdf",
        "supp": "",
        "pdf_size": 332606,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4590038409286783575&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "School of Information, Central University of Finance and Economics, Beijing, China; School of Computer Science, University of Birmingham, Birmingham, UK; Software School, Xiamen University, Xiamen, Fujian, China; Department of Computer Science, University of York, York, UK",
        "aff_domain": "HOTMAIL.COM;CS.BHAM.AC.UK;XMU.EDU.CN;CS.YORK.AC.UK",
        "email": "HOTMAIL.COM;CS.BHAM.AC.UK;XMU.EDU.CN;CS.YORK.AC.UK",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "Central University of Finance and Economics;University of Birmingham;Xiamen University;University of York",
        "aff_unique_dep": "School of Information;School of Computer Science;Software School;Department of Computer Science",
        "aff_unique_url": "http://www.cufe.edu.cn;https://www.birmingham.ac.uk;https://www.xmu.edu.cn;https://www.york.ac.uk",
        "aff_unique_abbr": "CUFE;UoB;XMU;York",
        "aff_campus_unique_index": "0;1;2;3",
        "aff_campus_unique": "Beijing;Birmingham;Xiamen;York",
        "aff_country_unique_index": "0;1;0;1",
        "aff_country_unique": "China;United Kingdom"
    },
    {
        "id": "8ae3f1bbd6",
        "title": "An Asynchronous Distributed Proximal Gradient Method for Composite Convex Optimization",
        "site": "https://proceedings.mlr.press/v37/aybat15.html",
        "author": "Necdet Aybat; Zi Wang; Garud Iyengar",
        "abstract": "We propose a distributed first-order augmented Lagrangian (DFAL) algorithm to minimize the sum of composite convex functions, where each term in the sum is a private cost function belonging to a node, and only nodes connected by an edge can directly communicate with each other. This optimization model abstracts a number of applications in distributed sensing and machine learning. We show that any limit point of DFAL iterates is optimal; and for any eps > 0, an eps-optimal and eps-feasible solution can be computed within O(log(1/eps)) DFAL iterations, which require O(\\psi_\\textmax^1.5/d_\\textmin \u22c51/\u03b5) proximal gradient computations and communications per node in total, where \\psi_\\textmax denotes the largest eigenvalue of the graph Laplacian, and d_\\textmin is the minimum degree of the graph. We also propose an asynchronous version of DFAL by incorporating randomized block coordinate descent methods; and demonstrate the efficiency of DFAL on large scale sparse-group LASSO problems.",
        "bibtex": "@InProceedings{pmlr-v37-aybat15,\n  title = \t {An Asynchronous Distributed Proximal Gradient Method for Composite Convex Optimization},\n  author = \t {Aybat, Necdet and Wang, Zi and Iyengar, Garud},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2454--2462},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/aybat15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/aybat15.html},\n  abstract = \t {We propose a distributed first-order augmented Lagrangian (DFAL) algorithm to minimize the sum of composite convex functions, where each term in the sum is a private cost function belonging to a node, and only nodes connected by an edge can directly communicate with each other. This optimization model abstracts a number of applications in distributed sensing and machine learning. We show that any limit point of DFAL iterates is optimal; and for any eps > 0, an eps-optimal and eps-feasible solution can be computed within O(log(1/eps)) DFAL iterations, which require O(\\psi_\\textmax^1.5/d_\\textmin \u22c51/\u03b5) proximal gradient computations and communications per node in total, where \\psi_\\textmax denotes the largest eigenvalue of the graph Laplacian, and d_\\textmin is the minimum degree of the graph. We also propose an asynchronous version of DFAL by incorporating randomized block coordinate descent methods; and demonstrate the efficiency of DFAL on large scale sparse-group LASSO problems.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/aybat15.pdf",
        "supp": "",
        "pdf_size": 216771,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6982291008236256086&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Penn State University; Penn State University; Columbia University",
        "aff_domain": "PSU.EDU;PSU.EDU;IEOR.COLUMBIA.EDU",
        "email": "PSU.EDU;PSU.EDU;IEOR.COLUMBIA.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Penn State University;Columbia University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.psu.edu;https://www.columbia.edu",
        "aff_unique_abbr": "PSU;Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a94912bb35",
        "title": "An Empirical Exploration of Recurrent Network Architectures",
        "site": "https://proceedings.mlr.press/v37/jozefowicz15.html",
        "author": "Rafal Jozefowicz; Wojciech Zaremba; Ilya Sutskever",
        "abstract": "The Recurrent Neural Network (RNN) is an extremely powerful sequence model that is often difficult to train. The Long Short-Term Memory (LSTM) is a specific RNN architecture whose design makes it much easier to train. While wildly successful in practice, the LSTM\u2019s architecture appears to be ad-hoc so it is not clear if it is optimal, and the significance of its individual components is unclear. In this work, we aim to determine whether the LSTM architecture is optimal or whether much better architectures exist. We conducted a thorough architecture search where we evaluated over ten thousand different RNN architectures, and identified an architecture that outperforms both the LSTM and the recently-introduced Gated Recurrent Unit (GRU) on some but not all tasks. We found that adding a bias of 1 to the LSTM\u2019s forget gate closes the gap between the LSTM and the GRU.",
        "bibtex": "@InProceedings{pmlr-v37-jozefowicz15,\n  title = \t {An Empirical Exploration of Recurrent Network Architectures},\n  author = \t {Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2342--2350},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/jozefowicz15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/jozefowicz15.html},\n  abstract = \t {The Recurrent Neural Network (RNN) is an extremely powerful sequence model that is often difficult to train. The Long Short-Term Memory (LSTM) is a specific RNN architecture whose design makes it much easier to train. While wildly successful in practice, the LSTM\u2019s architecture appears to be ad-hoc so it is not clear if it is optimal, and the significance of its individual components is unclear. In this work, we aim to determine whether the LSTM architecture is optimal or whether much better architectures exist. We conducted a thorough architecture search where we evaluated over ten thousand different RNN architectures, and identified an architecture that outperforms both the LSTM and the recently-introduced Gated Recurrent Unit (GRU) on some but not all tasks. We found that adding a bias of 1 to the LSTM\u2019s forget gate closes the gap between the LSTM and the GRU.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/jozefowicz15.pdf",
        "supp": "",
        "pdf_size": 217041,
        "gs_citation": 2562,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3565461567464548201&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Google Inc.; New York University + Facebook; Google Inc.",
        "aff_domain": "GOOGLE.COM;GMAIL.COM;GOOGLE.COM",
        "email": "GOOGLE.COM;GMAIL.COM;GOOGLE.COM",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2;0",
        "aff_unique_norm": "Google;New York University;Meta",
        "aff_unique_dep": "Google;;Facebook, Inc.",
        "aff_unique_url": "https://www.google.com;https://www.nyu.edu;https://www.facebook.com",
        "aff_unique_abbr": "Google;NYU;FB",
        "aff_campus_unique_index": "0;;0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6d38ce740e",
        "title": "An Empirical Study of Stochastic Variational Inference Algorithms for the Beta Bernoulli Process",
        "site": "https://proceedings.mlr.press/v37/shahb15.html",
        "author": "Amar Shah; David Knowles; Zoubin Ghahramani",
        "abstract": "Stochastic variational inference (SVI) is emerging as the most promising candidate for scaling inference in Bayesian probabilistic models to large datasets. However, the performance of these methods has been assessed primarily in the context of Bayesian topic models, particularly latent Dirichlet allocation (LDA). Deriving several new algorithms, and using synthetic, image and genomic datasets, we investigate whether the understanding gleaned from LDA applies in the setting of sparse latent factor models, specifically beta process factor analysis (BPFA). We demonstrate that the big picture is consistent: using Gibbs sampling within SVI to maintain certain posterior dependencies is extremely effective. However, we also show that different posterior dependencies are important in BPFA relative to LDA.",
        "bibtex": "@InProceedings{pmlr-v37-shahb15,\n  title = \t {An Empirical Study of Stochastic Variational Inference Algorithms for the Beta Bernoulli Process},\n  author = \t {Shah, Amar and Knowles, David and Ghahramani, Zoubin},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1594--1603},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/shahb15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/shahb15.html},\n  abstract = \t {Stochastic variational inference (SVI) is emerging as the most promising candidate for scaling inference in Bayesian probabilistic models to large datasets. However, the performance of these methods has been assessed primarily in the context of Bayesian topic models, particularly latent Dirichlet allocation (LDA). Deriving several new algorithms, and using synthetic, image and genomic datasets, we investigate whether the understanding gleaned from LDA applies in the setting of sparse latent factor models, specifically beta process factor analysis (BPFA). We demonstrate that the big picture is consistent: using Gibbs sampling within SVI to maintain certain posterior dependencies is extremely effective. However, we also show that different posterior dependencies are important in BPFA relative to LDA.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/shahb15.pdf",
        "supp": "",
        "pdf_size": 1524161,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10304523681008421657&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "University of Cambridge, Department of Engineering, Cambridge, UK; Stanford University, Department of Computer Science, Stanford, CA, USA; University of Cambridge, Department of Engineering, Cambridge, UK",
        "aff_domain": "CAM.AC.UK;CS.STANFORD.EDU;ENG.CAM.AC.UK",
        "email": "CAM.AC.UK;CS.STANFORD.EDU;ENG.CAM.AC.UK",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Cambridge;Stanford University",
        "aff_unique_dep": "Department of Engineering;Department of Computer Science",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.stanford.edu",
        "aff_unique_abbr": "Cambridge;Stanford",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Cambridge;Stanford",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "3153655850",
        "title": "An Explicit Sampling Dependent Spectral Error Bound for Column Subset Selection",
        "site": "https://proceedings.mlr.press/v37/yanga15.html",
        "author": "Tianbao Yang; Lijun Zhang; Rong Jin; Shenghuo Zhu",
        "abstract": "In this paper, we consider the problem of column subset selection. We present a novel analysis of the spectral norm reconstruction for a simple randomized algorithm and establish a new bound that depends explicitly on the sampling probabilities. The sampling dependent error bound (i) allows us to better understand the tradeoff in the reconstruction error due to sampling probabilities, (ii) exhibits more insights than existing error bounds that exploit specific probability distributions, and (iii) implies better sampling distributions. In particular, we show that a sampling distribution with probabilities proportional to the square root of the statistical leverage scores is better than uniform sampling, and is better than leverage-based sampling when the statistical leverage scores are very nonuniform. And by solving a constrained optimization problem related to the error bound with an efficient bisection search we are able to achieve better performance than using either the leverage-based distribution or that proportional to the square root of the statistical leverage scores. Numerical simulations demonstrate the benefits of the new sampling distributions for low-rank matrix approximation and least square approximation compared to state-of-the art algorithms.",
        "bibtex": "@InProceedings{pmlr-v37-yanga15,\n  title = \t {An Explicit Sampling Dependent Spectral Error Bound for Column Subset Selection},\n  author = \t {Yang, Tianbao and Zhang, Lijun and Jin, Rong and Zhu, Shenghuo},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {135--143},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/yanga15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/yanga15.html},\n  abstract = \t {In this paper, we consider the problem of column subset selection. We present a novel analysis of the spectral norm reconstruction for a simple randomized algorithm and establish a new bound that depends explicitly on the sampling probabilities. The sampling dependent error bound (i) allows us to better understand the tradeoff in the reconstruction error due to sampling probabilities, (ii) exhibits more insights than existing error bounds that exploit specific probability distributions, and (iii) implies better sampling distributions. In particular, we show that a sampling distribution with probabilities proportional to the square root of the statistical leverage scores is better than uniform sampling, and is better than leverage-based sampling when the statistical leverage scores are very nonuniform. And by solving a constrained optimization problem related to the error bound with an efficient bisection search we are able to achieve better performance than using either the leverage-based distribution or that proportional to the square root of the statistical leverage scores. Numerical simulations demonstrate the benefits of the new sampling distributions for low-rank matrix approximation and least square approximation compared to state-of-the art algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/yanga15.pdf",
        "supp": "",
        "pdf_size": 472418,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5445029578225571189&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computer Science, the University of Iowa, Iowa City, USA; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Department of Computer Science and Engineering, Michigan State University, East Lansing, USA+Institute of Data Science and Technologies at Alibaba Group, Seattle, USA; Institute of Data Science and Technologies at Alibaba Group, Seattle, USA",
        "aff_domain": "uiowa.edu;lamda.nju.edu.cn;cse.msu.edu;gmail.com",
        "email": "uiowa.edu;lamda.nju.edu.cn;cse.msu.edu;gmail.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2+3;3",
        "aff_unique_norm": "University of Iowa;Nanjing University;Michigan State University;Alibaba Group",
        "aff_unique_dep": "Department of Computer Science;National Key Laboratory for Novel Software Technology;Department of Computer Science and Engineering;Institute of Data Science and Technologies",
        "aff_unique_url": "https://www.uiowa.edu;http://www.nju.edu.cn;https://www.msu.edu;https://www.alibaba.com",
        "aff_unique_abbr": "UIowa;Nanjing U;MSU;Alibaba",
        "aff_campus_unique_index": "0;1;2+3;3",
        "aff_campus_unique": "Iowa City;Nanjing;East Lansing;Seattle",
        "aff_country_unique_index": "0;1;0+0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "902e18eeb1",
        "title": "An Online Learning Algorithm for Bilinear Models",
        "site": "https://proceedings.mlr.press/v37/wua15.html",
        "author": "Yuanbin Wu; Shiliang Sun",
        "abstract": "We investigate the bilinear model, which is a matrix form linear model with the rank 1 constraint. A new online learning algorithm is proposed to train the model parameters. Our algorithm runs in the manner of online mirror descent, and gradients are computed by the power iteration. To analyze it, we give a new second order approximation of the squared spectral norm, which helps us to get a regret bound. Experiments on two sequential labelling tasks give positive results.",
        "bibtex": "@InProceedings{pmlr-v37-wua15,\n  title = \t {An Online Learning Algorithm for Bilinear Models},\n  author = \t {Wu, Yuanbin and Sun, Shiliang},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {890--898},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/wua15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/wua15.html},\n  abstract = \t {We investigate the bilinear model, which is a matrix form linear model with the rank 1 constraint. A new online learning algorithm is proposed to train the model parameters. Our algorithm runs in the manner of online mirror descent, and gradients are computed by the power iteration. To analyze it, we give a new second order approximation of the squared spectral norm, which helps us to get a regret bound. Experiments on two sequential labelling tasks give positive results.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/wua15.pdf",
        "supp": "",
        "pdf_size": 271715,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=975354236206491759&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Shanghai Key Laboratory of Multidimensional Information Processing + Department of Computer Science and Technology, East China Normal University; Shanghai Key Laboratory of Multidimensional Information Processing + Department of Computer Science and Technology, East China Normal University",
        "aff_domain": "CS.ECNU.EDU.CN;CS.ECNU.EDU.CN",
        "email": "CS.ECNU.EDU.CN;CS.ECNU.EDU.CN",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Shanghai Key Laboratory of Multidimensional Information Processing;East China Normal University",
        "aff_unique_dep": "Multidimensional Information Processing;Department of Computer Science and Technology",
        "aff_unique_url": ";http://www.ecnu.edu.cn",
        "aff_unique_abbr": ";ECNU",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "04d1364f3f",
        "title": "An embarrassingly simple approach to zero-shot learning",
        "site": "https://proceedings.mlr.press/v37/romera-paredes15.html",
        "author": "Bernardino Romera-Paredes; Philip Torr",
        "abstract": "Zero-shot learning consists in learning how to recognize new concepts by just having a description of them. Many sophisticated approaches have been proposed to address the challenges this problem comprises. In this paper we describe a zero-shot learning approach that can be implemented in just one line of code, yet it is able to outperform state of the art approaches on standard datasets. The approach is based on a more general framework which models the relationships between features, attributes, and classes as a two linear layers network, where the weights of the top layer are not learned but are given by the environment. We further provide a learning bound on the generalization error of this kind of approaches, by casting them as domain adaptation methods. In experiments carried out on three standard real datasets, we found that our approach is able to perform significantly better than the state of art on all of them, obtaining a ratio of improvement up to 17%.",
        "bibtex": "@InProceedings{pmlr-v37-romera-paredes15,\n  title = \t {An embarrassingly simple approach to zero-shot learning},\n  author = \t {Romera-Paredes, Bernardino and Torr, Philip},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2152--2161},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/romera-paredes15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/romera-paredes15.html},\n  abstract = \t {Zero-shot learning consists in learning how to recognize new concepts by just having a description of them. Many sophisticated approaches have been proposed to address the challenges this problem comprises. In this paper we describe a zero-shot learning approach that can be implemented in just one line of code, yet it is able to outperform state of the art approaches on standard datasets. The approach is based on a more general framework which models the relationships between features, attributes, and classes as a two linear layers network, where the weights of the top layer are not learned but are given by the environment. We further provide a learning bound on the generalization error of this kind of approaches, by casting them as domain adaptation methods. In experiments carried out on three standard real datasets, we found that our approach is able to perform significantly better than the state of art on all of them, obtaining a ratio of improvement up to 17%.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/romera-paredes15.pdf",
        "supp": "",
        "pdf_size": 640175,
        "gs_citation": 1617,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6698468073175201896&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "University of Oxford, Department of Engineering Science, Parks Road, Oxford, OX1 3PJ, UK; University of Oxford, Department of Engineering Science, Parks Road, Oxford, OX1 3PJ, UK",
        "aff_domain": "ENG.OX.AC.UK;ENG.OX.AC.UK",
        "email": "ENG.OX.AC.UK;ENG.OX.AC.UK",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Oxford",
        "aff_unique_dep": "Department of Engineering Science",
        "aff_unique_url": "https://www.ox.ac.uk",
        "aff_unique_abbr": "Oxford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Oxford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "6245ee8087",
        "title": "Approval Voting and Incentives in Crowdsourcing",
        "site": "https://proceedings.mlr.press/v37/shaha15.html",
        "author": "Nihar Shah; Dengyong Zhou; Yuval Peres",
        "abstract": "The growing need for labeled training data has made crowdsourcing an important part of machine learning. The quality of crowdsourced labels is, however, adversely affected by three factors: (1) the workers are not experts; (2) the incentives of the workers are not aligned with those of the requesters; and (3) the interface does not allow workers to convey their knowledge accurately, by forcing them to make a single choice among a set of options. In this paper, we address these issues by introducing approval voting to utilize the expertise of workers who have partial knowledge of the true answer, and coupling it with a (\"strictly proper\") incentive-compatible compensation mechanism. We show rigorous theoretical guarantees of optimality of our mechanism together with a simple axiomatic characterization. We also conduct preliminary empirical studies on Amazon Mechanical Turk which validate our approach.",
        "bibtex": "@InProceedings{pmlr-v37-shaha15,\n  title = \t {Approval Voting and Incentives in Crowdsourcing},\n  author = \t {Shah, Nihar and Zhou, Dengyong and Peres, Yuval},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {10--19},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/shaha15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/shaha15.html},\n  abstract = \t {The growing need for labeled training data has made crowdsourcing an important part of machine learning. The quality of crowdsourced labels is, however, adversely affected by three factors: (1) the workers are not experts; (2) the incentives of the workers are not aligned with those of the requesters; and (3) the interface does not allow workers to convey their knowledge accurately, by forcing them to make a single choice among a set of options. In this paper, we address these issues by introducing approval voting to utilize the expertise of workers who have partial knowledge of the true answer, and coupling it with a (\"strictly proper\") incentive-compatible compensation mechanism. We show rigorous theoretical guarantees of optimality of our mechanism together with a simple axiomatic characterization. We also conduct preliminary empirical studies on Amazon Mechanical Turk which validate our approach.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/shaha15.pdf",
        "supp": "",
        "pdf_size": 625514,
        "gs_citation": 90,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3058109368412164451&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "University of California, Berkeley, CA 94720; Microsoft Research, Redmond, WA 98052; Microsoft Research, Redmond, WA 98052",
        "aff_domain": "eecs.berkeley.edu;microsoft.com;microsoft.com",
        "email": "eecs.berkeley.edu;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of California, Berkeley;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.berkeley.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "UC Berkeley;MSR",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Berkeley;Redmond",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d26d4951ba",
        "title": "Approximate Dynamic Programming for Two-Player Zero-Sum Markov Games",
        "site": "https://proceedings.mlr.press/v37/perolat15.html",
        "author": "Julien Perolat; Bruno Scherrer; Bilal Piot; Olivier Pietquin",
        "abstract": "This paper provides an analysis of error propagation in Approximate Dynamic Programming applied to zero-sum two-player Stochastic Games. We provide a novel and unified error propagation analysis in L_p-norm of three well-known algorithms adapted to Stochastic Games (namely Approximate Value Iteration, Approximate Policy Iteration and Approximate Generalized Policy Iteration). We show that we can achieve a stationary policy which is \\frac2\u03b3(1 - \u03b3)^2 \u03b5+ \\frac1(1 - \u03b3)^2\u03b5\u2019-optimal, where \u03b5is the value function approximation error and \u03b5\u2019 is the approximate greedy operator error. In addition, we provide a practical algorithm (AGPI-Q) to solve infinite horizon \u03b3-discounted two-player zero-sum stochastic games in a batch setting. It is an extension of the Fitted-Q algorithm (which solves Markov Decisions Processes in a batch setting) and can be non-parametric. Finally, we demonstrate experimentally the performance of AGPI-Q on a simultaneous two-player game, namely Alesia.",
        "bibtex": "@InProceedings{pmlr-v37-perolat15,\n  title = \t {Approximate Dynamic Programming for Two-Player Zero-Sum Markov Games},\n  author = \t {Perolat, Julien and Scherrer, Bruno and Piot, Bilal and Pietquin, Olivier},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1321--1329},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/perolat15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/perolat15.html},\n  abstract = \t {This paper provides an analysis of error propagation in Approximate Dynamic Programming applied to zero-sum two-player Stochastic Games. We provide a novel and unified error propagation analysis in L_p-norm of three well-known algorithms adapted to Stochastic Games (namely Approximate Value Iteration, Approximate Policy Iteration and Approximate Generalized Policy Iteration). We show that we can achieve a stationary policy which is \\frac2\u03b3(1 - \u03b3)^2 \u03b5+ \\frac1(1 - \u03b3)^2\u03b5\u2019-optimal, where \u03b5is the value function approximation error and \u03b5\u2019 is the approximate greedy operator error. In addition, we provide a practical algorithm (AGPI-Q) to solve infinite horizon \u03b3-discounted two-player zero-sum stochastic games in a batch setting. It is an extension of the Fitted-Q algorithm (which solves Markov Decisions Processes in a batch setting) and can be non-parametric. Finally, we demonstrate experimentally the performance of AGPI-Q on a simultaneous two-player game, namely Alesia.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/perolat15.pdf",
        "supp": "",
        "pdf_size": 342502,
        "gs_citation": 140,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7808432401217300211&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Univ. Lille, CRIStAL, SequeL team, France+Institut Universitaire de France (IUF), France; Inria, Villers-l `es-Nancy, F-54600, France; Univ. Lille, CRIStAL, SequeL team, France; Univ. Lille, CRIStAL, SequeL team, France+Institut Universitaire de France (IUF), France",
        "aff_domain": "ed.univ-lille1.fr;inria.fr;univ-lille3.fr;univ-lille1.fr",
        "email": "ed.univ-lille1.fr;inria.fr;univ-lille3.fr;univ-lille1.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;0;0+1",
        "aff_unique_norm": "University of Lille;Institut Universitaire de France;INRIA",
        "aff_unique_dep": "CRIStAL;;",
        "aff_unique_url": "https://www.univ-lille.fr;https://www.iuf.cnrs.fr;https://www.inria.fr",
        "aff_unique_abbr": "Univ. Lille;IUF;Inria",
        "aff_campus_unique_index": ";1;",
        "aff_campus_unique": ";Villers-l\u00e8s-Nancy",
        "aff_country_unique_index": "0+0;0;0;0+0",
        "aff_country_unique": "France"
    },
    {
        "id": "c28f6e857e",
        "title": "Asymmetric Transfer Learning with Deep Gaussian Processes",
        "site": "https://proceedings.mlr.press/v37/kandemir15.html",
        "author": "Melih Kandemir",
        "abstract": "We introduce a novel Gaussian process based Bayesian model for asymmetric transfer learning. We adopt a two-layer feed-forward deep Gaussian process as the task learner of source and target domains. The first layer projects the data onto a separate non-linear manifold for each task. We perform knowledge transfer by projecting the target data also onto the source domain and linearly combining its representations on the source and target domain manifolds. Our approach achieves the state-of-the-art in a benchmark real-world image categorization task, and improves on it in cross-tissue tumor detection from histopathology tissue slide images.",
        "bibtex": "@InProceedings{pmlr-v37-kandemir15,\n  title = \t {Asymmetric Transfer Learning with Deep Gaussian Processes},\n  author = \t {Kandemir, Melih},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {730--738},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/kandemir15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/kandemir15.html},\n  abstract = \t {We introduce a novel Gaussian process based Bayesian model for asymmetric transfer learning. We adopt a two-layer feed-forward deep Gaussian process as the task learner of source and target domains. The first layer projects the data onto a separate non-linear manifold for each task. We perform knowledge transfer by projecting the target data also onto the source domain and linearly combining its representations on the source and target domain manifolds. Our approach achieves the state-of-the-art in a benchmark real-world image categorization task, and improves on it in cross-tissue tumor detection from histopathology tissue slide images.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/kandemir15.pdf",
        "supp": "",
        "pdf_size": 584651,
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7303021440857576151&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Heidelberg University, HCI/IWR",
        "aff_domain": "IWR.UNI-HEIDELBERG.DE",
        "email": "IWR.UNI-HEIDELBERG.DE",
        "github": "https://github.com/melihkandemir/atldgp",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Heidelberg University",
        "aff_unique_dep": "Human-Computer Interaction / Institute for Visualization and Data Analysis",
        "aff_unique_url": "https://www.uni-heidelberg.de",
        "aff_unique_abbr": "Uni HD",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "78a07663de",
        "title": "Atomic Spatial Processes",
        "site": "https://proceedings.mlr.press/v37/jewell15.html",
        "author": "Sean Jewell; Neil Spencer; Alexandre Bouchard-C\u00f4t\u00e9",
        "abstract": "The emergence of compact GPS systems and the establishment of open data initiatives has resulted in widespread availability of spatial data for many urban centres. These data can be leveraged to develop data-driven intelligent resource allocation systems for urban issues such as policing, sanitation, and transportation. We employ techniques from Bayesian non-parametric statistics to develop a process which captures a common characteristic of urban spatial datasets. Specifically, our new spatial process framework models events which occur repeatedly at discrete spatial points, the number and locations of which are unknown a priori. We develop a representation of our spatial process which facilitates posterior simulation, resulting in an interpretable and computationally tractable model. The framework\u2019s superiority over both empirical grid-based models and Dirichlet process mixture models is demonstrated by fitting, interpreting, and comparing models of graffiti prevalence for both downtown Vancouver and Manhattan.",
        "bibtex": "@InProceedings{pmlr-v37-jewell15,\n  title = \t {Atomic Spatial Processes},\n  author = \t {Jewell, Sean and Spencer, Neil and Bouchard-C\u00f4t\u00e9, Alexandre},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {248--256},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/jewell15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/jewell15.html},\n  abstract = \t {The emergence of compact GPS systems and the establishment of open data initiatives has resulted in widespread availability of spatial data for many urban centres. These data can be leveraged to develop data-driven intelligent resource allocation systems for urban issues such as policing, sanitation, and transportation. We employ techniques from Bayesian non-parametric statistics to develop a process which captures a common characteristic of urban spatial datasets. Specifically, our new spatial process framework models events which occur repeatedly at discrete spatial points, the number and locations of which are unknown a priori. We develop a representation of our spatial process which facilitates posterior simulation, resulting in an interpretable and computationally tractable model. The framework\u2019s superiority over both empirical grid-based models and Dirichlet process mixture models is demonstrated by fitting, interpreting, and comparing models of graffiti prevalence for both downtown Vancouver and Manhattan.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/jewell15.pdf",
        "supp": "",
        "pdf_size": 1623893,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17220749334453984384&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Statistics, University of British Columbia, Vancouver, BC V6T 1Z4, Canada; Department of Statistics, University of British Columbia, Vancouver, BC V6T 1Z4, Canada; Department of Statistics, University of British Columbia, Vancouver, BC V6T 1Z4, Canada",
        "aff_domain": "STAT.UBC.CA;STAT.UBC.CA;STAT.UBC.CA",
        "email": "STAT.UBC.CA;STAT.UBC.CA;STAT.UBC.CA",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of British Columbia",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.ubc.ca",
        "aff_unique_abbr": "UBC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Vancouver",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "4d5193fc46",
        "title": "Attribute Efficient Linear Regression with Distribution-Dependent Sampling",
        "site": "https://proceedings.mlr.press/v37/kukliansky15.html",
        "author": "Doron Kukliansky; Ohad Shamir",
        "abstract": "We consider a budgeted learning setting, where the learner can only choose and observe a small subset of the attributes of each training example. We develop efficient algorithms for Ridge and Lasso linear regression, which utilize the geometry of the data by a novel distribution-dependent sampling scheme, and have excess risk bounds which are better a factor of up to O(d/k) over the state-of-the-art, where d is the dimension and k+1 is the number of observed attributes per example. Moreover, under reasonable assumptions, our algorithms are the first in our setting which can provably use *less* attributes than full-information algorithms, which is the main concern in budgeted learning. We complement our theoretical analysis with experiments which support our claims.",
        "bibtex": "@InProceedings{pmlr-v37-kukliansky15,\n  title = \t {Attribute Efficient Linear Regression with Distribution-Dependent Sampling},\n  author = \t {Kukliansky, Doron and Shamir, Ohad},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {153--161},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/kukliansky15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/kukliansky15.html},\n  abstract = \t {We consider a budgeted learning setting, where the learner can only choose and observe a small subset of the attributes of each training example. We develop efficient algorithms for Ridge and Lasso linear regression, which utilize the geometry of the data by a novel distribution-dependent sampling scheme, and have excess risk bounds which are better a factor of up to O(d/k) over the state-of-the-art, where d is the dimension and k+1 is the number of observed attributes per example. Moreover, under reasonable assumptions, our algorithms are the first in our setting which can provably use *less* attributes than full-information algorithms, which is the main concern in budgeted learning. We complement our theoretical analysis with experiments which support our claims.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/kukliansky15.pdf",
        "supp": "",
        "pdf_size": 424826,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4388479284542773462&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science and Applied Mathematics, Weizmann Institute of Science, Rehovot, Israel; Department of Computer Science and Applied Mathematics, Weizmann Institute of Science, Rehovot, Israel",
        "aff_domain": "WEIZMANN.AC.IL;WEIZMANN.AC.IL",
        "email": "WEIZMANN.AC.IL;WEIZMANN.AC.IL",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Weizmann Institute of Science",
        "aff_unique_dep": "Department of Computer Science and Applied Mathematics",
        "aff_unique_url": "https://www.weizmann.ac.il",
        "aff_unique_abbr": "Weizmann",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Rehovot",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "a1973bf006",
        "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
        "site": "https://proceedings.mlr.press/v37/ioffe15.html",
        "author": "Sergey Ioffe; Christian Szegedy",
        "abstract": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer\u2019s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.",
        "bibtex": "@InProceedings{pmlr-v37-ioffe15,\n  title = \t {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},\n  author = \t {Ioffe, Sergey and Szegedy, Christian},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {448--456},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/ioffe15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/ioffe15.html},\n  abstract = \t {Training Deep Neural Networks is complicated by the fact that the distribution of each layer\u2019s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/ioffe15.pdf",
        "supp": "",
        "pdf_size": 318669,
        "gs_citation": 62227,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9384364112097346204&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 31,
        "aff": "Google, 1600 Amphitheatre Pkwy, Mountain View, CA 94043; Google, 1600 Amphitheatre Pkwy, Mountain View, CA 94043",
        "aff_domain": "GOOGLE.COM;GOOGLE.COM",
        "email": "GOOGLE.COM;GOOGLE.COM",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google",
        "aff_unique_url": "https://www.google.com",
        "aff_unique_abbr": "Google",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "76fc79fd5a",
        "title": "Bayesian Multiple Target Localization",
        "site": "https://proceedings.mlr.press/v37/rajan15.html",
        "author": "Purnima Rajan; Weidong Han; Raphael Sznitman; Peter Frazier; Bruno Jedynak",
        "abstract": "We consider the problem of quickly localizing multiple targets by asking questions of the form \u201cHow many targets are within this set\" while obtaining noisy answers. This setting is a generalization to multiple targets of the game of 20 questions in which only a single target is queried. We assume that the targets are points on the real line, or in a two dimensional plane for the experiments, drawn independently from a known distribution. We evaluate the performance of a policy using the expected entropy of the posterior distribution after a fixed number of questions with noisy answers. We derive a lower bound for the value of this problem and study a specific policy, named the dyadic policy. We show that this policy achieves a value which is no more than twice this lower bound when answers are noise-free, and show a more general constant factor approximation guarantee for the noisy setting. We present an empirical evaluation of this policy on simulated data for the problem of detecting multiple instances of the same object in an image. Finally, we present experiments on localizing multiple faces simultaneously on real images.",
        "bibtex": "@InProceedings{pmlr-v37-rajan15,\n  title = \t {Bayesian Multiple Target Localization},\n  author = \t {Rajan, Purnima and Han, Weidong and Sznitman, Raphael and Frazier, Peter and Jedynak, Bruno},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1945--1953},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/rajan15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/rajan15.html},\n  abstract = \t {We consider the problem of quickly localizing multiple targets by asking questions of the form \u201cHow many targets are within this set\" while obtaining noisy answers. This setting is a generalization to multiple targets of the game of 20 questions in which only a single target is queried. We assume that the targets are points on the real line, or in a two dimensional plane for the experiments, drawn independently from a known distribution. We evaluate the performance of a policy using the expected entropy of the posterior distribution after a fixed number of questions with noisy answers. We derive a lower bound for the value of this problem and study a specific policy, named the dyadic policy. We show that this policy achieves a value which is no more than twice this lower bound when answers are noise-free, and show a more general constant factor approximation guarantee for the noisy setting. We present an empirical evaluation of this policy on simulated data for the problem of detecting multiple instances of the same object in an image. Finally, we present experiments on localizing multiple faces simultaneously on real images.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/rajan15.pdf",
        "supp": "",
        "pdf_size": 1019640,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17931583104307783606&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Science, Johns Hopkins University; Department of Operations Research and Financial Engineering, Princeton University; ARTORG Center, University of Bern; School of Operations Research and Information Engineering, Cornell University; Department of Applied Mathematics & Statistics, Johns Hopkins University",
        "aff_domain": "cs.jhu.edu;princeton.edu;artorg.unibe.ch;cornell.edu;jhu.edu",
        "email": "cs.jhu.edu;princeton.edu;artorg.unibe.ch;cornell.edu;jhu.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3;0",
        "aff_unique_norm": "Johns Hopkins University;Princeton University;University of Bern;Cornell University",
        "aff_unique_dep": "Department of Computer Science;Department of Operations Research and Financial Engineering;ARTORG Center;School of Operations Research and Information Engineering",
        "aff_unique_url": "https://www.jhu.edu;https://www.princeton.edu;https://www.unibe.ch;https://www.cornell.edu",
        "aff_unique_abbr": "JHU;Princeton;;Cornell",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0;0",
        "aff_country_unique": "United States;Switzerland"
    },
    {
        "id": "6b7a2fd3e7",
        "title": "Bayesian and Empirical Bayesian Forests",
        "site": "https://proceedings.mlr.press/v37/matthew15.html",
        "author": "Taddy Matthew; Chun-Sheng Chen; Jun Yu; Mitch Wyle",
        "abstract": "We derive ensembles of decision trees through a nonparametric Bayesian model, allowing us to view such ensembles as samples from a posterior distribution. This insight motivates a class of Bayesian Forest (BF) algorithms that provide small gains in performance and large gains in interpretability. Based on the BF framework, we are able to show that high-level tree hierarchy is stable in large samples. This motivates an empirical Bayesian Forest (EBF) algorithm for building approximate BFs on massive distributed datasets and we show that EBFs outperform sub-sampling based alternatives by a large margin.",
        "bibtex": "@InProceedings{pmlr-v37-matthew15,\n  title = \t {Bayesian and Empirical Bayesian Forests},\n  author = \t {Matthew, Taddy and Chen, Chun-Sheng and Yu, Jun and Wyle, Mitch},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {967--976},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/matthew15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/matthew15.html},\n  abstract = \t {We derive ensembles of decision trees through a nonparametric Bayesian model, allowing us to view such ensembles as samples from a posterior distribution. This insight motivates a class of Bayesian Forest (BF) algorithms that provide small gains in performance and large gains in interpretability. Based on the BF framework, we are able to show that high-level tree hierarchy is stable in large samples. This motivates an empirical Bayesian Forest (EBF) algorithm for building approximate BFs on massive distributed datasets and we show that EBFs outperform sub-sampling based alternatives by a large margin.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/matthew15.pdf",
        "supp": "",
        "pdf_size": 646415,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10617765103146942144&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Chicago Booth School of Business; eBay; eBay; eBay",
        "aff_domain": "chicagobooth.edu;ebay.com;ebay.com;ebay.com",
        "email": "chicagobooth.edu;ebay.com;ebay.com;ebay.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "University of Chicago;eBay Inc.",
        "aff_unique_dep": "Booth School of Business;",
        "aff_unique_url": "https://\u5e03\u65af\u829d\u52a0\u54e5\u5927\u5b66.com;https://www.ebay.com",
        "aff_unique_abbr": "UChicago;eBay",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Chicago;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b0fe005c63",
        "title": "BilBOWA: Fast Bilingual Distributed Representations without Word Alignments",
        "site": "https://proceedings.mlr.press/v37/gouws15.html",
        "author": "Stephan Gouws; Yoshua Bengio; Greg Corrado",
        "abstract": "We introduce BilBOWA (Bilingual Bag-of-Words without Alignments), a simple and computationally-efficient model for learning bilingual distributed representations of words which can scale to large monolingual datasets and does not require word-aligned parallel training data. Instead it trains directly on monolingual data and extracts a bilingual signal from a smaller set of raw-text sentence-aligned data. This is achieved using a novel sampled bag-of-words cross-lingual objective, which is used to regularize two noise-contrastive language models for efficient cross-lingual feature learning. We show that bilingual embeddings learned using the proposed model outperforms state-of-the-art methods on a cross-lingual document classification task as well as a lexical translation task on the WMT11 data.",
        "bibtex": "@InProceedings{pmlr-v37-gouws15,\n  title = \t {BilBOWA: Fast Bilingual Distributed Representations without Word Alignments},\n  author = \t {Gouws, Stephan and Bengio, Yoshua and Corrado, Greg},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {748--756},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/gouws15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/gouws15.html},\n  abstract = \t {We introduce BilBOWA (Bilingual Bag-of-Words without Alignments), a simple and computationally-efficient model for learning bilingual distributed representations of words which can scale to large monolingual datasets and does not require word-aligned parallel training data. Instead it trains directly on monolingual data and extracts a bilingual signal from a smaller set of raw-text sentence-aligned data. This is achieved using a novel sampled bag-of-words cross-lingual objective, which is used to regularize two noise-contrastive language models for efficient cross-lingual feature learning. We show that bilingual embeddings learned using the proposed model outperforms state-of-the-art methods on a cross-lingual document classification task as well as a lexical translation task on the WMT11 data.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/gouws15.pdf",
        "supp": "",
        "pdf_size": 739745,
        "gs_citation": 454,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2719170563629478824&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Google Inc., Mountain View, CA, USA; Dept. IRO, Universit \u00b4e de Montr \u00b4eal, QC, Canada & Canadian Institute for Advanced Research; Google Inc., Mountain View, CA, USA",
        "aff_domain": "GOOGLE.COM; ; ",
        "email": "GOOGLE.COM; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Google;Universit\u00e9 de Montr\u00e9al",
        "aff_unique_dep": "Google Inc.;Dept. IRO",
        "aff_unique_url": "https://www.google.com;https://www.umontreal.ca",
        "aff_unique_abbr": "Google;UdeM",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Mountain View;Montreal",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "9b823e18e4",
        "title": "Bimodal Modelling of Source Code and Natural Language",
        "site": "https://proceedings.mlr.press/v37/allamanis15.html",
        "author": "Miltos Allamanis; Daniel Tarlow; Andrew Gordon; Yi Wei",
        "abstract": "We consider the problem of building probabilistic models that jointly model short natural language utterances and source code snippets. The aim is to bring together recent work on statistical modelling of source code and work on bimodal models of images and natural language. The resulting models are useful for a variety of tasks that involve natural language and source code. We demonstrate their performance on two retrieval tasks: retrieving source code snippets given a natural language query, and retrieving natural language descriptions given a source code query (i.e., source code captioning). The experiments show there to be promise in this direction, and that modelling the structure of source code is helpful towards the retrieval tasks.",
        "bibtex": "@InProceedings{pmlr-v37-allamanis15,\n  title = \t {Bimodal Modelling of Source Code and Natural Language},\n  author = \t {Allamanis, Miltos and Tarlow, Daniel and Gordon, Andrew and Wei, Yi},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2123--2132},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/allamanis15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/allamanis15.html},\n  abstract = \t {We consider the problem of building probabilistic models that jointly model short natural language utterances and source code snippets. The aim is to bring together recent work on statistical modelling of source code and work on bimodal models of images and natural language. The resulting models are useful for a variety of tasks that involve natural language and source code. We demonstrate their performance on two retrieval tasks: retrieving source code snippets given a natural language query, and retrieving natural language descriptions given a source code query (i.e., source code captioning). The experiments show there to be promise in this direction, and that modelling the structure of source code is helpful towards the retrieval tasks.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/allamanis15.pdf",
        "supp": "",
        "pdf_size": 457830,
        "gs_citation": 257,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2060661297773656672&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "School of Informatics University of Edinburgh, Edinburgh, EH8 9AB, United Kingdom; Microsoft Research, 21 Station Road, Cambridge, CB1 2FB, United Kingdom; Microsoft Research, 21 Station Road, Cambridge, CB1 2FB, United Kingdom; Microsoft Research, 21 Station Road, Cambridge, CB1 2FB, United Kingdom",
        "aff_domain": "ED.AC.UK;MICROSOFT.COM;MICROSOFT.COM;MICROSOFT.COM",
        "email": "ED.AC.UK;MICROSOFT.COM;MICROSOFT.COM;MICROSOFT.COM",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "University of Edinburgh;Microsoft",
        "aff_unique_dep": "School of Informatics;Microsoft Research",
        "aff_unique_url": "https://www.ed.ac.uk;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "Edinburgh;MSR",
        "aff_campus_unique_index": "0;1;1;1",
        "aff_campus_unique": "Edinburgh;Cambridge",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "582c50df9a",
        "title": "Binary Embedding: Fundamental Limits and Fast Algorithm",
        "site": "https://proceedings.mlr.press/v37/yi15.html",
        "author": "Xinyang Yi; Constantine Caramanis; Eric Price",
        "abstract": "Binary embedding is a nonlinear dimension reduction methodology where high dimensional data are embedded into the Hamming cube while preserving the structure of the original space. Specifically, for an arbitrary N distinct points in \\mathbbS^p-1, our goal is to encode each point using m-dimensional binary strings such that we can reconstruct their geodesic distance up to \u03b4uniform distortion. Existing binary embedding algorithms either lack theoretical guarantees or suffer from running time O(mp). We make three contributions: (1) we establish a lower bound that shows any binary embedding oblivious to the set of points requires m =\u03a9(\\frac1\u03b4^2\\logN) bits and a similar lower bound for non-oblivious embeddings into Hamming distance; (2) we propose a novel fast binary embedding algorithm with provably optimal bit complexity m = O(\\frac1 \u03b4^2\\logN) and near linear running time O(p \\log p) whenever \\log N \u226a\u03b4\\sqrtp, with a slightly worse running time for larger \\log N; (3) we also provide an analytic result about embedding a general set of points K \u2286\\mathbbS^p-1 with even infinite size. Our theoretical findings are supported through experiments on both synthetic and real data sets.",
        "bibtex": "@InProceedings{pmlr-v37-yi15,\n  title = \t {Binary Embedding: Fundamental Limits and Fast Algorithm},\n  author = \t {Yi, Xinyang and Caramanis, Constantine and Price, Eric},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2162--2170},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/yi15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/yi15.html},\n  abstract = \t {Binary embedding is a nonlinear dimension reduction methodology where high dimensional data are embedded into the Hamming cube while preserving the structure of the original space. Specifically, for an arbitrary N distinct points in \\mathbbS^p-1, our goal is to encode each point using m-dimensional binary strings such that we can reconstruct their geodesic distance up to \u03b4uniform distortion. Existing binary embedding algorithms either lack theoretical guarantees or suffer from running time O(mp). We make three contributions: (1) we establish a lower bound that shows any binary embedding oblivious to the set of points requires m =\u03a9(\\frac1\u03b4^2\\logN) bits and a similar lower bound for non-oblivious embeddings into Hamming distance; (2) we propose a novel fast binary embedding algorithm with provably optimal bit complexity m = O(\\frac1 \u03b4^2\\logN) and near linear running time O(p \\log p) whenever \\log N \u226a\u03b4\\sqrtp, with a slightly worse running time for larger \\log N; (3) we also provide an analytic result about embedding a general set of points K \u2286\\mathbbS^p-1 with even infinite size. Our theoretical findings are supported through experiments on both synthetic and real data sets.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/yi15.pdf",
        "supp": "",
        "pdf_size": 362611,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7347549153263812661&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX 78712; Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX 78712; Department of Computer Science, The University of Texas at Austin, Austin, TX 78712",
        "aff_domain": "UTEXAS.EDU;UTEXAS.EDU;CS.UTEXAS.EDU",
        "email": "UTEXAS.EDU;UTEXAS.EDU;CS.UTEXAS.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "60c8e55979",
        "title": "Bipartite Edge Prediction via Transductive Learning over Product Graphs",
        "site": "https://proceedings.mlr.press/v37/liuc15.html",
        "author": "Hanxiao Liu; Yiming Yang",
        "abstract": "This paper addresses the problem of predicting the missing edges of a bipartite graph where each side of the vertices has its own intrinsic structure. We propose a new optimization framework to map the two sides of the intrinsic structures onto the manifold structure of the edges via a graph product, and to reduce the original problem to vertex label propagation over the product graph. This framework enjoys flexible choices in the formulation of graph products, and supports a rich family of graph transduction schemes with scalable inference. Experiments on benchmark datasets for collaborative filtering, citation network analysis and prerequisite prediction of online courses show advantageous performance of the proposed approach over other state-of-the-art methods.",
        "bibtex": "@InProceedings{pmlr-v37-liuc15,\n  title = \t {Bipartite Edge Prediction via Transductive Learning over Product Graphs},\n  author = \t {Liu, Hanxiao and Yang, Yiming},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1880--1888},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/liuc15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/liuc15.html},\n  abstract = \t {This paper addresses the problem of predicting the missing edges of a bipartite graph where each side of the vertices has its own intrinsic structure. We propose a new optimization framework to map the two sides of the intrinsic structures onto the manifold structure of the edges via a graph product, and to reduce the original problem to vertex label propagation over the product graph. This framework enjoys flexible choices in the formulation of graph products, and supports a rich family of graph transduction schemes with scalable inference. Experiments on benchmark datasets for collaborative filtering, citation network analysis and prerequisite prediction of online courses show advantageous performance of the proposed approach over other state-of-the-art methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/liuc15.pdf",
        "supp": "",
        "pdf_size": 340171,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3480541466937200372&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Carnegie Mellon University, Pittsburgh, PA 15213 USA; Carnegie Mellon University, Pittsburgh, PA 15213 USA",
        "aff_domain": "CS.CMU.EDU;CS.CMU.EDU",
        "email": "CS.CMU.EDU;CS.CMU.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ea3c8b4090",
        "title": "Blitz: A Principled Meta-Algorithm for Scaling Sparse Optimization",
        "site": "https://proceedings.mlr.press/v37/johnson15.html",
        "author": "Tyler Johnson; Carlos Guestrin",
        "abstract": "By reducing optimization to a sequence of small subproblems, working set methods achieve fast convergence times for many challenging problems. Despite excellent performance, theoretical understanding of working sets is limited, and implementations often resort to heuristics to determine subproblem size, makeup, and stopping criteria. We propose Blitz, a fast working set algorithm accompanied by useful guarantees. Making no assumptions on data, our theory relates subproblem size to progress toward convergence. This result motivates methods for optimizing algorithmic parameters and discarding irrelevant variables as iterations progress. Applied to L1-regularized learning, Blitz convincingly outperforms existing solvers in sequential, limited-memory, and distributed settings. Blitz is not specific to L1-regularized learning, making the algorithm relevant to many applications involving sparsity or constraints.",
        "bibtex": "@InProceedings{pmlr-v37-johnson15,\n  title = \t {Blitz: A Principled Meta-Algorithm for Scaling Sparse Optimization},\n  author = \t {Johnson, Tyler and Guestrin, Carlos},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1171--1179},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/johnson15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/johnson15.html},\n  abstract = \t {By reducing optimization to a sequence of small subproblems, working set methods achieve fast convergence times for many challenging problems. Despite excellent performance, theoretical understanding of working sets is limited, and implementations often resort to heuristics to determine subproblem size, makeup, and stopping criteria. We propose Blitz, a fast working set algorithm accompanied by useful guarantees. Making no assumptions on data, our theory relates subproblem size to progress toward convergence. This result motivates methods for optimizing algorithmic parameters and discarding irrelevant variables as iterations progress. Applied to L1-regularized learning, Blitz convincingly outperforms existing solvers in sequential, limited-memory, and distributed settings. Blitz is not specific to L1-regularized learning, making the algorithm relevant to many applications involving sparsity or constraints.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/johnson15.pdf",
        "supp": "",
        "pdf_size": 1687477,
        "gs_citation": 71,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2078777002040062901&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "University of Washington, Seattle, WA 98195, USA; University of Washington, Seattle, WA 98195, USA",
        "aff_domain": "WASHINGTON.EDU;CS.WASHINGTON.EDU",
        "email": "WASHINGTON.EDU;CS.WASHINGTON.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5485b93b0a",
        "title": "Boosted Categorical Restricted Boltzmann Machine for Computational Prediction of Splice Junctions",
        "site": "https://proceedings.mlr.press/v37/leeb15.html",
        "author": "Taehoon Lee; Sungroh Yoon",
        "abstract": "Splicing refers to the elimination of non-coding regions in transcribed pre-messenger ribonucleic acid (RNA). Discovering splice sites is an important machine learning task that helps us not only to identify the basic units of genetic heredity but also to understand how different proteins are produced. Existing methods for splicing prediction have produced promising results, but often show limited robustness and accuracy. In this paper, we propose a deep belief network-based methodology for computational splice junction prediction. Our proposal includes a novel method for training restricted Boltzmann machines for class-imbalanced prediction. The proposed method addresses the limitations of conventional contrastive divergence and provides regularization for datasets that have categorical features. We tested our approach using public human genome datasets and obtained significantly improved accuracy and reduced runtime compared to state-of-the-art alternatives. The proposed approach was less sensitive to the length of input sequences and more robust for handling false splicing signals. Furthermore, we could discover non-canonical splicing patterns that were otherwise difficult to recognize using conventional methods. Given the efficiency and robustness of our methodology, we anticipate that it can be extended to the discovery of primary structural patterns of other subtle genomic elements.",
        "bibtex": "@InProceedings{pmlr-v37-leeb15,\n  title = \t {Boosted Categorical Restricted Boltzmann Machine for Computational Prediction of Splice Junctions},\n  author = \t {Lee, Taehoon and Yoon, Sungroh},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2483--2492},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/leeb15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/leeb15.html},\n  abstract = \t {Splicing refers to the elimination of non-coding regions in transcribed pre-messenger ribonucleic acid (RNA). Discovering splice sites is an important machine learning task that helps us not only to identify the basic units of genetic heredity but also to understand how different proteins are produced. Existing methods for splicing prediction have produced promising results, but often show limited robustness and accuracy. In this paper, we propose a deep belief network-based methodology for computational splice junction prediction. Our proposal includes a novel method for training restricted Boltzmann machines for class-imbalanced prediction. The proposed method addresses the limitations of conventional contrastive divergence and provides regularization for datasets that have categorical features. We tested our approach using public human genome datasets and obtained significantly improved accuracy and reduced runtime compared to state-of-the-art alternatives. The proposed approach was less sensitive to the length of input sequences and more robust for handling false splicing signals. Furthermore, we could discover non-canonical splicing patterns that were otherwise difficult to recognize using conventional methods. Given the efficiency and robustness of our methodology, we anticipate that it can be extended to the discovery of primary structural patterns of other subtle genomic elements.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/leeb15.pdf",
        "supp": "",
        "pdf_size": 517930,
        "gs_citation": 93,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6745440623520187969&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Electrical and Computer Engineering, Seoul National University, Seoul 151-744, Korea; Department of Electrical and Computer Engineering, Seoul National University, Seoul 151-744, Korea",
        "aff_domain": "snu.ac.kr;snu.ac.kr",
        "email": "snu.ac.kr;snu.ac.kr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Seoul National University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.snu.ac.kr",
        "aff_unique_abbr": "SNU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Seoul",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "3e8aaf5239",
        "title": "Budget Allocation Problem with Multiple Advertisers: A Game Theoretic View",
        "site": "https://proceedings.mlr.press/v37/maehara15.html",
        "author": "Takanori Maehara; Akihiro Yabe; Ken-ichi Kawarabayashi",
        "abstract": "In marketing planning, advertisers seek to maximize the number of customers by allocating given budgets to each media channel effectively. The budget allocation problem with a bipartite influence model captures this scenario; however, the model is problematic because it assumes there is only one advertiser in the market. In reality, there are many advertisers which are in conflict of advertisement; thus we must extend the model for such a case. By extending the budget allocation problem with a bipartite influence model, we propose a game-theoretic model problem that considers many advertisers. By simulating our model, we can analyze the behavior of a media channel market, e.g., we can estimate which media channels are allocated by an advertiser, and which customers are influenced by an advertiser. Our model has many attractive features. First, our model is a potential game; therefore, it has a pure Nash equilibrium. Second, any Nash equilibrium of our game has 2-optimal social utility, i.e., the price of anarchy is 2. Finally, the proposed model can be simulated very efficiently; thus it can be used to analyze large markets.",
        "bibtex": "@InProceedings{pmlr-v37-maehara15,\n  title = \t {Budget Allocation Problem with Multiple Advertisers: A Game Theoretic View},\n  author = \t {Maehara, Takanori and Yabe, Akihiro and Kawarabayashi, Ken-ichi},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {428--437},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/maehara15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/maehara15.html},\n  abstract = \t {In marketing planning, advertisers seek to maximize the number of customers by allocating given budgets to each media channel effectively. The budget allocation problem with a bipartite influence model captures this scenario; however, the model is problematic because it assumes there is only one advertiser in the market. In reality, there are many advertisers which are in conflict of advertisement; thus we must extend the model for such a case. By extending the budget allocation problem with a bipartite influence model, we propose a game-theoretic model problem that considers many advertisers. By simulating our model, we can analyze the behavior of a media channel market, e.g., we can estimate which media channels are allocated by an advertiser, and which customers are influenced by an advertiser. Our model has many attractive features. First, our model is a potential game; therefore, it has a pure Nash equilibrium. Second, any Nash equilibrium of our game has 2-optimal social utility, i.e., the price of anarchy is 2. Finally, the proposed model can be simulated very efficiently; thus it can be used to analyze large markets.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/maehara15.pdf",
        "supp": "",
        "pdf_size": 212541,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5575721816295150734&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Shizuoka University+JST, ERATO, Kawarabayashi Large Graph Project; NEC Corporation+JST, ERATO, Kawarabayashi Large Graph Project; National Institute of Informatics+JST, ERATO, Kawarabayashi Large Graph Project",
        "aff_domain": "SHIZUOKA.AC.JP;CQ.JP.NEC.COM;NII.AC.JP",
        "email": "SHIZUOKA.AC.JP;CQ.JP.NEC.COM;NII.AC.JP",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2+1;3+1",
        "aff_unique_norm": "Shizuoka University;Japan Science and Technology Agency;NEC Corporation;National Institute of Informatics",
        "aff_unique_dep": ";Kawarabayashi Large Graph Project;;",
        "aff_unique_url": "https://www.shizuoka.ac.jp;https://www.jst.go.jp;https://www.nec.com;https://www.nii.ac.jp/",
        "aff_unique_abbr": "Shizuoka U;JST;NEC;NII",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "f29253df66",
        "title": "CUR Algorithm for Partially Observed Matrices",
        "site": "https://proceedings.mlr.press/v37/xua15.html",
        "author": "Miao Xu; Rong Jin; Zhi-Hua Zhou",
        "abstract": "CUR matrix decomposition computes the low rank approximation of a given matrix by using the actual rows and columns of the matrix. It has been a very useful tool for handling large matrices. One limitation with the existing algorithms for CUR matrix decomposition is that they cannot deal with entries in a \\it partially observed matrix, while incomplete matrices are found in many real world applications. In this work, we alleviate this limitation by developing a CUR decomposition algorithm for partially observed matrices. In particular, the proposed algorithm computes the low rank approximation of the target matrix based on (i) the randomly sampled rows and columns, and (ii) a subset of observed entries that are randomly sampled from the matrix. Our analysis shows the relative error bound, measured by spectral norm, for the proposed algorithm when the target matrix is of full rank. We also show that only O(n r\\ln r) observed entries are needed by the proposed algorithm to perfectly recover a rank r matrix of size n\\times n, which improves the sample complexity of the existing algorithms for matrix completion. Empirical studies on both synthetic and real-world datasets verify our theoretical claims and demonstrate the effectiveness of the proposed algorithm.",
        "bibtex": "@InProceedings{pmlr-v37-xua15,\n  title = \t {CUR Algorithm for Partially Observed Matrices},\n  author = \t {Xu, Miao and Jin, Rong and Zhou, Zhi-Hua},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1412--1421},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/xua15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/xua15.html},\n  abstract = \t {CUR matrix decomposition computes the low rank approximation of a given matrix by using the actual rows and columns of the matrix. It has been a very useful tool for handling large matrices. One limitation with the existing algorithms for CUR matrix decomposition is that they cannot deal with entries in a \\it partially observed matrix, while incomplete matrices are found in many real world applications. In this work, we alleviate this limitation by developing a CUR decomposition algorithm for partially observed matrices. In particular, the proposed algorithm computes the low rank approximation of the target matrix based on (i) the randomly sampled rows and columns, and (ii) a subset of observed entries that are randomly sampled from the matrix. Our analysis shows the relative error bound, measured by spectral norm, for the proposed algorithm when the target matrix is of full rank. We also show that only O(n r\\ln r) observed entries are needed by the proposed algorithm to perfectly recover a rank r matrix of size n\\times n, which improves the sample complexity of the existing algorithms for matrix completion. Empirical studies on both synthetic and real-world datasets verify our theoretical claims and demonstrate the effectiveness of the proposed algorithm.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/xua15.pdf",
        "supp": "",
        "pdf_size": 300517,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9260997701487588631&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "National Key Laboratory for Novel Software Technology, Nanjing University + Collaborative Innovation Center of Novel Software Technology and Industrialization, Nanjing, China; Institute of Data Science and Technologies at Alibaba Group, Seattle, USA; National Key Laboratory for Novel Software Technology, Nanjing University + Collaborative Innovation Center of Novel Software Technology and Industrialization, Nanjing, China",
        "aff_domain": "LAMDA.NJU.EDU.CN;CSE.MSU.EDU;LAMDA.NJU.EDU.CN",
        "email": "LAMDA.NJU.EDU.CN;CSE.MSU.EDU;LAMDA.NJU.EDU.CN",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;0+1",
        "aff_unique_norm": "Nanjing University;Collaborative Innovation Center of Novel Software Technology and Industrialization;Alibaba Group",
        "aff_unique_dep": "National Key Laboratory for Novel Software Technology;;Institute of Data Science and Technologies",
        "aff_unique_url": "http://www.nju.edu.cn;;https://www.alibaba.com",
        "aff_unique_abbr": "Nanjing University;;Alibaba",
        "aff_campus_unique_index": "1;2;1",
        "aff_campus_unique": ";Nanjing;Seattle",
        "aff_country_unique_index": "0+0;1;0+0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2da21376a8",
        "title": "Cascading Bandits: Learning to Rank in the Cascade Model",
        "site": "https://proceedings.mlr.press/v37/kveton15.html",
        "author": "Branislav Kveton; Csaba Szepesvari; Zheng Wen; Azin Ashkan",
        "abstract": "A search engine usually outputs a list of K web pages. The user examines this list, from the first web page to the last, and chooses the first attractive page. This model of user behavior is known as the cascade model. In this paper, we propose cascading bandits, a learning variant of the cascade model where the objective is to identify K most attractive items. We formulate our problem as a stochastic combinatorial partial monitoring problem. We propose two algorithms for solving it, CascadeUCB1 and CascadeKL-UCB. We also prove gap-dependent upper bounds on the regret of these algorithms and derive a lower bound on the regret in cascading bandits. The lower bound matches the upper bound of CascadeKL-UCB up to a logarithmic factor. We experiment with our algorithms on several problems. The algorithms perform surprisingly well even when our modeling assumptions are violated.",
        "bibtex": "@InProceedings{pmlr-v37-kveton15,\n  title = \t {Cascading Bandits: Learning to Rank in the Cascade Model},\n  author = \t {Kveton, Branislav and Szepesvari, Csaba and Wen, Zheng and Ashkan, Azin},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {767--776},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/kveton15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/kveton15.html},\n  abstract = \t {A search engine usually outputs a list of K web pages. The user examines this list, from the first web page to the last, and chooses the first attractive page. This model of user behavior is known as the cascade model. In this paper, we propose cascading bandits, a learning variant of the cascade model where the objective is to identify K most attractive items. We formulate our problem as a stochastic combinatorial partial monitoring problem. We propose two algorithms for solving it, CascadeUCB1 and CascadeKL-UCB. We also prove gap-dependent upper bounds on the regret of these algorithms and derive a lower bound on the regret in cascading bandits. The lower bound matches the upper bound of CascadeKL-UCB up to a logarithmic factor. We experiment with our algorithms on several problems. The algorithms perform surprisingly well even when our modeling assumptions are violated.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/kveton15.pdf",
        "supp": "",
        "pdf_size": 508886,
        "gs_citation": 338,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11280156416837811152&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Adobe Research, San Jose, CA; Department of Computing Science, University of Alberta; Yahoo Labs, Sunnyvale, CA; Technicolor Research, Los Altos, CA",
        "aff_domain": "ADOBE.COM;CS.UALBERTA.CA;YAHOO-INC.COM;TECHNICOLOR.COM",
        "email": "ADOBE.COM;CS.UALBERTA.CA;YAHOO-INC.COM;TECHNICOLOR.COM",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "Adobe;University of Alberta;Yahoo;Technicolor",
        "aff_unique_dep": "Adobe Research;Department of Computing Science;Yahoo Labs;Research",
        "aff_unique_url": "https://research.adobe.com;https://www.ualberta.ca;https://yahoo.com;https://www.technicolor.com",
        "aff_unique_abbr": "Adobe;UAlberta;Yahoo Labs;",
        "aff_campus_unique_index": "0;2;3",
        "aff_campus_unique": "San Jose;;Sunnyvale;Los Altos",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "9e929a840e",
        "title": "Causal Inference by Identification of Vector Autoregressive Processes with Hidden Components",
        "site": "https://proceedings.mlr.press/v37/geiger15.html",
        "author": "Philipp Geiger; Kun Zhang; Bernhard Schoelkopf; Mingming Gong; Dominik Janzing",
        "abstract": "A widely applied approach to causal inference from a time series X, often referred to as \u201c(linear) Granger causal analysis\u201d, is to simply regress present on past and interpret the regression matrix \\hatB causally. However, if there is an unmeasured time series Z that influences X, then this approach can lead to wrong causal conclusions, i.e., distinct from those one would draw if one had additional information such as Z. In this paper we take a different approach: We assume that X together with some hidden Z forms a first order vector autoregressive (VAR) process with transition matrix A, and argue why it is more valid to interpret A causally instead of \\hatB. Then we examine under which conditions the most important parts of A are identifiable or almost identifiable from only X. Essentially, sufficient conditions are (1) non-Gaussian, independent noise or (2) no influence from X to Z. We present two estimation algorithms that are tailored towards conditions (1) and (2), respectively, and evaluate them on synthetic and real-world data. We discuss how to check the model using X.",
        "bibtex": "@InProceedings{pmlr-v37-geiger15,\n  title = \t {Causal Inference by Identification of Vector Autoregressive Processes with Hidden Components},\n  author = \t {Geiger, Philipp and Zhang, Kun and Schoelkopf, Bernhard and Gong, Mingming and Janzing, Dominik},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1917--1925},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/geiger15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/geiger15.html},\n  abstract = \t {A widely applied approach to causal inference from a time series X, often referred to as \u201c(linear) Granger causal analysis\u201d, is to simply regress present on past and interpret the regression matrix \\hatB causally. However, if there is an unmeasured time series Z that influences X, then this approach can lead to wrong causal conclusions, i.e., distinct from those one would draw if one had additional information such as Z. In this paper we take a different approach: We assume that X together with some hidden Z forms a first order vector autoregressive (VAR) process with transition matrix A, and argue why it is more valid to interpret A causally instead of \\hatB. Then we examine under which conditions the most important parts of A are identifiable or almost identifiable from only X. Essentially, sufficient conditions are (1) non-Gaussian, independent noise or (2) no influence from X to Z. We present two estimation algorithms that are tailored towards conditions (1) and (2), respectively, and evaluate them on synthetic and real-world data. We discuss how to check the model using X.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/geiger15.pdf",
        "supp": "",
        "pdf_size": 276129,
        "gs_citation": 95,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10356800094709183572&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Empirical Inference Department, Max Planck Institute for Intelligent Systems, T\u00fcbingen, Germany; Empirical Inference Department, Max Planck Institute for Intelligent Systems, T\u00fcbingen, Germany+Information Sciences Institute, University of Southern California, USA; Centre for Quantum Computation and Intelligent Systems, University of Technology, Sydney, Australia; Empirical Inference Department, Max Planck Institute for Intelligent Systems, T\u00fcbingen, Germany; Empirical Inference Department, Max Planck Institute for Intelligent Systems, T\u00fcbingen, Germany",
        "aff_domain": "TUEBINGEN.MPG.DE;TUEBINGEN.MPG.DE;GMAIL.COM;TUEBINGEN.MPG.DE;TUEBINGEN.MPG.DE",
        "email": "TUEBINGEN.MPG.DE;TUEBINGEN.MPG.DE;GMAIL.COM;TUEBINGEN.MPG.DE;TUEBINGEN.MPG.DE",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;2;0;0",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems;University of Southern California;University of Technology Sydney",
        "aff_unique_dep": "Empirical Inference Department;Information Sciences Institute;Centre for Quantum Computation and Intelligent Systems",
        "aff_unique_url": "https://www.mpituebingen.mpg.de;https://isi.usc.edu;https://www.uts.edu.au",
        "aff_unique_abbr": "MPI-IS;USC;UTS",
        "aff_campus_unique_index": "0;0+1;2;0;0",
        "aff_campus_unique": "T\u00fcbingen;USA;Sydney",
        "aff_country_unique_index": "0;0+1;2;0;0",
        "aff_country_unique": "Germany;United States;Australia"
    },
    {
        "id": "00344647ca",
        "title": "Celeste: Variational inference for a generative model of astronomical images",
        "site": "https://proceedings.mlr.press/v37/regier15.html",
        "author": "Jeffrey Regier; Andrew Miller; Jon McAuliffe; Ryan Adams; Matt Hoffman; Dustin Lang; David Schlegel; Mr Prabhat",
        "abstract": "We present a new, fully generative model of optical telescope image sets, along with a variational procedure for inference. Each pixel intensity is treated as a Poisson random variable, with a rate parameter dependent on latent properties of stars and galaxies. Key latent properties are themselves random, with scientific prior distributions constructed from large ancillary data sets. We check our approach on synthetic images. We also run it on images from a major sky survey, where it exceeds the performance of the current state-of-the-art method for locating celestial bodies and measuring their colors.",
        "bibtex": "@InProceedings{pmlr-v37-regier15,\n  title = \t {Celeste: Variational inference for a generative model of astronomical images},\n  author = \t {Regier, Jeffrey and Miller, Andrew and McAuliffe, Jon and Adams, Ryan and Hoffman, Matt and Lang, Dustin and Schlegel, David and Prabhat, Mr},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2095--2103},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/regier15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/regier15.html},\n  abstract = \t {We present a new, fully generative model of optical telescope image sets, along with a variational procedure for inference. Each pixel intensity is treated as a Poisson random variable, with a rate parameter dependent on latent properties of stars and galaxies. Key latent properties are themselves random, with scientific prior distributions constructed from large ancillary data sets. We check our approach on synthetic images. We also run it on images from a major sky survey, where it exceeds the performance of the current state-of-the-art method for locating celestial bodies and measuring their colors.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/regier15.pdf",
        "supp": "",
        "pdf_size": 498988,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10735226797191848377&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "University of California, Berkeley; Harvard University; University of California, Berkeley; Harvard University; Adobe Research; Carnegie Mellon University; Lawrence Berkeley National Laboratory; Lawrence Berkeley National Laboratory",
        "aff_domain": "STAT.BERKELEY.EDU;SEAS.HARVARD.EDU;STAT.BERKELEY.EDU;SEAS.HARVARD.EDU;CS.PRINCETON.EDU;CMU.EDU;LBL.GOV;LBL.GOV",
        "email": "STAT.BERKELEY.EDU;SEAS.HARVARD.EDU;STAT.BERKELEY.EDU;SEAS.HARVARD.EDU;CS.PRINCETON.EDU;CMU.EDU;LBL.GOV;LBL.GOV",
        "github": "",
        "project": "",
        "author_num": 8,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;1;2;3;4;4",
        "aff_unique_norm": "University of California, Berkeley;Harvard University;Adobe;Carnegie Mellon University;Lawrence Berkeley National Laboratory",
        "aff_unique_dep": ";;Adobe Research;;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.harvard.edu;https://research.adobe.com;https://www.cmu.edu;https://www.lbl.gov",
        "aff_unique_abbr": "UC Berkeley;Harvard;Adobe;CMU;LBNL",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d0dec7f6a3",
        "title": "Cheap Bandits",
        "site": "https://proceedings.mlr.press/v37/hanawal15.html",
        "author": "Manjesh Hanawal; Venkatesh Saligrama; Michal Valko; Remi Munos",
        "abstract": "We consider stochastic sequential learning problems where the learner can observe the average reward of several actions. Such a setting is interesting in many applications involving monitoring and surveillance, where the set of the actions to observe represent some (geographical) area. The importance of this setting is that in these applications, it is actually cheaper to observe average reward of a group of actions rather than the reward of a single action. We show that when the reward is smooth over a given graph representing the neighboring actions, we can maximize the cumulative reward of learning while minimizing the sensing cost. In this paper we propose CheapUCB, an algorithm that matches the regret guarantees of the known algorithms for this setting and at the same time guarantees a linear cost again over them. As a by-product of our analysis, we establish a \u03a9(\\sqrt(dT)) lower bound on the cumulative regret of spectral bandits for a class of graphs with effective dimension d.",
        "bibtex": "@InProceedings{pmlr-v37-hanawal15,\n  title = \t {Cheap Bandits},\n  author = \t {Hanawal, Manjesh and Saligrama, Venkatesh and Valko, Michal and Munos, Remi},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2133--2142},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/hanawal15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/hanawal15.html},\n  abstract = \t {We consider stochastic sequential learning problems where the learner can observe the average reward of several actions. Such a setting is interesting in many applications involving monitoring and surveillance, where the set of the actions to observe represent some (geographical) area. The importance of this setting is that in these applications, it is actually cheaper to observe average reward of a group of actions rather than the reward of a single action. We show that when the reward is smooth over a given graph representing the neighboring actions, we can maximize the cumulative reward of learning while minimizing the sensing cost. In this paper we propose CheapUCB, an algorithm that matches the regret guarantees of the known algorithms for this setting and at the same time guarantees a linear cost again over them. As a by-product of our analysis, we establish a \u03a9(\\sqrt(dT)) lower bound on the cumulative regret of spectral bandits for a class of graphs with effective dimension d.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/hanawal15.pdf",
        "supp": "",
        "pdf_size": 1508127,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12688710453107849121&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Department of ECE, Boston University, Boston, Massachusetts, 02215 USA; Department of ECE, Boston University, Boston, Massachusetts, 02215 USA; INRIA Lille - Nord Europe, SequeL team, 40 avenue Halley 59650, Villeneuve d\u2019Ascq, France; INRIA Lille - Nord Europe, SequeL team, France + Google DeepMind, United Kingdom",
        "aff_domain": "bu.edu;bu.edu;inria.fr;inria.fr",
        "email": "bu.edu;bu.edu;inria.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2+3",
        "aff_unique_norm": "Boston University;INRIA Lille - Nord Europe;INRIA;Google",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;SequeL team;SequeL team;Google DeepMind",
        "aff_unique_url": "https://www.bu.edu;https://www.inria.fr/lille-nord-europe;https://www.inria.fr;https://deepmind.com",
        "aff_unique_abbr": "BU;INRIA;INRIA;DeepMind",
        "aff_campus_unique_index": "0;0;1;2",
        "aff_campus_unique": "Boston;Lille;Lille - Nord Europe;",
        "aff_country_unique_index": "0;0;1;1+2",
        "aff_country_unique": "United States;France;United Kingdom"
    },
    {
        "id": "fb5fcaa1d7",
        "title": "Classification with Low Rank and Missing Data",
        "site": "https://proceedings.mlr.press/v37/hazan15.html",
        "author": "Elad Hazan; Roi Livni; Yishay Mansour",
        "abstract": "We consider classification and regression tasks where we have missing data and assume that the (clean) data resides in a low rank subspace. Finding a hidden subspace is known to be computationally hard. Nevertheless, using a non-proper formulation we give an efficient agnostic algorithm that classifies as good as the best linear classifier coupled with the best low-dimensional subspace in which the data resides. A direct implication is that our algorithm can linearly (and non-linearly through kernels) classify provably as well as the best classifier that has access to the full data.",
        "bibtex": "@InProceedings{pmlr-v37-hazan15,\n  title = \t {Classification with Low Rank and Missing Data},\n  author = \t {Hazan, Elad and Livni, Roi and Mansour, Yishay},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {257--266},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/hazan15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/hazan15.html},\n  abstract = \t {We consider classification and regression tasks where we have missing data and assume that the (clean) data resides in a low rank subspace. Finding a hidden subspace is known to be computationally hard. Nevertheless, using a non-proper formulation we give an efficient agnostic algorithm that classifies as good as the best linear classifier coupled with the best low-dimensional subspace in which the data resides. A direct implication is that our algorithm can linearly (and non-linearly through kernels) classify provably as well as the best classifier that has access to the full data.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/hazan15.pdf",
        "supp": "",
        "pdf_size": 370874,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15785225915576881526&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Princeton University + Microsoft Research, Herzliya; The Hebrew University of Jerusalem + Microsoft Research, Herzliya; Microsoft Research, Hertzelia + Tel Aviv University",
        "aff_domain": "cs.princeton.edu;mail.huji.ac.il;gmail.com",
        "email": "cs.princeton.edu;mail.huji.ac.il;gmail.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2+1;1+3",
        "aff_unique_norm": "Princeton University;Microsoft;Hebrew University of Jerusalem;Tel Aviv University",
        "aff_unique_dep": ";Microsoft Research;;",
        "aff_unique_url": "https://www.princeton.edu;https://www.microsoft.com/en-us/research;https://www.huji.ac.il;https://www.tau.ac.il",
        "aff_unique_abbr": "Princeton;MSR;HUJI;TAU",
        "aff_campus_unique_index": "1;1;2",
        "aff_campus_unique": ";Herzliya;Hertzelia",
        "aff_country_unique_index": "0+1;1+1;1+1",
        "aff_country_unique": "United States;Israel"
    },
    {
        "id": "a5c2ab3588",
        "title": "Community Detection Using Time-Dependent Personalized PageRank",
        "site": "https://proceedings.mlr.press/v37/avron15.html",
        "author": "Haim Avron; Lior Horesh",
        "abstract": "Local graph diffusions have proven to be valuable tools for solving various graph clustering problems. As such, there has been much interest recently in efficient local algorithms for computing them. We present an efficient local algorithm for approximating a graph diffusion that generalizes both the celebrated personalized PageRank and its recent competitor/companion - the heat kernel. Our algorithm is based on writing the diffusion vector as the solution of an initial value problem, and then using a waveform relaxation approach to approximate the solution. Our experimental results suggest that it produces rankings that are distinct and competitive with the ones produced by high quality implementations of personalized PageRank and localized heat kernel, and that our algorithm is a useful addition to the toolset of localized graph diffusions.",
        "bibtex": "@InProceedings{pmlr-v37-avron15,\n  title = \t {Community Detection Using Time-Dependent Personalized PageRank},\n  author = \t {Avron, Haim and Horesh, Lior},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1795--1803},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/avron15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/avron15.html},\n  abstract = \t {Local graph diffusions have proven to be valuable tools for solving various graph clustering problems. As such, there has been much interest recently in efficient local algorithms for computing them. We present an efficient local algorithm for approximating a graph diffusion that generalizes both the celebrated personalized PageRank and its recent competitor/companion - the heat kernel. Our algorithm is based on writing the diffusion vector as the solution of an initial value problem, and then using a waveform relaxation approach to approximate the solution. Our experimental results suggest that it produces rankings that are distinct and competitive with the ones produced by high quality implementations of personalized PageRank and localized heat kernel, and that our algorithm is a useful addition to the toolset of localized graph diffusions.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/avron15.pdf",
        "supp": "",
        "pdf_size": 491223,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11217746088411905520&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "IBM T. J. Watson Research Center, Yorktown Heights, NY 10598; IBM T. J. Watson Research Center, Yorktown Heights, NY 10598",
        "aff_domain": "US.IBM.COM;US.IBM.COM",
        "email": "US.IBM.COM;US.IBM.COM",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "IBM",
        "aff_unique_dep": "IBM T. J. Watson Research Center",
        "aff_unique_url": "https://www.ibm.com/research/watson",
        "aff_unique_abbr": "IBM Watson",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Yorktown Heights",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b75bdf538a",
        "title": "Complete Dictionary Recovery Using Nonconvex Optimization",
        "site": "https://proceedings.mlr.press/v37/sund15.html",
        "author": "Ju Sun; Qing Qu; John Wright",
        "abstract": "We consider the problem of recovering a complete (i.e., square and invertible) dictionary mb A_0, from mb Y = mb A_0 mb X_0 with mb Y \u2208\\mathbb R^n \\times p. This recovery setting is central to the theoretical understanding of dictionary learning. We give the first efficient algorithm that provably recovers mb A_0 when mb X_0 has O(n) nonzeros per column, under suitable probability model for mb X_0. Prior results provide recovery guarantees when mb X_0 has only O(\\sqrtn) nonzeros per column. Our algorithm is based on nonconvex optimization with a spherical constraint, and hence is naturally phrased in the language of manifold optimization. Our proofs give a geometric characterization of the high-dimensional objective landscape, which shows that with high probability there are no spurious local minima. Experiments with synthetic data corroborate our theory. Full version of this paper is available online: \\urlhttp://arxiv.org/abs/1504.06785.",
        "bibtex": "@InProceedings{pmlr-v37-sund15,\n  title = \t {Complete Dictionary Recovery Using Nonconvex Optimization},\n  author = \t {Sun, Ju and Qu, Qing and Wright, John},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2351--2360},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/sund15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/sund15.html},\n  abstract = \t {We consider the problem of recovering a complete (i.e., square and invertible) dictionary mb A_0, from mb Y = mb A_0 mb X_0 with mb Y \u2208\\mathbb R^n \\times p. This recovery setting is central to the theoretical understanding of dictionary learning. We give the first efficient algorithm that provably recovers mb A_0 when mb X_0 has O(n) nonzeros per column, under suitable probability model for mb X_0. Prior results provide recovery guarantees when mb X_0 has only O(\\sqrtn) nonzeros per column. Our algorithm is based on nonconvex optimization with a spherical constraint, and hence is naturally phrased in the language of manifold optimization. Our proofs give a geometric characterization of the high-dimensional objective landscape, which shows that with high probability there are no spurious local minima. Experiments with synthetic data corroborate our theory. Full version of this paper is available online: \\urlhttp://arxiv.org/abs/1504.06785.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/sund15.pdf",
        "supp": "",
        "pdf_size": 1343291,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4453663925736913619&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Electrical Engineering, Columbia University, New York, NY, USA; Department of Electrical Engineering, Columbia University, New York, NY, USA; Department of Electrical Engineering, Columbia University, New York, NY, USA",
        "aff_domain": "COLUMBIA.EDU;COLUMBIA.EDU;COLUMBIA.EDU",
        "email": "COLUMBIA.EDU;COLUMBIA.EDU;COLUMBIA.EDU",
        "github": "",
        "project": "http://arxiv.org/abs/1504.06785",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "New York",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ab99af2c28",
        "title": "Complex Event Detection using Semantic Saliency and Nearly-Isotonic SVM",
        "site": "https://proceedings.mlr.press/v37/changa15.html",
        "author": "Xiaojun Chang; Yi Yang; Eric Xing; Yaoliang Yu",
        "abstract": "We aim to detect complex events in long Internet videos that may last for hours. A major challenge in this setting is that only a few shots in a long video are relevant to the event of interest while others are irrelevant or even misleading. Instead of indifferently pooling the shots, we first define a novel notion of semantic saliency that assesses the relevance of each shot with the event of interest. We then prioritize the shots according to their saliency scores since shots that are semantically more salient are expected to contribute more to the final event detector. Next, we propose a new isotonic regularizer that is able to exploit the semantic ordering information. The resulting nearly-isotonic SVM classifier exhibits higher discriminative power. Computationally, we develop an efficient implementation using the proximal gradient algorithm, and we prove new, closed-form proximal steps. We conduct extensive experiments on three real-world video datasets and confirm the effectiveness of the proposed approach.",
        "bibtex": "@InProceedings{pmlr-v37-changa15,\n  title = \t {Complex Event Detection using Semantic Saliency and Nearly-Isotonic SVM},\n  author = \t {Chang, Xiaojun and Yang, Yi and Xing, Eric and Yu, Yaoliang},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1348--1357},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/changa15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/changa15.html},\n  abstract = \t {We aim to detect complex events in long Internet videos that may last for hours. A major challenge in this setting is that only a few shots in a long video are relevant to the event of interest while others are irrelevant or even misleading. Instead of indifferently pooling the shots, we first define a novel notion of semantic saliency that assesses the relevance of each shot with the event of interest. We then prioritize the shots according to their saliency scores since shots that are semantically more salient are expected to contribute more to the final event detector. Next, we propose a new isotonic regularizer that is able to exploit the semantic ordering information. The resulting nearly-isotonic SVM classifier exhibits higher discriminative power. Computationally, we develop an efficient implementation using the proximal gradient algorithm, and we prove new, closed-form proximal steps. We conduct extensive experiments on three real-world video datasets and confirm the effectiveness of the proposed approach.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/changa15.pdf",
        "supp": "",
        "pdf_size": 1739853,
        "gs_citation": 83,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=290591480782453143&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Centre for Quantum Computation and Intelligent Systems, University of Technology Sydney, Sydney, Australia; Centre for Quantum Computation and Intelligent Systems, University of Technology Sydney, Sydney, Australia; Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA, USA; Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA, USA",
        "aff_domain": "GMAIL.COM;GMAIL.COM;CS.CMU.EDU;CS.CMU.EDU",
        "email": "GMAIL.COM;GMAIL.COM;CS.CMU.EDU;CS.CMU.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;1",
        "aff_unique_norm": "University of Technology Sydney;Carnegie Mellon University",
        "aff_unique_dep": "Centre for Quantum Computation and Intelligent Systems;Machine Learning Department",
        "aff_unique_url": "https://www.uts.edu.au;https://www.cmu.edu",
        "aff_unique_abbr": "UTS;CMU",
        "aff_campus_unique_index": "0;0;1;1",
        "aff_campus_unique": "Sydney;Pittsburgh",
        "aff_country_unique_index": "0;0;1;1",
        "aff_country_unique": "Australia;United States"
    },
    {
        "id": "ab9e1a7d89",
        "title": "Compressing Neural Networks with the Hashing Trick",
        "site": "https://proceedings.mlr.press/v37/chenc15.html",
        "author": "Wenlin Chen; James Wilson; Stephen Tyree; Kilian Weinberger; Yixin Chen",
        "abstract": "As deep nets are increasingly used in applications suited for mobile devices, a fundamental dilemma becomes apparent: the trend in deep learning is to grow models to absorb ever-increasing data set sizes; however mobile devices are designed with very little memory and cannot store such large models. We present a novel network architecture, HashedNets, that exploits inherent redundancy in neural networks to achieve drastic reductions in model sizes. HashedNets uses a low-cost hash function to randomly group connection weights into hash buckets, and all connections within the same hash bucket share a single parameter value. These parameters are tuned to adjust to the HashedNets weight sharing architecture with standard backprop during training. Our hashing procedure introduces no additional memory overhead, and we demonstrate on several benchmark data sets that HashedNets shrink the storage requirements of neural networks substantially while mostly preserving generalization performance.",
        "bibtex": "@InProceedings{pmlr-v37-chenc15,\n  title = \t {Compressing Neural Networks with the Hashing Trick},\n  author = \t {Chen, Wenlin and Wilson, James and Tyree, Stephen and Weinberger, Kilian and Chen, Yixin},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2285--2294},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/chenc15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/chenc15.html},\n  abstract = \t {As deep nets are increasingly used in applications suited for mobile devices, a fundamental dilemma becomes apparent: the trend in deep learning is to grow models to absorb ever-increasing data set sizes; however mobile devices are designed with very little memory and cannot store such large models. We present a novel network architecture, HashedNets, that exploits inherent redundancy in neural networks to achieve drastic reductions in model sizes. HashedNets uses a low-cost hash function to randomly group connection weights into hash buckets, and all connections within the same hash bucket share a single parameter value. These parameters are tuned to adjust to the HashedNets weight sharing architecture with standard backprop during training. Our hashing procedure introduces no additional memory overhead, and we demonstrate on several benchmark data sets that HashedNets shrink the storage requirements of neural networks substantially while mostly preserving generalization performance.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/chenc15.pdf",
        "supp": "",
        "pdf_size": 417218,
        "gs_citation": 1494,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5053947540904220409&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science and Engineering, Washington University in St. Louis, St. Louis, MO, USA+NVIDIA, Santa Clara, CA, USA; Department of Computer Science and Engineering, Washington University in St. Louis, St. Louis, MO, USA; NVIDIA, Santa Clara, CA, USA; Department of Computer Science and Engineering, Washington University in St. Louis, St. Louis, MO, USA; Department of Computer Science and Engineering, Washington University in St. Louis, St. Louis, MO, USA",
        "aff_domain": "WUSTL.EDU;WUSTL.EDU;NVIDIA.COM;WUSTL.EDU;CSE.WUSTL.EDU",
        "email": "WUSTL.EDU;WUSTL.EDU;NVIDIA.COM;WUSTL.EDU;CSE.WUSTL.EDU",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;1;0;0",
        "aff_unique_norm": "Washington University in St. Louis;NVIDIA",
        "aff_unique_dep": "Department of Computer Science and Engineering;NVIDIA",
        "aff_unique_url": "https://wustl.edu;https://www.nvidia.com",
        "aff_unique_abbr": "WashU;NV",
        "aff_campus_unique_index": "0+1;0;1;0;0",
        "aff_campus_unique": "St. Louis;Santa Clara",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7cbdecdc98",
        "title": "Consistent Multiclass Algorithms for Complex Performance Measures",
        "site": "https://proceedings.mlr.press/v37/narasimhanb15.html",
        "author": "Harikrishna Narasimhan; Harish Ramaswamy; Aadirupa Saha; Shivani Agarwal",
        "abstract": "This paper presents new consistent algorithms for multiclass learning with complex performance measures, defined by arbitrary functions of the confusion matrix. This setting includes as a special case all loss-based performance measures, which are simply linear functions of the confusion matrix, but also includes more complex performance measures such as the multiclass G-mean and micro F_1 measures. We give a general framework for designing consistent algorithms for such performance measures by viewing the learning problem as an optimization problem over the set of feasible confusion matrices, and give two specific instantiations based on the Frank-Wolfe method for concave performance measures and on the bisection method for ratio-of-linear performance measures. The resulting algorithms are provably consistent and outperform a multiclass version of the state-of-the-art SVMperf method in experiments; for large multiclass problems, the algorithms are also orders of magnitude faster than SVMperf.",
        "bibtex": "@InProceedings{pmlr-v37-narasimhanb15,\n  title = \t {Consistent Multiclass Algorithms for Complex Performance Measures},\n  author = \t {Narasimhan, Harikrishna and Ramaswamy, Harish and Saha, Aadirupa and Agarwal, Shivani},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2398--2407},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/narasimhanb15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/narasimhanb15.html},\n  abstract = \t {This paper presents new consistent algorithms for multiclass learning with complex performance measures, defined by arbitrary functions of the confusion matrix. This setting includes as a special case all loss-based performance measures, which are simply linear functions of the confusion matrix, but also includes more complex performance measures such as the multiclass G-mean and micro F_1 measures. We give a general framework for designing consistent algorithms for such performance measures by viewing the learning problem as an optimization problem over the set of feasible confusion matrices, and give two specific instantiations based on the Frank-Wolfe method for concave performance measures and on the bisection method for ratio-of-linear performance measures. The resulting algorithms are provably consistent and outperform a multiclass version of the state-of-the-art SVMperf method in experiments; for large multiclass problems, the algorithms are also orders of magnitude faster than SVMperf.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/narasimhanb15.pdf",
        "supp": "",
        "pdf_size": 602938,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4321116758296101031&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Indian Institute of Science, Bangalore 560012, INDIA; Indian Institute of Science, Bangalore 560012, INDIA; Indian Institute of Science, Bangalore 560012, INDIA; Indian Institute of Science, Bangalore 560012, INDIA",
        "aff_domain": "CSA.IISC.ERNET.IN;CSA.IISC.ERNET.IN;CSA.IISC.ERNET.IN;CSA.IISC.ERNET.IN",
        "email": "CSA.IISC.ERNET.IN;CSA.IISC.ERNET.IN;CSA.IISC.ERNET.IN;CSA.IISC.ERNET.IN",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Indian Institute of Science",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iisc.ac.in",
        "aff_unique_abbr": "IISc",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Bangalore",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "0c73b655b4",
        "title": "Consistent estimation of dynamic and multi-layer block models",
        "site": "https://proceedings.mlr.press/v37/hanb15.html",
        "author": "Qiuyi Han; Kevin Xu; Edoardo Airoldi",
        "abstract": "Significant progress has been made recently on theoretical analysis of estimators for the stochastic block model (SBM). In this paper, we consider the multi-graph SBM, which serves as a foundation for many application settings including dynamic and multi-layer networks. We explore the asymptotic properties of two estimators for the multi-graph SBM, namely spectral clustering and the maximum-likelihood estimate (MLE), as the number of layers of the multi-graph increases. We derive sufficient conditions for consistency of both estimators and propose a variational approximation to the MLE that is computationally feasible for large networks. We verify the sufficient conditions via simulation and demonstrate that they are practical. In addition, we apply the model to two real data sets: a dynamic social network and a multi-layer social network with several types of relations.",
        "bibtex": "@InProceedings{pmlr-v37-hanb15,\n  title = \t {Consistent estimation of dynamic and multi-layer block models},\n  author = \t {Han, Qiuyi and Xu, Kevin and Airoldi, Edoardo},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1511--1520},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/hanb15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/hanb15.html},\n  abstract = \t {Significant progress has been made recently on theoretical analysis of estimators for the stochastic block model (SBM). In this paper, we consider the multi-graph SBM, which serves as a foundation for many application settings including dynamic and multi-layer networks. We explore the asymptotic properties of two estimators for the multi-graph SBM, namely spectral clustering and the maximum-likelihood estimate (MLE), as the number of layers of the multi-graph increases. We derive sufficient conditions for consistency of both estimators and propose a variational approximation to the MLE that is computationally feasible for large networks. We verify the sufficient conditions via simulation and demonstrate that they are practical. In addition, we apply the model to two real data sets: a dynamic social network and a multi-layer social network with several types of relations.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/hanb15.pdf",
        "supp": "",
        "pdf_size": 472818,
        "gs_citation": 184,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=814888679165317437&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Statistics, Harvard University, Cambridge, MA, USA; Technicolor Research, Los Altos, CA, USA; Department of Statistics, Harvard University, Cambridge, MA, USA",
        "aff_domain": "fas.harvard.edu;outlook.com;fas.harvard.edu",
        "email": "fas.harvard.edu;outlook.com;fas.harvard.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Harvard University;Technicolor Research",
        "aff_unique_dep": "Department of Statistics;",
        "aff_unique_url": "https://www.harvard.edu;https://www.technicolor.com/en",
        "aff_unique_abbr": "Harvard;",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Cambridge;Los Altos",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3a9e473aee",
        "title": "Context-based Unsupervised Data Fusion for Decision Making",
        "site": "https://proceedings.mlr.press/v37/soltanmohammadi15.html",
        "author": "Erfan Soltanmohammadi; Mort Naraghi-Pour; Mihaela Schaar",
        "abstract": "Big Data received from sources such as social media, in-stream monitoring systems, networks, and markets is often mined for discovering patterns, detecting anomalies, and making decisions or predictions. In distributed learning and real-time processing of Big Data, ensemble-based systems in which a fusion center (FC) is used to combine the local decisions of several classifiers, have shown to be superior to single expert systems. However, optimal design of the FC requires knowledge of the accuracy of the individual classifiers which, in many cases, is not available. Moreover, in many applications supervised training of the FC is not feasible since the true labels of the data set are not available. In this paper, we propose an unsupervised joint estimation-detection scheme to estimate the accuracies of the local classifiers as functions of data context and to fuse the local decisions of the classifiers. Numerical results show the dramatic improvement of the proposed method as compared with the state of the art approaches.",
        "bibtex": "@InProceedings{pmlr-v37-soltanmohammadi15,\n  title = \t {Context-based Unsupervised Data Fusion for Decision Making},\n  author = \t {Soltanmohammadi, Erfan and Naraghi-Pour, Mort and Schaar, Mihaela},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2076--2084},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/soltanmohammadi15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/soltanmohammadi15.html},\n  abstract = \t {Big Data received from sources such as social media, in-stream monitoring systems, networks, and markets is often mined for discovering patterns, detecting anomalies, and making decisions or predictions. In distributed learning and real-time processing of Big Data, ensemble-based systems in which a fusion center (FC) is used to combine the local decisions of several classifiers, have shown to be superior to single expert systems. However, optimal design of the FC requires knowledge of the accuracy of the individual classifiers which, in many cases, is not available. Moreover, in many applications supervised training of the FC is not feasible since the true labels of the data set are not available. In this paper, we propose an unsupervised joint estimation-detection scheme to estimate the accuracies of the local classifiers as functions of data context and to fuse the local decisions of the classifiers. Numerical results show the dramatic improvement of the proposed method as compared with the state of the art approaches.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/soltanmohammadi15.pdf",
        "supp": "",
        "pdf_size": 1562640,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16138625844811648559&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Marvell Semiconductor, Inc., Santa Clara, CA, USA; Louisiana State University, Baton Rouge, LA, USA; University of California, Los Angeles, CA, USA",
        "aff_domain": "MARVELL.COM;LSU.EDU;EE.UCLA.EDU",
        "email": "MARVELL.COM;LSU.EDU;EE.UCLA.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Marvell Semiconductor, Inc.;Louisiana State University;University of California, Los Angeles",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.marvell.com;https://www.lsu.edu;https://www.ucla.edu",
        "aff_unique_abbr": "Marvell;LSU;UCLA",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "Santa Clara;Baton Rouge;Los Angeles",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "49853cd68b",
        "title": "Controversy in mechanistic modelling with Gaussian processes",
        "site": "https://proceedings.mlr.press/v37/macdonald15.html",
        "author": "Benn Macdonald; Catherine Higham; Dirk Husmeier",
        "abstract": "Parameter inference in mechanistic models based on non-affine differential equations is computationally onerous, and various faster alternatives based on gradient matching have been proposed. A particularly promising approach is based on nonparametric Bayesian modelling with Gaussian processes, which exploits the fact that a Gaussian process is closed under differentiation. However, two alternative paradigms have been proposed. The first paradigm, proposed at NIPS 2008 and AISTATS 2013, is based on a product of experts approach and a marginalization over the derivatives of the state variables. The second paradigm, proposed at ICML 2014, is based on a probabilistic generative model and a marginalization over the state variables. The claim has been made that this leads to better inference results. In the present article, we offer a new interpretation of the second paradigm, which highlights the underlying assumptions, approximations and limitations. In particular, we show that the second paradigm suffers from an intrinsic identifiability problem, which the first paradigm is not affected by.",
        "bibtex": "@InProceedings{pmlr-v37-macdonald15,\n  title = \t {Controversy in mechanistic modelling with Gaussian processes},\n  author = \t {Macdonald, Benn and Higham, Catherine and Husmeier, Dirk},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1539--1547},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/macdonald15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/macdonald15.html},\n  abstract = \t {Parameter inference in mechanistic models based on non-affine differential equations is computationally onerous, and various faster alternatives based on gradient matching have been proposed. A particularly promising approach is based on nonparametric Bayesian modelling with Gaussian processes, which exploits the fact that a Gaussian process is closed under differentiation. However, two alternative paradigms have been proposed. The first paradigm, proposed at NIPS 2008 and AISTATS 2013, is based on a product of experts approach and a marginalization over the derivatives of the state variables. The second paradigm, proposed at ICML 2014, is based on a probabilistic generative model and a marginalization over the state variables. The claim has been made that this leads to better inference results. In the present article, we offer a new interpretation of the second paradigm, which highlights the underlying assumptions, approximations and limitations. In particular, we show that the second paradigm suffers from an intrinsic identifiability problem, which the first paradigm is not affected by.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/macdonald15.pdf",
        "supp": "",
        "pdf_size": 3098605,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17667627634118374763&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "School of Mathematics & Statistics, University of Glasgow; School of Mathematics & Statistics, University of Glasgow; School of Mathematics & Statistics, University of Glasgow",
        "aff_domain": "research.gla.ac.uk;glasgow.ac.uk;glasgow.ac.uk",
        "email": "research.gla.ac.uk;glasgow.ac.uk;glasgow.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Glasgow",
        "aff_unique_dep": "School of Mathematics & Statistics",
        "aff_unique_url": "https://www.gla.ac.uk",
        "aff_unique_abbr": "UoG",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "ccf063c427",
        "title": "Convergence rate of Bayesian tensor estimator and its minimax optimality",
        "site": "https://proceedings.mlr.press/v37/suzuki15.html",
        "author": "Taiji Suzuki",
        "abstract": "We investigate the statistical convergence rate of a Bayesian low-rank tensor estimator, and derive the minimax optimal rate for learning a low-rank tensor. Our problem setting is the regression problem where the regression coefficient forms a tensor structure. This problem setting occurs in many practical applications, such as collaborative filtering, multi-task learning, and spatio-temporal data analysis. The convergence rate of the Bayes tensor estimator is analyzed in terms of both in-sample and out-of-sample predictive accuracies. It is shown that a fast learning rate is achieved without any strong convexity of the observation. Moreover, we show that the method has adaptivity to the unknown rank of the true tensor, that is, the near optimal rate depending on the true rank is achieved even if it is not known a priori. Finally, we show the minimax optimal learning rate for the tensor estimation problem, and thus show that the derived bound of the Bayes estimator is tight and actually near minimax optimal.",
        "bibtex": "@InProceedings{pmlr-v37-suzuki15,\n  title = \t {Convergence rate of Bayesian tensor estimator and its minimax optimality},\n  author = \t {Suzuki, Taiji},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1273--1282},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/suzuki15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/suzuki15.html},\n  abstract = \t {We investigate the statistical convergence rate of a Bayesian low-rank tensor estimator, and derive the minimax optimal rate for learning a low-rank tensor. Our problem setting is the regression problem where the regression coefficient forms a tensor structure. This problem setting occurs in many practical applications, such as collaborative filtering, multi-task learning, and spatio-temporal data analysis. The convergence rate of the Bayes tensor estimator is analyzed in terms of both in-sample and out-of-sample predictive accuracies. It is shown that a fast learning rate is achieved without any strong convexity of the observation. Moreover, we show that the method has adaptivity to the unknown rank of the true tensor, that is, the near optimal rate depending on the true rank is achieved even if it is not known a priori. Finally, we show the minimax optimal learning rate for the tensor estimation problem, and thus show that the derived bound of the Bayes estimator is tight and actually near minimax optimal.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/suzuki15.pdf",
        "supp": "",
        "pdf_size": 150502,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3668923580353937459&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "TokyoInstitute of Technology,O-okayama 2-12-1, Meguro-ku,Tokyo152-8552, JAPAN+PRESTO,Japan Science and TechnologyAgency,JAPAN",
        "aff_domain": "IS.TITECH.AC.JP",
        "email": "IS.TITECH.AC.JP",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1",
        "aff_unique_norm": "Tokyo Institute of Technology;Japan Science and Technology Agency",
        "aff_unique_dep": ";PRESTO",
        "aff_unique_url": "https://www.titech.ac.jp;https://www.jst.go.jp",
        "aff_unique_abbr": "Titech;JST",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Ookayama;",
        "aff_country_unique_index": "0+0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "4f5a47f15f",
        "title": "Convex Calibrated Surrogates for Hierarchical Classification",
        "site": "https://proceedings.mlr.press/v37/ramaswamy15.html",
        "author": "Harish Ramaswamy; Ambuj Tewari; Shivani Agarwal",
        "abstract": "Hierarchical classification problems are multiclass supervised learning problems with a pre-defined hierarchy over the set of class labels. In this work, we study the consistency of hierarchical classification algorithms with respect to a natural loss, namely the tree distance metric on the hierarchy tree of class labels, via the usage of calibrated surrogates. We first show that the Bayes optimal classifier for this loss classifies an instance according to the deepest node in the hierarchy such that the total conditional probability of the subtree rooted at the node is greater than \\frac12. We exploit this insight to develop new consistent algorithm for hierarchical classification, that makes use of an algorithm known to be consistent for the \u201cmulticlass classification with reject option (MCRO)\u201d problem as a sub-routine. Our experiments on a number of benchmark datasets show that the resulting algorithm, which we term OvA-Cascade, gives improved performance over other state-of-the-art hierarchical classification algorithms.",
        "bibtex": "@InProceedings{pmlr-v37-ramaswamy15,\n  title = \t {Convex Calibrated Surrogates for Hierarchical Classification},\n  author = \t {Ramaswamy, Harish and Tewari, Ambuj and Agarwal, Shivani},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1852--1860},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/ramaswamy15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/ramaswamy15.html},\n  abstract = \t {Hierarchical classification problems are multiclass supervised learning problems with a pre-defined hierarchy over the set of class labels. In this work, we study the consistency of hierarchical classification algorithms with respect to a natural loss, namely the tree distance metric on the hierarchy tree of class labels, via the usage of calibrated surrogates. We first show that the Bayes optimal classifier for this loss classifies an instance according to the deepest node in the hierarchy such that the total conditional probability of the subtree rooted at the node is greater than \\frac12. We exploit this insight to develop new consistent algorithm for hierarchical classification, that makes use of an algorithm known to be consistent for the \u201cmulticlass classification with reject option (MCRO)\u201d problem as a sub-routine. Our experiments on a number of benchmark datasets show that the resulting algorithm, which we term OvA-Cascade, gives improved performance over other state-of-the-art hierarchical classification algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/ramaswamy15.pdf",
        "supp": "",
        "pdf_size": 349543,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10119082706611120940&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Indian Institute of Science, Bangalore, INDIA; University of Michigan, Ann Arbor, USA; Indian Institute of Science, Bangalore, INDIA",
        "aff_domain": "CSA.IISC.ERNET.IN;UMICH.EDU;CSA.IISC.ERNET.IN",
        "email": "CSA.IISC.ERNET.IN;UMICH.EDU;CSA.IISC.ERNET.IN",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Indian Institute of Science;University of Michigan",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iisc.ac.in;https://www.umich.edu",
        "aff_unique_abbr": "IISc;UM",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Bangalore;Ann Arbor",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "India;United States"
    },
    {
        "id": "ce1c0d1f1c",
        "title": "Convex Formulation for Learning from Positive and Unlabeled Data",
        "site": "https://proceedings.mlr.press/v37/plessis15.html",
        "author": "Marthinus Du Plessis; Gang Niu; Masashi Sugiyama",
        "abstract": "We discuss binary classification from only from positive and unlabeled data (PU classification), which is conceivable in various real-world machine learning problems. Since unlabeled data consists of both positive and negative data, simply separating positive and unlabeled data yields a biased solution. Recently, it was shown that the bias can be canceled by using a particular non-convex loss such as the ramp loss. However, classifier training with a non-convex loss is not straightforward in practice. In this paper, we discuss a convex formulation for PU classification that can still cancel the bias. The key idea is to use different loss functions for positive and unlabeled samples. However, in this setup, the hinge loss is not permissible. As an alternative, we propose the double hinge loss. Theoretically, we prove that the estimators converge to the optimal solutions at the optimal parametric rate. Experimentally, we demonstrate that PU classification with the double hinge loss performs as accurate as the non-convex method, with a much lower computational cost.",
        "bibtex": "@InProceedings{pmlr-v37-plessis15,\n  title = \t {Convex Formulation for Learning from Positive and Unlabeled Data},\n  author = \t {Plessis, Marthinus Du and Niu, Gang and Sugiyama, Masashi},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1386--1394},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/plessis15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/plessis15.html},\n  abstract = \t {We discuss binary classification from only from positive and unlabeled data (PU classification), which is conceivable in various real-world machine learning problems. Since unlabeled data consists of both positive and negative data, simply separating positive and unlabeled data yields a biased solution. Recently, it was shown that the bias can be canceled by using a particular non-convex loss such as the ramp loss. However, classifier training with a non-convex loss is not straightforward in practice. In this paper, we discuss a convex formulation for PU classification that can still cancel the bias. The key idea is to use different loss functions for positive and unlabeled samples. However, in this setup, the hinge loss is not permissible. As an alternative, we propose the double hinge loss. Theoretically, we prove that the estimators converge to the optimal solutions at the optimal parametric rate. Experimentally, we demonstrate that PU classification with the double hinge loss performs as accurate as the non-convex method, with a much lower computational cost.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/plessis15.pdf",
        "supp": "",
        "pdf_size": 421406,
        "gs_citation": 407,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1809204439380716960&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo, Japan; Baidu Inc., Beijing, 100085, China; The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo, Japan",
        "aff_domain": "ms.k.u-tokyo.ac.jp;baidu.cn;k.u-tokyo.ac.jp",
        "email": "ms.k.u-tokyo.ac.jp;baidu.cn;k.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Tokyo;Baidu",
        "aff_unique_dep": ";Baidu Inc.",
        "aff_unique_url": "https://www.u-tokyo.ac.jp;https://www.baidu.com",
        "aff_unique_abbr": "UTokyo;Baidu",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Tokyo;Beijing",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Japan;China"
    },
    {
        "id": "23c3d4e4d3",
        "title": "Convex Learning of Multiple Tasks and their Structure",
        "site": "https://proceedings.mlr.press/v37/ciliberto15.html",
        "author": "Carlo Ciliberto; Youssef Mroueh; Tomaso Poggio; Lorenzo Rosasco",
        "abstract": "Reducing the amount of human supervision is a key problem in machine learning and a natural approach is that of exploiting the relations (structure) among different tasks. This is the idea at the core of multi-task learning. In this context a fundamental question is how to incorporate the tasks structure in the learning problem. We tackle this question by studying a general computational framework that allows to encode a-priori knowledge of the tasks structure in the form of a convex penalty; in this setting a variety of previously proposed methods can be recovered as special cases, including linear and non-linear approaches. Within this framework, we show that tasks and their structure can be efficiently learned considering a convex optimization problem that can be approached by means of block coordinate methods such as alternating minimization and for which we prove convergence to the global minimum.",
        "bibtex": "@InProceedings{pmlr-v37-ciliberto15,\n  title = \t {Convex Learning of Multiple Tasks and their Structure},\n  author = \t {Ciliberto, Carlo and Mroueh, Youssef and Poggio, Tomaso and Rosasco, Lorenzo},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1548--1557},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/ciliberto15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/ciliberto15.html},\n  abstract = \t {Reducing the amount of human supervision is a key problem in machine learning and a natural approach is that of exploiting the relations (structure) among different tasks. This is the idea at the core of multi-task learning. In this context a fundamental question is how to incorporate the tasks structure in the learning problem. We tackle this question by studying a general computational framework that allows to encode a-priori knowledge of the tasks structure in the form of a convex penalty; in this setting a variety of previously proposed methods can be recovered as special cases, including linear and non-linear approaches. Within this framework, we show that tasks and their structure can be efficiently learned considering a convex optimization problem that can be approached by means of block coordinate methods such as alternating minimization and for which we prove convergence to the global minimum.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/ciliberto15.pdf",
        "supp": "",
        "pdf_size": 321119,
        "gs_citation": 94,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9938841166994363265&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Laboratory for Computational and Statistical Learning, Istituto Italiano di Tecnologia, Via Morego 30, Genova, Italy+Center for Brains Minds and Machines, Massachusetts Institute of Technology, Cambridge, MA 02139 USA; Laboratory for Computational and Statistical Learning, Istituto Italiano di Tecnologia, Via Morego 30, Genova, Italy+Center for Brains Minds and Machines, Massachusetts Institute of Technology, Cambridge, MA 02139 USA; Laboratory for Computational and Statistical Learning, Istituto Italiano di Tecnologia, Via Morego 30, Genova, Italy+Center for Brains Minds and Machines, Massachusetts Institute of Technology, Cambridge, MA 02139 USA; Laboratory for Computational and Statistical Learning, Istituto Italiano di Tecnologia, Via Morego 30, Genova, Italy+Center for Brains Minds and Machines, Massachusetts Institute of Technology, Cambridge, MA 02139 USA+DIBRIS, Universit `a di Genova, Via Dodecaneso, 35, 16146, Genova, Italy",
        "aff_domain": "MIT.EDU;MIT.EDU;AI.MIT.EDU;MIT.EDU",
        "email": "MIT.EDU;MIT.EDU;AI.MIT.EDU;MIT.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1;0+1+2",
        "aff_unique_norm": "Istituto Italiano di Tecnologia;Massachusetts Institute of Technology;Universit\u00e0 di Genova",
        "aff_unique_dep": "Laboratory for Computational and Statistical Learning;Center for Brains Minds and Machines;DIBRIS",
        "aff_unique_url": "https://www.iit.it;https://web.mit.edu/;https://www.unige.it",
        "aff_unique_abbr": "IIT;MIT;",
        "aff_campus_unique_index": "0+1;0+1;0+1;0+1",
        "aff_campus_unique": "Genova;Cambridge;",
        "aff_country_unique_index": "0+1;0+1;0+1;0+1+0",
        "aff_country_unique": "Italy;United States"
    },
    {
        "id": "2231717114",
        "title": "Coordinate Descent Converges Faster with the Gauss-Southwell Rule Than Random Selection",
        "site": "https://proceedings.mlr.press/v37/nutini15.html",
        "author": "Julie Nutini; Mark Schmidt; Issam Laradji; Michael Friedlander; Hoyt Koepke",
        "abstract": "There has been significant recent work on the theory and application of randomized coordinate descent algorithms, beginning with the work of\u00a0 Nesterov [SIAM J. Optim., 22(2), 2012], who showed that a random-coordinate selection rule achieves the same convergence rate as the Gauss-Southwell selection rule. This result suggests that we should never use the Gauss-Southwell rule, as it is typically much more expensive than random selection. However, the empirical behaviours of these algorithms contradict this theoretical result: in applications where the computational costs of the selection rules are comparable, the Gauss-Southwell selection rule tends to perform substantially better than random coordinate selection. We give a simple analysis of the Gauss-Southwell rule showing that\u2014except in extreme cases\u2014it\u2019s convergence rate is faster than choosing random coordinates. Further, in this work we (i) show that exact coordinate optimization improves the convergence rate for certain sparse problems, (ii) propose a Gauss-Southwell-Lipschitz rule that gives an even faster convergence rate given knowledge of the Lipschitz constants of the partial derivatives, (iii) analyze the effect of approximate Gauss-Southwell rules, and (iv) analyze proximal-gradient variants of the Gauss-Southwell rule.",
        "bibtex": "@InProceedings{pmlr-v37-nutini15,\n  title = \t {Coordinate Descent Converges Faster with the Gauss-Southwell Rule Than Random Selection},\n  author = \t {Nutini, Julie and Schmidt, Mark and Laradji, Issam and Friedlander, Michael and Koepke, Hoyt},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1632--1641},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/nutini15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/nutini15.html},\n  abstract = \t {There has been significant recent work on the theory and application of randomized coordinate descent algorithms, beginning with the work of\u00a0 Nesterov [SIAM J. Optim., 22(2), 2012], who showed that a random-coordinate selection rule achieves the same convergence rate as the Gauss-Southwell selection rule. This result suggests that we should never use the Gauss-Southwell rule, as it is typically much more expensive than random selection. However, the empirical behaviours of these algorithms contradict this theoretical result: in applications where the computational costs of the selection rules are comparable, the Gauss-Southwell selection rule tends to perform substantially better than random coordinate selection. We give a simple analysis of the Gauss-Southwell rule showing that\u2014except in extreme cases\u2014it\u2019s convergence rate is faster than choosing random coordinates. Further, in this work we (i) show that exact coordinate optimization improves the convergence rate for certain sparse problems, (ii) propose a Gauss-Southwell-Lipschitz rule that gives an even faster convergence rate given knowledge of the Lipschitz constants of the partial derivatives, (iii) analyze the effect of approximate Gauss-Southwell rules, and (iv) analyze proximal-gradient variants of the Gauss-Southwell rule.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/nutini15.pdf",
        "supp": "",
        "pdf_size": 348743,
        "gs_citation": 285,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6332805312280430863&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "University of British Columbia; University of British Columbia; University of British Columbia; University of California, Davis; Dato",
        "aff_domain": "CS.UBC.CA;CS.UBC.CA;CS.UBC.CA;MATH.UCDAVIS.EDU;DATO.COM",
        "email": "CS.UBC.CA;CS.UBC.CA;CS.UBC.CA;MATH.UCDAVIS.EDU;DATO.COM",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;2",
        "aff_unique_norm": "University of British Columbia;University of California, Davis;Dato",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ubc.ca;https://www.ucdavis.edu;",
        "aff_unique_abbr": "UBC;UC Davis;",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Davis",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "Canada;United States;"
    },
    {
        "id": "b85c293d1c",
        "title": "Coresets for Nonparametric Estimation - the Case of DP-Means",
        "site": "https://proceedings.mlr.press/v37/bachem15.html",
        "author": "Olivier Bachem; Mario Lucic; Andreas Krause",
        "abstract": "Scalable training of Bayesian nonparametric models is a notoriously difficult challenge. We explore the use of coresets - a data summarization technique originating from computational geometry - for this task. Coresets are weighted subsets of the data such that models trained on these coresets are provably competitive with models trained on the full dataset. Coresets sublinear in the dataset size allow for fast approximate inference with provable guarantees. Existing constructions, however, are limited to parametric problems. Using novel techniques in coreset construction we show the existence of coresets for DP-Means - a prototypical nonparametric clustering problem - and provide a practical construction algorithm. We empirically demonstrate that our algorithm allows us to efficiently trade off computation time and approximation error and thus scale DP-Means to large datasets. For instance, with coresets we can obtain a computational speedup of 45x at an approximation error of only 2.4% compared to solving on the full data set. In contrast, for the same subsample size, the \u201cnaive\u201d approach of uniformly subsampling the data incurs an approximation error of 22.5%.",
        "bibtex": "@InProceedings{pmlr-v37-bachem15,\n  title = \t {Coresets for Nonparametric Estimation - the Case of DP-Means},\n  author = \t {Bachem, Olivier and Lucic, Mario and Krause, Andreas},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {209--217},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/bachem15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/bachem15.html},\n  abstract = \t {Scalable training of Bayesian nonparametric models is a notoriously difficult challenge. We explore the use of coresets - a data summarization technique originating from computational geometry - for this task. Coresets are weighted subsets of the data such that models trained on these coresets are provably competitive with models trained on the full dataset. Coresets sublinear in the dataset size allow for fast approximate inference with provable guarantees. Existing constructions, however, are limited to parametric problems. Using novel techniques in coreset construction we show the existence of coresets for DP-Means - a prototypical nonparametric clustering problem - and provide a practical construction algorithm. We empirically demonstrate that our algorithm allows us to efficiently trade off computation time and approximation error and thus scale DP-Means to large datasets. For instance, with coresets we can obtain a computational speedup of 45x at an approximation error of only 2.4% compared to solving on the full data set. In contrast, for the same subsample size, the \u201cnaive\u201d approach of uniformly subsampling the data incurs an approximation error of 22.5%.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/bachem15.pdf",
        "supp": "",
        "pdf_size": 566618,
        "gs_citation": 120,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12350713923938486149&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "ETH Zurich, Switzerland; ETH Zurich, Switzerland; ETH Zurich, Switzerland",
        "aff_domain": "inf.ethz.ch;inf.ethz.ch;ethz.ch",
        "email": "inf.ethz.ch;inf.ethz.ch;ethz.ch",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "ea1ac8aa5c",
        "title": "Correlation Clustering in Data Streams",
        "site": "https://proceedings.mlr.press/v37/ahn15.html",
        "author": "KookJin Ahn; Graham Cormode; Sudipto Guha; Andrew McGregor; Anthony Wirth",
        "abstract": "In this paper, we address the problem of \\emphcorrelation clustering in the dynamic data stream model. The stream consists of updates to the edge weights of a graph on\u00a0n nodes and the goal is to find a node-partition such that the end-points of negative-weight edges are typically in different clusters whereas the end-points of positive-weight edges are typically in the same cluster. We present polynomial-time, O(n\u22c5\\textpolylog n)-space approximation algorithms for natural problems that arise. We first develop data structures based on linear sketches that allow the \u201cquality\u201d of a given node-partition to be measured. We then combine these data structures with convex programming and sampling techniques to solve the relevant approximation problem. However the standard LP and SDP formulations are not obviously solvable in O(n\u22c5\\textpolylog n)-space. Our work presents space-efficient algorithms for the convex programming required, as well as approaches to reduce the adaptivity of the sampling. Note that the improved space and running-time bounds achieved from streaming algorithms are also useful for offline settings such as MapReduce models.",
        "bibtex": "@InProceedings{pmlr-v37-ahn15,\n  title = \t {Correlation Clustering in Data Streams},\n  author = \t {Ahn, KookJin and Cormode, Graham and Guha, Sudipto and McGregor, Andrew and Wirth, Anthony},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2237--2246},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/ahn15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/ahn15.html},\n  abstract = \t {In this paper, we address the problem of \\emphcorrelation clustering in the dynamic data stream model. The stream consists of updates to the edge weights of a graph on\u00a0n nodes and the goal is to find a node-partition such that the end-points of negative-weight edges are typically in different clusters whereas the end-points of positive-weight edges are typically in the same cluster. We present polynomial-time, O(n\u22c5\\textpolylog n)-space approximation algorithms for natural problems that arise. We first develop data structures based on linear sketches that allow the \u201cquality\u201d of a given node-partition to be measured. We then combine these data structures with convex programming and sampling techniques to solve the relevant approximation problem. However the standard LP and SDP formulations are not obviously solvable in O(n\u22c5\\textpolylog n)-space. Our work presents space-efficient algorithms for the convex programming required, as well as approaches to reduce the adaptivity of the sampling. Note that the improved space and running-time bounds achieved from streaming algorithms are also useful for offline settings such as MapReduce models.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/ahn15.pdf",
        "supp": "",
        "pdf_size": 353503,
        "gs_citation": 120,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5531893415205488519&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 24,
        "aff": "University of Pennsylvania, Philadelphia, PA 19104, USA + Google, Inc.; University of Warwick, Coventry CV4 7AL, UK; University of Pennsylvania, Philadelphia, PA 19104, USA; University of Massachusetts, Amherst, MA 01003, USA; Department of Computing and Information Systems, The University of Melbourne, Vic 3010, Australia",
        "aff_domain": "google.com;warwick.ac.uk;cis.upenn.edu;cs.umass.edu;unimelb.edu.au",
        "email": "google.com;warwick.ac.uk;cis.upenn.edu;cs.umass.edu;unimelb.edu.au",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;0;3;4",
        "aff_unique_norm": "University of Pennsylvania;Google;University of Warwick;University of Massachusetts Amherst;University of Melbourne",
        "aff_unique_dep": ";Google;;;Department of Computing and Information Systems",
        "aff_unique_url": "https://www.upenn.edu;https://www.google.com;https://www.warwick.ac.uk;https://www.umass.edu;https://www.unimelb.edu.au",
        "aff_unique_abbr": "UPenn;Google;Warwick;UMass Amherst;UniMelb",
        "aff_campus_unique_index": "0+1;2;0;3;4",
        "aff_campus_unique": "Philadelphia;Mountain View;Coventry;Amherst;Melbourne",
        "aff_country_unique_index": "0+0;1;0;0;2",
        "aff_country_unique": "United States;United Kingdom;Australia"
    },
    {
        "id": "2f7ab6248d",
        "title": "Counterfactual Risk Minimization: Learning from Logged Bandit Feedback",
        "site": "https://proceedings.mlr.press/v37/swaminathan15.html",
        "author": "Adith Swaminathan; Thorsten Joachims",
        "abstract": "We develop a learning principle and an efficient algorithm for batch learning from logged bandit feedback. This learning setting is ubiquitous in online systems (e.g., ad placement, web search, recommendation), where an algorithm makes a prediction (e.g., ad ranking) for a given input (e.g., query) and observes bandit feedback (e.g., user clicks on presented ads). We first address the counterfactual nature of the learning problem through propensity scoring. Next, we prove generalization error bounds that account for the variance of the propensity-weighted empirical risk estimator. These constructive bounds give rise to the Counterfactual Risk Minimization (CRM) principle. We show how CRM can be used to derive a new learning method \u2013 called Policy Optimizer for Exponential Models (POEM) \u2013 for learning stochastic linear rules for structured output prediction. We present a decomposition of the POEM objective that enables efficient stochastic gradient optimization. POEM is evaluated on several multi-label classification problems showing substantially improved robustness and generalization performance compared to the state-of-the-art.",
        "bibtex": "@InProceedings{pmlr-v37-swaminathan15,\n  title = \t {Counterfactual Risk Minimization: Learning from Logged Bandit Feedback},\n  author = \t {Swaminathan, Adith and Joachims, Thorsten},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {814--823},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/swaminathan15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/swaminathan15.html},\n  abstract = \t {We develop a learning principle and an efficient algorithm for batch learning from logged bandit feedback. This learning setting is ubiquitous in online systems (e.g., ad placement, web search, recommendation), where an algorithm makes a prediction (e.g., ad ranking) for a given input (e.g., query) and observes bandit feedback (e.g., user clicks on presented ads). We first address the counterfactual nature of the learning problem through propensity scoring. Next, we prove generalization error bounds that account for the variance of the propensity-weighted empirical risk estimator. These constructive bounds give rise to the Counterfactual Risk Minimization (CRM) principle. We show how CRM can be used to derive a new learning method \u2013 called Policy Optimizer for Exponential Models (POEM) \u2013 for learning stochastic linear rules for structured output prediction. We present a decomposition of the POEM objective that enables efficient stochastic gradient optimization. POEM is evaluated on several multi-label classification problems showing substantially improved robustness and generalization performance compared to the state-of-the-art.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/swaminathan15.pdf",
        "supp": "",
        "pdf_size": 401082,
        "gs_citation": 425,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10969158151126414974&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Cornell University, Ithaca, NY 14853 USA; Cornell University, Ithaca, NY 14853 USA",
        "aff_domain": "cs.cornell.edu;cs.cornell.edu",
        "email": "cs.cornell.edu;cs.cornell.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Cornell University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cornell.edu",
        "aff_unique_abbr": "Cornell",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Ithaca",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "68bf2f4afd",
        "title": "DP-space: Bayesian Nonparametric Subspace Clustering with Small-variance Asymptotics",
        "site": "https://proceedings.mlr.press/v37/wanga15.html",
        "author": "Yining Wang; Jun Zhu",
        "abstract": "Subspace clustering separates data points approximately lying on union of affine subspaces into several clusters. This paper presents a novel nonparametric Bayesian subspace clustering model that infers both the number of subspaces and the dimension of each subspace from the observed data. Though the posterior inference is hard, our model leads to a very efficient deterministic algorithm, DP-space, which retains the nonparametric ability under a small-variance asymptotic analysis. DP-space monotonically minimizes an intuitive objective with an explicit tradeoff between data fitness and model complexity. Experimental results demonstrate that DP-space outperforms various competitors in terms of clustering accuracy and at the same time it is highly efficient.",
        "bibtex": "@InProceedings{pmlr-v37-wanga15,\n  title = \t {DP-space: Bayesian Nonparametric Subspace Clustering with Small-variance Asymptotics},\n  author = \t {Wang, Yining and Zhu, Jun},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {862--870},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/wanga15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/wanga15.html},\n  abstract = \t {Subspace clustering separates data points approximately lying on union of affine subspaces into several clusters. This paper presents a novel nonparametric Bayesian subspace clustering model that infers both the number of subspaces and the dimension of each subspace from the observed data. Though the posterior inference is hard, our model leads to a very efficient deterministic algorithm, DP-space, which retains the nonparametric ability under a small-variance asymptotic analysis. DP-space monotonically minimizes an intuitive objective with an explicit tradeoff between data fitness and model complexity. Experimental results demonstrate that DP-space outperforms various competitors in terms of clustering accuracy and at the same time it is highly efficient.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/wanga15.pdf",
        "supp": "",
        "pdf_size": 557066,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17222806510604998983&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15213, USA; Dept. of Comp. Sci. & Tech., State Key Lab of Intell. Tech. & Sys., TNList, CBICR Center, Tsinghua University, China",
        "aff_domain": "CS.CMU.EDU;TSINGHUA.EDU.CN",
        "email": "CS.CMU.EDU;TSINGHUA.EDU.CN",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Carnegie Mellon University;Tsinghua University",
        "aff_unique_dep": "Machine Learning Department;Dept. of Comp. Sci. & Tech.",
        "aff_unique_url": "https://www.cmu.edu;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "CMU;THU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Pittsburgh;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "368e7fcf6f",
        "title": "DRAW: A Recurrent Neural Network For Image Generation",
        "site": "https://proceedings.mlr.press/v37/gregor15.html",
        "author": "Karol Gregor; Ivo Danihelka; Alex Graves; Danilo Rezende; Daan Wierstra",
        "abstract": "This paper introduces the Deep Recurrent Attentive Writer (DRAW) architecture for image generation with neural networks. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it is able to generate images that are indistinguishable from real data with the naked eye.",
        "bibtex": "@InProceedings{pmlr-v37-gregor15,\n  title = \t {DRAW: A Recurrent Neural Network For Image Generation},\n  author = \t {Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo and Wierstra, Daan},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1462--1471},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/gregor15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/gregor15.html},\n  abstract = \t {This paper introduces the Deep Recurrent Attentive Writer (DRAW) architecture for image generation with neural networks. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it is able to generate images that are indistinguishable from real data with the naked eye.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/gregor15.pdf",
        "supp": "",
        "pdf_size": 1783902,
        "gs_citation": 2594,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8022513888710268841&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Google DeepMind; Google DeepMind; Google DeepMind; Google DeepMind; Google DeepMind",
        "aff_domain": "GOOGLE.COM;GOOGLE.COM;GOOGLE.COM;GOOGLE.COM;GOOGLE.COM",
        "email": "GOOGLE.COM;GOOGLE.COM;GOOGLE.COM;GOOGLE.COM;GOOGLE.COM",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google DeepMind",
        "aff_unique_url": "https://deepmind.com",
        "aff_unique_abbr": "DeepMind",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "97432ed73e",
        "title": "Dealing with small data: On the generalization of context trees",
        "site": "https://proceedings.mlr.press/v37/eggeling15.html",
        "author": "Ralf Eggeling; Mikko Koivisto; Ivo Grosse",
        "abstract": "Context trees (CT) are a widely used tool in machine learning for representing context-specific independences in conditional probability distributions. Parsimonious context trees (PCTs) are a recently proposed generalization of CTs that can enable statistically more efficient learning due to a higher structural flexibility, which is particularly useful for small-data settings. However, this comes at the cost of a computationally expensive structure learning algorithm, which is feasible only for domains with small alphabets and tree depths. In this work, we investigate to which degree CTs can be generalized to increase statistical efficiency while still keeping the learning computationally feasible. Approaching this goal from two different angles, we (i) propose algorithmic improvements to the PCT learning algorithm, and (ii) study further generalizations of CTs, which are inspired by PCTs, but trade structural flexibility for computational efficiency. By empirical studies both on simulated and real-world data, we demonstrate that the synergy of combining of both orthogonal approaches yields a substantial improvement in obtaining statistically efficient and computationally feasible generalizations of CTs.",
        "bibtex": "@InProceedings{pmlr-v37-eggeling15,\n  title = \t {Dealing with small data: On the generalization of context trees},\n  author = \t {Eggeling, Ralf and Koivisto, Mikko and Grosse, Ivo},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1245--1253},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/eggeling15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/eggeling15.html},\n  abstract = \t {Context trees (CT) are a widely used tool in machine learning for representing context-specific independences in conditional probability distributions. Parsimonious context trees (PCTs) are a recently proposed generalization of CTs that can enable statistically more efficient learning due to a higher structural flexibility, which is particularly useful for small-data settings. However, this comes at the cost of a computationally expensive structure learning algorithm, which is feasible only for domains with small alphabets and tree depths. In this work, we investigate to which degree CTs can be generalized to increase statistical efficiency while still keeping the learning computationally feasible. Approaching this goal from two different angles, we (i) propose algorithmic improvements to the PCT learning algorithm, and (ii) study further generalizations of CTs, which are inspired by PCTs, but trade structural flexibility for computational efficiency. By empirical studies both on simulated and real-world data, we demonstrate that the synergy of combining of both orthogonal approaches yields a substantial improvement in obtaining statistically efficient and computationally feasible generalizations of CTs.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/eggeling15.pdf",
        "supp": "",
        "pdf_size": 292261,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1199806516883561402&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Martin Luther University Halle-Wittenberg, Germany; Helsinki Institute for Information Technology, Department of Computer Science, University of Helsinki, Finland; Martin Luther University Halle-Wittenberg, Germany+German Center for Integrative Biodiversity Research (iDiv) Halle-Jena-Leipzig, Leipzig, Germany",
        "aff_domain": "INFORMATIK.UNI-HALLE.DE;CS.HELSINKI.FI;INFORMATIK.UNI-HALLE.DE",
        "email": "INFORMATIK.UNI-HALLE.DE;CS.HELSINKI.FI;INFORMATIK.UNI-HALLE.DE",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0+2",
        "aff_unique_norm": "Martin Luther University Halle-Wittenberg;University of Helsinki;German Center for Integrative Biodiversity Research",
        "aff_unique_dep": ";Department of Computer Science;Integrative Biodiversity Research",
        "aff_unique_url": "https://www.uni-halle.de;https://www.helsinki.fi;https://www.idiv.de",
        "aff_unique_abbr": "MLU;UH;iDiv",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Leipzig",
        "aff_country_unique_index": "0;1;0+0",
        "aff_country_unique": "Germany;Finland"
    },
    {
        "id": "0a6ce87297",
        "title": "Deep Edge-Aware Filters",
        "site": "https://proceedings.mlr.press/v37/xub15.html",
        "author": "Li Xu; Jimmy Ren; Qiong Yan; Renjie Liao; Jiaya Jia",
        "abstract": "There are many edge-aware filters varying in their construction forms and filtering properties. It seems impossible to uniformly represent and accelerate them in a single framework. We made the attempt to learn a big and important family of edge-aware operators from data. Our method is based on a deep convolutional neural network with a gradient domain training procedure, which gives rise to a powerful tool to approximate various filters without knowing the original models and implementation details. The only difference among these operators in our system becomes merely the learned parameters. Our system enables fast approximation for complex edge-aware filters and achieves up to 200x acceleration, regardless of their originally very different implementation. Fast speed can also be achieved when creating new effects using spatially varying filter or filter combination, bearing out the effectiveness of our deep edge-aware filters.",
        "bibtex": "@InProceedings{pmlr-v37-xub15,\n  title = \t {Deep Edge-Aware Filters},\n  author = \t {Xu, Li and Ren, Jimmy and Yan, Qiong and Liao, Renjie and Jia, Jiaya},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1669--1678},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/xub15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/xub15.html},\n  abstract = \t {There are many edge-aware filters varying in their construction forms and filtering properties. It seems impossible to uniformly represent and accelerate them in a single framework. We made the attempt to learn a big and important family of edge-aware operators from data. Our method is based on a deep convolutional neural network with a gradient domain training procedure, which gives rise to a powerful tool to approximate various filters without knowing the original models and implementation details. The only difference among these operators in our system becomes merely the learned parameters. Our system enables fast approximation for complex edge-aware filters and achieves up to 200x acceleration, regardless of their originally very different implementation. Fast speed can also be achieved when creating new effects using spatially varying filter or filter combination, bearing out the effectiveness of our deep edge-aware filters.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/xub15.pdf",
        "supp": "",
        "pdf_size": 3308439,
        "gs_citation": 268,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12709500558987039729&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "SenseTime Group Limited; SenseTime Group Limited; SenseTime Group Limited; The Chinese University of Hong Kong; The Chinese University of Hong Kong",
        "aff_domain": "SENSETIME.COM;SENSETIME.COM;SENSETIME.COM;CSE.CUHK.EDU.HK;CSE.CUHK.EDU.HK",
        "email": "SENSETIME.COM;SENSETIME.COM;SENSETIME.COM;CSE.CUHK.EDU.HK;CSE.CUHK.EDU.HK",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;1",
        "aff_unique_norm": "SenseTime Group Limited;Chinese University of Hong Kong",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sensetime.com;https://www.cuhk.edu.hk",
        "aff_unique_abbr": "SenseTime;CUHK",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "00cf8fdc78",
        "title": "Deep Learning with Limited Numerical Precision",
        "site": "https://proceedings.mlr.press/v37/gupta15.html",
        "author": "Suyog Gupta; Ankur Agrawal; Kailash Gopalakrishnan; Pritish Narayanan",
        "abstract": "Training of large-scale deep neural networks is often constrained by the available computational resources. We study the effect of limited precision data representation and computation on neural network training. Within the context of low-precision fixed-point computations, we observe the rounding scheme to play a crucial role in determining the network\u2019s behavior during training. Our results show that deep networks can be trained using only 16-bit wide fixed-point number representation when using stochastic rounding, and incur little to no degradation in the classification accuracy. We also demonstrate an energy-efficient hardware accelerator that implements low-precision fixed-point arithmetic with stochastic rounding",
        "bibtex": "@InProceedings{pmlr-v37-gupta15,\n  title = \t {Deep Learning with Limited Numerical Precision},\n  author = \t {Gupta, Suyog and Agrawal, Ankur and Gopalakrishnan, Kailash and Narayanan, Pritish},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1737--1746},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/gupta15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/gupta15.html},\n  abstract = \t {Training of large-scale deep neural networks is often constrained by the available computational resources. We study the effect of limited precision data representation and computation on neural network training. Within the context of low-precision fixed-point computations, we observe the rounding scheme to play a crucial role in determining the network\u2019s behavior during training. Our results show that deep networks can be trained using only 16-bit wide fixed-point number representation when using stochastic rounding, and incur little to no degradation in the classification accuracy. We also demonstrate an energy-efficient hardware accelerator that implements low-precision fixed-point arithmetic with stochastic rounding}\n}",
        "pdf": "http://proceedings.mlr.press/v37/gupta15.pdf",
        "supp": "",
        "pdf_size": 406008,
        "gs_citation": 2815,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14109789955727767115&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "IBM T. J. Watson Research Center, Yorktown Heights, NY 10598; IBM T. J. Watson Research Center, Yorktown Heights, NY 10598; IBM T. J. Watson Research Center, Yorktown Heights, NY 10598; IBM Almaden Research Center, San Jose, CA 95120",
        "aff_domain": "US.IBM.COM;US.IBM.COM;US.IBM.COM;US.IBM.COM",
        "email": "US.IBM.COM;US.IBM.COM;US.IBM.COM;US.IBM.COM",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "IBM",
        "aff_unique_dep": "IBM T. J. Watson Research Center",
        "aff_unique_url": "https://www.ibm.com/research/watson",
        "aff_unique_abbr": "IBM Watson",
        "aff_campus_unique_index": "0;0;0;1",
        "aff_campus_unique": "Yorktown Heights;San Jose",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1888cc8f27",
        "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics",
        "site": "https://proceedings.mlr.press/v37/sohl-dickstein15.html",
        "author": "Jascha Sohl-Dickstein; Eric Weiss; Niru Maheswaranathan; Surya Ganguli",
        "abstract": "A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.",
        "bibtex": "@InProceedings{pmlr-v37-sohl-dickstein15,\n  title = \t {Deep Unsupervised Learning using Nonequilibrium Thermodynamics},\n  author = \t {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2256--2265},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/sohl-dickstein15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/sohl-dickstein15.html},\n  abstract = \t {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/sohl-dickstein15.pdf",
        "supp": "",
        "pdf_size": 3363574,
        "gs_citation": 8219,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7270166379090138707&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Stanford University; University of California, Berkeley; Stanford University; Stanford University",
        "aff_domain": "STANFORD.EDU;BERKELEY.EDU;STANFORD.EDU;STANFORD.EDU",
        "email": "STANFORD.EDU;BERKELEY.EDU;STANFORD.EDU;STANFORD.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Stanford University;University of California, Berkeley",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.stanford.edu;https://www.berkeley.edu",
        "aff_unique_abbr": "Stanford;UC Berkeley",
        "aff_campus_unique_index": "0;1;0;0",
        "aff_campus_unique": "Stanford;Berkeley",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "368523f563",
        "title": "Deterministic Independent Component Analysis",
        "site": "https://proceedings.mlr.press/v37/huangb15.html",
        "author": "Ruitong Huang; Andras Gyorgy; Csaba Szepesv\u00e1ri",
        "abstract": "We study independent component analysis with noisy observations. We present, for the first time in the literature, consistent, polynomial-time algorithms to recover non-Gaussian source signals and the mixing matrix with a reconstruction error that vanishes at a 1/\\sqrtT rate using T observations and scales only polynomially with the natural parameters of the problem. Our algorithms and analysis also extend to deterministic source signals whose empirical distributions are approximately independent.",
        "bibtex": "@InProceedings{pmlr-v37-huangb15,\n  title = \t {Deterministic Independent Component Analysis},\n  author = \t {Huang, Ruitong and Gyorgy, Andras and Szepesv\u00e1ri, Csaba},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2521--2530},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/huangb15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/huangb15.html},\n  abstract = \t {We study independent component analysis with noisy observations. We present, for the first time in the literature, consistent, polynomial-time algorithms to recover non-Gaussian source signals and the mixing matrix with a reconstruction error that vanishes at a 1/\\sqrtT rate using T observations and scales only polynomially with the natural parameters of the problem. Our algorithms and analysis also extend to deterministic source signals whose empirical distributions are approximately independent.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/huangb15.pdf",
        "supp": "",
        "pdf_size": 459572,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12989360526018166126&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computing Science, University of Alberta, Edmonton, AB T6G2E8 Canada; Department of Computing Science, University of Alberta, Edmonton, AB T6G2E8 Canada; Department of Computing Science, University of Alberta, Edmonton, AB T6G2E8 Canada",
        "aff_domain": "ualberta.ca;ualberta.ca;ualberta.ca",
        "email": "ualberta.ca;ualberta.ca;ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "Department of Computing Science",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Edmonton",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "84e9586ff4",
        "title": "DiSCO: Distributed Optimization for Self-Concordant Empirical Loss",
        "site": "https://proceedings.mlr.press/v37/zhangb15.html",
        "author": "Yuchen Zhang; Xiao Lin",
        "abstract": "We propose a new distributed algorithm for empirical risk minimization in machine learning. The algorithm is based on an inexact damped Newton method, where the inexact Newton steps are computed by a distributed preconditioned conjugate gradient method. We analyze its iteration complexity and communication efficiency for minimizing self-concordant empirical loss functions, and discuss the results for distributed ridge regression, logistic regression and binary classification with a smoothed hinge loss. In a standard setting for supervised learning, where the n data points are i.i.d. sampled and when the regularization parameter scales as 1/\\sqrtn, we show that the proposed algorithm is communication efficient: the required round of communication does not increase with the sample size n, and only grows slowly with the number of machines.",
        "bibtex": "@InProceedings{pmlr-v37-zhangb15,\n  title = \t {DiSCO: Distributed Optimization for Self-Concordant Empirical Loss},\n  author = \t {Zhang, Yuchen and Lin, Xiao},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {362--370},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/zhangb15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/zhangb15.html},\n  abstract = \t {We propose a new distributed algorithm for empirical risk minimization in machine learning. The algorithm is based on an inexact damped Newton method, where the inexact Newton steps are computed by a distributed preconditioned conjugate gradient method. We analyze its iteration complexity and communication efficiency for minimizing self-concordant empirical loss functions, and discuss the results for distributed ridge regression, logistic regression and binary classification with a smoothed hinge loss. In a standard setting for supervised learning, where the n data points are i.i.d. sampled and when the regularization parameter scales as 1/\\sqrtn, we show that the proposed algorithm is communication efficient: the required round of communication does not increase with the sample size n, and only grows slowly with the number of machines.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/zhangb15.pdf",
        "supp": "",
        "pdf_size": 403671,
        "gs_citation": 273,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3808619345077471001&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of California Berkeley, Berkeley, CA 94720, USA; Microsoft Research, Redmond, WA 98053, USA",
        "aff_domain": "EECS.BERKELEY.EDU;MICROSOFT.COM",
        "email": "EECS.BERKELEY.EDU;MICROSOFT.COM",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of California, Berkeley;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.berkeley.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "UC Berkeley;MSR",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Berkeley;Redmond",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "bfef2339c2",
        "title": "Differentially Private Bayesian Optimization",
        "site": "https://proceedings.mlr.press/v37/kusnera15.html",
        "author": "Matt Kusner; Jacob Gardner; Roman Garnett; Kilian Weinberger",
        "abstract": "Bayesian optimization is a powerful tool for fine-tuning the hyper-parameters of a wide variety of machine learning models. The success of machine learning has led practitioners in diverse real-world settings to learn classifiers for practical problems. As machine learning becomes commonplace, Bayesian optimization becomes an attractive method for practitioners to automate the process of classifier hyper-parameter tuning. A key observation is that the data used for tuning models in these settings is often sensitive. Certain data such as genetic predisposition, personal email statistics, and car accident history, if not properly private, may be at risk of being inferred from Bayesian optimization outputs. To address this, we introduce methods for releasing the best hyper-parameters and classifier accuracy privately. Leveraging the strong theoretical guarantees of differential privacy and known Bayesian optimization convergence bounds, we prove that under a GP assumption these private quantities are often near-optimal. Finally, even if this assumption is not satisfied, we can use different smoothness guarantees to protect privacy.",
        "bibtex": "@InProceedings{pmlr-v37-kusnera15,\n  title = \t {Differentially Private Bayesian Optimization},\n  author = \t {Kusner, Matt and Gardner, Jacob and Garnett, Roman and Weinberger, Kilian},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {918--927},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/kusnera15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/kusnera15.html},\n  abstract = \t {Bayesian optimization is a powerful tool for fine-tuning the hyper-parameters of a wide variety of machine learning models. The success of machine learning has led practitioners in diverse real-world settings to learn classifiers for practical problems. As machine learning becomes commonplace, Bayesian optimization becomes an attractive method for practitioners to automate the process of classifier hyper-parameter tuning. A key observation is that the data used for tuning models in these settings is often sensitive. Certain data such as genetic predisposition, personal email statistics, and car accident history, if not properly private, may be at risk of being inferred from Bayesian optimization outputs. To address this, we introduce methods for releasing the best hyper-parameters and classifier accuracy privately. Leveraging the strong theoretical guarantees of differential privacy and known Bayesian optimization convergence bounds, we prove that under a GP assumption these private quantities are often near-optimal. Finally, even if this assumption is not satisfied, we can use different smoothness guarantees to protect privacy.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/kusnera15.pdf",
        "supp": "",
        "pdf_size": 274841,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11423023877788293986&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Washington University in St. Louis, 1 Brookings Dr., St. Louis, MO 63130; Washington University in St. Louis, 1 Brookings Dr., St. Louis, MO 63130; Washington University in St. Louis, 1 Brookings Dr., St. Louis, MO 63130; Washington University in St. Louis, 1 Brookings Dr., St. Louis, MO 63130",
        "aff_domain": "WUSTL.EDU;WUSTL.EDU;WUSTL.EDU;WUSTL.EDU",
        "email": "WUSTL.EDU;WUSTL.EDU;WUSTL.EDU;WUSTL.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Washington University in St. Louis",
        "aff_unique_dep": "",
        "aff_unique_url": "https://wustl.edu",
        "aff_unique_abbr": "WUSTL",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "St. Louis",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c616dc4cec",
        "title": "Discovering Temporal Causal Relations from Subsampled Data",
        "site": "https://proceedings.mlr.press/v37/gongb15.html",
        "author": "Mingming Gong; Kun Zhang; Bernhard Schoelkopf; Dacheng Tao; Philipp Geiger",
        "abstract": "Granger causal analysis has been an important tool for causal analysis for time series in various fields, including neuroscience and economics, and recently it has been extended to include instantaneous effects between the time series to explain the contemporaneous dependence in the residuals. In this paper, we assume that the time series at the true causal frequency follow the vector autoregressive model. We show that when the data resolution becomes lower due to subsampling, neither the original Granger causal analysis nor the extended one is able to discover the underlying causal relations. We then aim to answer the following question: can we estimate the temporal causal relations at the right causal frequency from the subsampled data? Traditionally this suffers from the identifiability problems: under the Gaussianity assumption of the data, the solutions are generally not unique. We prove that, however, if the noise terms are non-Gaussian, the underlying model for the high frequency data is identifiable from subsampled data under mild conditions. We then propose an Expectation-Maximization (EM) approach and a variational inference approach to recover temporal causal relations from such subsampled data. Experimental results on both simulated and real data are reported to illustrate the performance of the proposed approaches.",
        "bibtex": "@InProceedings{pmlr-v37-gongb15,\n  title = \t {Discovering Temporal Causal Relations from Subsampled Data},\n  author = \t {Gong, Mingming and Zhang, Kun and Schoelkopf, Bernhard and Tao, Dacheng and Geiger, Philipp},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1898--1906},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/gongb15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/gongb15.html},\n  abstract = \t {Granger causal analysis has been an important tool for causal analysis for time series in various fields, including neuroscience and economics, and recently it has been extended to include instantaneous effects between the time series to explain the contemporaneous dependence in the residuals. In this paper, we assume that the time series at the true causal frequency follow the vector autoregressive model. We show that when the data resolution becomes lower due to subsampling, neither the original Granger causal analysis nor the extended one is able to discover the underlying causal relations. We then aim to answer the following question: can we estimate the temporal causal relations at the right causal frequency from the subsampled data? Traditionally this suffers from the identifiability problems: under the Gaussianity assumption of the data, the solutions are generally not unique. We prove that, however, if the noise terms are non-Gaussian, the underlying model for the high frequency data is identifiable from subsampled data under mild conditions. We then propose an Expectation-Maximization (EM) approach and a variational inference approach to recover temporal causal relations from such subsampled data. Experimental results on both simulated and real data are reported to illustrate the performance of the proposed approaches.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/gongb15.pdf",
        "supp": "",
        "pdf_size": 695488,
        "gs_citation": 110,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5229466014222980256&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Centre for Quantum Computation and Intelligent Systems, FEIT, University of Technology, Sydney, NSW, Australia; Max Plank Institute for Intelligent Systems, T\u00fcbingen 72076, Germany+Information Sciences Institute, University of Southern California; Max Plank Institute for Intelligent Systems, T\u00fcbingen 72076, Germany; Centre for Quantum Computation and Intelligent Systems, FEIT, University of Technology, Sydney, NSW, Australia; Max Plank Institute for Intelligent Systems, T\u00fcbingen 72076, Germany",
        "aff_domain": "student.uts.edu.au;tuebingen.mpg.de;tuebingen.mpg.de;uts.edu.au;tuebingen.mpg.de",
        "email": "student.uts.edu.au;tuebingen.mpg.de;tuebingen.mpg.de;uts.edu.au;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2;1;0;1",
        "aff_unique_norm": "University of Technology Sydney;Max Planck Institute for Intelligent Systems;University of Southern California",
        "aff_unique_dep": "Centre for Quantum Computation and Intelligent Systems;;Information Sciences Institute",
        "aff_unique_url": "https://www.uts.edu.au;https://www.mpi-iis.mpg.de;https://www.usc.edu",
        "aff_unique_abbr": "UTS;MPI-IS;USC",
        "aff_campus_unique_index": "0;1+2;1;0;1",
        "aff_campus_unique": "Sydney;T\u00fcbingen;Los Angeles",
        "aff_country_unique_index": "0;1+2;1;0;1",
        "aff_country_unique": "Australia;Germany;United States"
    },
    {
        "id": "71eb24dda8",
        "title": "Distributed Box-Constrained Quadratic Optimization for Dual Linear SVM",
        "site": "https://proceedings.mlr.press/v37/leea15.html",
        "author": "Ching-Pei Lee; Dan Roth",
        "abstract": "Training machine learning models sometimes needs to be done on large amounts of data that exceed the capacity of a single machine, motivating recent works on developing algorithms that train in a distributed fashion. This paper proposes an efficient box-constrained quadratic optimization algorithm for distributedly training linear support vector machines (SVMs) with large data. Our key technical contribution is an analytical solution to the problem of computing the optimal step size at each iteration, using an efficient method that requires only O(1) communication cost to ensure fast convergence. With this optimal step size, our approach is superior to other methods by possessing global linear convergence, or, equivalently, O(\\log(1/\u03b5)) iteration complexity for an epsilon-accurate solution, for distributedly solving the non-strongly-convex linear SVM dual problem. Experiments also show that our method is significantly faster than state-of- the-art distributed linear SVM algorithms including DSVM-AVE, DisDCA and TRON.",
        "bibtex": "@InProceedings{pmlr-v37-leea15,\n  title = \t {Distributed Box-Constrained Quadratic Optimization for Dual Linear SVM},\n  author = \t {Lee, Ching-Pei and Roth, Dan},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {987--996},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/leea15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/leea15.html},\n  abstract = \t {Training machine learning models sometimes needs to be done on large amounts of data that exceed the capacity of a single machine, motivating recent works on developing algorithms that train in a distributed fashion. This paper proposes an efficient box-constrained quadratic optimization algorithm for distributedly training linear support vector machines (SVMs) with large data. Our key technical contribution is an analytical solution to the problem of computing the optimal step size at each iteration, using an efficient method that requires only O(1) communication cost to ensure fast convergence. With this optimal step size, our approach is superior to other methods by possessing global linear convergence, or, equivalently, O(\\log(1/\u03b5)) iteration complexity for an epsilon-accurate solution, for distributedly solving the non-strongly-convex linear SVM dual problem. Experiments also show that our method is significantly faster than state-of- the-art distributed linear SVM algorithms including DSVM-AVE, DisDCA and TRON.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/leea15.pdf",
        "supp": "",
        "pdf_size": 457892,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2571492043820279668&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign",
        "aff_domain": "gmail.com;illinois.edu",
        "email": "gmail.com;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign",
        "aff_unique_dep": "",
        "aff_unique_url": "https://illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Urbana-Champaign",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "217dea8fdb",
        "title": "Distributed Estimation of Generalized Matrix Rank: Efficient Algorithms and Lower Bounds",
        "site": "https://proceedings.mlr.press/v37/zhangc15.html",
        "author": "Yuchen Zhang; Martin Wainwright; Michael Jordan",
        "abstract": "We study the following generalized matrix rank estimation problem: given an n-by-n matrix and a constant c > 0, estimate the number of eigenvalues that are greater than c. In the distributed setting, the matrix of interest is the sum of m matrices held by separate machines. We show that any deterministic algorithm solving this problem must communicate \u03a9(n^2) bits, which is order-equivalent to transmitting the whole matrix. In contrast, we propose a randomized algorithm that communicates only O(n) bits. The upper bound is matched by an \u03a9(n) lower bound on the randomized communication complexity. We demonstrate the practical effectiveness of the proposed algorithm with some numerical experiments.",
        "bibtex": "@InProceedings{pmlr-v37-zhangc15,\n  title = \t {Distributed Estimation of Generalized Matrix Rank: Efficient Algorithms and Lower Bounds},\n  author = \t {Zhang, Yuchen and Wainwright, Martin and Jordan, Michael},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {457--465},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/zhangc15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/zhangc15.html},\n  abstract = \t {We study the following generalized matrix rank estimation problem: given an n-by-n matrix and a constant c > 0, estimate the number of eigenvalues that are greater than c. In the distributed setting, the matrix of interest is the sum of m matrices held by separate machines. We show that any deterministic algorithm solving this problem must communicate \u03a9(n^2) bits, which is order-equivalent to transmitting the whole matrix. In contrast, we propose a randomized algorithm that communicates only O(n) bits. The upper bound is matched by an \u03a9(n) lower bound on the randomized communication complexity. We demonstrate the practical effectiveness of the proposed algorithm with some numerical experiments.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/zhangc15.pdf",
        "supp": "",
        "pdf_size": 305409,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14405543891580083005&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "University of California, Berkeley, CA 94720, USA; University of California, Berkeley, CA 94720, USA; University of California, Berkeley, CA 94720, USA",
        "aff_domain": "EECS.BERKELEY.EDU;STAT.BERKELEY.EDU;CS.BERKELEY.EDU",
        "email": "EECS.BERKELEY.EDU;STAT.BERKELEY.EDU;CS.BERKELEY.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "54a56918f7",
        "title": "Distributed Gaussian Processes",
        "site": "https://proceedings.mlr.press/v37/deisenroth15.html",
        "author": "Marc Deisenroth; Jun Wei Ng",
        "abstract": "To scale Gaussian processes (GPs) to large data sets we introduce the robust Bayesian Committee Machine (rBCM), a practical and scalable product-of-experts model for large-scale distributed GP regression. Unlike state-of-the-art sparse GP approximations, the rBCM is conceptually simple and does not rely on inducing or variational parameters. The key idea is to recursively distribute computations to independent computational units and, subsequently, recombine them to form an overall result. Efficient closed-form inference allows for straightforward parallelisation and distributed computations with a small memory footprint. The rBCM is independent of the computational graph and can be used on heterogeneous computing infrastructures, ranging from laptops to clusters. With sufficient computing resources our distributed GP model can handle arbitrarily large data sets.",
        "bibtex": "@InProceedings{pmlr-v37-deisenroth15,\n  title = \t {Distributed Gaussian Processes},\n  author = \t {Deisenroth, Marc and Ng, Jun Wei},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1481--1490},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/deisenroth15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/deisenroth15.html},\n  abstract = \t {To scale Gaussian processes (GPs) to large data sets we introduce the robust Bayesian Committee Machine (rBCM), a practical and scalable product-of-experts model for large-scale distributed GP regression. Unlike state-of-the-art sparse GP approximations, the rBCM is conceptually simple and does not rely on inducing or variational parameters. The key idea is to recursively distribute computations to independent computational units and, subsequently, recombine them to form an overall result. Efficient closed-form inference allows for straightforward parallelisation and distributed computations with a small memory footprint. The rBCM is independent of the computational graph and can be used on heterogeneous computing infrastructures, ranging from laptops to clusters. With sufficient computing resources our distributed GP model can handle arbitrarily large data sets.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/deisenroth15.pdf",
        "supp": "",
        "pdf_size": 641050,
        "gs_citation": 467,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12328519706788425295&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 21,
        "aff": "Department of Computing, Imperial College London, United Kingdom; Department of Computing, Imperial College London, United Kingdom",
        "aff_domain": "IMPERIAL.AC.UK;ALUMNI.IMPERIAL.AC.UK",
        "email": "IMPERIAL.AC.UK;ALUMNI.IMPERIAL.AC.UK",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Imperial College London",
        "aff_unique_dep": "Department of Computing",
        "aff_unique_url": "https://www.imperial.ac.uk",
        "aff_unique_abbr": "Imperial",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "95fd7d3fb4",
        "title": "Distributed Inference for Dirichlet Process Mixture Models",
        "site": "https://proceedings.mlr.press/v37/gea15.html",
        "author": "Hong Ge; Yutian Chen; Moquan Wan; Zoubin Ghahramani",
        "abstract": "Bayesian nonparametric mixture models based on the Dirichlet process (DP) have been widely used for solving problems like clustering, density estimation and topic modelling. These models make weak assumptions about the underlying process that generated the observed data. Thus, when more data are collected, the complexity of these models can change accordingly. These theoretical properties often lead to superior predictive performance when compared to traditional finite mixture models. However, despite the increasing amount of data available, the application of Bayesian nonparametric mixture models is so far limited to relatively small data sets. In this paper, we propose an efficient distributed inference algorithm for the DP and the HDP mixture model. The proposed method is based on a variant of the slice sampler for DPs. Since this sampler does not involve a pre-determined truncation, the stationary distribution of the sampling algorithm is unbiased. We provide both local thread-level and distributed machine-level parallel implementations and study the performance of this sampler through an extensive set of experiments on image and text data. When compared to existing inference algorithms, the proposed method exhibits state-of-the-art accuracy and strong scalability with up to 512 cores.",
        "bibtex": "@InProceedings{pmlr-v37-gea15,\n  title = \t {Distributed Inference for Dirichlet Process Mixture Models},\n  author = \t {Ge, Hong and Chen, Yutian and Wan, Moquan and Ghahramani, Zoubin},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2276--2284},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/gea15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/gea15.html},\n  abstract = \t {Bayesian nonparametric mixture models based on the Dirichlet process (DP) have been widely used for solving problems like clustering, density estimation and topic modelling. These models make weak assumptions about the underlying process that generated the observed data. Thus, when more data are collected, the complexity of these models can change accordingly. These theoretical properties often lead to superior predictive performance when compared to traditional finite mixture models. However, despite the increasing amount of data available, the application of Bayesian nonparametric mixture models is so far limited to relatively small data sets. In this paper, we propose an efficient distributed inference algorithm for the DP and the HDP mixture model. The proposed method is based on a variant of the slice sampler for DPs. Since this sampler does not involve a pre-determined truncation, the stationary distribution of the sampling algorithm is unbiased. We provide both local thread-level and distributed machine-level parallel implementations and study the performance of this sampler through an extensive set of experiments on image and text data. When compared to existing inference algorithms, the proposed method exhibits state-of-the-art accuracy and strong scalability with up to 512 cores.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/gea15.pdf",
        "supp": "",
        "pdf_size": 337355,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6046749817431977309&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Engineering, University of Cambridge, Cambridge CB2 1PZ, UK; Department of Engineering, University of Cambridge, Cambridge CB2 1PZ, UK; Department of Engineering, University of Cambridge, Cambridge CB2 1PZ, UK; Department of Engineering, University of Cambridge, Cambridge CB2 1PZ, UK",
        "aff_domain": "CAM.AC.UK;ENG.CAM.AC.UK;CAM.AC.UK;ENG.CAM.AC.UK",
        "email": "CAM.AC.UK;ENG.CAM.AC.UK;CAM.AC.UK;ENG.CAM.AC.UK",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "Department of Engineering",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "b5b42aa257",
        "title": "Distributional Rank Aggregation, and an Axiomatic Analysis",
        "site": "https://proceedings.mlr.press/v37/prasad15.html",
        "author": "Adarsh Prasad; Harsh Pareek; Pradeep Ravikumar",
        "abstract": "The rank aggregation problem has been studied with varying desiderata in varied communities such as Theoretical Computer Science, Statistics, Information Retrieval and Social Welfare Theory. We introduce a variant of this problem we call distributional rank aggregation, where the ranking data is only available via the induced distribution over the set of all permutations. We provide a novel translation of the usual social welfare theory axioms to this setting. As we show this allows for a more quantitative characterization of these axioms: which then are not only less prone to misinterpretation, but also allow simpler proofs for some key impossibility theorems. Most importantly, these quantitative characterizations lead to natural and novel relaxations of these axioms, which as we show, allow us to get around celebrated impossibility results in social choice theory. We are able to completely characterize the class of positional scoring rules with respect to our axioms and show that Borda Count is optimal in a certain sense.",
        "bibtex": "@InProceedings{pmlr-v37-prasad15,\n  title = \t {Distributional Rank Aggregation, and an Axiomatic Analysis},\n  author = \t {Prasad, Adarsh and Pareek, Harsh and Ravikumar, Pradeep},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2104--2112},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/prasad15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/prasad15.html},\n  abstract = \t {The rank aggregation problem has been studied with varying desiderata in varied communities such as Theoretical Computer Science, Statistics, Information Retrieval and Social Welfare Theory. We introduce a variant of this problem we call distributional rank aggregation, where the ranking data is only available via the induced distribution over the set of all permutations. We provide a novel translation of the usual social welfare theory axioms to this setting. As we show this allows for a more quantitative characterization of these axioms: which then are not only less prone to misinterpretation, but also allow simpler proofs for some key impossibility theorems. Most importantly, these quantitative characterizations lead to natural and novel relaxations of these axioms, which as we show, allow us to get around celebrated impossibility results in social choice theory. We are able to completely characterize the class of positional scoring rules with respect to our axioms and show that Borda Count is optimal in a certain sense.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/prasad15.pdf",
        "supp": "",
        "pdf_size": 375415,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15468380711419378414&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, The University of Texas, Austin, TX 78712, USA; Department of Computer Science, The University of Texas, Austin, TX 78712, USA; Department of Computer Science, The University of Texas, Austin, TX 78712, USA",
        "aff_domain": "cs.utexas.edu;cs.utexas.edu;cs.utexas.edu",
        "email": "cs.utexas.edu;cs.utexas.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6017dd3bc7",
        "title": "Double Nystr\u00f6m Method: An Efficient and Accurate Nystr\u00f6m Scheme for Large-Scale Data Sets",
        "site": "https://proceedings.mlr.press/v37/lima15.html",
        "author": "Woosang Lim; Minhwan Kim; Haesun Park; Kyomin Jung",
        "abstract": "The Nystr\u00f6m method has been one of the most effective techniques for kernel-based approach that scales well to large data sets. Since its introduction, there has been a large body of work that improves the approximation accuracy while maintaining computational efficiency. In this paper, we present a novel Nystr\u00f6m method that improves both accuracy and efficiency based on a new theoretical analysis. We first provide a generalized sampling scheme, CAPS, that minimizes a novel error bound based on the subspace distance. We then present our double Nystr\u00f6m method that reduces the size of the decomposition in two stages. We show that our method is highly efficient and accurate compared to other state-of-the-art Nystr\u00f6m methods by evaluating them on a number of real data sets.",
        "bibtex": "@InProceedings{pmlr-v37-lima15,\n  title = \t {Double Nystr\\\"om Method: An Efficient and Accurate Nystr\\\"om Scheme for Large-Scale Data Sets},\n  author = \t {Lim, Woosang and Kim, Minhwan and Park, Haesun and Jung, Kyomin},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1367--1375},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/lima15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/lima15.html},\n  abstract = \t {The Nystr\u00f6m method has been one of the most effective techniques for kernel-based approach that scales well to large data sets. Since its introduction, there has been a large body of work that improves the approximation accuracy while maintaining computational efficiency. In this paper, we present a novel Nystr\u00f6m method that improves both accuracy and efficiency based on a new theoretical analysis. We first provide a generalized sampling scheme, CAPS, that minimizes a novel error bound based on the subspace distance. We then present our double Nystr\u00f6m method that reduces the size of the decomposition in two stages. We show that our method is highly efficient and accurate compared to other state-of-the-art Nystr\u00f6m methods by evaluating them on a number of real data sets.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/lima15.pdf",
        "supp": "",
        "pdf_size": 480584,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14334031755070809899&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "School of Computing, KAIST, Daejeon, Korea; LG Electronics, Seoul, Korea; School of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, GA 30332, USA; Department of Electrical and Computer Engineering, Seoul National University, Seoul, Korea",
        "aff_domain": "KAIST.AC.KR;LGE.COM;CC.GATECH.EDU;SNU.AC.KR",
        "email": "KAIST.AC.KR;LGE.COM;CC.GATECH.EDU;SNU.AC.KR",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "KAIST;LG;Georgia Institute of Technology;Seoul National University",
        "aff_unique_dep": "School of Computing;LG Electronics;School of Computational Science and Engineering;Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.kaist.ac.kr;https://www.lg.com;https://www.gatech.edu;https://www.snu.ac.kr",
        "aff_unique_abbr": "KAIST;LG;Georgia Tech;SNU",
        "aff_campus_unique_index": "0;2;3",
        "aff_campus_unique": "Daejeon;;Atlanta;Seoul",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "South Korea;United States"
    },
    {
        "id": "bc2efdca96",
        "title": "Dynamic Sensing: Better Classification under Acquisition Constraints",
        "site": "https://proceedings.mlr.press/v37/richman15.html",
        "author": "Oran Richman; Shie Mannor",
        "abstract": "In many machine learning applications the quality of the data is limited by resource constraints (may it be power, bandwidth, memory, ...). In such cases, the constraints are on the average resources allocated, therefore there is some control over each sample\u2019s quality. In most cases this option remains unused and the data\u2019s quality is uniform over the samples. In this paper we propose to actively allocate resources to each sample such that resources are used optimally overall. We propose a method to compute the optimal resource allocation. We further derive generalization bounds for the case where the problem\u2019s model is unknown. We demonstrate the potential benefit of this approach on both simulated and real-life problems.",
        "bibtex": "@InProceedings{pmlr-v37-richman15,\n  title = \t {Dynamic Sensing: Better Classification under Acquisition Constraints},\n  author = \t {Richman, Oran and Mannor, Shie},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {267--275},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/richman15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/richman15.html},\n  abstract = \t {In many machine learning applications the quality of the data is limited by resource constraints (may it be power, bandwidth, memory, ...). In such cases, the constraints are on the average resources allocated, therefore there is some control over each sample\u2019s quality. In most cases this option remains unused and the data\u2019s quality is uniform over the samples. In this paper we propose to actively allocate resources to each sample such that resources are used optimally overall. We propose a method to compute the optimal resource allocation. We further derive generalization bounds for the case where the problem\u2019s model is unknown. We demonstrate the potential benefit of this approach on both simulated and real-life problems.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/richman15.pdf",
        "supp": "",
        "pdf_size": 267162,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18406177191419811945&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Electrical Engineering, Technion - Israel Institute of Technology, Haifa 32000, Israel; Department of Electrical Engineering, Technion - Israel Institute of Technology, Haifa 32000, Israel",
        "aff_domain": "TX.TECHNION.AC.IL;EE.TECHNION.AC.IL",
        "email": "TX.TECHNION.AC.IL;EE.TECHNION.AC.IL",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Technion - Israel Institute of Technology",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.technion.ac.il",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Haifa",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "eb43f913bb",
        "title": "Efficient Learning in Large-Scale Combinatorial Semi-Bandits",
        "site": "https://proceedings.mlr.press/v37/wen15.html",
        "author": "Zheng Wen; Branislav Kveton; Azin Ashkan",
        "abstract": "A stochastic combinatorial semi-bandit is an online learning problem where at each step a learning agent chooses a subset of ground items subject to combinatorial constraints, and then observes stochastic weights of these items and receives their sum as a payoff. In this paper, we consider efficient learning in large-scale combinatorial semi-bandits with linear generalization, and as a solution, propose two learning algorithms called Combinatorial Linear Thompson Sampling (CombLinTS) and Combinatorial Linear UCB (CombLinUCB). Both algorithms are computationally efficient as long as the offline version of the combinatorial problem can be solved efficiently. We establish that CombLinTS and CombLinUCB are also provably statistically efficient under reasonable assumptions, by developing regret bounds that are independent of the problem scale (number of items) and sublinear in time. We also evaluate CombLinTS on a variety of problems with thousands of items. Our experiment results demonstrate that CombLinTS is scalable, robust to the choice of algorithm parameters, and significantly outperforms the best of our baselines.",
        "bibtex": "@InProceedings{pmlr-v37-wen15,\n  title = \t {Efficient Learning in Large-Scale Combinatorial Semi-Bandits},\n  author = \t {Wen, Zheng and Kveton, Branislav and Ashkan, Azin},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1113--1122},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/wen15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/wen15.html},\n  abstract = \t {A stochastic combinatorial semi-bandit is an online learning problem where at each step a learning agent chooses a subset of ground items subject to combinatorial constraints, and then observes stochastic weights of these items and receives their sum as a payoff. In this paper, we consider efficient learning in large-scale combinatorial semi-bandits with linear generalization, and as a solution, propose two learning algorithms called Combinatorial Linear Thompson Sampling (CombLinTS) and Combinatorial Linear UCB (CombLinUCB). Both algorithms are computationally efficient as long as the offline version of the combinatorial problem can be solved efficiently. We establish that CombLinTS and CombLinUCB are also provably statistically efficient under reasonable assumptions, by developing regret bounds that are independent of the problem scale (number of items) and sublinear in time. We also evaluate CombLinTS on a variety of problems with thousands of items. Our experiment results demonstrate that CombLinTS is scalable, robust to the choice of algorithm parameters, and significantly outperforms the best of our baselines.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/wen15.pdf",
        "supp": "",
        "pdf_size": 505020,
        "gs_citation": 125,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9485295956602794558&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Yahoo Labs, Sunnyvale, CA; Adobe Research, San Jose, CA; Technicolor Research, Los Altos, CA",
        "aff_domain": "yahoo-inc.com;adobe.com;technicolor.com",
        "email": "yahoo-inc.com;adobe.com;technicolor.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Yahoo;Adobe;Technicolor",
        "aff_unique_dep": "Yahoo Labs;Adobe Research;Research",
        "aff_unique_url": "https://yahoo.com;https://research.adobe.com;https://www.technicolor.com",
        "aff_unique_abbr": "Yahoo Labs;Adobe;",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "Sunnyvale;San Jose;Los Altos",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4399ad6500",
        "title": "Efficient Training of LDA on a GPU by Mean-for-Mode Estimation",
        "site": "https://proceedings.mlr.press/v37/tristan15.html",
        "author": "Jean-Baptiste Tristan; Joseph Tassarotti; Guy Steele",
        "abstract": "We introduce Mean-for-Mode estimation, a variant of an uncollapsed Gibbs sampler that we use to train LDA on a GPU. The algorithm combines benefits of both uncollapsed and collapsed Gibbs samplers. Like a collapsed Gibbs sampler \u2014 and unlike an uncollapsed Gibbs sampler \u2014 it has good statistical performance, and can use sampling complexity reduction techniques such as sparsity. Meanwhile, like an uncollapsed Gibbs sampler \u2014 and unlike a collapsed Gibbs sampler \u2014 it is embarrassingly parallel, and can use approximate counters.",
        "bibtex": "@InProceedings{pmlr-v37-tristan15,\n  title = \t {Efficient Training of LDA on a GPU by Mean-for-Mode Estimation},\n  author = \t {Tristan, Jean-Baptiste and Tassarotti, Joseph and Steele, Guy},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {59--68},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/tristan15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/tristan15.html},\n  abstract = \t {We introduce Mean-for-Mode estimation, a variant of an uncollapsed Gibbs sampler that we use to train LDA on a GPU. The algorithm combines benefits of both uncollapsed and collapsed Gibbs samplers. Like a collapsed Gibbs sampler \u2014 and unlike an uncollapsed Gibbs sampler \u2014 it has good statistical performance, and can use sampling complexity reduction techniques such as sparsity. Meanwhile, like an uncollapsed Gibbs sampler \u2014 and unlike a collapsed Gibbs sampler \u2014 it is embarrassingly parallel, and can use approximate counters.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/tristan15.pdf",
        "supp": "",
        "pdf_size": 364154,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5381372716869087594&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Oracle Labs, USA; Department of Computer Science, Carnegie Mellon University, USA; Oracle Labs, USA",
        "aff_domain": "ORACLE.COM;CS.CMU.EDU;ORACLE.COM",
        "email": "ORACLE.COM;CS.CMU.EDU;ORACLE.COM",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Oracle Labs;Carnegie Mellon University",
        "aff_unique_dep": ";Department of Computer Science",
        "aff_unique_url": "https://labs.oracle.com;https://www.cmu.edu",
        "aff_unique_abbr": "Oracle Labs;CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d17c5d5116",
        "title": "Enabling scalable stochastic gradient-based inference for Gaussian processes by employing the Unbiased LInear System SolvEr (ULISSE)",
        "site": "https://proceedings.mlr.press/v37/filippone15.html",
        "author": "Maurizio Filippone; Raphael Engler",
        "abstract": "In applications of Gaussian processes where quantification of uncertainty is of primary interest, it is necessary to accurately characterize the posterior distribution over covariance parameters. This paper proposes an adaptation of the Stochastic Gradient Langevin Dynamics algorithm to draw samples from the posterior distribution over covariance parameters with negligible bias and without the need to compute the marginal likelihood. In Gaussian process regression, this has the enormous advantage that stochastic gradients can be computed by solving linear systems only. A novel unbiased linear systems solver based on parallelizable covariance matrix-vector products is developed to accelerate the unbiased estimation of gradients. The results demonstrate the possibility to enable scalable and exact (in a Monte Carlo sense) quantification of uncertainty in Gaussian processes without imposing any special structure on the covariance or reducing the number of input vectors.",
        "bibtex": "@InProceedings{pmlr-v37-filippone15,\n  title = \t {Enabling scalable stochastic gradient-based inference for Gaussian processes by employing the Unbiased LInear System SolvEr (ULISSE)},\n  author = \t {Filippone, Maurizio and Engler, Raphael},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1015--1024},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/filippone15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/filippone15.html},\n  abstract = \t {In applications of Gaussian processes where quantification of uncertainty is of primary interest, it is necessary to accurately characterize the posterior distribution over covariance parameters. This paper proposes an adaptation of the Stochastic Gradient Langevin Dynamics algorithm to draw samples from the posterior distribution over covariance parameters with negligible bias and without the need to compute the marginal likelihood. In Gaussian process regression, this has the enormous advantage that stochastic gradients can be computed by solving linear systems only. A novel unbiased linear systems solver based on parallelizable covariance matrix-vector products is developed to accelerate the unbiased estimation of gradients. The results demonstrate the possibility to enable scalable and exact (in a Monte Carlo sense) quantification of uncertainty in Gaussian processes without imposing any special structure on the covariance or reducing the number of input vectors.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/filippone15.pdf",
        "supp": "",
        "pdf_size": 531454,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12064449647885342725&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "School of Computing Science, University of Glasgow, UK; School of Computing Science, University of Glasgow, UK",
        "aff_domain": "GLASGOW.AC.UK;WEB.DE",
        "email": "GLASGOW.AC.UK;WEB.DE",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Glasgow",
        "aff_unique_dep": "School of Computing Science",
        "aff_unique_url": "https://www.gla.ac.uk",
        "aff_unique_abbr": "UofG",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "05b04d8f35",
        "title": "Entropic Graph-based Posterior Regularization",
        "site": "https://proceedings.mlr.press/v37/libbrecht15.html",
        "author": "Maxwell Libbrecht; Michael Hoffman; Jeff Bilmes; William Noble",
        "abstract": "Graph smoothness objectives have achieved great success in semi-supervised learning but have not yet been applied extensively to unsupervised generative models. We define a new class of entropic graph-based posterior regularizers that augment a probabilistic model by encouraging pairs of nearby variables in a regularization graph to have similar posterior distributions. We present a three-way alternating optimization algorithm with closed-form updates for performing inference on this joint model and learning its parameters. This method admits updates linear in the degree of the regularization graph, exhibits monotone convergence and is easily parallelizable. We are motivated by applications in computational biology in which temporal models such as hidden Markov models are used to learn a human-interpretable representation of genomic data. On a synthetic problem, we show that our method outperforms existing methods for graph-based regularization and a comparable strategy for incorporating long-range interactions using existing methods for approximate inference. Using genome-scale functional genomics data, we integrate genome 3D interaction data into existing models for genome annotation and demonstrate significant improvements in predicting genomic activity.",
        "bibtex": "@InProceedings{pmlr-v37-libbrecht15,\n  title = \t {Entropic Graph-based Posterior Regularization},\n  author = \t {Libbrecht, Maxwell and Hoffman, Michael and Bilmes, Jeff and Noble, William},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1992--2001},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/libbrecht15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/libbrecht15.html},\n  abstract = \t {Graph smoothness objectives have achieved great success in semi-supervised learning but have not yet been applied extensively to unsupervised generative models. We define a new class of entropic graph-based posterior regularizers that augment a probabilistic model by encouraging pairs of nearby variables in a regularization graph to have similar posterior distributions. We present a three-way alternating optimization algorithm with closed-form updates for performing inference on this joint model and learning its parameters. This method admits updates linear in the degree of the regularization graph, exhibits monotone convergence and is easily parallelizable. We are motivated by applications in computational biology in which temporal models such as hidden Markov models are used to learn a human-interpretable representation of genomic data. On a synthetic problem, we show that our method outperforms existing methods for graph-based regularization and a comparable strategy for incorporating long-range interactions using existing methods for approximate inference. Using genome-scale functional genomics data, we integrate genome 3D interaction data into existing models for genome annotation and demonstrate significant improvements in predicting genomic activity.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/libbrecht15.pdf",
        "supp": "",
        "pdf_size": 538535,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2437066990254887799&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Genome Sciences, University of Washington; Princess Margaret Cancer Centre; Department of Electrical Engineering, University of Washington; Genome Sciences, University of Washington",
        "aff_domain": "CS.WASHINGTON.EDU;UTORONTO.CA;EE.WASHINGTON.EDU;U.WASHINGTON.EDU",
        "email": "CS.WASHINGTON.EDU;UTORONTO.CA;EE.WASHINGTON.EDU;U.WASHINGTON.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "University of Washington;Princess Margaret Cancer Centre",
        "aff_unique_dep": "Genome Sciences;",
        "aff_unique_url": "https://www.washington.edu;https://www.pmcc.ca",
        "aff_unique_abbr": "UW;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Seattle;",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "960090582d",
        "title": "Entropy evaluation based on confidence intervals of frequency estimates : Application to the learning of decision trees",
        "site": "https://proceedings.mlr.press/v37/serrurier15.html",
        "author": "Mathieu Serrurier; Henri Prade",
        "abstract": "Entropy gain is widely used for learning decision trees. However, as we go deeper downward the tree, the examples become rarer and the faithfulness of entropy decreases. Thus, misleading choices and over-fitting may occur and the tree has to be adjusted by using an early-stop criterion or post pruning algorithms. However, these methods still depends on the choices previously made, which may be unsatisfactory. We propose a new cumulative entropy function based on confidence intervals on frequency estimates that together considers the entropy of the probability distribution and the uncertainty around the estimation of its parameters. This function takes advantage of the ability of a possibility distribution to upper bound a family of probabilities previously estimated from a limited set of examples and of the link between possibilistic specificity order and entropy. The proposed measure has several advantages over the classical one. It performs significant choices of split and provides a statistically relevant stopping criterion that allows the learning of trees whose size is well-suited w.r.t. the available data. On the top of that, it also provides a reasonable estimator of the performances of a decision tree. Finally, we show that it can be used for designing a simple and efficient online learning algorithm.",
        "bibtex": "@InProceedings{pmlr-v37-serrurier15,\n  title = \t {Entropy evaluation based on confidence intervals of frequency estimates : Application to the learning of decision trees},\n  author = \t {Serrurier, Mathieu and Prade, Henri},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1576--1584},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/serrurier15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/serrurier15.html},\n  abstract = \t {Entropy gain is widely used for learning decision trees. However, as we go deeper downward the tree, the examples become rarer and the faithfulness of entropy decreases. Thus, misleading choices and over-fitting may occur and the tree has to be adjusted by using an early-stop criterion or post pruning algorithms. However, these methods still depends on the choices previously made, which may be unsatisfactory. We propose a new cumulative entropy function based on confidence intervals on frequency estimates that together considers the entropy of the probability distribution and the uncertainty around the estimation of its parameters. This function takes advantage of the ability of a possibility distribution to upper bound a family of probabilities previously estimated from a limited set of examples and of the link between possibilistic specificity order and entropy. The proposed measure has several advantages over the classical one. It performs significant choices of split and provides a statistically relevant stopping criterion that allows the learning of trees whose size is well-suited w.r.t. the available data. On the top of that, it also provides a reasonable estimator of the performances of a decision tree. Finally, we show that it can be used for designing a simple and efficient online learning algorithm.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/serrurier15.pdf",
        "supp": "",
        "pdf_size": 2438554,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10705699986223076165&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "IRIT - Universit \u00b4e Paul Sabatier 118 route de Narbonne 31062, Toulouse Cedex 9, France; IRIT - Universit \u00b4e Paul Sabatier, Toulouse, France + QCIS, University of Technology, Sydney, Australia",
        "aff_domain": "IRIT.FR;IRIT.FR",
        "email": "IRIT.FR;IRIT.FR",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "Universit\u00e9 Paul Sabatier;University of Technology Sydney",
        "aff_unique_dep": "IRIT;QCIS",
        "aff_unique_url": "https://www.univ-tlse3.fr;https://www.uts.edu.au",
        "aff_unique_abbr": "UPS;UTS",
        "aff_campus_unique_index": "0;0+1",
        "aff_campus_unique": "Toulouse;Sydney",
        "aff_country_unique_index": "0;0+1",
        "aff_country_unique": "France;Australia"
    },
    {
        "id": "bec39603e2",
        "title": "Entropy-Based Concentration Inequalities for Dependent Variables",
        "site": "https://proceedings.mlr.press/v37/ralaivola15.html",
        "author": "Liva Ralaivola; Massih-Reza Amini",
        "abstract": "We provide new concentration inequalities for functions of dependent variables. The work extends that of Janson (2004), which proposes concentration inequalities using a combination of the Laplace transform and the idea of fractional graph coloring, as well as many works that derive concentration inequalities using the entropy method (see, e.g., (Boucheron et al., 2003)). We give inequalities for fractionally sub-additive and fractionally self-bounding functions. In the way, we prove a new Talagrand concentration inequality for fractionally sub-additive functions of dependent variables. The results allow us to envision the derivation of generalization bounds for various applications where dependent variables naturally appear, such as in bipartite ranking.",
        "bibtex": "@InProceedings{pmlr-v37-ralaivola15,\n  title = \t {Entropy-Based Concentration Inequalities for Dependent Variables},\n  author = \t {Ralaivola, Liva and Amini, Massih-Reza},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2436--2444},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/ralaivola15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/ralaivola15.html},\n  abstract = \t {We provide new concentration inequalities for functions of dependent variables. The work extends that of Janson (2004), which proposes concentration inequalities using a combination of the Laplace transform and the idea of fractional graph coloring, as well as many works that derive concentration inequalities using the entropy method (see, e.g., (Boucheron et al., 2003)). We give inequalities for fractionally sub-additive and fractionally self-bounding functions. In the way, we prove a new Talagrand concentration inequality for fractionally sub-additive functions of dependent variables. The results allow us to envision the derivation of generalization bounds for various applications where dependent variables naturally appear, such as in bipartite ranking.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/ralaivola15.pdf",
        "supp": "",
        "pdf_size": 318404,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15540520203727985995&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "QARMA, LIF, CNRS, Aix-Marseille University, F\u201313288 Marseille cedex 9, France; AMA, LIG, CNRS, University Grenoble Alpes, Centre Equation 4, BP 53, F\u201338041 Grenoble Cedex 9, France",
        "aff_domain": "LIF.UNIV-MRS.FR;IMAG.FR",
        "email": "LIF.UNIV-MRS.FR;IMAG.FR",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Aix-Marseille University;University Grenoble Alpes",
        "aff_unique_dep": "QARMA, LIF, CNRS;Centre Equation 4",
        "aff_unique_url": "https://www.univ-amu.fr;https://www.univ-grenoble-alpes.fr",
        "aff_unique_abbr": "AMU;UGA",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Marseille;Grenoble",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "61af547745",
        "title": "Exponential Integration for Hamiltonian Monte Carlo",
        "site": "https://proceedings.mlr.press/v37/chao15.html",
        "author": "Wei-Lun Chao; Justin Solomon; Dominik Michels; Fei Sha",
        "abstract": "We investigate numerical integration of ordinary differential equations (ODEs) for Hamiltonian Monte Carlo (HMC). High-quality integration is crucial for designing efficient and effective proposals for HMC. While the standard method is leapfrog (Stormer-Verlet) integration, we propose the use of an exponential integrator, which is robust to stiff ODEs with highly-oscillatory components. This oscillation is difficult to reproduce using leapfrog integration, even with carefully selected integration parameters and preconditioning. Concretely, we use a Gaussian distribution approximation to segregate stiff components of the ODE. We integrate this term analytically for stability and account for deviation from the approximation using variation of constants. We consider various ways to derive Gaussian approximations and conduct extensive empirical studies applying the proposed \u201cexponential HMC\u201d to several benchmarked learning problems. We compare to state-of-the-art methods for improving leapfrog HMC and demonstrate the advantages of our method in generating many effective samples with high acceptance rates in short running times.",
        "bibtex": "@InProceedings{pmlr-v37-chao15,\n  title = \t {Exponential Integration for Hamiltonian Monte Carlo},\n  author = \t {Chao, Wei-Lun and Solomon, Justin and Michels, Dominik and Sha, Fei},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1142--1151},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/chao15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/chao15.html},\n  abstract = \t {We investigate numerical integration of ordinary differential equations (ODEs) for Hamiltonian Monte Carlo (HMC). High-quality integration is crucial for designing efficient and effective proposals for HMC. While the standard method is leapfrog (Stormer-Verlet) integration, we propose the use of an exponential integrator, which is robust to stiff ODEs with highly-oscillatory components. This oscillation is difficult to reproduce using leapfrog integration, even with carefully selected integration parameters and preconditioning. Concretely, we use a Gaussian distribution approximation to segregate stiff components of the ODE. We integrate this term analytically for stability and account for deviation from the approximation using variation of constants. We consider various ways to derive Gaussian approximations and conduct extensive empirical studies applying the proposed \u201cexponential HMC\u201d to several benchmarked learning problems. We compare to state-of-the-art methods for improving leapfrog HMC and demonstrate the advantages of our method in generating many effective samples with high acceptance rates in short running times.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/chao15.pdf",
        "supp": "",
        "pdf_size": 719994,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1469263057063326079&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Science, University of Southern California, Los Angeles, CA 90089; Department of Computer Science, Stanford University, 353 Serra Mall, Stanford, California 94305 USA; Department of Computer Science, Stanford University, 353 Serra Mall, Stanford, California 94305 USA; Department of Computer Science, University of Southern California, Los Angeles, CA 90089",
        "aff_domain": "USC.EDU;STANFORD.EDU;CS.STANFORD.EDU;USC.EDU",
        "email": "USC.EDU;STANFORD.EDU;CS.STANFORD.EDU;USC.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "University of Southern California;Stanford University",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.usc.edu;https://www.stanford.edu",
        "aff_unique_abbr": "USC;Stanford",
        "aff_campus_unique_index": "0;1;1;0",
        "aff_campus_unique": "Los Angeles;Stanford",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f7f5e86c28",
        "title": "Fast Kronecker Inference in Gaussian Processes with non-Gaussian Likelihoods",
        "site": "https://proceedings.mlr.press/v37/flaxman15.html",
        "author": "Seth Flaxman; Andrew Wilson; Daniel Neill; Hannes Nickisch; Alex Smola",
        "abstract": "Gaussian processes (GPs) are a flexible class of methods with state of the art performance on spatial statistics applications. However, GPs require O(n^3) computations and O(n^2) storage, and popular GP kernels are typically limited to smoothing and interpolation. To address these difficulties, Kronecker methods have been used to exploit structure in the GP covariance matrix for scalability, while allowing for expressive kernel learning (Wilson et al., 2014). However, fast Kronecker methods have been confined to Gaussian likelihoods. We propose new scalable Kronecker methods for Gaussian processes with non-Gaussian likelihoods, using a Laplace approximation which involves linear conjugate gradients for inference, and a lower bound on the GP marginal likelihood for kernel learning. Our approach has near linear scaling, requiring O(D n^(D+1)/D) operations and O(D n^2/D) storage, for n training data-points on a dense D > 1 dimensional grid. Moreover, we introduce a log Gaussian Cox process, with highly expressive kernels, for modelling spatiotemporal count processes, and apply it to a point pattern (n = 233,088) of a decade of crime events in Chicago. Using our model, we discover spatially varying multiscale seasonal trends and produce highly accurate long-range local area forecasts.",
        "bibtex": "@InProceedings{pmlr-v37-flaxman15,\n  title = \t {Fast Kronecker Inference in Gaussian Processes with non-Gaussian Likelihoods},\n  author = \t {Flaxman, Seth and Wilson, Andrew and Neill, Daniel and Nickisch, Hannes and Smola, Alex},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {607--616},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/flaxman15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/flaxman15.html},\n  abstract = \t {Gaussian processes (GPs) are a flexible class of methods with state of the art performance on spatial statistics applications. However, GPs require O(n^3) computations and O(n^2) storage, and popular GP kernels are typically limited to smoothing and interpolation. To address these difficulties, Kronecker methods have been used to exploit structure in the GP covariance matrix for scalability, while allowing for expressive kernel learning (Wilson et al., 2014). However, fast Kronecker methods have been confined to Gaussian likelihoods. We propose new scalable Kronecker methods for Gaussian processes with non-Gaussian likelihoods, using a Laplace approximation which involves linear conjugate gradients for inference, and a lower bound on the GP marginal likelihood for kernel learning. Our approach has near linear scaling, requiring O(D n^(D+1)/D) operations and O(D n^2/D) storage, for n training data-points on a dense D > 1 dimensional grid. Moreover, we introduce a log Gaussian Cox process, with highly expressive kernels, for modelling spatiotemporal count processes, and apply it to a point pattern (n = 233,088) of a decade of crime events in Chicago. Using our model, we discover spatially varying multiscale seasonal trends and produce highly accurate long-range local area forecasts.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/flaxman15.pdf",
        "supp": "",
        "pdf_size": 539168,
        "gs_citation": 123,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17351008656514514224&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Philips Research Hamburg; Carnegie Mellon University+Marianas Labs",
        "aff_domain": "CS.CMU.EDU;CS.CMU.EDU;CS.CMU.EDU;NICKISCH.ORG;SMOLA.ORG",
        "email": "CS.CMU.EDU;CS.CMU.EDU;CS.CMU.EDU;NICKISCH.ORG;SMOLA.ORG",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;0+2",
        "aff_unique_norm": "Carnegie Mellon University;Philips Research;Marianas Labs",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.cmu.edu;https://www.philips.com/research;",
        "aff_unique_abbr": "CMU;Philips Research;",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";Hamburg",
        "aff_country_unique_index": "0;0;0;1;0+0",
        "aff_country_unique": "United States;Germany"
    },
    {
        "id": "8adab303e2",
        "title": "Faster Rates for the Frank-Wolfe Method over Strongly-Convex Sets",
        "site": "https://proceedings.mlr.press/v37/garbera15.html",
        "author": "Dan Garber; Elad Hazan",
        "abstract": "The Frank-Wolfe method (a.k.a. conditional gradient algorithm) for smooth optimization has regained much interest in recent years in the context of large scale optimization and machine learning. A key advantage of the method is that it avoids projections - the computational bottleneck in many applications - replacing it by a linear optimization step. Despite this advantage, the known convergence rates of the FW method fall behind standard first order methods for most settings of interest. It is an active line of research to derive faster linear optimization-based algorithms for various settings of convex optimization. In this paper we consider the special case of optimization over strongly convex sets, for which we prove that the vanila FW method converges at a rate of \\frac1t^2. This gives a quadratic improvement in convergence rate compared to the general case, in which convergence is of the order \\frac1t, and known to be tight. We show that various balls induced by \\ell_p norms, Schatten norms and group norms are strongly convex on one hand and on the other hand, linear optimization over these sets is straightforward and admits a closed-form solution. We further show how several previous fast-rate results for the FW method follow easily from our analysis.",
        "bibtex": "@InProceedings{pmlr-v37-garbera15,\n  title = \t {Faster Rates for the Frank-Wolfe Method over Strongly-Convex Sets},\n  author = \t {Garber, Dan and Hazan, Elad},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {541--549},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/garbera15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/garbera15.html},\n  abstract = \t {The Frank-Wolfe method (a.k.a. conditional gradient algorithm) for smooth optimization has regained much interest in recent years in the context of large scale optimization and machine learning. A key advantage of the method is that it avoids projections - the computational bottleneck in many applications - replacing it by a linear optimization step. Despite this advantage, the known convergence rates of the FW method fall behind standard first order methods for most settings of interest. It is an active line of research to derive faster linear optimization-based algorithms for various settings of convex optimization. In this paper we consider the special case of optimization over strongly convex sets, for which we prove that the vanila FW method converges at a rate of \\frac1t^2. This gives a quadratic improvement in convergence rate compared to the general case, in which convergence is of the order \\frac1t, and known to be tight. We show that various balls induced by \\ell_p norms, Schatten norms and group norms are strongly convex on one hand and on the other hand, linear optimization over these sets is straightforward and admits a closed-form solution. We further show how several previous fast-rate results for the FW method follow easily from our analysis.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/garbera15.pdf",
        "supp": "",
        "pdf_size": 588264,
        "gs_citation": 231,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1192277194574172525&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Technion - Israel Institute of Technology; Princeton University",
        "aff_domain": "TX.TECHNION.AC.IL;CS.PRINCETON.EDU",
        "email": "TX.TECHNION.AC.IL;CS.PRINCETON.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Technion - Israel Institute of Technology;Princeton University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.technion.ac.il/en/;https://www.princeton.edu",
        "aff_unique_abbr": "Technion;Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Israel;United States"
    },
    {
        "id": "43c3d556b4",
        "title": "Faster cover trees",
        "site": "https://proceedings.mlr.press/v37/izbicki15.html",
        "author": "Mike Izbicki; Christian Shelton",
        "abstract": "The cover tree data structure speeds up exact nearest neighbor queries over arbitrary metric spaces. This paper makes cover trees even faster. In particular, we provide (1) a simpler definition of the cover tree that reduces the number of nodes from O(n) to exactly n, (2) an additional invariant that makes queries faster in practice, (3) algorithms for constructing and querying the tree in parallel on multiprocessor systems, and (4) a more cache efficient memory layout. On standard benchmark datasets, we reduce the number of distance computations by 10\u201350%. On a large-scale bioinformatics dataset, we reduce the number of distance computations by 71%. On a large-scale image dataset, our parallel algorithm with 16 cores reduces tree construction time from 3.5 hours to 12 minutes.",
        "bibtex": "@InProceedings{pmlr-v37-izbicki15,\n  title = \t {Faster cover trees},\n  author = \t {Izbicki, Mike and Shelton, Christian},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1162--1170},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/izbicki15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/izbicki15.html},\n  abstract = \t {The cover tree data structure speeds up exact nearest neighbor queries over arbitrary metric spaces. This paper makes cover trees even faster. In particular, we provide (1) a simpler definition of the cover tree that reduces the number of nodes from O(n) to exactly n, (2) an additional invariant that makes queries faster in practice, (3) algorithms for constructing and querying the tree in parallel on multiprocessor systems, and (4) a more cache efficient memory layout. On standard benchmark datasets, we reduce the number of distance computations by 10\u201350%. On a large-scale bioinformatics dataset, we reduce the number of distance computations by 71%. On a large-scale image dataset, our parallel algorithm with 16 cores reduces tree construction time from 3.5 hours to 12 minutes.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/izbicki15.pdf",
        "supp": "",
        "pdf_size": 158899,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1010541527517696005&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "University of California Riverside; University of California Riverside",
        "aff_domain": "ucr.edu;cs.ucr.edu",
        "email": "ucr.edu;cs.ucr.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Riverside",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucr.edu",
        "aff_unique_abbr": "UCR",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Riverside",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5b842c68a6",
        "title": "Feature-Budgeted Random Forest",
        "site": "https://proceedings.mlr.press/v37/nan15.html",
        "author": "Feng Nan; Joseph Wang; Venkatesh Saligrama",
        "abstract": "We seek decision rules for \\it prediction-time cost reduction, where complete data is available for training, but during prediction-time, each feature can only be acquired for an additional cost. We propose a novel random forest algorithm to minimize prediction error for a user-specified \\it average feature acquisition budget. While random forests yield strong generalization performance, they do not explicitly account for feature costs and furthermore require low correlation among trees, which amplifies costs. Our random forest grows trees with low acquisition cost and high strength based on greedy minimax cost-weighted-impurity splits. Theoretically, we establish near-optimal acquisition cost guarantees for our algorithm. Empirically, on a number of benchmark datasets we demonstrate competitive accuracy-cost curves against state-of-the-art prediction-time algorithms.",
        "bibtex": "@InProceedings{pmlr-v37-nan15,\n  title = \t {Feature-Budgeted Random Forest},\n  author = \t {Nan, Feng and Wang, Joseph and Saligrama, Venkatesh},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1983--1991},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/nan15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/nan15.html},\n  abstract = \t {We seek decision rules for \\it prediction-time cost reduction, where complete data is available for training, but during prediction-time, each feature can only be acquired for an additional cost. We propose a novel random forest algorithm to minimize prediction error for a user-specified \\it average feature acquisition budget. While random forests yield strong generalization performance, they do not explicitly account for feature costs and furthermore require low correlation among trees, which amplifies costs. Our random forest grows trees with low acquisition cost and high strength based on greedy minimax cost-weighted-impurity splits. Theoretically, we establish near-optimal acquisition cost guarantees for our algorithm. Empirically, on a number of benchmark datasets we demonstrate competitive accuracy-cost curves against state-of-the-art prediction-time algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/nan15.pdf",
        "supp": "",
        "pdf_size": 554646,
        "gs_citation": 82,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2761540775964021696&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Boston University; Boston University; Boston University",
        "aff_domain": "BU.EDU;BU.EDU;BU.EDU",
        "email": "BU.EDU;BU.EDU;BU.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Boston University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.bu.edu",
        "aff_unique_abbr": "BU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6ea7fb56c7",
        "title": "Fictitious Self-Play in Extensive-Form Games",
        "site": "https://proceedings.mlr.press/v37/heinrich15.html",
        "author": "Johannes Heinrich; Marc Lanctot; David Silver",
        "abstract": "Fictitious play is a popular game-theoretic model of learning in games. However, it has received little attention in practical applications to large problems. This paper introduces two variants of fictitious play that are implemented in behavioural strategies of an extensive-form game. The first variant is a full-width process that is realization equivalent to its normal-form counterpart and therefore inherits its convergence guarantees. However, its computational requirements are linear in time and space rather than exponential. The second variant, Fictitious Self-Play, is a machine learning framework that implements fictitious play in a sample-based fashion. Experiments in imperfect-information poker games compare our approaches and demonstrate their convergence to approximate Nash equilibria.",
        "bibtex": "@InProceedings{pmlr-v37-heinrich15,\n  title = \t {Fictitious Self-Play in Extensive-Form Games},\n  author = \t {Heinrich, Johannes and Lanctot, Marc and Silver, David},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {805--813},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/heinrich15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/heinrich15.html},\n  abstract = \t {Fictitious play is a popular game-theoretic model of learning in games. However, it has received little attention in practical applications to large problems. This paper introduces two variants of fictitious play that are implemented in behavioural strategies of an extensive-form game. The first variant is a full-width process that is realization equivalent to its normal-form counterpart and therefore inherits its convergence guarantees. However, its computational requirements are linear in time and space rather than exponential. The second variant, Fictitious Self-Play, is a machine learning framework that implements fictitious play in a sample-based fashion. Experiments in imperfect-information poker games compare our approaches and demonstrate their convergence to approximate Nash equilibria.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/heinrich15.pdf",
        "supp": "",
        "pdf_size": 392176,
        "gs_citation": 449,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13759448641839275825&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "University College London, UK; Google DeepMind, London, UK; Google DeepMind, London, UK",
        "aff_domain": "CS.UCL.AC.UK;GOOGLE.COM;GOOGLE.COM",
        "email": "CS.UCL.AC.UK;GOOGLE.COM;GOOGLE.COM",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University College London;Google",
        "aff_unique_dep": ";Google DeepMind",
        "aff_unique_url": "https://www.ucl.ac.uk;https://deepmind.com",
        "aff_unique_abbr": "UCL;DeepMind",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";London",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "4914f4ce34",
        "title": "Finding Galaxies in the Shadows of Quasars with Gaussian Processes",
        "site": "https://proceedings.mlr.press/v37/garnett15.html",
        "author": "Roman Garnett; Shirley Ho; Jeff Schneider",
        "abstract": "We develop an automated technique for detecting damped Lyman-\u03b1absorbers (DLAs) along spectroscopic sightlines to quasi-stellar objects (QSOs or quasars). The detection of DLAs in large-scale spectroscopic surveys such as SDSS\u2013III is critical to address outstanding cosmological questions, such as the nature of galaxy formation. We use nearly 50000 QSO spectra to learn a tailored Gaussian process model for quasar emission spectra, which we apply to the DLA detection problem via Bayesian model selection. We demonstrate our method\u2019s effectiveness with a large-scale validation experiment on over 100000 spectra, with excellent performance.",
        "bibtex": "@InProceedings{pmlr-v37-garnett15,\n  title = \t {Finding Galaxies in the Shadows of Quasars with Gaussian Processes},\n  author = \t {Garnett, Roman and Ho, Shirley and Schneider, Jeff},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1025--1033},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/garnett15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/garnett15.html},\n  abstract = \t {We develop an automated technique for detecting damped Lyman-\u03b1absorbers (DLAs) along spectroscopic sightlines to quasi-stellar objects (QSOs or quasars). The detection of DLAs in large-scale spectroscopic surveys such as SDSS\u2013III is critical to address outstanding cosmological questions, such as the nature of galaxy formation. We use nearly 50000 QSO spectra to learn a tailored Gaussian process model for quasar emission spectra, which we apply to the DLA detection problem via Bayesian model selection. We demonstrate our method\u2019s effectiveness with a large-scale validation experiment on over 100000 spectra, with excellent performance.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/garnett15.pdf",
        "supp": "",
        "pdf_size": 1089440,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17337654103023790282&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Washington University in St. Louis, St. Louis, MO 63130, United States; Carnegie Mellon University, Pittsburgh, PA 15213, United States; Carnegie Mellon University, Pittsburgh, PA 15213, United States",
        "aff_domain": "WUSTL.EDU;ANDREW.CMU.EDU;CS.CMU.EDU",
        "email": "WUSTL.EDU;ANDREW.CMU.EDU;CS.CMU.EDU",
        "github": "",
        "project": "http://www.sdss.org/",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Washington University in St. Louis;Carnegie Mellon University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://wustl.edu;https://www.cmu.edu",
        "aff_unique_abbr": "WUSTL;CMU",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "St. Louis;Pittsburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "210c15a16f",
        "title": "Finding Linear Structure in Large Datasets with Scalable Canonical Correlation Analysis",
        "site": "https://proceedings.mlr.press/v37/maa15.html",
        "author": "Zhuang Ma; Yichao Lu; Dean Foster",
        "abstract": "Canonical Correlation Analysis (CCA) is a widely used spectral technique for finding correlation structures in multi-view datasets. In this paper, we tackle the problem of large scale CCA, where classical algorithms, usually requiring computing the product of two huge matrices and huge matrix decomposition, are computationally and storage expensive. We recast CCA from a novel perspective and propose a scalable and memory efficient \\textitAugmented Approximate Gradient (AppGrad) scheme for finding top k dimensional canonical subspace which only involves large matrix multiplying a thin matrix of width k and small matrix decomposition of dimension k\\times k. Further, \\textitAppGrad achieves optimal storage complexity O(k(p_1+p_2)), compared with classical algorithms which usually require O(p_1^2+p_2^2) space to store two dense whitening matrices. The proposed scheme naturally generalizes to stochastic optimization regime, especially efficient for huge datasets where batch algorithms are prohibitive. The online property of stochastic \\textitAppGrad is also well suited to the streaming scenario, where data comes sequentially. To the best of our knowledge, it is the first stochastic algorithm for CCA. Experiments on four real data sets are provided to show the effectiveness of the proposed methods.",
        "bibtex": "@InProceedings{pmlr-v37-maa15,\n  title = \t {Finding Linear Structure in Large Datasets with Scalable Canonical Correlation Analysis},\n  author = \t {Ma, Zhuang and Lu, Yichao and Foster, Dean},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {169--178},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/maa15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/maa15.html},\n  abstract = \t {Canonical Correlation Analysis (CCA) is a widely used spectral technique for finding correlation structures in multi-view datasets. In this paper, we tackle the problem of large scale CCA, where classical algorithms, usually requiring computing the product of two huge matrices and huge matrix decomposition, are computationally and storage expensive. We recast CCA from a novel perspective and propose a scalable and memory efficient \\textitAugmented Approximate Gradient (AppGrad) scheme for finding top k dimensional canonical subspace which only involves large matrix multiplying a thin matrix of width k and small matrix decomposition of dimension k\\times k. Further, \\textitAppGrad achieves optimal storage complexity O(k(p_1+p_2)), compared with classical algorithms which usually require O(p_1^2+p_2^2) space to store two dense whitening matrices. The proposed scheme naturally generalizes to stochastic optimization regime, especially efficient for huge datasets where batch algorithms are prohibitive. The online property of stochastic \\textitAppGrad is also well suited to the streaming scenario, where data comes sequentially. To the best of our knowledge, it is the first stochastic algorithm for CCA. Experiments on four real data sets are provided to show the effectiveness of the proposed methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/maa15.pdf",
        "supp": "",
        "pdf_size": 386403,
        "gs_citation": 102,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4912101023191495597&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Statistics, The Wharton School, University of Pennsylvania, Philadelphia, PA 19104 U.S.A; Department of Statistics, The Wharton School, University of Pennsylvania, Philadelphia, PA 19104 U.S.A; Department of Statistics, The Wharton School, University of Pennsylvania, Philadelphia, PA 19104 U.S.A",
        "aff_domain": "WHARTON.UPENN.EDU;WHARTON.UPENN.EDU;FOSTER.NET",
        "email": "WHARTON.UPENN.EDU;WHARTON.UPENN.EDU;FOSTER.NET",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Philadelphia",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "debb9a539c",
        "title": "Fixed-point algorithms for learning determinantal point processes",
        "site": "https://proceedings.mlr.press/v37/mariet15.html",
        "author": "Zelda Mariet; Suvrit Sra",
        "abstract": "Determinantal point processes (DPPs) offer an elegant tool for encoding probabilities over subsets of a ground set. Discrete DPPs are parametrized by a positive semidefinite matrix (called the DPP kernel), and estimating this kernel is key to learning DPPs from observed data. We consider the task of learning the DPP kernel, and develop for it a surprisingly simple yet effective new algorithm. Our algorithm offers the following benefits over previous approaches: (a) it is much simpler; (b) it yields equally good and sometimes even better local maxima; and (c) it runs an order of magnitude faster on large problems. We present experimental results on both real and simulated data to illustrate the numerical performance of our technique.",
        "bibtex": "@InProceedings{pmlr-v37-mariet15,\n  title = \t {Fixed-point algorithms for learning determinantal point processes},\n  author = \t {Mariet, Zelda and Sra, Suvrit},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2389--2397},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/mariet15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/mariet15.html},\n  abstract = \t {Determinantal point processes (DPPs) offer an elegant tool for encoding probabilities over subsets of a ground set. Discrete DPPs are parametrized by a positive semidefinite matrix (called the DPP kernel), and estimating this kernel is key to learning DPPs from observed data. We consider the task of learning the DPP kernel, and develop for it a surprisingly simple yet effective new algorithm. Our algorithm offers the following benefits over previous approaches: (a) it is much simpler; (b) it yields equally good and sometimes even better local maxima; and (c) it runs an order of magnitude faster on large problems. We present experimental results on both real and simulated data to illustrate the numerical performance of our technique.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/mariet15.pdf",
        "supp": "",
        "pdf_size": 362473,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9159476369485574433&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Massachusetts Institute of Technology; Massachusetts Institute of Technology",
        "aff_domain": "CSAIL.MIT.EDU;MIT.EDU",
        "email": "CSAIL.MIT.EDU;MIT.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ad5e43416c",
        "title": "Following the Perturbed Leader for Online Structured Learning",
        "site": "https://proceedings.mlr.press/v37/cohena15.html",
        "author": "Alon Cohen; Tamir Hazan",
        "abstract": "We investigate a new Follow the Perturbed Leader (FTPL) algorithm for online structured prediction problems. We show a regret bound which is comparable to the state of the art of FTPL algorithms and is comparable with the best possible regret in some cases. To better understand FTPL algorithms for online structured learning, we present a lower bound on the regret for a large and natural class of FTPL algorithms that use logconcave perturbations. We complete our investigation with an online shortest path experiment and empirically show that our algorithm is both statistically and computationally efficient.",
        "bibtex": "@InProceedings{pmlr-v37-cohena15,\n  title = \t {Following the Perturbed Leader for Online Structured Learning},\n  author = \t {Cohen, Alon and Hazan, Tamir},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1034--1042},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/cohena15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/cohena15.html},\n  abstract = \t {We investigate a new Follow the Perturbed Leader (FTPL) algorithm for online structured prediction problems. We show a regret bound which is comparable to the state of the art of FTPL algorithms and is comparable with the best possible regret in some cases. To better understand FTPL algorithms for online structured learning, we present a lower bound on the regret for a large and natural class of FTPL algorithms that use logconcave perturbations. We complete our investigation with an online shortest path experiment and empirically show that our algorithm is both statistically and computationally efficient.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/cohena15.pdf",
        "supp": "",
        "pdf_size": 365367,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5738423163280140780&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "University of Haifa, University of Haifa, Dept. of Computer Science, 31905 Haifa, Israel; University of Haifa, University of Haifa, Dept. of Computer Science, 31905 Haifa, Israel",
        "aff_domain": "csweb.haifa.ac.il;cs.haifa.ac.il",
        "email": "csweb.haifa.ac.il;cs.haifa.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Haifa",
        "aff_unique_dep": "Dept. of Computer Science",
        "aff_unique_url": "https://www.haifa.ac.il",
        "aff_unique_abbr": "Haifa U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "e8c7a75cc4",
        "title": "From Word Embeddings To Document Distances",
        "site": "https://proceedings.mlr.press/v37/kusnerb15.html",
        "author": "Matt Kusner; Yu Sun; Nicholas Kolkin; Kilian Weinberger",
        "abstract": "We present the Word Mover\u2019s Distance (WMD), a novel distance function between text documents. Our work is based on recent results in word embeddings that learn semantically meaningful representations for words from local co-occurrences in sentences. The WMD distance measures the dissimilarity between two text documents as the minimum amount of distance that the embedded words of one document need to \"travel\" to reach the embedded words of another document. We show that this distance metric can be cast as an instance of the Earth Mover\u2019s Distance, a well studied transportation problem for which several highly efficient solvers have been developed. Our metric has no hyperparameters and is straight-forward to implement. Further, we demonstrate on eight real world document classification data sets, in comparison with seven state-of-the-art baselines, that the WMD metric leads to unprecedented low k-nearest neighbor document classification error rates.",
        "bibtex": "@InProceedings{pmlr-v37-kusnerb15,\n  title = \t {From Word Embeddings To Document Distances},\n  author = \t {Kusner, Matt and Sun, Yu and Kolkin, Nicholas and Weinberger, Kilian},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {957--966},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/kusnerb15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/kusnerb15.html},\n  abstract = \t {We present the Word Mover\u2019s Distance (WMD), a novel distance function between text documents. Our work is based on recent results in word embeddings that learn semantically meaningful representations for words from local co-occurrences in sentences. The WMD distance measures the dissimilarity between two text documents as the minimum amount of distance that the embedded words of one document need to \"travel\" to reach the embedded words of another document. We show that this distance metric can be cast as an instance of the Earth Mover\u2019s Distance, a well studied transportation problem for which several highly efficient solvers have been developed. Our metric has no hyperparameters and is straight-forward to implement. Further, we demonstrate on eight real world document classification data sets, in comparison with seven state-of-the-art baselines, that the WMD metric leads to unprecedented low k-nearest neighbor document classification error rates.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/kusnerb15.pdf",
        "supp": "",
        "pdf_size": 1194029,
        "gs_citation": 3059,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6676593072521907897&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Washington University in St. Louis; Washington University in St. Louis; Washington University in St. Louis; Washington University in St. Louis",
        "aff_domain": "WUSTL.EDU;WUSTL.EDU;WUSTL.EDU;WUSTL.EDU",
        "email": "WUSTL.EDU;WUSTL.EDU;WUSTL.EDU;WUSTL.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Washington University in St. Louis",
        "aff_unique_dep": "",
        "aff_unique_url": "https://wustl.edu",
        "aff_unique_abbr": "WashU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "St. Louis",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c34c6f9b00",
        "title": "Functional Subspace Clustering with Application to Time Series",
        "site": "https://proceedings.mlr.press/v37/bahadori15.html",
        "author": "Mohammad Taha Bahadori; David Kale; Yingying Fan; Yan Liu",
        "abstract": "Functional data, where samples are random functions, are increasingly common and important in a variety of applications, such as health care and traffic analysis. They are naturally high dimensional and lie along complex manifolds. These properties warrant use of the subspace assumption, but most state-of-the-art subspace learning algorithms are limited to linear or other simple settings. To address these challenges, we propose a new framework called Functional Subspace Clustering (FSC). FSC assumes that functional samples lie in deformed linear subspaces and formulates the subspace learning problem as a sparse regression over operators. The resulting problem can be efficiently solved via greedy variable selection, given access to a fast deformation oracle. We provide theoretical guarantees for FSC and show how it can be applied to time series with warped alignments. Experimental results on both synthetic data and real clinical time series show that FSC outperforms both standard time series clustering and state-of-the-art subspace clustering.",
        "bibtex": "@InProceedings{pmlr-v37-bahadori15,\n  title = \t {Functional Subspace Clustering with Application to Time Series},\n  author = \t {Bahadori, Mohammad Taha and Kale, David and Fan, Yingying and Liu, Yan},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {228--237},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/bahadori15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/bahadori15.html},\n  abstract = \t {Functional data, where samples are random functions, are increasingly common and important in a variety of applications, such as health care and traffic analysis. They are naturally high dimensional and lie along complex manifolds. These properties warrant use of the subspace assumption, but most state-of-the-art subspace learning algorithms are limited to linear or other simple settings. To address these challenges, we propose a new framework called Functional Subspace Clustering (FSC). FSC assumes that functional samples lie in deformed linear subspaces and formulates the subspace learning problem as a sparse regression over operators. The resulting problem can be efficiently solved via greedy variable selection, given access to a fast deformation oracle. We provide theoretical guarantees for FSC and show how it can be applied to time series with warped alignments. Experimental results on both synthetic data and real clinical time series show that FSC outperforms both standard time series clustering and state-of-the-art subspace clustering.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/bahadori15.pdf",
        "supp": "",
        "pdf_size": 433767,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11508415028128219998&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "University of Southern California, Los Angeles, CA 90089+Laura P. and Leland K. Whittier Virtual PICU, Children\u2019s Hospital Los Angeles, Los Angeles, CA 90027; University of Southern California, Los Angeles, CA 90089; University of Southern California, Los Angeles, CA 90089; University of Southern California, Los Angeles, CA 90089",
        "aff_domain": "USC.EDU;USC.EDU;MARSHALL.USC.EDU;USC.EDU",
        "email": "USC.EDU;USC.EDU;MARSHALL.USC.EDU;USC.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0;0",
        "aff_unique_norm": "University of Southern California;Children\u2019s Hospital Los Angeles",
        "aff_unique_dep": ";Laura P. and Leland K. Whittier Virtual PICU",
        "aff_unique_url": "https://www.usc.edu;https://www.chla.org",
        "aff_unique_abbr": "USC;",
        "aff_campus_unique_index": "0+0;0;0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7d28c4a17d",
        "title": "Gated Feedback Recurrent Neural Networks",
        "site": "https://proceedings.mlr.press/v37/chung15.html",
        "author": "Junyoung Chung; Caglar Gulcehre; Kyunghyun Cho; Yoshua Bengio",
        "abstract": "In this work, we propose a novel recurrent neural network (RNN) architecture. The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of stacking multiple recurrent layers by allowing and controlling signals flowing from upper recurrent layers to lower layers using a global gating unit for each pair of layers. The recurrent signals exchanged between layers are gated adaptively based on the previous hidden states and the current input. We evaluated the proposed GF-RNN with different types of recurrent units, such as tanh, long short-term memory and gated recurrent units, on the tasks of character-level language modeling and Python program evaluation. Our empirical evaluation of different RNN units, revealed that in both tasks, the GF-RNN outperforms the conventional approaches to build deep stacked RNNs. We suggest that the improvement arises because the GF-RNN can adaptively assign different layers to different timescales and layer-to-layer interactions (including the top-down ones which are not usually present in a stacked RNN) by learning to gate these interactions.",
        "bibtex": "@InProceedings{pmlr-v37-chung15,\n  title = \t {Gated Feedback Recurrent Neural Networks},\n  author = \t {Chung, Junyoung and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2067--2075},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/chung15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/chung15.html},\n  abstract = \t {In this work, we propose a novel recurrent neural network (RNN) architecture. The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of stacking multiple recurrent layers by allowing and controlling signals flowing from upper recurrent layers to lower layers using a global gating unit for each pair of layers. The recurrent signals exchanged between layers are gated adaptively based on the previous hidden states and the current input. We evaluated the proposed GF-RNN with different types of recurrent units, such as tanh, long short-term memory and gated recurrent units, on the tasks of character-level language modeling and Python program evaluation. Our empirical evaluation of different RNN units, revealed that in both tasks, the GF-RNN outperforms the conventional approaches to build deep stacked RNNs. We suggest that the improvement arises because the GF-RNN can adaptively assign different layers to different timescales and layer-to-layer interactions (including the top-down ones which are not usually present in a stacked RNN) by learning to gate these interactions.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/chung15.pdf",
        "supp": "",
        "pdf_size": 1558678,
        "gs_citation": 1213,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9646941875979474208&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 11,
        "aff": "Dept. IRO, Universit\u00e9 de Montr\u00e9al; Dept. IRO, Universit\u00e9 de Montr\u00e9al; Dept. IRO, Universit\u00e9 de Montr\u00e9al; Dept. IRO, Universit\u00e9 de Montr\u00e9al + CIFAR Senior Fellow",
        "aff_domain": "UMONTREAL.CA;UMONTREAL.CA;UMONTREAL.CA;THE.WEB",
        "email": "UMONTREAL.CA;UMONTREAL.CA;UMONTREAL.CA;THE.WEB",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0+1",
        "aff_unique_norm": "Universit\u00e9 de Montr\u00e9al;CIFAR",
        "aff_unique_dep": "Dept. IRO;Senior Fellow",
        "aff_unique_url": "https://www.umontreal.ca;https://www.cifar.ca",
        "aff_unique_abbr": "UdeM;CIFAR",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Montr\u00e9al;",
        "aff_country_unique_index": "0;0;0;0+0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "40b5693159",
        "title": "Generalization error bounds for learning to rank: Does the length of document lists matter?",
        "site": "https://proceedings.mlr.press/v37/tewari15.html",
        "author": "Ambuj Tewari; Sougata Chaudhuri",
        "abstract": "We consider the generalization ability of algorithms for learning to rank at a query level, a problem also called subset ranking. Existing generalization error bounds necessarily degrade as the size of the document list associated with a query increases. We show that such a degradation is not intrinsic to the problem. For several loss functions, including the cross-entropy loss used in the well known ListNet method, there is no degradation in generalization ability as document lists become longer. We also provide novel generalization error bounds under \\ell_1 regularization and faster convergence rates if the loss function is smooth.",
        "bibtex": "@InProceedings{pmlr-v37-tewari15,\n  title = \t {Generalization error bounds for learning to rank: Does the length of document lists matter?},\n  author = \t {Tewari, Ambuj and Chaudhuri, Sougata},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {315--323},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/tewari15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/tewari15.html},\n  abstract = \t {We consider the generalization ability of algorithms for learning to rank at a query level, a problem also called subset ranking. Existing generalization error bounds necessarily degrade as the size of the document list associated with a query increases. We show that such a degradation is not intrinsic to the problem. For several loss functions, including the cross-entropy loss used in the well known ListNet method, there is no degradation in generalization ability as document lists become longer. We also provide novel generalization error bounds under \\ell_1 regularization and faster convergence rates if the loss function is smooth.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/tewari15.pdf",
        "supp": "",
        "pdf_size": 582945,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2300556061157975834&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Michigan, Ann Arbor; University of Michigan, Ann Arbor",
        "aff_domain": "UMICH.EDU;UMICH.EDU",
        "email": "UMICH.EDU;UMICH.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d2226f7c7e",
        "title": "Generative Moment Matching Networks",
        "site": "https://proceedings.mlr.press/v37/li15.html",
        "author": "Yujia Li; Kevin Swersky; Rich Zemel",
        "abstract": "We consider the problem of learning deep generative models from data. We formulate a method that generates an independent sample via a single feedforward pass through a multilayer preceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014). Training a generative adversarial network, however, requires careful optimization of a difficult minimax program. Instead, we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model, and can be trained by backpropagation. We further boost the performance of this approach by combining our generative network with an auto-encoder network, using MMD to learn to generate codes that can then be decoded to produce samples. We show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on MNIST and the Toronto Face Database.",
        "bibtex": "@InProceedings{pmlr-v37-li15,\n  title = \t {Generative Moment Matching Networks},\n  author = \t {Li, Yujia and Swersky, Kevin and Zemel, Rich},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1718--1727},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/li15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/li15.html},\n  abstract = \t {We consider the problem of learning deep generative models from data. We formulate a method that generates an independent sample via a single feedforward pass through a multilayer preceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014). Training a generative adversarial network, however, requires careful optimization of a difficult minimax program. Instead, we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model, and can be trained by backpropagation. We further boost the performance of this approach by combining our generative network with an auto-encoder network, using MMD to learn to generate codes that can then be decoded to produce samples. We show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on MNIST and the Toronto Face Database.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/li15.pdf",
        "supp": "",
        "pdf_size": 1070941,
        "gs_citation": 1101,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18115566463777766587&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Science, University of Toronto, Toronto, ON, CANADA; Department of Computer Science, University of Toronto, Toronto, ON, CANADA; Department of Computer Science, University of Toronto, Toronto, ON, CANADA + Canadian Institute for Advanced Research, Toronto, ON, CANADA",
        "aff_domain": "CS.TORONTO.EDU;CS.TORONTO.EDU;CS.TORONTO.EDU",
        "email": "CS.TORONTO.EDU;CS.TORONTO.EDU;CS.TORONTO.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "University of Toronto;Canadian Institute for Advanced Research",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.utoronto.ca;https://www.cifar.ca",
        "aff_unique_abbr": "U of T;CIFAR",
        "aff_campus_unique_index": "0;0;0+0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2b3feb543e",
        "title": "Geometric Conditions for Subspace-Sparse Recovery",
        "site": "https://proceedings.mlr.press/v37/you15.html",
        "author": "Chong You; Rene Vidal",
        "abstract": "Given a dictionary \\Pi and a signal \u03be= \\Pi \\mathbf x generated by a few \\textitlinearly independent columns of \\Pi, classical sparse recovery theory deals with the problem of uniquely recovering the sparse representation \\mathbf x of \u03be. In this work, we consider the more general case where \u03belies in a low-dimensional subspace spanned by a few columns of \\Pi, which are possibly \\textitlinearly dependent. In this case, \\mathbf x may not unique, and the goal is to recover any subset of the columns of \\Pi that spans the subspace containing \u03be. We call such a representation \\mathbf x \\textitsubspace-sparse. We study conditions under which existing pursuit methods recover a subspace-sparse representation. Such conditions reveal important geometric insights and have implications for the theory of classical sparse recovery as well as subspace clustering.",
        "bibtex": "@InProceedings{pmlr-v37-you15,\n  title = \t {Geometric Conditions for Subspace-Sparse Recovery},\n  author = \t {You, Chong and Vidal, Rene},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1585--1593},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/you15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/you15.html},\n  abstract = \t {Given a dictionary \\Pi and a signal \u03be= \\Pi \\mathbf x generated by a few \\textitlinearly independent columns of \\Pi, classical sparse recovery theory deals with the problem of uniquely recovering the sparse representation \\mathbf x of \u03be. In this work, we consider the more general case where \u03belies in a low-dimensional subspace spanned by a few columns of \\Pi, which are possibly \\textitlinearly dependent. In this case, \\mathbf x may not unique, and the goal is to recover any subset of the columns of \\Pi that spans the subspace containing \u03be. We call such a representation \\mathbf x \\textitsubspace-sparse. We study conditions under which existing pursuit methods recover a subspace-sparse representation. Such conditions reveal important geometric insights and have implications for the theory of classical sparse recovery as well as subspace clustering.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/you15.pdf",
        "supp": "",
        "pdf_size": 273293,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15852044049902160626&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Center for Imaging Science, Johns Hopkins University, Baltimore, MD, 21218, USA; Center for Imaging Science, Johns Hopkins University, Baltimore, MD, 21218, USA",
        "aff_domain": "CIS.JHU.EDU;CIS.JHU.EDU",
        "email": "CIS.JHU.EDU;CIS.JHU.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Johns Hopkins University",
        "aff_unique_dep": "Center for Imaging Science",
        "aff_unique_url": "https://www.jhu.edu",
        "aff_unique_abbr": "JHU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Baltimore",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9281be7b3f",
        "title": "Global Convergence of Stochastic Gradient Descent for Some Non-convex Matrix Problems",
        "site": "https://proceedings.mlr.press/v37/sa15.html",
        "author": "Christopher De Sa; Christopher Re; Kunle Olukotun",
        "abstract": "Stochastic gradient descent (SGD) on a low-rank factorization is commonly employed to speed up matrix problems including matrix completion, subspace tracking, and SDP relaxation. In this paper, we exhibit a step size scheme for SGD on a low-rank least-squares problem, and we prove that, under broad sampling conditions, our method converges globally from a random starting point within O(\u03b5^-1 n \\log n) steps with constant probability for constant-rank problems. Our modification of SGD relates it to stochastic power iteration. We also show some experiments to illustrate the runtime and convergence of the algorithm.",
        "bibtex": "@InProceedings{pmlr-v37-sa15,\n  title = \t {Global Convergence of Stochastic Gradient Descent for Some Non-convex Matrix Problems},\n  author = \t {Sa, Christopher De and Re, Christopher and Olukotun, Kunle},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2332--2341},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/sa15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/sa15.html},\n  abstract = \t {Stochastic gradient descent (SGD) on a low-rank factorization is commonly employed to speed up matrix problems including matrix completion, subspace tracking, and SDP relaxation. In this paper, we exhibit a step size scheme for SGD on a low-rank least-squares problem, and we prove that, under broad sampling conditions, our method converges globally from a random starting point within O(\u03b5^-1 n \\log n) steps with constant probability for constant-rank problems. Our modification of SGD relates it to stochastic power iteration. We also show some experiments to illustrate the runtime and convergence of the algorithm.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/sa15.pdf",
        "supp": "",
        "pdf_size": 333904,
        "gs_citation": 206,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5543643794865731267&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Electrical Engineering, Stanford University, Stanford, CA 94309; Department of Electrical Engineering, Stanford University, Stanford, CA 94309; Department of Computer Science, Stanford University, Stanford, CA 94309",
        "aff_domain": "STANFORD.EDU;STANFORD.EDU;STANFORD.EDU",
        "email": "STANFORD.EDU;STANFORD.EDU;STANFORD.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "acfa8c7f81",
        "title": "Gradient-based Hyperparameter Optimization through Reversible Learning",
        "site": "https://proceedings.mlr.press/v37/maclaurin15.html",
        "author": "Dougal Maclaurin; David Duvenaud; Ryan Adams",
        "abstract": "Tuning hyperparameters of learning algorithms is hard because gradients are usually unavailable. We compute exact gradients of cross-validation performance with respect to all hyperparameters by chaining derivatives backwards through the entire training procedure. These gradients allow us to optimize thousands of hyperparameters, including step-size and momentum schedules, weight initialization distributions, richly parameterized regularization schemes, and neural network architectures. We compute hyperparameter gradients by exactly reversing the dynamics of stochastic gradient descent with momentum.",
        "bibtex": "@InProceedings{pmlr-v37-maclaurin15,\n  title = \t {Gradient-based Hyperparameter Optimization through Reversible Learning},\n  author = \t {Maclaurin, Dougal and Duvenaud, David and Adams, Ryan},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2113--2122},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/maclaurin15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/maclaurin15.html},\n  abstract = \t {Tuning hyperparameters of learning algorithms is hard because gradients are usually unavailable. We compute exact gradients of cross-validation performance with respect to all hyperparameters by chaining derivatives backwards through the entire training procedure. These gradients allow us to optimize thousands of hyperparameters, including step-size and momentum schedules, weight initialization distributions, richly parameterized regularization schemes, and neural network architectures. We compute hyperparameter gradients by exactly reversing the dynamics of stochastic gradient descent with momentum.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/maclaurin15.pdf",
        "supp": "",
        "pdf_size": 994066,
        "gs_citation": 1174,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16427522673612533152&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Harvard University; Harvard University; Harvard University",
        "aff_domain": "PHYSICS.HARVARD.EDU;SEAS.HARVARD.EDU;SEAS.HARVARD.EDU",
        "email": "PHYSICS.HARVARD.EDU;SEAS.HARVARD.EDU;SEAS.HARVARD.EDU",
        "github": "github.com/hips/author-roulette",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Harvard University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.harvard.edu",
        "aff_unique_abbr": "Harvard",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ccab4c457f",
        "title": "Guaranteed Tensor Decomposition: A Moment Approach",
        "site": "https://proceedings.mlr.press/v37/tanga15.html",
        "author": "Gongguo Tang; Parikshit Shah",
        "abstract": "We develop a theoretical and computational framework to perform guaranteed tensor decomposition, which also has the potential to accomplish other tensor tasks such as tensor completion and denoising. We formulate tensor decomposition as a problem of measure estimation from moments. By constructing a dual polynomial, we demonstrate that measure optimization returns the correct CP decomposition under an incoherence condition on the rank-one factors. To address the computational challenge, we present a hierarchy of semidefinite programs based on sums-of-squares relaxations of the measure optimization problem. By showing that the constructed dual polynomial is a sum-of-squares modulo the sphere, we prove that the smallest SDP in the relaxation hierarchy is exact and the decomposition can be extracted from the solution under the same incoherence condition. One implication is that the tensor nuclear norm can be computed exactly using the smallest SDP as long as the rank-one factors of the tensor are incoherent. Numerical experiments are conducted to test the performance of the moment approach.",
        "bibtex": "@InProceedings{pmlr-v37-tanga15,\n  title = \t {Guaranteed Tensor Decomposition: A Moment Approach},\n  author = \t {Tang, Gongguo and Shah, Parikshit},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1491--1500},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/tanga15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/tanga15.html},\n  abstract = \t {We develop a theoretical and computational framework to perform guaranteed tensor decomposition, which also has the potential to accomplish other tensor tasks such as tensor completion and denoising. We formulate tensor decomposition as a problem of measure estimation from moments. By constructing a dual polynomial, we demonstrate that measure optimization returns the correct CP decomposition under an incoherence condition on the rank-one factors. To address the computational challenge, we present a hierarchy of semidefinite programs based on sums-of-squares relaxations of the measure optimization problem. By showing that the constructed dual polynomial is a sum-of-squares modulo the sphere, we prove that the smallest SDP in the relaxation hierarchy is exact and the decomposition can be extracted from the solution under the same incoherence condition. One implication is that the tensor nuclear norm can be computed exactly using the smallest SDP as long as the rank-one factors of the tensor are incoherent. Numerical experiments are conducted to test the performance of the moment approach.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/tanga15.pdf",
        "supp": "",
        "pdf_size": 929823,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8397930001379944252&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 6,
        "aff": "Colorado School of Mines; Yahoo! Labs",
        "aff_domain": "MINES.EDU;DISCOVERY.WISC.EDU",
        "email": "MINES.EDU;DISCOVERY.WISC.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Colorado School of Mines;Yahoo!",
        "aff_unique_dep": ";Yahoo! Labs",
        "aff_unique_url": "https://www.mines.edu;https://yahoo.com",
        "aff_unique_abbr": "CSM;Yahoo!",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "64c81e5a5c",
        "title": "Harmonic Exponential Families on Manifolds",
        "site": "https://proceedings.mlr.press/v37/cohenb15.html",
        "author": "Taco Cohen; Max Welling",
        "abstract": "In a range of fields including the geosciences, molecular biology, robotics and computer vision, one encounters problems that involve random variables on manifolds. Currently, there is a lack of flexible probabilistic models on manifolds that are fast and easy to train. We define an extremely flexible class of exponential family distributions on manifolds such as the torus, sphere, and rotation groups, and show that for these distributions the gradient of the log-likelihood can be computed efficiently using a non-commutative generalization of the Fast Fourier Transform (FFT). We discuss applications to Bayesian camera motion estimation (where harmonic exponential families serve as conjugate priors), and modelling of the spatial distribution of earthquakes on the surface of the earth. Our experimental results show that harmonic densities yield a significantly higher likelihood than the best competing method, while being orders of magnitude faster to train.",
        "bibtex": "@InProceedings{pmlr-v37-cohenb15,\n  title = \t {Harmonic Exponential Families on Manifolds},\n  author = \t {Cohen, Taco and Welling, Max},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1757--1765},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/cohenb15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/cohenb15.html},\n  abstract = \t {In a range of fields including the geosciences, molecular biology, robotics and computer vision, one encounters problems that involve random variables on manifolds. Currently, there is a lack of flexible probabilistic models on manifolds that are fast and easy to train. We define an extremely flexible class of exponential family distributions on manifolds such as the torus, sphere, and rotation groups, and show that for these distributions the gradient of the log-likelihood can be computed efficiently using a non-commutative generalization of the Fast Fourier Transform (FFT). We discuss applications to Bayesian camera motion estimation (where harmonic exponential families serve as conjugate priors), and modelling of the spatial distribution of earthquakes on the surface of the earth. Our experimental results show that harmonic densities yield a significantly higher likelihood than the best competing method, while being orders of magnitude faster to train.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/cohenb15.pdf",
        "supp": "",
        "pdf_size": 1075515,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1059605640610488684&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "University of Amsterdam; University of Amsterdam + University of California Irvine + Canadian Institute for Advanced Research",
        "aff_domain": "UVA.NL;UVA.NL",
        "email": "UVA.NL;UVA.NL",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1+2",
        "aff_unique_norm": "University of Amsterdam;University of California, Irvine;Canadian Institute for Advanced Research",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.uva.nl;https://www.uci.edu;https://www.cifar.ca",
        "aff_unique_abbr": "UvA;UCI;CIFAR",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Irvine",
        "aff_country_unique_index": "0;0+1+2",
        "aff_country_unique": "Netherlands;United States;Canada"
    },
    {
        "id": "8f52f5b316",
        "title": "Hashing for Distributed Data",
        "site": "https://proceedings.mlr.press/v37/leng15.html",
        "author": "Cong Leng; Jiaxiang Wu; Jian Cheng; Xi Zhang; Hanqing Lu",
        "abstract": "Recently, hashing based approximate nearest neighbors search has attracted much attention. Extensive centralized hashing algorithms have been proposed and achieved promising performance. However, due to the large scale of many applications, the data is often stored or even collected in a distributed manner. Learning hash functions by aggregating all the data into a fusion center is infeasible because of the prohibitively expensive communication and computation overhead. In this paper, we develop a novel hashing model to learn hash functions in a distributed setting. We cast a centralized hashing model as a set of subproblems with consensus constraints. We find these subproblems can be analytically solved in parallel on the distributed compute nodes. Since no training data is transmitted across the nodes in the learning process, the communication cost of our model is independent to the data size. Extensive experiments on several large scale datasets containing up to 100 million samples demonstrate the efficacy of our method.",
        "bibtex": "@InProceedings{pmlr-v37-leng15,\n  title = \t {Hashing for Distributed Data},\n  author = \t {Leng, Cong and Wu, Jiaxiang and Cheng, Jian and Zhang, Xi and Lu, Hanqing},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1642--1650},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/leng15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/leng15.html},\n  abstract = \t {Recently, hashing based approximate nearest neighbors search has attracted much attention. Extensive centralized hashing algorithms have been proposed and achieved promising performance. However, due to the large scale of many applications, the data is often stored or even collected in a distributed manner. Learning hash functions by aggregating all the data into a fusion center is infeasible because of the prohibitively expensive communication and computation overhead. In this paper, we develop a novel hashing model to learn hash functions in a distributed setting. We cast a centralized hashing model as a set of subproblems with consensus constraints. We find these subproblems can be analytically solved in parallel on the distributed compute nodes. Since no training data is transmitted across the nodes in the learning process, the communication cost of our model is independent to the data size. Extensive experiments on several large scale datasets containing up to 100 million samples demonstrate the efficacy of our method.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/leng15.pdf",
        "supp": "",
        "pdf_size": 349081,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8970016007010442859&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing",
        "aff_domain": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "email": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Automation",
        "aff_unique_url": "http://www.ia.cas.cn",
        "aff_unique_abbr": "CAS",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "8c209a6214",
        "title": "HawkesTopic: A Joint Model for Network Inference and Topic Modeling from Text-Based Cascades",
        "site": "https://proceedings.mlr.press/v37/he15.html",
        "author": "Xinran He; Theodoros Rekatsinas; James Foulds; Lise Getoor; Yan Liu",
        "abstract": "Understanding the diffusion of information in social network and social media requires modeling the text diffusion process. In this work, we develop the HawkesTopic model (HTM) for analyzing text-based cascades, such as \"retweeting a post\" or \"publishing a follow-up blog post\". HTM combines Hawkes processes and topic modeling to simultaneously reason about the information diffusion pathways and the topics characterizing the observed textual information. We show how to jointly infer them with a mean-field variational inference algorithm and validate our approach on both synthetic and real-world data sets, including a news media dataset for modeling information diffusion, and an ArXiv publication dataset for modeling scientific influence. The results show that HTM is significantly more accurate than several baselines for both tasks.",
        "bibtex": "@InProceedings{pmlr-v37-he15,\n  title = \t {HawkesTopic: A Joint Model for Network Inference and Topic Modeling from Text-Based Cascades},\n  author = \t {He, Xinran and Rekatsinas, Theodoros and Foulds, James and Getoor, Lise and Liu, Yan},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {871--880},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/he15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/he15.html},\n  abstract = \t {Understanding the diffusion of information in social network and social media requires modeling the text diffusion process. In this work, we develop the HawkesTopic model (HTM) for analyzing text-based cascades, such as \"retweeting a post\" or \"publishing a follow-up blog post\". HTM combines Hawkes processes and topic modeling to simultaneously reason about the information diffusion pathways and the topics characterizing the observed textual information. We show how to jointly infer them with a mean-field variational inference algorithm and validate our approach on both synthetic and real-world data sets, including a news media dataset for modeling information diffusion, and an ArXiv publication dataset for modeling scientific influence. The results show that HTM is significantly more accurate than several baselines for both tasks.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/he15.pdf",
        "supp": "",
        "pdf_size": 839586,
        "gs_citation": 123,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7256771740501863570&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "University of Southern California; University of Maryland, College Park; University of California, Santa Cruz; University of California, Santa Cruz; University of Southern California",
        "aff_domain": "USC.EDU;CS.UMD.EDU;UCSC.EDU;SOE.UCSC.EDU;USC.EDU",
        "email": "USC.EDU;CS.UMD.EDU;UCSC.EDU;SOE.UCSC.EDU;USC.EDU",
        "github": "",
        "project": "http://eventregistry.org",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;2;0",
        "aff_unique_norm": "University of Southern California;University of Maryland;University of California, Santa Cruz",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.usc.edu;https://www/umd.edu;https://www.ucsc.edu",
        "aff_unique_abbr": "USC;UMD;UCSC",
        "aff_campus_unique_index": "0;1;2;2;0",
        "aff_campus_unique": "Los Angeles;College Park;Santa Cruz",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "bedb6383f0",
        "title": "Hidden Markov Anomaly Detection",
        "site": "https://proceedings.mlr.press/v37/goernitz15.html",
        "author": "Nico Goernitz; Mikio Braun; Marius Kloft",
        "abstract": "We introduce a new anomaly detection methodology for data with latent dependency structure. As a particular instantiation, we derive a hidden Markov anomaly detector that extends the regular one-class support vector machine. We optimize the approach, which is non-convex, via a DC (difference of convex functions) algorithm, and show that the parameter v can be conveniently used to control the number of outliers in the model. The empirical evaluation on artificial and real data from the domains of computational biology and computational sustainability shows that the approach can achieve significantly higher anomaly detection performance than the regular one-class SVM.",
        "bibtex": "@InProceedings{pmlr-v37-goernitz15,\n  title = \t {Hidden Markov Anomaly Detection},\n  author = \t {Goernitz, Nico and Braun, Mikio and Kloft, Marius},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1833--1842},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/goernitz15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/goernitz15.html},\n  abstract = \t {We introduce a new anomaly detection methodology for data with latent dependency structure. As a particular instantiation, we derive a hidden Markov anomaly detector that extends the regular one-class support vector machine. We optimize the approach, which is non-convex, via a DC (difference of convex functions) algorithm, and show that the parameter v can be conveniently used to control the number of outliers in the model. The empirical evaluation on artificial and real data from the domains of computational biology and computational sustainability shows that the approach can achieve significantly higher anomaly detection performance than the regular one-class SVM.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/goernitz15.pdf",
        "supp": "",
        "pdf_size": 2740124,
        "gs_citation": 90,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4395035814436596606&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Berlin Institute of Technology; Berlin Institute of Technology; Humboldt University of Berlin",
        "aff_domain": "TU-BERLIN.DE;TU-BERLIN.DE;HU-BERLIN.DE",
        "email": "TU-BERLIN.DE;TU-BERLIN.DE;HU-BERLIN.DE",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Berlin Institute of Technology;Humboldt University of Berlin",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tu-berlin.de;https://www.hu-berlin.de",
        "aff_unique_abbr": "TU Berlin;HU Berlin",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berlin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "f6681ff969",
        "title": "High Confidence Policy Improvement",
        "site": "https://proceedings.mlr.press/v37/thomas15.html",
        "author": "Philip Thomas; Georgios Theocharous; Mohammad Ghavamzadeh",
        "abstract": "We present a batch reinforcement learning (RL) algorithm that provides probabilistic guarantees about the quality of each policy that it proposes, and which has no hyper-parameter that requires expert tuning. Specifically, the user may select any performance lower-bound and confidence level and our algorithm will ensure that the probability that it returns a policy with performance below the lower bound is at most the specified confidence level. We then propose an incremental algorithm that executes our policy improvement algorithm repeatedly to generate multiple policy improvements. We show the viability of our approach with a simple 4 x 4 gridworld and the standard mountain car problem, as well as with a digital marketing application that uses real world data.",
        "bibtex": "@InProceedings{pmlr-v37-thomas15,\n  title = \t {High Confidence Policy Improvement},\n  author = \t {Thomas, Philip and Theocharous, Georgios and Ghavamzadeh, Mohammad},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2380--2388},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/thomas15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/thomas15.html},\n  abstract = \t {We present a batch reinforcement learning (RL) algorithm that provides probabilistic guarantees about the quality of each policy that it proposes, and which has no hyper-parameter that requires expert tuning. Specifically, the user may select any performance lower-bound and confidence level and our algorithm will ensure that the probability that it returns a policy with performance below the lower bound is at most the specified confidence level. We then propose an incremental algorithm that executes our policy improvement algorithm repeatedly to generate multiple policy improvements. We show the viability of our approach with a simple 4 x 4 gridworld and the standard mountain car problem, as well as with a digital marketing application that uses real world data.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/thomas15.pdf",
        "supp": "",
        "pdf_size": 711379,
        "gs_citation": 233,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6828708791549663288&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "University of Massachusetts Amherst; Adobe Research; Adobe Research + INRIA",
        "aff_domain": "CS.UMASS.EDU;ADOBE.COM;ADOBE.COM",
        "email": "CS.UMASS.EDU;ADOBE.COM;ADOBE.COM",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1+2",
        "aff_unique_norm": "University of Massachusetts Amherst;Adobe;INRIA",
        "aff_unique_dep": ";Adobe Research;",
        "aff_unique_url": "https://www.umass.edu;https://research.adobe.com;https://www.inria.fr",
        "aff_unique_abbr": "UMass Amherst;Adobe;INRIA",
        "aff_campus_unique_index": "0;",
        "aff_campus_unique": "Amherst;",
        "aff_country_unique_index": "0;0;0+1",
        "aff_country_unique": "United States;France"
    },
    {
        "id": "5fc81f5b6f",
        "title": "High Dimensional Bayesian Optimisation and Bandits via Additive Models",
        "site": "https://proceedings.mlr.press/v37/kandasamy15.html",
        "author": "Kirthevasan Kandasamy; Jeff Schneider; Barnabas Poczos",
        "abstract": "Bayesian Optimisation (BO) is a technique used in optimising a D-dimensional function which is typically expensive to evaluate. While there have been many successes for BO in low dimensions, scaling it to high dimensions has been notoriously difficult. Existing literature on the topic are under very restrictive settings. In this paper, we identify two key challenges in this endeavour. We tackle these challenges by assuming an additive structure for the function. This setting is substantially more expressive and contains a richer class of functions than previous work. We prove that, for additive functions the regret has only linear dependence on D even though the function depends on all D dimensions. We also demonstrate several other statistical and computational benefits in our framework. Via synthetic examples, a scientific simulation and a face detection problem we demonstrate that our method outperforms naive BO on additive functions and on several examples where the function is not additive.",
        "bibtex": "@InProceedings{pmlr-v37-kandasamy15,\n  title = \t {High Dimensional Bayesian Optimisation and Bandits via Additive Models},\n  author = \t {Kandasamy, Kirthevasan and Schneider, Jeff and Poczos, Barnabas},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {295--304},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/kandasamy15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/kandasamy15.html},\n  abstract = \t {Bayesian Optimisation (BO) is a technique used in optimising a D-dimensional function which is typically expensive to evaluate. While there have been many successes for BO in low dimensions, scaling it to high dimensions has been notoriously difficult. Existing literature on the topic are under very restrictive settings. In this paper, we identify two key challenges in this endeavour. We tackle these challenges by assuming an additive structure for the function. This setting is substantially more expressive and contains a richer class of functions than previous work. We prove that, for additive functions the regret has only linear dependence on D even though the function depends on all D dimensions. We also demonstrate several other statistical and computational benefits in our framework. Via synthetic examples, a scientific simulation and a face detection problem we demonstrate that our method outperforms naive BO on additive functions and on several examples where the function is not additive.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/kandasamy15.pdf",
        "supp": "",
        "pdf_size": 674990,
        "gs_citation": 472,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15502383080264998413&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Carnegie Mellon University, Pittsburgh, PA, USA; Carnegie Mellon University, Pittsburgh, PA, USA; Carnegie Mellon University, Pittsburgh, PA, USA",
        "aff_domain": "CS.CMU.EDU;CS.CMU.EDU;CS.CMU.EDU",
        "email": "CS.CMU.EDU;CS.CMU.EDU;CS.CMU.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f58c765fcf",
        "title": "How Can Deep Rectifier Networks Achieve Linear Separability and Preserve Distances?",
        "site": "https://proceedings.mlr.press/v37/an15.html",
        "author": "Senjian An; Farid Boussaid; Mohammed Bennamoun",
        "abstract": "This paper investigates how hidden layers of deep rectifier networks are capable of transforming two or more pattern sets to be linearly separable while preserving the distances with a guaranteed degree, and proves the universal classification power of such distance preserving rectifier networks. Through the nearly isometric nonlinear transformation in the hidden layers, the margin of the linear separating plane in the output layer and the margin of the nonlinear separating boundary in the original data space can be closely related so that the maximum margin classification in the input data space can be achieved approximately via the maximum margin linear classifiers in the output layer. The generalization performance of such distance preserving deep rectifier neural networks can be well justified by the distance-preserving properties of their hidden layers and the maximum margin property of the linear classifiers in the output layer.",
        "bibtex": "@InProceedings{pmlr-v37-an15,\n  title = \t {How Can Deep Rectifier Networks Achieve Linear Separability and Preserve Distances?},\n  author = \t {An, Senjian and Boussaid, Farid and Bennamoun, Mohammed},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {514--523},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/an15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/an15.html},\n  abstract = \t {This paper investigates how hidden layers of deep rectifier networks are capable of transforming two or more pattern sets to be linearly separable while preserving the distances with a guaranteed degree, and proves the universal classification power of such distance preserving rectifier networks. Through the nearly isometric nonlinear transformation in the hidden layers, the margin of the linear separating plane in the output layer and the margin of the nonlinear separating boundary in the original data space can be closely related so that the maximum margin classification in the input data space can be achieved approximately via the maximum margin linear classifiers in the output layer. The generalization performance of such distance preserving deep rectifier neural networks can be well justified by the distance-preserving properties of their hidden layers and the maximum margin property of the linear classifiers in the output layer.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/an15.pdf",
        "supp": "",
        "pdf_size": 329832,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6742148970318459037&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "School of Computer Science and Software Engineering, The University of Western Australia, Australia; School of Electrical, Electronic and Computer Engineering, The University of Western Australia, Australia; School of Computer Science and Software Engineering, The University of Western Australia, Australia",
        "aff_domain": "UWA.EDU.AU;UWA.EDU.AU;UWA.EDU.AU",
        "email": "UWA.EDU.AU;UWA.EDU.AU;UWA.EDU.AU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Western Australia",
        "aff_unique_dep": "School of Computer Science and Software Engineering",
        "aff_unique_url": "https://www.uwa.edu.au",
        "aff_unique_abbr": "UWA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "9258c0b381",
        "title": "How Hard is Inference for Structured Prediction?",
        "site": "https://proceedings.mlr.press/v37/globerson15.html",
        "author": "Amir Globerson; Tim Roughgarden; David Sontag; Cafer Yildirim",
        "abstract": "Structured prediction tasks in machine learning involve the simultaneous prediction of multiple labels. This is often done by maximizing a score function on the space of labels, which decomposes as a sum of pairwise elements, each depending on two specific labels. The goal of this paper is to develop a theoretical explanation of the empirical effectiveness of heuristic inference algorithms for solving such structured prediction problems. We study the minimum-achievable expected Hamming error in such problems, highlighting the case of 2D grid graphs, which are common in machine vision applications. Our main theorems provide tight upper and lower bounds on this error, as well as a polynomial-time algorithm that achieves the bound.",
        "bibtex": "@InProceedings{pmlr-v37-globerson15,\n  title = \t {How Hard is Inference for Structured Prediction?},\n  author = \t {Globerson, Amir and Roughgarden, Tim and Sontag, David and Yildirim, Cafer},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2181--2190},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/globerson15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/globerson15.html},\n  abstract = \t {Structured prediction tasks in machine learning involve the simultaneous prediction of multiple labels. This is often done by maximizing a score function on the space of labels, which decomposes as a sum of pairwise elements, each depending on two specific labels. The goal of this paper is to develop a theoretical explanation of the empirical effectiveness of heuristic inference algorithms for solving such structured prediction problems. We study the minimum-achievable expected Hamming error in such problems, highlighting the case of 2D grid graphs, which are common in machine vision applications. Our main theorems provide tight upper and lower bounds on this error, as well as a polynomial-time algorithm that achieves the bound.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/globerson15.pdf",
        "supp": "",
        "pdf_size": 454308,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2434508049237376828&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "GAMIR @CS.HUJI .AC.IL; TIM@CS.STANFORD .EDU; DSONTAG @CS.NYU .EDU; CAFERTYILDIRIM @GMAIL .COM",
        "aff_domain": "cs.huji.ac.il;cs.stanford.edu;cs.nyu.edu;gmail.com",
        "email": "cs.huji.ac.il;cs.stanford.edu;cs.nyu.edu;gmail.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Hebrew University of Jerusalem;Stanford University;New York University;",
        "aff_unique_dep": "Computer Science;Department of Computer Science;Computer Science;",
        "aff_unique_url": "http://www.cs.huji.ac.il;https://www.stanford.edu;https://www.nyu.edu;",
        "aff_unique_abbr": "HUJI;Stanford;NYU;",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Israel;United States;"
    },
    {
        "id": "32f70451dd",
        "title": "Improved Regret Bounds for Undiscounted Continuous Reinforcement Learning",
        "site": "https://proceedings.mlr.press/v37/lakshmanan15.html",
        "author": "K. Lakshmanan; Ronald Ortner; Daniil Ryabko",
        "abstract": "We consider the problem of undiscounted reinforcement learning in continuous state space. Regret bounds in this setting usually hold under various assumptions on the structure of the reward and transition function. Under the assumption that the rewards and transition probabilities are Lipschitz, for 1-dimensional state space a regret bound of O(T^3/4) after any T steps has been given by Ortner and Ryabko (2012). Here we improve upon this result by using non-parametric kernel density estimation for estimating the transition probability distributions, and obtain regret bounds that depend on the smoothness of the transition probability distributions. In particular, under the assumption that the transition probability functions are smoothly differentiable, the regret bound is shown to be O(T^2/3) asymptotically for reinforcement learning in 1-dimensional state space. Finally, we also derive improved regret bounds for higher dimensional state space.",
        "bibtex": "@InProceedings{pmlr-v37-lakshmanan15,\n  title = \t {Improved Regret Bounds for Undiscounted Continuous Reinforcement Learning},\n  author = \t {Lakshmanan, K. and Ortner, Ronald and Ryabko, Daniil},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {524--532},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/lakshmanan15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/lakshmanan15.html},\n  abstract = \t {We consider the problem of undiscounted reinforcement learning in continuous state space. Regret bounds in this setting usually hold under various assumptions on the structure of the reward and transition function. Under the assumption that the rewards and transition probabilities are Lipschitz, for 1-dimensional state space a regret bound of O(T^3/4) after any T steps has been given by Ortner and Ryabko (2012). Here we improve upon this result by using non-parametric kernel density estimation for estimating the transition probability distributions, and obtain regret bounds that depend on the smoothness of the transition probability distributions. In particular, under the assumption that the transition probability functions are smoothly differentiable, the regret bound is shown to be O(T^2/3) asymptotically for reinforcement learning in 1-dimensional state space. Finally, we also derive improved regret bounds for higher dimensional state space.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/lakshmanan15.pdf",
        "supp": "",
        "pdf_size": 110996,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10260696474025457217&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Montanuniversit\u00e4t Leoben; Montanuniversit\u00e4t Leoben; INRIA Lille - Nord Europe",
        "aff_domain": "GMAIL.COM;UNILEOBEN.AC.AT;RYABKO.NET",
        "email": "GMAIL.COM;UNILEOBEN.AC.AT;RYABKO.NET",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Montanuniversit\u00e4t Leoben;INRIA",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.montanuni-leoben.at;https://www.inria.fr",
        "aff_unique_abbr": "MUL;INRIA",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Lille - Nord Europe",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Austria;France"
    },
    {
        "id": "148637bca0",
        "title": "Improving the Gaussian Process Sparse Spectrum Approximation by Representing Uncertainty in Frequency Inputs",
        "site": "https://proceedings.mlr.press/v37/galb15.html",
        "author": "Yarin Gal; Richard Turner",
        "abstract": "Standard sparse pseudo-input approximations to the Gaussian process (GP) cannot handle complex functions well. Sparse spectrum alternatives attempt to answer this but are known to over-fit. We suggest the use of variational inference for the sparse spectrum approximation to avoid both issues. We model the covariance function with a finite Fourier series approximation and treat it as a random variable. The random covariance function has a posterior, on which a variational distribution is placed. The variational distribution transforms the random covariance function to fit the data. We study the properties of our approximate inference, compare it to alternative ones, and extend it to the distributed and stochastic domains. Our approximation captures complex functions better than standard approaches and avoids over-fitting.",
        "bibtex": "@InProceedings{pmlr-v37-galb15,\n  title = \t {Improving the Gaussian Process Sparse Spectrum Approximation by Representing Uncertainty in Frequency Inputs},\n  author = \t {Gal, Yarin and Turner, Richard},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {655--664},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/galb15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/galb15.html},\n  abstract = \t {Standard sparse pseudo-input approximations to the Gaussian process (GP) cannot handle complex functions well. Sparse spectrum alternatives attempt to answer this but are known to over-fit. We suggest the use of variational inference for the sparse spectrum approximation to avoid both issues. We model the covariance function with a finite Fourier series approximation and treat it as a random variable. The random covariance function has a posterior, on which a variational distribution is placed. The variational distribution transforms the random covariance function to fit the data. We study the properties of our approximate inference, compare it to alternative ones, and extend it to the distributed and stochastic domains. Our approximation captures complex functions better than standard approaches and avoids over-fitting.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/galb15.pdf",
        "supp": "",
        "pdf_size": 1265588,
        "gs_citation": 102,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2099767999510365161&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "University of Cambridge; University of Cambridge",
        "aff_domain": "CAM.AC.UK;CAM.AC.UK",
        "email": "CAM.AC.UK;CAM.AC.UK",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "a0afa5e0ef",
        "title": "Inference in a Partially Observed Queuing Model with Applications in Ecology",
        "site": "https://proceedings.mlr.press/v37/winner15.html",
        "author": "Kevin Winner; Garrett Bernstein; Dan Sheldon",
        "abstract": "We consider the problem of inference in a probabilistic model for transient populations where we wish to learn about arrivals, departures, and population size over all time, but the only available data are periodic counts of the population size at specific observation times. The underlying model arises in queueing theory (as an M/G/inf queue) and also in ecological models for short-lived animals such as insects. Our work applies to both systems. Previous work in the ecology literature focused on maximum likelihood estimation and made a simplifying independence assumption that prevents inference over unobserved random variables such as arrivals and departures. The contribution of this paper is to formulate a latent variable model and develop a novel Gibbs sampler based on Markov bases to perform inference using the correct, but intractable, likelihood function. We empirically validate the convergence behavior of our sampler and demonstrate the ability of our model to make much finer-grained inferences than the previous approach.",
        "bibtex": "@InProceedings{pmlr-v37-winner15,\n  title = \t {Inference in a Partially Observed Queuing Model with Applications in Ecology},\n  author = \t {Winner, Kevin and Bernstein, Garrett and Sheldon, Dan},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2512--2520},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/winner15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/winner15.html},\n  abstract = \t {We consider the problem of inference in a probabilistic model for transient populations where we wish to learn about arrivals, departures, and population size over all time, but the only available data are periodic counts of the population size at specific observation times. The underlying model arises in queueing theory (as an M/G/inf queue) and also in ecological models for short-lived animals such as insects. Our work applies to both systems. Previous work in the ecology literature focused on maximum likelihood estimation and made a simplifying independence assumption that prevents inference over unobserved random variables such as arrivals and departures. The contribution of this paper is to formulate a latent variable model and develop a novel Gibbs sampler based on Markov bases to perform inference using the correct, but intractable, likelihood function. We empirically validate the convergence behavior of our sampler and demonstrate the ability of our model to make much finer-grained inferences than the previous approach.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/winner15.pdf",
        "supp": "",
        "pdf_size": 671682,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3289793218921941650&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "College of Information and Computer Sciences, University of Massachusetts, Amherst, MA 01002, USA; College of Information and Computer Sciences, University of Massachusetts, Amherst, MA 01002, USA; College of Information and Computer Sciences, University of Massachusetts, Amherst, MA 01002, USA + Department of Computer Science, Mount Holyoke College, South Hadley, MA 01075, USA",
        "aff_domain": "CS.UMASS.EDU;CS.UMASS.EDU;CS.UMASS.EDU",
        "email": "CS.UMASS.EDU;CS.UMASS.EDU;CS.UMASS.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "University of Massachusetts Amherst;Mount Holyoke College",
        "aff_unique_dep": "College of Information and Computer Sciences;Department of Computer Science",
        "aff_unique_url": "https://www.cics.umass.edu;https://www.mtholyoke.edu",
        "aff_unique_abbr": "UMass Amherst;MHC",
        "aff_campus_unique_index": "0;0;0+1",
        "aff_campus_unique": "Amherst;South Hadley",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "41117fdbde",
        "title": "Inferring Graphs from Cascades: A Sparse Recovery Framework",
        "site": "https://proceedings.mlr.press/v37/pouget-abadie15.html",
        "author": "Jean Pouget-Abadie; Thibaut Horel",
        "abstract": "In the Graph Inference problem, one seeks to recover the edges of an unknown graph from the observations of cascades propagating over this graph. In this paper, we approach this problem from the sparse recovery perspective. We introduce a general model of cascades, including the voter model and the independent cascade model, for which we provide the first algorithm which recovers the graph\u2019s edges with high probability and O(s log m) measurements where s is the maximum degree of the graph and m is the number of nodes. Furthermore, we show that our algorithm also recovers the edge weights (the parameters of the diffusion process) and is robust in the context of approximate sparsity. Finally we prove an almost matching lower bound of \u03a9(s \\log m/s) and validate our approach empirically on synthetic graphs.",
        "bibtex": "@InProceedings{pmlr-v37-pouget-abadie15,\n  title = \t {Inferring Graphs from Cascades: A Sparse Recovery Framework},\n  author = \t {Pouget-Abadie, Jean and Horel, Thibaut},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {977--986},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/pouget-abadie15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/pouget-abadie15.html},\n  abstract = \t {In the Graph Inference problem, one seeks to recover the edges of an unknown graph from the observations of cascades propagating over this graph. In this paper, we approach this problem from the sparse recovery perspective. We introduce a general model of cascades, including the voter model and the independent cascade model, for which we provide the first algorithm which recovers the graph\u2019s edges with high probability and O(s log m) measurements where s is the maximum degree of the graph and m is the number of nodes. Furthermore, we show that our algorithm also recovers the edge weights (the parameters of the diffusion process) and is robust in the context of approximate sparsity. Finally we prove an almost matching lower bound of \u03a9(s \\log m/s) and validate our approach empirically on synthetic graphs.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/pouget-abadie15.pdf",
        "supp": "",
        "pdf_size": 1013802,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10295576779952756840&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Harvard University; Harvard University",
        "aff_domain": "G.HARVARD.EDU;SEAS.HARVARD.EDU",
        "email": "G.HARVARD.EDU;SEAS.HARVARD.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Harvard University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.harvard.edu",
        "aff_unique_abbr": "Harvard",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6f58077d56",
        "title": "Information Geometry and Minimum Description Length Networks",
        "site": "https://proceedings.mlr.press/v37/suna15.html",
        "author": "Ke Sun; Jun Wang; Alexandros Kalousis; Stephan Marchand-Maillet",
        "abstract": "We study parametric unsupervised mixture learning. We measure the loss of intrinsic information from the observations to complex mixture models, and then to simple mixture models. We present a geometric picture, where all these representations are regarded as free points in the space of probability distributions. Based on minimum description length, we derive a simple geometric principle to learn all these models together. We present a new learning machine with theories, algorithms, and simulations.",
        "bibtex": "@InProceedings{pmlr-v37-suna15,\n  title = \t {Information Geometry and Minimum Description Length Networks},\n  author = \t {Sun, Ke and Wang, Jun and Kalousis, Alexandros and Marchand-Maillet, Stephan},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {49--58},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/suna15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/suna15.html},\n  abstract = \t {We study parametric unsupervised mixture learning. We measure the loss of intrinsic information from the observations to complex mixture models, and then to simple mixture models. We present a geometric picture, where all these representations are regarded as free points in the space of probability distributions. Based on minimum description length, we derive a simple geometric principle to learn all these models together. We present a new learning machine with theories, algorithms, and simulations.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/suna15.pdf",
        "supp": "",
        "pdf_size": 1205580,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14764502055070436075&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Viper Group, Computer Vision and Multimedia Laboratory, University of Geneva, Switzerland; Expedia, Switzerland; Business Informatics Department, University of Applied Sciences, Western Switzerland+Department of Computer Science, University of Geneva, Switzerland; Viper Group, Computer Vision and Multimedia Laboratory, University of Geneva, Switzerland",
        "aff_domain": "GMAIL.COM;EXPEDIA.COM;HESGE.CH;UNIGE.CH",
        "email": "GMAIL.COM;EXPEDIA.COM;HESGE.CH;UNIGE.CH",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2+0;0",
        "aff_unique_norm": "University of Geneva;Expedia;University of Applied Sciences Western Switzerland",
        "aff_unique_dep": "Computer Vision and Multimedia Laboratory;;Business Informatics Department",
        "aff_unique_url": "https://www.unige.ch;https://www.expedia.com;https://www.hes-so.ch/en",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "50ab162b08",
        "title": "Intersecting Faces: Non-negative Matrix Factorization With New Guarantees",
        "site": "https://proceedings.mlr.press/v37/geb15.html",
        "author": "Rong Ge; James Zou",
        "abstract": "Non-negative matrix factorization (NMF) is a natural model of admixture and is widely used in science and engineering. A plethora of algorithms have been developed to tackle NMF, but due to the non-convex nature of the problem, there is little guarantee on how well these methods work. Recently a surge of research have focused on a very restricted class of NMFs, called separable NMF, where provably correct algorithms have been developed. In this paper, we propose the notion of subset-separable NMF, which substantially generalizes the property of separability. We show that subset-separability is a natural necessary condition for the factorization to be unique or to have minimum volume. We developed the Face-Intersect algorithm which provably and efficiently solves subset-separable NMF under natural conditions, and we prove that our algorithm is robust to small noise. We explored the performance of Face-Intersect on simulations and discuss settings where it empirically outperformed the state-of-art methods. Our work is a step towards finding provably correct algorithms that solve large classes of NMF problems.",
        "bibtex": "@InProceedings{pmlr-v37-geb15,\n  title = \t {Intersecting Faces: Non-negative Matrix Factorization With New Guarantees},\n  author = \t {Ge, Rong and Zou, James},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2295--2303},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/geb15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/geb15.html},\n  abstract = \t {Non-negative matrix factorization (NMF) is a natural model of admixture and is widely used in science and engineering. A plethora of algorithms have been developed to tackle NMF, but due to the non-convex nature of the problem, there is little guarantee on how well these methods work. Recently a surge of research have focused on a very restricted class of NMFs, called separable NMF, where provably correct algorithms have been developed. In this paper, we propose the notion of subset-separable NMF, which substantially generalizes the property of separability. We show that subset-separability is a natural necessary condition for the factorization to be unique or to have minimum volume. We developed the Face-Intersect algorithm which provably and efficiently solves subset-separable NMF under natural conditions, and we prove that our algorithm is robust to small noise. We explored the performance of Face-Intersect on simulations and discuss settings where it empirically outperformed the state-of-art methods. Our work is a step towards finding provably correct algorithms that solve large classes of NMF problems.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/geb15.pdf",
        "supp": "",
        "pdf_size": 462171,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15983555689941550951&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Microsoft Research New England; Microsoft Research New England",
        "aff_domain": "microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Microsoft Research",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/group/microsoft-research-new-england",
        "aff_unique_abbr": "MSR NE",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "New England",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b5cf4c196b",
        "title": "Is Feature Selection Secure against Training Data Poisoning?",
        "site": "https://proceedings.mlr.press/v37/xiao15.html",
        "author": "Huang Xiao; Battista Biggio; Gavin Brown; Giorgio Fumera; Claudia Eckert; Fabio Roli",
        "abstract": "Learning in adversarial settings is becoming an important task for application domains where attackers may inject malicious data into the training set to subvert normal operation of data-driven technologies. Feature selection has been widely used in machine learning for security applications to improve generalization and computational efficiency, although it is not clear whether its use may be beneficial or even counterproductive when training data are poisoned by intelligent attackers. In this work, we shed light on this issue by providing a framework to investigate the robustness of popular feature selection methods, including LASSO, ridge regression and the elastic net. Our results on malware detection show that feature selection methods can be significantly compromised under attack (we can reduce LASSO to almost random choices of feature sets by careful insertion of less than 5% poisoned training samples), highlighting the need for specific countermeasures.",
        "bibtex": "@InProceedings{pmlr-v37-xiao15,\n  title = \t {Is Feature Selection Secure against Training Data Poisoning?},\n  author = \t {Xiao, Huang and Biggio, Battista and Brown, Gavin and Fumera, Giorgio and Eckert, Claudia and Roli, Fabio},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1689--1698},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/xiao15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/xiao15.html},\n  abstract = \t {Learning in adversarial settings is becoming an important task for application domains where attackers may inject malicious data into the training set to subvert normal operation of data-driven technologies. Feature selection has been widely used in machine learning for security applications to improve generalization and computational efficiency, although it is not clear whether its use may be beneficial or even counterproductive when training data are poisoned by intelligent attackers. In this work, we shed light on this issue by providing a framework to investigate the robustness of popular feature selection methods, including LASSO, ridge regression and the elastic net. Our results on malware detection show that feature selection methods can be significantly compromised under attack (we can reduce LASSO to almost random choices of feature sets by careful insertion of less than 5% poisoned training samples), highlighting the need for specific countermeasures.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/xiao15.pdf",
        "supp": "",
        "pdf_size": 1610313,
        "gs_citation": 545,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1517576158054536968&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Computer Science, Technische Universit\u00a8at M\u00a8unchen; Department of Electrical and Electronic Engineering, University of Cagliari; School of Computer Science, University of Manchester; Department of Electrical and Electronic Engineering, University of Cagliari; Department of Computer Science, Technische Universit\u00a8at M\u00a8unchen; Department of Electrical and Electronic Engineering, University of Cagliari",
        "aff_domain": "IN.TUM.DE;DIEE.UNICA.IT;MANCHESTER.AC.UK;DIEE.UNICA.IT;IN.TUM.DE;DIEE.UNICA.IT",
        "email": "IN.TUM.DE;DIEE.UNICA.IT;MANCHESTER.AC.UK;DIEE.UNICA.IT;IN.TUM.DE;DIEE.UNICA.IT",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;1;0;1",
        "aff_unique_norm": "Technische Universit\u00e4t M\u00fcnchen;University of Cagliari;University of Manchester",
        "aff_unique_dep": "Department of Computer Science;Department of Electrical and Electronic Engineering;School of Computer Science",
        "aff_unique_url": "https://www.tum.de;https://www.unica.it;https://www.manchester.ac.uk",
        "aff_unique_abbr": "TUM;;UoM",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Manchester",
        "aff_country_unique_index": "0;1;2;1;0;1",
        "aff_country_unique": "Germany;Italy;United Kingdom"
    },
    {
        "id": "88d83c214e",
        "title": "JUMP-Means: Small-Variance Asymptotics for Markov Jump Processes",
        "site": "https://proceedings.mlr.press/v37/hugginsa15.html",
        "author": "Jonathan Huggins; Karthik Narasimhan; Ardavan Saeedi; Vikash Mansinghka",
        "abstract": "Markov jump processes (MJPs) are used to model a wide range of phenomenon from disease progression to RNA path folding. However, existing methods suffer from a number of shortcomings: degenerate trajectories in the case of ML estimation of parametric models and poor inferential performance in the case of nonparametric models. We take a small-variance asymptotics (SVA) approach to overcome these limitations. We derive the small-variance asymptotics for parametric and nonparametric MJPs for both directly observed and hidden state models. In the parametric case we obtain a novel objective function which leads to non-degenerate trajectories. To derive the nonparametric version we introduce the gamma-gamma process, a novel extension to the gamma-exponential process. We propose algorithms for each of these formulations, which we call \\emphJUMP-means. Our experiments demonstrate that JUMP-means is competitive with or outperforms widely used MJP inference approaches in terms of both speed and reconstruction accuracy.",
        "bibtex": "@InProceedings{pmlr-v37-hugginsa15,\n  title = \t {JUMP-Means: Small-Variance Asymptotics for Markov Jump Processes},\n  author = \t {Huggins, Jonathan and Narasimhan, Karthik and Saeedi, Ardavan and Mansinghka, Vikash},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {693--701},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/hugginsa15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/hugginsa15.html},\n  abstract = \t {Markov jump processes (MJPs) are used to model a wide range of phenomenon from disease progression to RNA path folding. However, existing methods suffer from a number of shortcomings: degenerate trajectories in the case of ML estimation of parametric models and poor inferential performance in the case of nonparametric models. We take a small-variance asymptotics (SVA) approach to overcome these limitations. We derive the small-variance asymptotics for parametric and nonparametric MJPs for both directly observed and hidden state models. In the parametric case we obtain a novel objective function which leads to non-degenerate trajectories. To derive the nonparametric version we introduce the gamma-gamma process, a novel extension to the gamma-exponential process. We propose algorithms for each of these formulations, which we call \\emphJUMP-means. Our experiments demonstrate that JUMP-means is competitive with or outperforms widely used MJP inference approaches in terms of both speed and reconstruction accuracy.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/hugginsa15.pdf",
        "supp": "",
        "pdf_size": 570840,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9964894117430232098&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Computer Science and Artificial Intelligence Laboratory, MIT; Computer Science and Artificial Intelligence Laboratory, MIT; Computer Science and Artificial Intelligence Laboratory, MIT; Computer Science and Artificial Intelligence Laboratory, MIT",
        "aff_domain": "MIT.EDU;MIT.EDU;MIT.EDU;MIT.EDU",
        "email": "MIT.EDU;MIT.EDU;MIT.EDU;MIT.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.csail.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0e0b7f61d4",
        "title": "K-hyperplane Hinge-Minimax Classifier",
        "site": "https://proceedings.mlr.press/v37/osadchy15.html",
        "author": "Margarita Osadchy; Tamir Hazan; Daniel Keren",
        "abstract": "We explore a novel approach to upper bound the misclassification error for problems with data comprising a small number of positive samples and a large number of negative samples. We assign the hinge-loss to upper bound the misclassification error of the positive examples and use the minimax risk to upper bound the misclassification error with respect to the worst case distribution that generates the negative examples. This approach is computationally appealing since the majority of training examples (belonging to the negative class) are represented by the statistics of their distribution, in contrast to kernel SVM which produces a very large number of support vectors in such settings. We derive empirical risk bounds for linear and non-linear classification and show that they are dimensionally independent and decay as 1/\\sqrtm for m samples. We propose an efficient algorithm for training an intersection of finite number of hyperplane and demonstrate its effectiveness on real data, including letter and scene recognition.",
        "bibtex": "@InProceedings{pmlr-v37-osadchy15,\n  title = \t {K-hyperplane Hinge-Minimax Classifier},\n  author = \t {Osadchy, Margarita and Hazan, Tamir and Keren, Daniel},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1558--1566},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/osadchy15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/osadchy15.html},\n  abstract = \t {We explore a novel approach to upper bound the misclassification error for problems with data comprising a small number of positive samples and a large number of negative samples. We assign the hinge-loss to upper bound the misclassification error of the positive examples and use the minimax risk to upper bound the misclassification error with respect to the worst case distribution that generates the negative examples. This approach is computationally appealing since the majority of training examples (belonging to the negative class) are represented by the statistics of their distribution, in contrast to kernel SVM which produces a very large number of support vectors in such settings. We derive empirical risk bounds for linear and non-linear classification and show that they are dimensionally independent and decay as 1/\\sqrtm for m samples. We propose an efficient algorithm for training an intersection of finite number of hyperplane and demonstrate its effectiveness on real data, including letter and scene recognition.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/osadchy15.pdf",
        "supp": "",
        "pdf_size": 524606,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4421782756746325638&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computer Science, University of Haifa, Mount Carmel, Haifa 31905, Israel; Department of Computer Science, University of Haifa, Mount Carmel, Haifa 31905, Israel; Department of Computer Science, University of Haifa, Mount Carmel, Haifa 31905, Israel",
        "aff_domain": "CS.HAIFA.AC.IL;CS.HAIFA.AC.IL;CS.HAIFA.AC.IL",
        "email": "CS.HAIFA.AC.IL;CS.HAIFA.AC.IL;CS.HAIFA.AC.IL",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Haifa",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.haifa.ac.il",
        "aff_unique_abbr": "UoH",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Haifa",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "0a110e783f",
        "title": "Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP)",
        "site": "https://proceedings.mlr.press/v37/wilson15.html",
        "author": "Andrew Wilson; Hannes Nickisch",
        "abstract": "We introduce a new structured kernel interpolation (SKI) framework, which generalises and unifies inducing point methods for scalable Gaussian processes (GPs). SKI methods produce kernel approximations for fast computations through kernel interpolation. The SKI framework clarifies how the quality of an inducing point approach depends on the number of inducing (aka interpolation) points, interpolation strategy, and GP covariance kernel. SKI also provides a mechanism to create new scalable kernel methods, through choosing different kernel interpolation strategies. Using SKI, with local cubic kernel interpolation, we introduce KISS-GP, which is 1) more scalable than inducing point alternatives, 2) naturally enables Kronecker and Toeplitz algebra for substantial additional gains in scalability, without requiring any grid data, and 3) can be used for fast and expressive kernel learning. KISS-GP costs O(n) time and storage for GP inference. We evaluate KISS-GP for kernel matrix approximation, kernel learning, and natural sound modelling.",
        "bibtex": "@InProceedings{pmlr-v37-wilson15,\n  title = \t {Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP)},\n  author = \t {Wilson, Andrew and Nickisch, Hannes},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1775--1784},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/wilson15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/wilson15.html},\n  abstract = \t {We introduce a new structured kernel interpolation (SKI) framework, which generalises and unifies inducing point methods for scalable Gaussian processes (GPs). SKI methods produce kernel approximations for fast computations through kernel interpolation. The SKI framework clarifies how the quality of an inducing point approach depends on the number of inducing (aka interpolation) points, interpolation strategy, and GP covariance kernel. SKI also provides a mechanism to create new scalable kernel methods, through choosing different kernel interpolation strategies. Using SKI, with local cubic kernel interpolation, we introduce KISS-GP, which is 1) more scalable than inducing point alternatives, 2) naturally enables Kronecker and Toeplitz algebra for substantial additional gains in scalability, without requiring any grid data, and 3) can be used for fast and expressive kernel learning. KISS-GP costs O(n) time and storage for GP inference. We evaluate KISS-GP for kernel matrix approximation, kernel learning, and natural sound modelling.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/wilson15.pdf",
        "supp": "",
        "pdf_size": 1650220,
        "gs_citation": 684,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10688700732600694368&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Carnegie Mellon University; Philips Research Hamburg",
        "aff_domain": "CS.CMU.EDU;NICKISCH.ORG",
        "email": "CS.CMU.EDU;NICKISCH.ORG",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Carnegie Mellon University;Philips Research",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;https://www.philips.com/research",
        "aff_unique_abbr": "CMU;Philips Research",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Hamburg",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;Germany"
    },
    {
        "id": "4ac69b1011",
        "title": "Landmarking Manifolds with Gaussian Processes",
        "site": "https://proceedings.mlr.press/v37/liang15.html",
        "author": "Dawen Liang; John Paisley",
        "abstract": "We present an algorithm for finding landmarks along a manifold. These landmarks provide a small set of locations spaced out along the manifold such that they capture the low-dimensional non-linear structure of the data embedded in the high-dimensional space. The approach does not select points directly from the dataset, but instead we optimize each landmark by moving along the continuous manifold space (as approximated by the data) according to the gradient of an objective function. We borrow ideas from active learning with Gaussian processes to define the objective, which has the property that a new landmark is \"repelled\" by those currently selected, allowing for exploration of the manifold. We derive a stochastic algorithm for learning with large datasets and show results on several datasets, including the Million Song Dataset and articles from the New York Times.",
        "bibtex": "@InProceedings{pmlr-v37-liang15,\n  title = \t {Landmarking Manifolds with Gaussian Processes},\n  author = \t {Liang, Dawen and Paisley, John},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {466--474},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/liang15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/liang15.html},\n  abstract = \t {We present an algorithm for finding landmarks along a manifold. These landmarks provide a small set of locations spaced out along the manifold such that they capture the low-dimensional non-linear structure of the data embedded in the high-dimensional space. The approach does not select points directly from the dataset, but instead we optimize each landmark by moving along the continuous manifold space (as approximated by the data) according to the gradient of an objective function. We borrow ideas from active learning with Gaussian processes to define the objective, which has the property that a new landmark is \"repelled\" by those currently selected, allowing for exploration of the manifold. We derive a stochastic algorithm for learning with large datasets and show results on several datasets, including the Million Song Dataset and articles from the New York Times.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/liang15.pdf",
        "supp": "",
        "pdf_size": 1063095,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=747993389582643157&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Electrical Engineering, Columbia University, New York, NY, USA; Department of Electrical Engineering, Columbia University, New York, NY, USA",
        "aff_domain": "EE.COLUMBIA.EDU;COLUMBIA.EDU",
        "email": "EE.COLUMBIA.EDU;COLUMBIA.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "New York",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4bcf89fc22",
        "title": "Large-Scale Markov Decision Problems with KL Control Cost and its Application to Crowdsourcing",
        "site": "https://proceedings.mlr.press/v37/abbasi-yadkori15.html",
        "author": "Yasin Abbasi-Yadkori; Peter Bartlett; Xi Chen; Alan Malek",
        "abstract": "We study average and total cost Markov decision problems with large state spaces. Since the computational and statistical costs of finding the optimal policy scale with the size of the state space, we focus on searching for near-optimality in a low-dimensional family of policies. In particular, we show that for problems with a Kullback-Leibler divergence cost function, we can reduce policy optimization to a convex optimization and solve it approximately using a stochastic subgradient algorithm. We show that the performance of the resulting policy is close to the best in the low-dimensional family. We demonstrate the efficacy of our approach by controlling the important crowdsourcing application of budget allocation in crowd labeling.",
        "bibtex": "@InProceedings{pmlr-v37-abbasi-yadkori15,\n  title = \t {Large-Scale Markov Decision Problems with KL Control Cost and its Application to Crowdsourcing},\n  author = \t {Abbasi-Yadkori, Yasin and Bartlett, Peter and Chen, Xi and Malek, Alan},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1053--1062},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/abbasi-yadkori15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/abbasi-yadkori15.html},\n  abstract = \t {We study average and total cost Markov decision problems with large state spaces. Since the computational and statistical costs of finding the optimal policy scale with the size of the state space, we focus on searching for near-optimality in a low-dimensional family of policies. In particular, we show that for problems with a Kullback-Leibler divergence cost function, we can reduce policy optimization to a convex optimization and solve it approximately using a stochastic subgradient algorithm. We show that the performance of the resulting policy is close to the best in the low-dimensional family. We demonstrate the efficacy of our approach by controlling the important crowdsourcing application of budget allocation in crowd labeling.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/abbasi-yadkori15.pdf",
        "supp": "",
        "pdf_size": 336762,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1470686836091579223&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Queensland University of Technology; University of California, Berkeley + Queensland University of Technology; Stern School of Business, New York University; University of California, Berkeley",
        "aff_domain": "QUT.EDU.AU;CS.BERKELEY.EDU;NYU.EDU;EECS.BERKELEY.EDU",
        "email": "QUT.EDU.AU;CS.BERKELEY.EDU;NYU.EDU;EECS.BERKELEY.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;2;1",
        "aff_unique_norm": "Queensland University of Technology;University of California, Berkeley;New York University",
        "aff_unique_dep": ";;Stern School of Business",
        "aff_unique_url": "https://www.qut.edu.au;https://www.berkeley.edu;https://www.nyu.edu",
        "aff_unique_abbr": "QUT;UC Berkeley;NYU",
        "aff_campus_unique_index": "1;2;1",
        "aff_campus_unique": ";Berkeley;New York",
        "aff_country_unique_index": "0;1+0;1;1",
        "aff_country_unique": "Australia;United States"
    },
    {
        "id": "2fa8efd9bc",
        "title": "Large-scale Distributed Dependent Nonparametric Trees",
        "site": "https://proceedings.mlr.press/v37/hu15.html",
        "author": "Zhiting Hu; Ho Qirong; Avinava Dubey; Eric Xing",
        "abstract": "Practical applications of Bayesian nonparametric (BNP) models have been limited, due to their high computational complexity and poor scaling on large data. In this paper, we consider dependent nonparametric trees (DNTs), a powerful infinite model that captures time-evolving hierarchies, and develop a large-scale distributed training system. Our major contributions include: (1) an effective memoized variational inference for DNTs, with a novel birth-merge strategy for exploring the unbounded tree space; (2) a model-parallel scheme for concurrent tree growing/pruning and efficient model alignment, through conflict-free model partitioning and lightweight synchronization; (3) a data-parallel scheme for variational parameter updates that allows distributed processing of massive data. Using 64 cores in 36 hours, our system learns a 10K-node DNT topic model on 8M documents that captures both high-frequency and long-tail topics. Our data and model scales are orders-of-magnitude larger than recent results on the hierarchical Dirichlet process, and the near-linear scalability indicates great potential for even bigger problem sizes.",
        "bibtex": "@InProceedings{pmlr-v37-hu15,\n  title = \t {Large-scale Distributed Dependent Nonparametric Trees},\n  author = \t {Hu, Zhiting and Qirong, Ho and Dubey, Avinava and Xing, Eric},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1651--1659},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/hu15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/hu15.html},\n  abstract = \t {Practical applications of Bayesian nonparametric (BNP) models have been limited, due to their high computational complexity and poor scaling on large data. In this paper, we consider dependent nonparametric trees (DNTs), a powerful infinite model that captures time-evolving hierarchies, and develop a large-scale distributed training system. Our major contributions include: (1) an effective memoized variational inference for DNTs, with a novel birth-merge strategy for exploring the unbounded tree space; (2) a model-parallel scheme for concurrent tree growing/pruning and efficient model alignment, through conflict-free model partitioning and lightweight synchronization; (3) a data-parallel scheme for variational parameter updates that allows distributed processing of massive data. Using 64 cores in 36 hours, our system learns a 10K-node DNT topic model on 8M documents that captures both high-frequency and long-tail topics. Our data and model scales are orders-of-magnitude larger than recent results on the hierarchical Dirichlet process, and the near-linear scalability indicates great potential for even bigger problem sizes.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/hu15.pdf",
        "supp": "",
        "pdf_size": 751664,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7630735540056745487&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Language Technologies Institute, Carnegie Mellon University; Institute for Infocomm Research, A*STAR; Machine Learning Department, Carnegie Mellon University; Machine Learning Department, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;gmail.com;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;gmail.com;cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Carnegie Mellon University;Institute for Infocomm Research",
        "aff_unique_dep": "Language Technologies Institute;",
        "aff_unique_url": "https://www.cmu.edu;https://www.i2r.a-star.edu.sg",
        "aff_unique_abbr": "CMU;I2R",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Pittsburgh;",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "United States;Singapore"
    },
    {
        "id": "f6d995ed85",
        "title": "Large-scale log-determinant computation through stochastic Chebyshev expansions",
        "site": "https://proceedings.mlr.press/v37/hana15.html",
        "author": "Insu Han; Dmitry Malioutov; Jinwoo Shin",
        "abstract": "Logarithms of determinants of large positive definite matrices appear ubiquitously in machine learning applications including Gaussian graphical and Gaussian process models, partition functions of discrete graphical models, minimum-volume ellipsoids and metric and kernel learning. Log-determinant computation involves the Cholesky decomposition at the cost cubic in the number of variables (i.e., the matrix dimension), which makes it prohibitive for large-scale applications. We propose a linear-time randomized algorithm to approximate log-determinants for very large-scale positive definite and general non-singular matrices using a stochastic trace approximation, called the Hutchinson method, coupled with Chebyshev polynomial expansions that both rely on efficient matrix-vector multiplications. We establish rigorous additive and multiplicative approximation error bounds depending on the condition number of the input matrix. In our experiments, the proposed algorithm can provide very high accuracy solutions at orders of magnitude faster time than the Cholesky decomposition and Shur completion, and enables us to compute log-determinants of matrices involving tens of millions of variables.",
        "bibtex": "@InProceedings{pmlr-v37-hana15,\n  title = \t {Large-scale log-determinant computation through stochastic Chebyshev expansions},\n  author = \t {Han, Insu and Malioutov, Dmitry and Shin, Jinwoo},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {908--917},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/hana15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/hana15.html},\n  abstract = \t {Logarithms of determinants of large positive definite matrices appear ubiquitously in machine learning applications including Gaussian graphical and Gaussian process models, partition functions of discrete graphical models, minimum-volume ellipsoids and metric and kernel learning. Log-determinant computation involves the Cholesky decomposition at the cost cubic in the number of variables (i.e., the matrix dimension), which makes it prohibitive for large-scale applications. We propose a linear-time randomized algorithm to approximate log-determinants for very large-scale positive definite and general non-singular matrices using a stochastic trace approximation, called the Hutchinson method, coupled with Chebyshev polynomial expansions that both rely on efficient matrix-vector multiplications. We establish rigorous additive and multiplicative approximation error bounds depending on the condition number of the input matrix. In our experiments, the proposed algorithm can provide very high accuracy solutions at orders of magnitude faster time than the Cholesky decomposition and Shur completion, and enables us to compute log-determinants of matrices involving tens of millions of variables.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/hana15.pdf",
        "supp": "",
        "pdf_size": 1072028,
        "gs_citation": 121,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1482496054070063032&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Korea; Business Analytics and Mathematical Sciences, IBM Research, Yorktown Heights, NY, USA; Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Korea",
        "aff_domain": "kaist.ac.kr;us.ibm.com;kaist.ac.kr",
        "email": "kaist.ac.kr;us.ibm.com;kaist.ac.kr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology;IBM",
        "aff_unique_dep": "Department of Electrical Engineering;Business Analytics and Mathematical Sciences",
        "aff_unique_url": "https://www.kaist.ac.kr;https://www.ibm.com/research",
        "aff_unique_abbr": "KAIST;IBM",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Yorktown Heights",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "South Korea;United States"
    },
    {
        "id": "707329c6fb",
        "title": "Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data",
        "site": "https://proceedings.mlr.press/v37/gala15.html",
        "author": "Yarin Gal; Yutian Chen; Zoubin Ghahramani",
        "abstract": "Multivariate categorical data occur in many applications of machine learning. One of the main difficulties with these vectors of categorical variables is sparsity. The number of possible observations grows exponentially with vector length, but dataset diversity might be poor in comparison. Recent models have gained significant improvement in supervised tasks with this data. These models embed observations in a continuous space to capture similarities between them. Building on these ideas we propose a Bayesian model for the unsupervised task of distribution estimation of multivariate categorical data. We model vectors of categorical variables as generated from a non-linear transformation of a continuous latent space. Non-linearity captures multi-modality in the distribution. The continuous representation addresses sparsity. Our model ties together many existing models, linking the linear categorical latent Gaussian model, the Gaussian process latent variable model, and Gaussian process classification. We derive inference for our model based on recent developments in sampling based variational inference. We show empirically that the model outperforms its linear and discrete counterparts in imputation tasks of sparse data.",
        "bibtex": "@InProceedings{pmlr-v37-gala15,\n  title = \t {Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data},\n  author = \t {Gal, Yarin and Chen, Yutian and Ghahramani, Zoubin},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {645--654},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/gala15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/gala15.html},\n  abstract = \t {Multivariate categorical data occur in many applications of machine learning. One of the main difficulties with these vectors of categorical variables is sparsity. The number of possible observations grows exponentially with vector length, but dataset diversity might be poor in comparison. Recent models have gained significant improvement in supervised tasks with this data. These models embed observations in a continuous space to capture similarities between them. Building on these ideas we propose a Bayesian model for the unsupervised task of distribution estimation of multivariate categorical data. We model vectors of categorical variables as generated from a non-linear transformation of a continuous latent space. Non-linearity captures multi-modality in the distribution. The continuous representation addresses sparsity. Our model ties together many existing models, linking the linear categorical latent Gaussian model, the Gaussian process latent variable model, and Gaussian process classification. We derive inference for our model based on recent developments in sampling based variational inference. We show empirically that the model outperforms its linear and discrete counterparts in imputation tasks of sparse data.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/gala15.pdf",
        "supp": "",
        "pdf_size": 1131487,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3140798470563876667&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "University of Cambridge; University of Cambridge; University of Cambridge",
        "aff_domain": "CAM.AC.UK;CAM.AC.UK;CAM.AC.UK",
        "email": "CAM.AC.UK;CAM.AC.UK;CAM.AC.UK",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "3874b1cf94",
        "title": "Latent Topic Networks: A Versatile Probabilistic Programming Framework for Topic Models",
        "site": "https://proceedings.mlr.press/v37/foulds15.html",
        "author": "James Foulds; Shachi Kumar; Lise Getoor",
        "abstract": "Topic models have become increasingly prominent text-analytic machine learning tools for research in the social sciences and the humanities. In particular, custom topic models can be developed to answer specific research questions. The design of these models requires a non-trivial amount of effort and expertise, motivating general-purpose topic modeling frameworks. In this paper we introduce latent topic networks, a flexible class of richly structured topic models designed to facilitate applied research. Custom models can straightforwardly be developed in our framework with an intuitive first-order logical probabilistic programming language. Latent topic networks admit scalable training via a parallelizable EM algorithm which leverages ADMM in the M-step. We demonstrate the broad applicability of the models with case studies on modeling influence in citation networks, and U.S. Presidential State of the Union addresses.",
        "bibtex": "@InProceedings{pmlr-v37-foulds15,\n  title = \t {Latent Topic Networks: A Versatile Probabilistic Programming Framework for Topic Models},\n  author = \t {Foulds, James and Kumar, Shachi and Getoor, Lise},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {777--786},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/foulds15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/foulds15.html},\n  abstract = \t {Topic models have become increasingly prominent text-analytic machine learning tools for research in the social sciences and the humanities. In particular, custom topic models can be developed to answer specific research questions. The design of these models requires a non-trivial amount of effort and expertise, motivating general-purpose topic modeling frameworks. In this paper we introduce latent topic networks, a flexible class of richly structured topic models designed to facilitate applied research. Custom models can straightforwardly be developed in our framework with an intuitive first-order logical probabilistic programming language. Latent topic networks admit scalable training via a parallelizable EM algorithm which leverages ADMM in the M-step. We demonstrate the broad applicability of the models with case studies on modeling influence in citation networks, and U.S. Presidential State of the Union addresses.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/foulds15.pdf",
        "supp": "",
        "pdf_size": 391706,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13015430980853393559&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, University of California, Santa Cruz, CA 95064 USA; Department of Computer Science, University of California, Santa Cruz, CA 95064 USA; Department of Computer Science, University of California, Santa Cruz, CA 95064 USA",
        "aff_domain": "UCSC.EDU;SOE.UCSC.EDU;SOE.UCSC.EDU",
        "email": "UCSC.EDU;SOE.UCSC.EDU;SOE.UCSC.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Santa Cruz",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ucsc.edu",
        "aff_unique_abbr": "UCSC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Santa Cruz",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2d619a6fe8",
        "title": "Learning Deep Structured Models",
        "site": "https://proceedings.mlr.press/v37/chenb15.html",
        "author": "Liang-Chieh Chen; Alexander Schwing; Alan Yuille; Raquel Urtasun",
        "abstract": "Many problems in real-world applications involve predicting several random variables that are statistically related. Markov random fields (MRFs) are a great mathematical tool to encode such dependencies. The goal of this paper is to combine MRFs with deep learning to estimate complex representations while taking into account the dependencies between the output random variables. Towards this goal, we propose a training algorithm that is able to learn structured models jointly with deep features that form the MRF potentials. Our approach is efficient as it blends learning and inference and makes use of GPU acceleration. We demonstrate the effectiveness of our algorithm in the tasks of predicting words from noisy images, as well as tagging of Flickr photographs. We show that joint learning of the deep features and the MRF parameters results in significant performance gains.",
        "bibtex": "@InProceedings{pmlr-v37-chenb15,\n  title = \t {Learning Deep Structured Models},\n  author = \t {Chen, Liang-Chieh and Schwing, Alexander and Yuille, Alan and Urtasun, Raquel},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1785--1794},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/chenb15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/chenb15.html},\n  abstract = \t {Many problems in real-world applications involve predicting several random variables that are statistically related. Markov random fields (MRFs) are a great mathematical tool to encode such dependencies. The goal of this paper is to combine MRFs with deep learning to estimate complex representations while taking into account the dependencies between the output random variables. Towards this goal, we propose a training algorithm that is able to learn structured models jointly with deep features that form the MRF potentials. Our approach is efficient as it blends learning and inference and makes use of GPU acceleration. We demonstrate the effectiveness of our algorithm in the tasks of predicting words from noisy images, as well as tagging of Flickr photographs. We show that joint learning of the deep features and the MRF parameters results in significant performance gains.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/chenb15.pdf",
        "supp": "",
        "pdf_size": 1096916,
        "gs_citation": 315,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4006467637565747418&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 21,
        "aff": "University of California Los Angeles, USA; University of Toronto, 10 King\u2019s College Rd., Toronto, Canada; University of California Los Angeles, USA; University of Toronto, 10 King\u2019s College Rd., Toronto, Canada",
        "aff_domain": "CS.UCLA.EDU;CS.TORONTO.EDU;STAT.UCLA.EDU;CS.TORONTO.EDU",
        "email": "CS.UCLA.EDU;CS.TORONTO.EDU;STAT.UCLA.EDU;CS.TORONTO.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "University of California, Los Angeles;University of Toronto",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucla.edu;https://www.utoronto.ca",
        "aff_unique_abbr": "UCLA;U of T",
        "aff_campus_unique_index": "0;1;0;1",
        "aff_campus_unique": "Los Angeles;Toronto",
        "aff_country_unique_index": "0;1;0;1",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "1fbb33f548",
        "title": "Learning Fast-Mixing Models for Structured Prediction",
        "site": "https://proceedings.mlr.press/v37/steinhardtb15.html",
        "author": "Jacob Steinhardt; Percy Liang",
        "abstract": "Markov Chain Monte Carlo (MCMC) algorithms are often used for approximate inference inside learning, but their slow mixing can be difficult to diagnose and the resulting approximate gradients can seriously degrade learning. To alleviate these issues, we define a new model family using strong Doeblin Markov chains, whose mixing times can be precisely controlled by a parameter. We also develop an algorithm to learn such models, which involves maximizing the data likelihood under the induced stationary distribution of these chains. We show empirical improvements on two challenging inference tasks.",
        "bibtex": "@InProceedings{pmlr-v37-steinhardtb15,\n  title = \t {Learning Fast-Mixing Models for Structured Prediction},\n  author = \t {Steinhardt, Jacob and Liang, Percy},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1063--1072},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/steinhardtb15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/steinhardtb15.html},\n  abstract = \t {Markov Chain Monte Carlo (MCMC) algorithms are often used for approximate inference inside learning, but their slow mixing can be difficult to diagnose and the resulting approximate gradients can seriously degrade learning. To alleviate these issues, we define a new model family using strong Doeblin Markov chains, whose mixing times can be precisely controlled by a parameter. We also develop an algorithm to learn such models, which involves maximizing the data likelihood under the induced stationary distribution of these chains. We show empirical improvements on two challenging inference tasks.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/steinhardtb15.pdf",
        "supp": "",
        "pdf_size": 1340204,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11336983583133963539&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Stanford University; Stanford University",
        "aff_domain": "CS.STANFORD.EDU;CS.STANFORD.EDU",
        "email": "CS.STANFORD.EDU;CS.STANFORD.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "89e6a67355",
        "title": "Learning Local Invariant Mahalanobis Distances",
        "site": "https://proceedings.mlr.press/v37/fetaya15.html",
        "author": "Ethan Fetaya; Shimon Ullman",
        "abstract": "For many tasks and data types, there are natural transformations to which the data should be invariant or insensitive. For instance, in visual recognition, natural images should be insensitive to rotation and translation. This requirement and its implications have been important in many machine learning applications, and tolerance for image transformations was primarily achieved by using robust feature vectors. In this paper we propose a novel and computationally efficient way to learn a local Mahalanobis metric per datum, and show how we can learn a local invariant metric to any transformation in order to improve performance.",
        "bibtex": "@InProceedings{pmlr-v37-fetaya15,\n  title = \t {Learning Local Invariant Mahalanobis Distances},\n  author = \t {Fetaya, Ethan and Ullman, Shimon},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {162--168},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/fetaya15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/fetaya15.html},\n  abstract = \t {For many tasks and data types, there are natural transformations to which the data should be invariant or insensitive. For instance, in visual recognition, natural images should be insensitive to rotation and translation. This requirement and its implications have been important in many machine learning applications, and tolerance for image transformations was primarily achieved by using robust feature vectors. In this paper we propose a novel and computationally efficient way to learn a local Mahalanobis metric per datum, and show how we can learn a local invariant metric to any transformation in order to improve performance.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/fetaya15.pdf",
        "supp": "",
        "pdf_size": 332629,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5200310223338526325&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Weizmann Institute of science; Weizmann Institute of science",
        "aff_domain": "weizmann.ac.il;weizmann.ac.il",
        "email": "weizmann.ac.il;weizmann.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Weizmann Institute of Science",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.weizmann.org.il",
        "aff_unique_abbr": "Weizmann",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "fcd1875fba",
        "title": "Learning Parametric-Output HMMs with Two Aliased States",
        "site": "https://proceedings.mlr.press/v37/weiss15.html",
        "author": "Roi Weiss; Boaz Nadler",
        "abstract": "In various applications involving hidden Markov models (HMMs), some of the hidden states are aliased, having identical output distributions. The minimality, identifiability and learnability of such aliased HMMs have been long standing problems, with only partial solutions provided thus far. In this paper we focus on parametric-output HMMs, whose output distributions come from a parametric family, and that have exactly two aliased states. For this class, we present a complete characterization of their minimality and identifiability. Furthermore, for a large family of parametric output distributions, we derive computationally efficient and statistically consistent algorithms to detect the presence of aliasing and learn the aliased HMM transition and emission parameters. We illustrate our theoretical analysis by several simulations.",
        "bibtex": "@InProceedings{pmlr-v37-weiss15,\n  title = \t {Learning Parametric-Output HMMs with Two Aliased States},\n  author = \t {Weiss, Roi and Nadler, Boaz},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {635--644},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/weiss15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/weiss15.html},\n  abstract = \t {In various applications involving hidden Markov models (HMMs), some of the hidden states are aliased, having identical output distributions. The minimality, identifiability and learnability of such aliased HMMs have been long standing problems, with only partial solutions provided thus far. In this paper we focus on parametric-output HMMs, whose output distributions come from a parametric family, and that have exactly two aliased states. For this class, we present a complete characterization of their minimality and identifiability. Furthermore, for a large family of parametric output distributions, we derive computationally efficient and statistically consistent algorithms to detect the presence of aliasing and learn the aliased HMM transition and emission parameters. We illustrate our theoretical analysis by several simulations.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/weiss15.pdf",
        "supp": "",
        "pdf_size": 452696,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13495634522678619880&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff": "Department of Computer Science, Ben-Gurion University, Beer Sheva, 84105, Israel; Department of Computer Science and Applied Mathematics, The Weizmann Institute of Science, Rehovot, 76100, Israel",
        "aff_domain": "CS.BGU.AC.IL;WEIZMANN.AC.IL",
        "email": "CS.BGU.AC.IL;WEIZMANN.AC.IL",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Ben-Gurion University;Weizmann Institute of Science",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science and Applied Mathematics",
        "aff_unique_url": "https://www.bgu.ac.il;https://www.weizmann.ac.il",
        "aff_unique_abbr": "BGU;Weizmann",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Beer Sheva;Rehovot",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "d38883c1bf",
        "title": "Learning Program Embeddings to Propagate Feedback on Student Code",
        "site": "https://proceedings.mlr.press/v37/piech15.html",
        "author": "Chris Piech; Jonathan Huang; Andy Nguyen; Mike Phulsuksombati; Mehran Sahami; Leonidas Guibas",
        "abstract": "Providing feedback, both assessing final work and giving hints to stuck students, is difficult for open-ended assignments in massive online classes which can range from thousands to millions of students. We introduce a neural network method to encode programs as a linear mapping from an embedded precondition space to an embedded postcondition space and propose an algorithm for feedback at scale using these linear maps as features. We apply our algorithm to assessments from the Code.org Hour of Code and Stanford University\u2019s CS1 course, where we propagate human comments on student assignments to orders of magnitude more submissions.",
        "bibtex": "@InProceedings{pmlr-v37-piech15,\n  title = \t {Learning Program Embeddings to Propagate Feedback on Student Code},\n  author = \t {Piech, Chris and Huang, Jonathan and Nguyen, Andy and Phulsuksombati, Mike and Sahami, Mehran and Guibas, Leonidas},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1093--1102},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/piech15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/piech15.html},\n  abstract = \t {Providing feedback, both assessing final work and giving hints to stuck students, is difficult for open-ended assignments in massive online classes which can range from thousands to millions of students. We introduce a neural network method to encode programs as a linear mapping from an embedded precondition space to an embedded postcondition space and propose an algorithm for feedback at scale using these linear maps as features. We apply our algorithm to assessments from the Code.org Hour of Code and Stanford University\u2019s CS1 course, where we propagate human comments on student assignments to orders of magnitude more submissions.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/piech15.pdf",
        "supp": "",
        "pdf_size": 767694,
        "gs_citation": 249,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13016376150314691200&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Stanford University; Google; Stanford University; Stanford University; Stanford University; Stanford University",
        "aff_domain": "CS.STANFORD.EDU;GOOGLE.COM;CS.STANFORD.EDU;CS.STANFORD.EDU;CS.STANFORD.EDU;CS.STANFORD.EDU",
        "email": "CS.STANFORD.EDU;GOOGLE.COM;CS.STANFORD.EDU;CS.STANFORD.EDU;CS.STANFORD.EDU;CS.STANFORD.EDU",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0;0;0",
        "aff_unique_norm": "Stanford University;Google",
        "aff_unique_dep": ";Google",
        "aff_unique_url": "https://www.stanford.edu;https://www.google.com",
        "aff_unique_abbr": "Stanford;Google",
        "aff_campus_unique_index": "0;1;0;0;0;0",
        "aff_campus_unique": "Stanford;Mountain View",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1d9b50dea9",
        "title": "Learning Scale-Free Networks by Dynamic Node Specific Degree Prior",
        "site": "https://proceedings.mlr.press/v37/tangb15.html",
        "author": "Qingming Tang; Siqi Sun; Jinbo Xu",
        "abstract": "Learning network structure underlying data is an important problem in machine learning. This paper presents a novel degree prior to study the inference of scale-free networks, which are widely used to model social and biological networks. In particular, this paper formulates scale-free network inference using Gaussian Graphical model (GGM) regularized by a node degree prior. Our degree prior not only promotes a desirable global degree distribution, but also exploits the estimated degree of an individual node and the relative strength of all the edges of a single node. To fulfill this, this paper proposes a ranking-based method to dynamically estimate the degree of a node, which makes the resultant optimization problem challenging to solve. To deal with this, this paper presents a novel ADMM (alternating direction method of multipliers) procedure. Our experimental results on both synthetic and real data show that our prior not only yields a scale-free network, but also produces many more correctly predicted edges than existing scale-free inducing prior, hub-inducing prior and the l_1 norm.",
        "bibtex": "@InProceedings{pmlr-v37-tangb15,\n  title = \t {Learning Scale-Free Networks by Dynamic Node Specific Degree Prior},\n  author = \t {Tang, Qingming and Sun, Siqi and Xu, Jinbo},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2247--2255},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/tangb15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/tangb15.html},\n  abstract = \t {Learning network structure underlying data is an important problem in machine learning. This paper presents a novel degree prior to study the inference of scale-free networks, which are widely used to model social and biological networks. In particular, this paper formulates scale-free network inference using Gaussian Graphical model (GGM) regularized by a node degree prior. Our degree prior not only promotes a desirable global degree distribution, but also exploits the estimated degree of an individual node and the relative strength of all the edges of a single node. To fulfill this, this paper proposes a ranking-based method to dynamically estimate the degree of a node, which makes the resultant optimization problem challenging to solve. To deal with this, this paper presents a novel ADMM (alternating direction method of multipliers) procedure. Our experimental results on both synthetic and real data show that our prior not only yields a scale-free network, but also produces many more correctly predicted edges than existing scale-free inducing prior, hub-inducing prior and the l_1 norm.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/tangb15.pdf",
        "supp": "",
        "pdf_size": 803923,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2256057102690424476&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Toyota Technological Institute at Chicago; Toyota Technological Institute at Chicago; Toyota Technological Institute at Chicago",
        "aff_domain": "TTIC.EDU;TTIC.EDU;GMAIL.COM",
        "email": "TTIC.EDU;TTIC.EDU;GMAIL.COM",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Toyota Technological Institute at Chicago",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tti-chicago.org",
        "aff_unique_abbr": "TTI Chicago",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Chicago",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a4f46f2dd5",
        "title": "Learning Submodular Losses with the Lovasz Hinge",
        "site": "https://proceedings.mlr.press/v37/yub15.html",
        "author": "Jiaqian Yu; Matthew Blaschko",
        "abstract": "Learning with non-modular losses is an important problem when sets of predictions are made simultaneously. The main tools for constructing convex surrogate loss functions for set prediction are margin rescaling and slack rescaling. In this work, we show that these strategies lead to tight convex surrogates iff the underlying loss function is increasing in the number of incorrect predictions. However, gradient or cutting-plane computation for these functions is NP-hard for non-supermodular loss functions. We propose instead a novel convex surrogate loss function for submodular losses, the Lovasz hinge, which leads to O(p log p) complexity with O(p) oracle accesses to the loss function to compute a gradient or cutting-plane. As a result, we have developed the first tractable convex surrogates in the literature for submodular losses. We demonstrate the utility of this novel convex surrogate through a real world image labeling task.",
        "bibtex": "@InProceedings{pmlr-v37-yub15,\n  title = \t {Learning Submodular Losses with the Lovasz Hinge},\n  author = \t {Yu, Jiaqian and Blaschko, Matthew},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1623--1631},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/yub15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/yub15.html},\n  abstract = \t {Learning with non-modular losses is an important problem when sets of predictions are made simultaneously. The main tools for constructing convex surrogate loss functions for set prediction are margin rescaling and slack rescaling. In this work, we show that these strategies lead to tight convex surrogates iff the underlying loss function is increasing in the number of incorrect predictions. However, gradient or cutting-plane computation for these functions is NP-hard for non-supermodular loss functions. We propose instead a novel convex surrogate loss function for submodular losses, the Lovasz hinge, which leads to O(p log p) complexity with O(p) oracle accesses to the loss function to compute a gradient or cutting-plane. As a result, we have developed the first tractable convex surrogates in the literature for submodular losses. We demonstrate the utility of this novel convex surrogate through a real world image labeling task.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/yub15.pdf",
        "supp": "",
        "pdf_size": 1067441,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1683302357410202931&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "CentraleSup \u00b4elec & Inria; CentraleSup \u00b4elec & Inria",
        "aff_domain": "CENTRALESUPELEC.FR;INRIA.FR",
        "email": "CENTRALESUPELEC.FR;INRIA.FR",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "CentraleSup\u00e9lec",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.centralesupelec.fr",
        "aff_unique_abbr": "CentraleSup\u00e9lec",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "c1a8d4e734",
        "title": "Learning Transferable Features with Deep Adaptation Networks",
        "site": "https://proceedings.mlr.press/v37/long15.html",
        "author": "Mingsheng Long; Yue Cao; Jianmin Wang; Michael Jordan",
        "abstract": "Recent studies reveal that a deep neural network can learn transferable features which generalize well to novel tasks for domain adaptation. However, as deep features eventually transition from general to specific along the network, the feature transferability drops significantly in higher layers with increasing domain discrepancy. Hence, it is important to formally reduce the dataset bias and enhance the transferability in task-specific layers. In this paper, we propose a new Deep Adaptation Network (DAN) architecture, which generalizes deep convolutional neural network to the domain adaptation scenario. In DAN, hidden representations of all task-specific layers are embedded in a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched. The domain discrepancy is further reduced using an optimal multi-kernel selection method for mean embedding matching. DAN can learn transferable features with statistical guarantees, and can scale linearly by unbiased estimate of kernel embedding. Extensive empirical evidence shows that the proposed architecture yields state-of-the-art image classification error rates on standard domain adaptation benchmarks.",
        "bibtex": "@InProceedings{pmlr-v37-long15,\n  title = \t {Learning Transferable Features with Deep Adaptation Networks},\n  author = \t {Long, Mingsheng and Cao, Yue and Wang, Jianmin and Jordan, Michael},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {97--105},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/long15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/long15.html},\n  abstract = \t {Recent studies reveal that a deep neural network can learn transferable features which generalize well to novel tasks for domain adaptation. However, as deep features eventually transition from general to specific along the network, the feature transferability drops significantly in higher layers with increasing domain discrepancy. Hence, it is important to formally reduce the dataset bias and enhance the transferability in task-specific layers. In this paper, we propose a new Deep Adaptation Network (DAN) architecture, which generalizes deep convolutional neural network to the domain adaptation scenario. In DAN, hidden representations of all task-specific layers are embedded in a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched. The domain discrepancy is further reduced using an optimal multi-kernel selection method for mean embedding matching. DAN can learn transferable features with statistical guarantees, and can scale linearly by unbiased estimate of kernel embedding. Extensive empirical evidence shows that the proposed architecture yields state-of-the-art image classification error rates on standard domain adaptation benchmarks.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/long15.pdf",
        "supp": "",
        "pdf_size": 336634,
        "gs_citation": 6646,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10097353709258117195&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "School of Software, TNList Lab for Info. Sci. & Tech., Institute for Data Science, Tsinghua University, China+Department of Electrical Engineering and Computer Science, University of California, Berkeley, CA, USA; School of Software, TNList Lab for Info. Sci. & Tech., Institute for Data Science, Tsinghua University, China; School of Software, TNList Lab for Info. Sci. & Tech., Institute for Data Science, Tsinghua University, China; Department of Electrical Engineering and Computer Science, University of California, Berkeley, CA, USA",
        "aff_domain": "tsinghua.edu.cn;mails.tsinghua.edu.cn;tsinghua.edu.cn;berkeley.edu",
        "email": "tsinghua.edu.cn;mails.tsinghua.edu.cn;tsinghua.edu.cn;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0;1",
        "aff_unique_norm": "Tsinghua University;University of California, Berkeley",
        "aff_unique_dep": "School of Software;Department of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.berkeley.edu",
        "aff_unique_abbr": "THU;UC Berkeley",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0+1;0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "1c247ca530",
        "title": "Learning Word Representations with Hierarchical Sparse Coding",
        "site": "https://proceedings.mlr.press/v37/yogatama15.html",
        "author": "Dani Yogatama; Manaal Faruqui; Chris Dyer; Noah Smith",
        "abstract": "We propose a new method for learning word representations using hierarchical regularization in sparse coding inspired by the linguistic study of word meanings. We show an efficient learning algorithm based on stochastic proximal methods that is significantly faster than previous approaches, making it possible to perform hierarchical sparse coding on a corpus of billions of word tokens. Experiments on various benchmark tasks\u2014word similarity ranking, syntactic and semantic analogies, sentence completion, and sentiment analysis\u2014demonstrate that the method outperforms or is competitive with state-of-the-art methods.",
        "bibtex": "@InProceedings{pmlr-v37-yogatama15,\n  title = \t {Learning Word Representations with Hierarchical Sparse Coding},\n  author = \t {Yogatama, Dani and Faruqui, Manaal and Dyer, Chris and Smith, Noah},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {87--96},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/yogatama15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/yogatama15.html},\n  abstract = \t {We propose a new method for learning word representations using hierarchical regularization in sparse coding inspired by the linguistic study of word meanings. We show an efficient learning algorithm based on stochastic proximal methods that is significantly faster than previous approaches, making it possible to perform hierarchical sparse coding on a corpus of billions of word tokens. Experiments on various benchmark tasks\u2014word similarity ranking, syntactic and semantic analogies, sentence completion, and sentiment analysis\u2014demonstrate that the method outperforms or is competitive with state-of-the-art methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/yogatama15.pdf",
        "supp": "",
        "pdf_size": 1250695,
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8486666919681513100&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "Language Technologies Institute, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA; Language Technologies Institute, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA; Language Technologies Institute, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA; Language Technologies Institute, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "http://www.ark.cs.cmu.edu/dyogatam/wordvecs/",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Language Technologies Institute, School of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "387adb72f6",
        "title": "Learning from Corrupted Binary Labels via Class-Probability Estimation",
        "site": "https://proceedings.mlr.press/v37/menon15.html",
        "author": "Aditya Menon; Brendan Van Rooyen; Cheng Soon Ong; Bob Williamson",
        "abstract": "Many supervised learning problems involve learning from samples whose labels are corrupted in some way. For example, each sample may have some constant probability of being incorrectly labelled (learning with label noise), or one may have a pool of unlabelled samples in lieu of negative samples (learning from positive and unlabelled data). This paper uses class-probability estimation to study these and other corruption processes belonging to the mutually contaminated distributions framework (Scott et al., 2013), with three conclusions. First, one can optimise balanced error and AUC without knowledge of the corruption process parameters. Second, given estimates of the corruption parameters, one can minimise a range of classification risks. Third, one can estimate the corruption parameters using only corrupted data. Experiments confirm the efficacy of class-probability estimation in learning from corrupted labels.",
        "bibtex": "@InProceedings{pmlr-v37-menon15,\n  title = \t {Learning from Corrupted Binary Labels via Class-Probability Estimation},\n  author = \t {Menon, Aditya and Rooyen, Brendan Van and Ong, Cheng Soon and Williamson, Bob},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {125--134},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/menon15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/menon15.html},\n  abstract = \t {Many supervised learning problems involve learning from samples whose labels are corrupted in some way. For example, each sample may have some constant probability of being incorrectly labelled (learning with label noise), or one may have a pool of unlabelled samples in lieu of negative samples (learning from positive and unlabelled data). This paper uses class-probability estimation to study these and other corruption processes belonging to the mutually contaminated distributions framework (Scott et al., 2013), with three conclusions. First, one can optimise balanced error and AUC without knowledge of the corruption process parameters. Second, given estimates of the corruption parameters, one can minimise a range of classification risks. Third, one can estimate the corruption parameters using only corrupted data. Experiments confirm the efficacy of class-probability estimation in learning from corrupted labels.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/menon15.pdf",
        "supp": "",
        "pdf_size": 344714,
        "gs_citation": 295,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17151561943065646798&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "National ICT Australia and The Australian National University, Canberra; The Australian National University and National ICT Australia, Canberra; National ICT Australia and The Australian National University, Canberra; National ICT Australia and The Australian National University, Canberra",
        "aff_domain": "nicta.com.au;nicta.com.au;nicta.com.au;nicta.com.au",
        "email": "nicta.com.au;nicta.com.au;nicta.com.au;nicta.com.au",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "National ICT Australia;Australian National University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.nicta.com.au;https://www.anu.edu.au",
        "aff_unique_abbr": "NICTA;ANU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Canberra",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "d5a3886ba3",
        "title": "Learning to Search Better than Your Teacher",
        "site": "https://proceedings.mlr.press/v37/changb15.html",
        "author": "Kai-Wei Chang; Akshay Krishnamurthy; Alekh Agarwal; Hal Daum\u00e9 III; John Langford",
        "abstract": "Methods for learning to search for structured prediction typically imitate a reference policy, with existing theoretical guarantees demonstrating low regret compared to that reference. This is unsatisfactory in many applications where the reference policy is suboptimal and the goal of learning is to improve upon it. Can learning to search work even when the reference is poor? We provide a new learning to search algorithm, LOLS, which does well relative to the reference policy, but additionally guarantees low regret compared to deviations from the learned policy: a local-optimality guarantee. Consequently, LOLS can improve upon the reference policy, unlike previous algorithms. This enables us to develop structured contextual bandits, a partial information structured prediction setting with many potential applications.",
        "bibtex": "@InProceedings{pmlr-v37-changb15,\n  title = \t {Learning to Search Better than Your Teacher},\n  author =       {Chang, Kai-Wei and Krishnamurthy, Akshay and Daum\\'e, III, Hal and Langford, John},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2058--2066},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/changb15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/changb15.html},\n  abstract = \t {Methods for learning to search for structured prediction typically imitate a reference policy, with existing theoretical guarantees demonstrating low regret compared to that reference. This is unsatisfactory in many applications where the reference policy is suboptimal and the goal of learning is to improve upon it. Can learning to search work even when the reference is poor? We provide a new learning to search algorithm, LOLS, which does well relative to the reference policy, but additionally guarantees low regret compared to deviations from the learned policy: a local-optimality guarantee. Consequently, LOLS can improve upon the reference policy, unlike previous algorithms. This enables us to develop structured contextual bandits, a partial information structured prediction setting with many potential applications.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/changb15.pdf",
        "supp": "",
        "pdf_size": 609671,
        "gs_citation": 237,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18133093559567092087&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "University of Illinois at Urbana Champaign, IL; Carnegie Mellon University, Pittsburgh, PA; Microsoft Research, New York, NY; University of Maryland, College Park, MD; Microsoft Research, New York, NY",
        "aff_domain": "ILLINOIS.EDU;CS.CMU.EDU;MICROSOFT.COM;UMIACS.UMD.EDU;MICROSOFT.COM",
        "email": "ILLINOIS.EDU;CS.CMU.EDU;MICROSOFT.COM;UMIACS.UMD.EDU;MICROSOFT.COM",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3;2",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;Carnegie Mellon University;Microsoft;University of Maryland",
        "aff_unique_dep": ";;Microsoft Research;",
        "aff_unique_url": "https://illinois.edu;https://www.cmu.edu;https://www.microsoft.com/en-us/research;https://www/umd.edu",
        "aff_unique_abbr": "UIUC;CMU;MSR;UMD",
        "aff_campus_unique_index": "0;1;2;3;2",
        "aff_campus_unique": "Urbana-Champaign;Pittsburgh;New York;College Park",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e725fc528d",
        "title": "Log-Euclidean Metric Learning on Symmetric Positive Definite Manifold with Application to Image Set Classification",
        "site": "https://proceedings.mlr.press/v37/huanga15.html",
        "author": "Zhiwu Huang; Ruiping Wang; Shiguang Shan; Xianqiu Li; Xilin Chen",
        "abstract": "The manifold of Symmetric Positive Definite (SPD) matrices has been successfully used for data representation in image set classification. By endowing the SPD manifold with Log-Euclidean Metric, existing methods typically work on vector-forms of SPD matrix logarithms. This however not only inevitably distorts the geometrical structure of the space of SPD matrix logarithms but also brings low efficiency especially when the dimensionality of SPD matrix is high. To overcome this limitation, we propose a novel metric learning approach to work directly on logarithms of SPD matrices. Specifically, our method aims to learn a tangent map that can directly transform the matrix logarithms from the original tangent space to a new tangent space of more discriminability. Under the tangent map framework, the novel metric learning can then be formulated as an optimization problem of seeking a Mahalanobis-like matrix, which can take the advantage of traditional metric learning techniques. Extensive evaluations on several image set classification tasks demonstrate the effectiveness of our proposed metric learning method.",
        "bibtex": "@InProceedings{pmlr-v37-huanga15,\n  title = \t {Log-Euclidean Metric Learning on Symmetric Positive Definite Manifold with Application to Image Set Classification},\n  author = \t {Huang, Zhiwu and Wang, Ruiping and Shan, Shiguang and Li, Xianqiu and Chen, Xilin},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {720--729},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/huanga15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/huanga15.html},\n  abstract = \t {The manifold of Symmetric Positive Definite (SPD) matrices has been successfully used for data representation in image set classification. By endowing the SPD manifold with Log-Euclidean Metric, existing methods typically work on vector-forms of SPD matrix logarithms. This however not only inevitably distorts the geometrical structure of the space of SPD matrix logarithms but also brings low efficiency especially when the dimensionality of SPD matrix is high. To overcome this limitation, we propose a novel metric learning approach to work directly on logarithms of SPD matrices. Specifically, our method aims to learn a tangent map that can directly transform the matrix logarithms from the original tangent space to a new tangent space of more discriminability. Under the tangent map framework, the novel metric learning can then be formulated as an optimization problem of seeking a Mahalanobis-like matrix, which can take the advantage of traditional metric learning techniques. Extensive evaluations on several image set classification tasks demonstrate the effectiveness of our proposed metric learning method.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/huanga15.pdf",
        "supp": "",
        "pdf_size": 480062,
        "gs_citation": 313,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5577007721565968327&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 11,
        "aff": "Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China+University of Chinese Academy of Sciences, Beijing, 100049, China; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China+Cooperative Medianet Innovation Center, China; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China+Cooperative Medianet Innovation Center, China; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China+University of Chinese Academy of Sciences, Beijing, 100049, China; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China+Cooperative Medianet Innovation Center, China",
        "aff_domain": "VIPL.ICT.AC.CN;ICT.AC.CN;ICT.AC.CN;VIPL.ICT.AC.CN;ICT.AC.CN",
        "email": "VIPL.ICT.AC.CN;ICT.AC.CN;ICT.AC.CN;VIPL.ICT.AC.CN;ICT.AC.CN",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+2;0+2;0+1;0+2",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Cooperative Medianet Innovation Center",
        "aff_unique_dep": "Institute of Computing Technology;;",
        "aff_unique_url": "http://www.cas.ac.cn;http://www.ucas.ac.cn;",
        "aff_unique_abbr": "CAS;UCAS;",
        "aff_campus_unique_index": "0+0;0;0;0+0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "bf8df04469",
        "title": "Long Short-Term Memory Over Recursive Structures",
        "site": "https://proceedings.mlr.press/v37/zhub15.html",
        "author": "Xiaodan Zhu; Parinaz Sobihani; Hongyu Guo",
        "abstract": "The chain-structured long short-term memory (LSTM) has showed to be effective in a wide range of problems such as speech recognition and machine translation. In this paper, we propose to extend it to tree structures, in which a memory cell can reflect the history memories of multiple child cells or multiple descendant cells in a recursive process. We call the model S-LSTM, which provides a principled way of considering long-distance interaction over hierarchies, e.g., language or image parse structures. We leverage the models for semantic composition to understand the meaning of text, a fundamental problem in natural language understanding, and show that it outperforms a state-of-the-art recursive model by replacing its composition layers with the S-LSTM memory blocks. We also show that utilizing the given structures is helpful in achieving a performance better than that without considering the structures.",
        "bibtex": "@InProceedings{pmlr-v37-zhub15,\n  title = \t {Long Short-Term Memory Over Recursive Structures},\n  author = \t {Zhu, Xiaodan and Sobihani, Parinaz and Guo, Hongyu},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1604--1612},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/zhub15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/zhub15.html},\n  abstract = \t {The chain-structured long short-term memory (LSTM) has showed to be effective in a wide range of problems such as speech recognition and machine translation. In this paper, we propose to extend it to tree structures, in which a memory cell can reflect the history memories of multiple child cells or multiple descendant cells in a recursive process. We call the model S-LSTM, which provides a principled way of considering long-distance interaction over hierarchies, e.g., language or image parse structures. We leverage the models for semantic composition to understand the meaning of text, a fundamental problem in natural language understanding, and show that it outperforms a state-of-the-art recursive model by replacing its composition layers with the S-LSTM memory blocks. We also show that utilizing the given structures is helpful in achieving a performance better than that without considering the structures.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/zhub15.pdf",
        "supp": "",
        "pdf_size": 469960,
        "gs_citation": 466,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15043878037627773482&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "National Research Council Canada; School of Electrical Engineering and Computer Science, University of Ottawa; National Research Council Canada",
        "aff_domain": "NRC-CNRC.GC.CA;UOTTAWA.CA;NRC-CNRC.GC.CA",
        "email": "NRC-CNRC.GC.CA;UOTTAWA.CA;NRC-CNRC.GC.CA",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "National Research Council Canada;University of Ottawa",
        "aff_unique_dep": ";School of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.nrc-cnrc.gc.ca;https://www.uottawa.ca",
        "aff_unique_abbr": "NRC-CNRC;U Ottawa",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "929b331fdc",
        "title": "Low Rank Approximation using Error Correcting Coding Matrices",
        "site": "https://proceedings.mlr.press/v37/ubaru15.html",
        "author": "Shashanka Ubaru; Arya Mazumdar; Yousef Saad",
        "abstract": "Low-rank matrix approximation is an integral component of tools such as principal component analysis (PCA), as well as is an important instrument used in applications like web search models, text mining and computer vision, e.g., face recognition. Recently, randomized algorithms were proposed to effectively construct low rank approximations of large matrices. In this paper, we show how matrices from error correcting codes can be used to find such low rank approximations. The benefits of using these code matrices are the following: (i) They are easy to generate and they reduce randomness significantly. (ii) Code matrices have low coherence and have a better chance of preserving the geometry of an entire subspace of vectors; (iii) Unlike Fourier transforms or Hadamard matrices, which require sampling O(k\\log k) columns for a rank-k approximation, the log factor is not necessary in the case of code matrices. (iv) Under certain conditions, the approximation errors can be better and the singular values obtained can be more accurate, than those obtained using Gaussian random matrices and other structured random matrices.",
        "bibtex": "@InProceedings{pmlr-v37-ubaru15,\n  title = \t {Low Rank Approximation using Error Correcting Coding Matrices},\n  author = \t {Ubaru, Shashanka and Mazumdar, Arya and Saad, Yousef},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {702--710},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/ubaru15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/ubaru15.html},\n  abstract = \t {Low-rank matrix approximation is an integral component of tools such as principal component analysis (PCA), as well as is an important instrument used in applications like web search models, text mining and computer vision, e.g., face recognition. Recently, randomized algorithms were proposed to effectively construct low rank approximations of large matrices. In this paper, we show how matrices from error correcting codes can be used to find such low rank approximations. The benefits of using these code matrices are the following: (i) They are easy to generate and they reduce randomness significantly. (ii) Code matrices have low coherence and have a better chance of preserving the geometry of an entire subspace of vectors; (iii) Unlike Fourier transforms or Hadamard matrices, which require sampling O(k\\log k) columns for a rank-k approximation, the log factor is not necessary in the case of code matrices. (iv) Under certain conditions, the approximation errors can be better and the singular values obtained can be more accurate, than those obtained using Gaussian random matrices and other structured random matrices.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/ubaru15.pdf",
        "supp": "",
        "pdf_size": 467863,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15971015931848905053&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff": "University of Minnesota-Twin Cities, MN USA; University of Minnesota-Twin Cities, MN USA; University of Minnesota-Twin Cities, MN USA",
        "aff_domain": "UMN.EDU;UMN.EDU;CS.UMN.EDU",
        "email": "UMN.EDU;UMN.EDU;CS.UMN.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Minnesota",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.minnesota.edu",
        "aff_unique_abbr": "UMN",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Twin Cities",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7c4ec3c15d",
        "title": "Low-Rank Matrix Recovery from Row-and-Column Affine Measurements",
        "site": "https://proceedings.mlr.press/v37/zuk15.html",
        "author": "Or Zuk; Avishai Wagner",
        "abstract": "We propose and study a row-and-column affine measurement scheme for low-rank matrix recovery. Each measurement is a linear combination of elements in one row or one column of a matrix X. This setting arises naturally in applications from different domains. However, current algorithms developed for standard matrix recovery problems do not perform well in our case, hence the need for developing new algorithms and theory for our problem. We propose a simple algorithm for the problem based on Singular Value Decomposition (SVD) and least-squares (LS), which we term alg. We prove that (a simplified version of) our algorithm can recover X exactly with the minimum possible number of measurements in the noiseless case. In the general noisy case, we prove performance guarantees on the reconstruction accuracy under the Frobenius norm. In simulations, our row-and-column design and alg algorithm show improved speed, and comparable and in some cases better accuracy compared to standard measurements designs and algorithms. Our theoretical and experimental results suggest that the proposed row-and-column affine measurements scheme, together with our recovery algorithm, may provide a powerful framework for affine matrix reconstruction.",
        "bibtex": "@InProceedings{pmlr-v37-zuk15,\n  title = \t {Low-Rank Matrix Recovery from Row-and-Column Affine Measurements},\n  author = \t {Zuk, Or and Wagner, Avishai},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2012--2020},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/zuk15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/zuk15.html},\n  abstract = \t {We propose and study a row-and-column affine measurement scheme for low-rank matrix recovery. Each measurement is a linear combination of elements in one row or one column of a matrix X. This setting arises naturally in applications from different domains. However, current algorithms developed for standard matrix recovery problems do not perform well in our case, hence the need for developing new algorithms and theory for our problem. We propose a simple algorithm for the problem based on Singular Value Decomposition (SVD) and least-squares (LS), which we term alg. We prove that (a simplified version of) our algorithm can recover X exactly with the minimum possible number of measurements in the noiseless case. In the general noisy case, we prove performance guarantees on the reconstruction accuracy under the Frobenius norm. In simulations, our row-and-column design and alg algorithm show improved speed, and comparable and in some cases better accuracy compared to standard measurements designs and algorithms. Our theoretical and experimental results suggest that the proposed row-and-column affine measurements scheme, together with our recovery algorithm, may provide a powerful framework for affine matrix reconstruction.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/zuk15.pdf",
        "supp": "",
        "pdf_size": 219904,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1201948536971216564&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Dept. of Statistics, The Hebrew University of Jerusalem, Mt. Scopus, Jerusalem, 91905, Israel; Dept. of Statistics, The Hebrew University of Jerusalem, Mt. Scopus, Jerusalem, 91905, Israel",
        "aff_domain": "mail.huji.ac.il;mail.huji.ac.il",
        "email": "mail.huji.ac.il;mail.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hebrew University of Jerusalem",
        "aff_unique_dep": "Dept. of Statistics",
        "aff_unique_url": "http://www.huji.ac.il",
        "aff_unique_abbr": "HUJI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Mt. Scopus",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "e2b2e28eb9",
        "title": "MADE: Masked Autoencoder for Distribution Estimation",
        "site": "https://proceedings.mlr.press/v37/germain15.html",
        "author": "Mathieu Germain; Karol Gregor; Iain Murray; Hugo Larochelle",
        "abstract": "There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoder\u2019s parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability. We can also train a single network that can decompose the joint probability in multiple different orderings. Our simple framework can be applied to multiple architectures, including deep ones. Vectorized implementations, such as on GPUs, are simple and fast. Experiments demonstrate that this approach is competitive with state-of-the-art tractable distribution estimators. At test time, the method is significantly faster and scales better than other autoregressive estimators.",
        "bibtex": "@InProceedings{pmlr-v37-germain15,\n  title = \t {MADE: Masked Autoencoder for Distribution Estimation},\n  author = \t {Germain, Mathieu and Gregor, Karol and Murray, Iain and Larochelle, Hugo},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {881--889},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/germain15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/germain15.html},\n  abstract = \t {There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoder\u2019s parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability. We can also train a single network that can decompose the joint probability in multiple different orderings. Our simple framework can be applied to multiple architectures, including deep ones. Vectorized implementations, such as on GPUs, are simple and fast. Experiments demonstrate that this approach is competitive with state-of-the-art tractable distribution estimators. At test time, the method is significantly faster and scales better than other autoregressive estimators.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/germain15.pdf",
        "supp": "",
        "pdf_size": 376051,
        "gs_citation": 1114,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3269243854142729843&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Universit \u00b4e de Sherbrooke, Canada; Google DeepMind; University of Edinburgh, United Kingdom; Universit \u00b4e de Sherbrooke, Canada",
        "aff_domain": "usherbrooke.ca;gmail.com;ed.ac.uk;usherbrooke.ca",
        "email": "usherbrooke.ca;gmail.com;ed.ac.uk;usherbrooke.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Universit\u00e9 de Sherbrooke;Google;University of Edinburgh",
        "aff_unique_dep": ";Google DeepMind;",
        "aff_unique_url": "https://www.usherbrooke.ca;https://deepmind.com;https://www.ed.ac.uk",
        "aff_unique_abbr": "UdeS;DeepMind;Edinburgh",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "Canada;United Kingdom"
    },
    {
        "id": "4a9ef02d50",
        "title": "MRA-based Statistical Learning from Incomplete Rankings",
        "site": "https://proceedings.mlr.press/v37/sibony15.html",
        "author": "Eric Sibony; St\u00e9phan Clemen\u00e7on; J\u00e9r\u00e9mie Jakubowicz",
        "abstract": "Statistical analysis of rank data describing preferences over small and variable subsets of a potentially large ensemble of items 1, ..., n is a very challenging problem. It is motivated by a wide variety of modern applications, such as recommender systems or search engines. However, very few inference methods have been documented in the literature to learn a ranking model from such incomplete rank data. The goal of this paper is twofold: it develops a rigorous mathematical framework for the problem of learning a ranking model from incomplete rankings and introduces a novel general statistical method to address it. Based on an original concept of multi-resolution analysis (MRA) of incomplete rankings, it finely adapts to any observation setting, leading to a statistical accuracy and an algorithmic complexity that depend directly on the complexity of the observed data. Beyond theoretical guarantees, we also provide experimental results that show its statistical performance.",
        "bibtex": "@InProceedings{pmlr-v37-sibony15,\n  title = \t {MRA-based Statistical Learning from Incomplete Rankings},\n  author = \t {Sibony, Eric and Clemen\u00e7on, St\u00e9phan and Jakubowicz, J\u00e9r\u00e9mie},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1432--1441},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/sibony15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/sibony15.html},\n  abstract = \t {Statistical analysis of rank data describing preferences over small and variable subsets of a potentially large ensemble of items 1, ..., n is a very challenging problem. It is motivated by a wide variety of modern applications, such as recommender systems or search engines. However, very few inference methods have been documented in the literature to learn a ranking model from such incomplete rank data. The goal of this paper is twofold: it develops a rigorous mathematical framework for the problem of learning a ranking model from incomplete rankings and introduces a novel general statistical method to address it. Based on an original concept of multi-resolution analysis (MRA) of incomplete rankings, it finely adapts to any observation setting, leading to a statistical accuracy and an algorithmic complexity that depend directly on the complexity of the observed data. Beyond theoretical guarantees, we also provide experimental results that show its statistical performance.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/sibony15.pdf",
        "supp": "",
        "pdf_size": 430583,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9479234695283976839&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "LTCI UMR No. 5141, Telecom ParisTech/CNRS, Institut Mines-Telecom, Paris, 75013, France; LTCI UMR No. 5141, Telecom ParisTech/CNRS, Institut Mines-Telecom, Paris, 75013, France; SAMOV AR UMR No. 5157, Telecom SudParis/CNRS, Institut Mines-Telecom, Evry, 91000, France",
        "aff_domain": "TELECOM-PARISTECH.FR;TELECOM-PARISTECH.FR;TELECOM-SUDPARIS.EDU",
        "email": "TELECOM-PARISTECH.FR;TELECOM-PARISTECH.FR;TELECOM-SUDPARIS.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Telecom ParisTech;Telecom SudParis",
        "aff_unique_dep": "LTCI UMR No. 5141;SAMOV AR UMR No. 5157",
        "aff_unique_url": "https://www.telecom-paristech.fr;https://www.telecom-sudparis.eu",
        "aff_unique_abbr": "Telecom ParisTech;TSP",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Paris;Evry",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "a16177f430",
        "title": "Manifold-valued Dirichlet Processes",
        "site": "https://proceedings.mlr.press/v37/kim15.html",
        "author": "Hyunwoo Kim; Jia Xu; Baba Vemuri; Vikas Singh",
        "abstract": "Statistical models for manifold-valued data permit capturing the intrinsic nature of the curved spaces in which the data lie and have been a topic of research for several decades. Typically, these formulations use geodesic curves and distances defined locally for most cases - this makes it hard to design parametric models globally on smooth manifolds. Thus, most (manifold specific) parametric models available today assume that the data lie in a small neighborhood on the manifold. To address this \u2019locality\u2019 problem, we propose a novel nonparametric model which unifies multivariate general linear models (MGLMs) using multiple tangent spaces. Our framework generalizes existing work on (both Euclidean and non-Euclidean) general linear models providing a recipe to globally extend the locally-defined parametric models (using a mixture of local models). By grouping observations into sub-populations at multiple tangent spaces, our method provides insights into the hidden structure (geodesic relationships) in the data. This yields a framework to group observations and discover geodesic relationships between covariates X and manifold-valued responses Y, which we call Dirichlet process mixtures of multivariate general linear models (DP-MGLM) on Riemannian manifolds. Finally, we present proof of concept experiments to validate our model.",
        "bibtex": "@InProceedings{pmlr-v37-kim15,\n  title = \t {Manifold-valued Dirichlet Processes},\n  author = \t {Kim, Hyunwoo and Xu, Jia and Vemuri, Baba and Singh, Vikas},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1199--1208},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/kim15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/kim15.html},\n  abstract = \t {Statistical models for manifold-valued data permit capturing the intrinsic nature of the curved spaces in which the data lie and have been a topic of research for several decades. Typically, these formulations use geodesic curves and distances defined locally for most cases - this makes it hard to design parametric models globally on smooth manifolds. Thus, most (manifold specific) parametric models available today assume that the data lie in a small neighborhood on the manifold. To address this \u2019locality\u2019 problem, we propose a novel nonparametric model which unifies multivariate general linear models (MGLMs) using multiple tangent spaces. Our framework generalizes existing work on (both Euclidean and non-Euclidean) general linear models providing a recipe to globally extend the locally-defined parametric models (using a mixture of local models). By grouping observations into sub-populations at multiple tangent spaces, our method provides insights into the hidden structure (geodesic relationships) in the data. This yields a framework to group observations and discover geodesic relationships between covariates X and manifold-valued responses Y, which we call Dirichlet process mixtures of multivariate general linear models (DP-MGLM) on Riemannian manifolds. Finally, we present proof of concept experiments to validate our model.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/kim15.pdf",
        "supp": "",
        "pdf_size": 4103094,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10930379038941008158&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "University of Wisconsin-Madison; University of Wisconsin-Madison; University of Florida; University of Wisconsin-Madison",
        "aff_domain": "CS.WISC.EDU;CS.WISC.EDU;CISE.UFL.EDU;BIOSTAT.WISC.EDU",
        "email": "CS.WISC.EDU;CS.WISC.EDU;CISE.UFL.EDU;BIOSTAT.WISC.EDU",
        "github": "",
        "project": "http://pages.cs.wisc.edu/~hwkim/projects/dp-mglm/",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Wisconsin-Madison;University of Florida",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.wisc.edu;https://www.ufl.edu",
        "aff_unique_abbr": "UW-Madison;UF",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Madison;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "267f8df823",
        "title": "Markov Chain Monte Carlo and Variational Inference: Bridging the Gap",
        "site": "https://proceedings.mlr.press/v37/salimans15.html",
        "author": "Tim Salimans; Diederik Kingma; Max Welling",
        "abstract": "Recent advances in stochastic gradient variational inference have made it possible to perform variational Bayesian inference with posterior approximations containing auxiliary random variables. This enables us to explore a new synthesis of variational inference and Monte Carlo methods where we incorporate one or more steps of MCMC into our variational approximation. By doing so we obtain a rich class of inference algorithms bridging the gap between variational methods and MCMC, and offering the best of both worlds: fast posterior approximation through the maximization of an explicit objective, with the option of trading off additional computation for additional accuracy. We describe the theoretical foundations that make this possible and show some promising first results.",
        "bibtex": "@InProceedings{pmlr-v37-salimans15,\n  title = \t {Markov Chain Monte Carlo and Variational Inference: Bridging the Gap},\n  author = \t {Salimans, Tim and Kingma, Diederik and Welling, Max},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1218--1226},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/salimans15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/salimans15.html},\n  abstract = \t {Recent advances in stochastic gradient variational inference have made it possible to perform variational Bayesian inference with posterior approximations containing auxiliary random variables. This enables us to explore a new synthesis of variational inference and Monte Carlo methods where we incorporate one or more steps of MCMC into our variational approximation. By doing so we obtain a rich class of inference algorithms bridging the gap between variational methods and MCMC, and offering the best of both worlds: fast posterior approximation through the maximization of an explicit objective, with the option of trading off additional computation for additional accuracy. We describe the theoretical foundations that make this possible and show some promising first results.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/salimans15.pdf",
        "supp": "",
        "pdf_size": 760272,
        "gs_citation": 767,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1527223141103092286&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Algoritmica; University of Amsterdam; University of Amsterdam",
        "aff_domain": "ALGORITMICA.NL;UV A.NL;UV A.NL",
        "email": "ALGORITMICA.NL;UV A.NL;UV A.NL",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Algoritmica;University of Amsterdam",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.uva.nl",
        "aff_unique_abbr": ";UvA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";Netherlands"
    },
    {
        "id": "4eed64b821",
        "title": "Markov Mixed Membership Models",
        "site": "https://proceedings.mlr.press/v37/zhangd15.html",
        "author": "Aonan Zhang; John Paisley",
        "abstract": "We present a Markov mixed membership model (Markov M3) for grouped data that learns a fully connected graph structure among mixing components. A key feature of Markov M3 is that it interprets the mixed membership assignment as a Markov random walk over this graph of nodes. This is in contrast to tree-structured models in which the assignment is done according to a tree structure on the mixing components. The Markov structure results in a simple parametric model that can learn a complex dependency structure between nodes, while still maintaining full conjugacy for closed-form stochastic variational inference. Empirical results demonstrate that Markov M3 performs well compared with tree structured topic models, and can learn meaningful dependency structure between topics.",
        "bibtex": "@InProceedings{pmlr-v37-zhangd15,\n  title = \t {Markov Mixed Membership Models},\n  author = \t {Zhang, Aonan and Paisley, John},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {475--483},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/zhangd15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/zhangd15.html},\n  abstract = \t {We present a Markov mixed membership model (Markov M3) for grouped data that learns a fully connected graph structure among mixing components. A key feature of Markov M3 is that it interprets the mixed membership assignment as a Markov random walk over this graph of nodes. This is in contrast to tree-structured models in which the assignment is done according to a tree structure on the mixing components. The Markov structure results in a simple parametric model that can learn a complex dependency structure between nodes, while still maintaining full conjugacy for closed-form stochastic variational inference. Empirical results demonstrate that Markov M3 performs well compared with tree structured topic models, and can learn meaningful dependency structure between topics.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/zhangd15.pdf",
        "supp": "",
        "pdf_size": 769635,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7642099399591250043&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Electrical Engineering, Columbia University, New York, NY, USA; Department of Electrical Engineering, Columbia University, New York, NY, USA",
        "aff_domain": "columbia.edu;columbia.edu",
        "email": "columbia.edu;columbia.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "New York",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "87a4fed2cd",
        "title": "Message Passing for Collective Graphical Models",
        "site": "https://proceedings.mlr.press/v37/sunc15.html",
        "author": "Tao Sun; Dan Sheldon; Akshat Kumar",
        "abstract": "Collective graphical models (CGMs) are a formalism for inference and learning about a population of independent and identically distributed individuals when only noisy aggregate data are available. We highlight a close connection between approximate MAP inference in CGMs and marginal inference in standard graphical models. The connection leads us to derive a novel Belief Propagation (BP) style algorithm for collective graphical models. Mathematically, the algorithm is a strict generalization of BP\u2014it can be viewed as an extension to minimize the Bethe free energy plus additional energy terms that are non-linear functions of the marginals. For CGMs, the algorithm is much more efficient than previous approaches to inference. We demonstrate its performance on two synthetic experiments concerning bird migration and collective human mobility.",
        "bibtex": "@InProceedings{pmlr-v37-sunc15,\n  title = \t {Message Passing for Collective Graphical Models},\n  author = \t {Sun, Tao and Sheldon, Dan and Kumar, Akshat},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {853--861},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/sunc15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/sunc15.html},\n  abstract = \t {Collective graphical models (CGMs) are a formalism for inference and learning about a population of independent and identically distributed individuals when only noisy aggregate data are available. We highlight a close connection between approximate MAP inference in CGMs and marginal inference in standard graphical models. The connection leads us to derive a novel Belief Propagation (BP) style algorithm for collective graphical models. Mathematically, the algorithm is a strict generalization of BP\u2014it can be viewed as an extension to minimize the Bethe free energy plus additional energy terms that are non-linear functions of the marginals. For CGMs, the algorithm is much more efficient than previous approaches to inference. We demonstrate its performance on two synthetic experiments concerning bird migration and collective human mobility.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/sunc15.pdf",
        "supp": "",
        "pdf_size": 1569388,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=461884870129701561&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "University of Massachusetts Amherst; University of Massachusetts Amherst+Mount Holyoke College; Singapore Management University",
        "aff_domain": "CS.UMASS.EDU;CS.UMASS.EDU;SMU.EDU.SG",
        "email": "CS.UMASS.EDU;CS.UMASS.EDU;SMU.EDU.SG",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;2",
        "aff_unique_norm": "University of Massachusetts Amherst;Mount Holyoke College;Singapore Management University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.umass.edu;https://www.mtholyoke.edu;https://www.smu.edu.sg",
        "aff_unique_abbr": "UMass Amherst;MHC;SMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Amherst;",
        "aff_country_unique_index": "0;0+0;1",
        "aff_country_unique": "United States;Singapore"
    },
    {
        "id": "08f89242b6",
        "title": "Metadata Dependent Mondrian Processes",
        "site": "https://proceedings.mlr.press/v37/wangd15.html",
        "author": "Yi Wang; Bin Li; Yang Wang; Fang Chen",
        "abstract": "Stochastic partition processes in a product space play an important role in modeling relational data. Recent studies on the Mondrian process have introduced more flexibility into the block structure in relational models. A side-effect of such high flexibility is that, in data sparsity scenarios, the model is prone to overfit. In reality, relational entities are always associated with meta information, such as user profiles in a social network. In this paper, we propose a metadata dependent Mondrian process (MDMP) to incorporate meta information into the stochastic partition process in the product space and the entity allocation process on the resulting block structure. MDMP can not only encourage homogeneous relational interactions within blocks but also discourage meta-label diversity within blocks. Regularized by meta information, MDMP becomes more robust in data sparsity scenarios and easier to converge in posterior inference. We apply MDMP to link prediction and rating prediction and demonstrate that MDMP is more effective than the baseline models in prediction accuracy with a more parsimonious model structure.",
        "bibtex": "@InProceedings{pmlr-v37-wangd15,\n  title = \t {Metadata Dependent Mondrian Processes},\n  author = \t {Wang, Yi and Li, Bin and Wang, Yang and Chen, Fang},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1339--1347},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/wangd15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/wangd15.html},\n  abstract = \t {Stochastic partition processes in a product space play an important role in modeling relational data. Recent studies on the Mondrian process have introduced more flexibility into the block structure in relational models. A side-effect of such high flexibility is that, in data sparsity scenarios, the model is prone to overfit. In reality, relational entities are always associated with meta information, such as user profiles in a social network. In this paper, we propose a metadata dependent Mondrian process (MDMP) to incorporate meta information into the stochastic partition process in the product space and the entity allocation process on the resulting block structure. MDMP can not only encourage homogeneous relational interactions within blocks but also discourage meta-label diversity within blocks. Regularized by meta information, MDMP becomes more robust in data sparsity scenarios and easier to converge in posterior inference. We apply MDMP to link prediction and rating prediction and demonstrate that MDMP is more effective than the baseline models in prediction accuracy with a more parsimonious model structure.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/wangd15.pdf",
        "supp": "",
        "pdf_size": 570123,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17698854098143922503&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Machine Learning Research Group, National ICT Australia, Eveleigh, NSW 2015, Australia+School of Computer Science & Engineering, University of New South Wales, Kensington, NSW 2033, Australia; Machine Learning Research Group, National ICT Australia, Eveleigh, NSW 2015, Australia; Machine Learning Research Group, National ICT Australia, Eveleigh, NSW 2015, Australia; Machine Learning Research Group, National ICT Australia, Eveleigh, NSW 2015, Australia+School of Computer Science & Engineering, University of New South Wales, Kensington, NSW 2033, Australia",
        "aff_domain": "nicta.com.au;nicta.com.au;nicta.com.au;nicta.com.au",
        "email": "nicta.com.au;nicta.com.au;nicta.com.au;nicta.com.au",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0;0+1",
        "aff_unique_norm": "National ICT Australia;University of New South Wales",
        "aff_unique_dep": "Machine Learning Research Group;School of Computer Science & Engineering",
        "aff_unique_url": "https://www.nicta.com.au;https://www.unsw.edu.au",
        "aff_unique_abbr": "NICTA;UNSW",
        "aff_campus_unique_index": "0+1;0;0;0+1",
        "aff_campus_unique": "Eveleigh;Kensington",
        "aff_country_unique_index": "0+0;0;0;0+0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "7bdab66e15",
        "title": "Mind the duality gap: safer rules for the Lasso",
        "site": "https://proceedings.mlr.press/v37/fercoq15.html",
        "author": "Olivier Fercoq; Alexandre Gramfort; Joseph Salmon",
        "abstract": "Screening rules allow to early discard irrelevant variables from the optimization in Lasso problems, or its derivatives, making solvers faster. In this paper, we propose new versions of the so-called \\textitsafe rules for the Lasso. Based on duality gap considerations, our new rules create safe test regions whose diameters converge to zero, provided that one relies on a converging solver. This property helps screening out more variables, for a wider range of regularization parameter values. In addition to faster convergence, we prove that we correctly identify the active sets (supports) of the solutions in finite time. While our proposed strategy can cope with any solver, its performance is demonstrated using a coordinate descent algorithm particularly adapted to machine learning use cases. Significant computing time reductions are obtained with respect to previous safe rules.",
        "bibtex": "@InProceedings{pmlr-v37-fercoq15,\n  title = \t {Mind the duality gap: safer rules for the Lasso},\n  author = \t {Fercoq, Olivier and Gramfort, Alexandre and Salmon, Joseph},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {333--342},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/fercoq15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/fercoq15.html},\n  abstract = \t {Screening rules allow to early discard irrelevant variables from the optimization in Lasso problems, or its derivatives, making solvers faster. In this paper, we propose new versions of the so-called \\textitsafe rules for the Lasso. Based on duality gap considerations, our new rules create safe test regions whose diameters converge to zero, provided that one relies on a converging solver. This property helps screening out more variables, for a wider range of regularization parameter values. In addition to faster convergence, we prove that we correctly identify the active sets (supports) of the solutions in finite time. While our proposed strategy can cope with any solver, its performance is demonstrated using a coordinate descent algorithm particularly adapted to machine learning use cases. Significant computing time reductions are obtained with respect to previous safe rules.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/fercoq15.pdf",
        "supp": "",
        "pdf_size": 552409,
        "gs_citation": 162,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16026253777692897449&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Institut Mines-T\u00e9l\u00e9com, T\u00e9l\u00e9com ParisTech, CNRS LTCI; Institut Mines-T\u00e9l\u00e9com, T\u00e9l\u00e9com ParisTech, CNRS LTCI; Institut Mines-T\u00e9l\u00e9com, T\u00e9l\u00e9com ParisTech, CNRS LTCI",
        "aff_domain": "TELECOM-PARISTECH.FR;TELECOM-PARISTECH.FR;TELECOM-PARISTECH.FR",
        "email": "TELECOM-PARISTECH.FR;TELECOM-PARISTECH.FR;TELECOM-PARISTECH.FR",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Institut Mines-T\u00e9l\u00e9com",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.imt.fr",
        "aff_unique_abbr": "IMT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "e762421760",
        "title": "Modeling Order in Neural Word Embeddings at Scale",
        "site": "https://proceedings.mlr.press/v37/trask15.html",
        "author": "Andrew Trask; David Gilmore; Matthew Russell",
        "abstract": "Natural Language Processing (NLP) systems commonly leverage bag-of-words co-occurrence techniques to capture semantic and syntactic word relationships. The resulting word-level distributed representations often ignore morphological information, though character-level embeddings have proven valuable to NLP tasks. We propose a new neural language model incorporating both word order and character order in its embedding. The model produces several vector spaces with meaningful substructure, as evidenced by its performance of 85.8% on a recent word-analogy task, exceeding best published syntactic word-analogy scores by a 58% error margin. Furthermore, the model includes several parallel training methods, most notably allowing a skip-gram network with 160 billion parameters to be trained overnight on 3 multi-core CPUs, 14x larger than the previous largest neural network.",
        "bibtex": "@InProceedings{pmlr-v37-trask15,\n  title = \t {Modeling Order in Neural Word Embeddings at Scale},\n  author = \t {Trask, Andrew and Gilmore, David and Russell, Matthew},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2266--2275},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/trask15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/trask15.html},\n  abstract = \t {Natural Language Processing (NLP) systems commonly leverage bag-of-words co-occurrence techniques to capture semantic and syntactic word relationships. The resulting word-level distributed representations often ignore morphological information, though character-level embeddings have proven valuable to NLP tasks. We propose a new neural language model incorporating both word order and character order in its embedding. The model produces several vector spaces with meaningful substructure, as evidenced by its performance of 85.8% on a recent word-analogy task, exceeding best published syntactic word-analogy scores by a 58% error margin. Furthermore, the model includes several parallel training methods, most notably allowing a skip-gram network with 160 billion parameters to be trained overnight on 3 multi-core CPUs, 14x larger than the previous largest neural network.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/trask15.pdf",
        "supp": "",
        "pdf_size": 750213,
        "gs_citation": 70,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3179250763402008167&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Digital Reasoning Systems, Inc., Nashville, TN USA; Digital Reasoning Systems, Inc., Nashville, TN USA; Digital Reasoning Systems, Inc., Nashville, TN USA",
        "aff_domain": "DIGITALREASONING.COM;DIGITALREASONING.COM;DIGITALREASONING.COM",
        "email": "DIGITALREASONING.COM;DIGITALREASONING.COM;DIGITALREASONING.COM",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Digital Reasoning Systems, Inc.",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "eebe735ea8",
        "title": "Moderated and Drifting Linear Dynamical Systems",
        "site": "https://proceedings.mlr.press/v37/guan15.html",
        "author": "Jinyan Guan; Kyle Simek; Ernesto Brau; Clayton Morrison; Emily Butler; Kobus Barnard",
        "abstract": "We consider linear dynamical systems, particularly coupled linear oscillators, where the parameters represent meaningful values in a domain theory and thus learning what affects them contributes to explanation. Rather than allow perturbations of latent states, we assume that temporal variation beyond noise is explained by parameter drift, and variation across coupled systems is a function of moderating variables. This change of focus reduces opportunities for efficient inference, and we propose sampling procedures to learn and fit the models. We test our approach on a real dataset of physiological measures of heterosexual couples engaged in a conversation about a potentially emotional topic, with body mass index (BMI) being considered as a moderator. We evaluate several models on their ability to predict future conversation dynamics (the last 20% of the data for each test couple), with shared parameters being learned using held out data. As proof of concept, we validate the hypothesis that BMI affects the conversation dynamic in the experimentally chosen topic.",
        "bibtex": "@InProceedings{pmlr-v37-guan15,\n  title = \t {Moderated and Drifting Linear Dynamical Systems},\n  author = \t {Guan, Jinyan and Simek, Kyle and Brau, Ernesto and Morrison, Clayton and Butler, Emily and Barnard, Kobus},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2473--2482},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/guan15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/guan15.html},\n  abstract = \t {We consider linear dynamical systems, particularly coupled linear oscillators, where the parameters represent meaningful values in a domain theory and thus learning what affects them contributes to explanation. Rather than allow perturbations of latent states, we assume that temporal variation beyond noise is explained by parameter drift, and variation across coupled systems is a function of moderating variables. This change of focus reduces opportunities for efficient inference, and we propose sampling procedures to learn and fit the models. We test our approach on a real dataset of physiological measures of heterosexual couples engaged in a conversation about a potentially emotional topic, with body mass index (BMI) being considered as a moderator. We evaluate several models on their ability to predict future conversation dynamics (the last 20% of the data for each test couple), with shared parameters being learned using held out data. As proof of concept, we validate the hypothesis that BMI affects the conversation dynamic in the experimentally chosen topic.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/guan15.pdf",
        "supp": "",
        "pdf_size": 382360,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10950621757269101954&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Science, University of Arizona; Department of Computer Science, University of Arizona; Department of Computer Science, Boston College; School of Information: Science, Technology, and Arts, University of Arizona; Norton School of Family and Consumer Sciences, University of Arizona; Department of Computer Science, University of Arizona",
        "aff_domain": "email.arizona.edu;email.arizona.edu;bc.edu;email.arizona.edu;email.arizona.edu;cs.arizona.edu",
        "email": "email.arizona.edu;email.arizona.edu;bc.edu;email.arizona.edu;email.arizona.edu;cs.arizona.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0;0;0",
        "aff_unique_norm": "University of Arizona;Boston College",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.arizona.edu;https://www.bostoncollege.edu",
        "aff_unique_abbr": "UArizona;BC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b6b5213b90",
        "title": "Multi-Task Learning for Subspace Segmentation",
        "site": "https://proceedings.mlr.press/v37/wangc15.html",
        "author": "Yu Wang; David Wipf; Qing Ling; Wei Chen; Ian Wassell",
        "abstract": "Subspace segmentation is the process of clustering a set of data points that are assumed to lie on the union of multiple linear or affine subspaces, and is increasingly being recognized as a fundamental tool for data analysis in high dimensional settings. Arguably one of the most successful approaches is based on the observation that the sparsest representation of a given point with respect to a dictionary formed by the others involves nonzero coefficients associated with points originating in the same subspace. Such sparse representations are computed independently for each data point via \\ell_1-norm minimization and then combined into an affinity matrix for use by a final spectral clustering step. The downside of this procedure is two-fold. First, unlike canonical compressive sensing scenarios with ideally-randomized dictionaries, the data-dependent dictionaries here are unavoidably highly structured, disrupting many of the favorable properties of the \\ell_1 norm. Secondly, by treating each data point independently, we ignore useful relationships between points that can be leveraged for jointly computing such sparse representations. Consequently, we motivate a multi-task learning-based framework for learning coupled sparse representations leading to a segmentation pipeline that is both robust against correlation structure and tailored to generate an optimal affinity matrix. Theoretical analysis and empirical tests are provided to support these claims.",
        "bibtex": "@InProceedings{pmlr-v37-wangc15,\n  title = \t {Multi-Task Learning for Subspace Segmentation},\n  author = \t {Wang, Yu and Wipf, David and Ling, Qing and Chen, Wei and Wassell, Ian},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1209--1217},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/wangc15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/wangc15.html},\n  abstract = \t {Subspace segmentation is the process of clustering a set of data points that are assumed to lie on the union of multiple linear or affine subspaces, and is increasingly being recognized as a fundamental tool for data analysis in high dimensional settings. Arguably one of the most successful approaches is based on the observation that the sparsest representation of a given point with respect to a dictionary formed by the others involves nonzero coefficients associated with points originating in the same subspace. Such sparse representations are computed independently for each data point via \\ell_1-norm minimization and then combined into an affinity matrix for use by a final spectral clustering step. The downside of this procedure is two-fold. First, unlike canonical compressive sensing scenarios with ideally-randomized dictionaries, the data-dependent dictionaries here are unavoidably highly structured, disrupting many of the favorable properties of the \\ell_1 norm. Secondly, by treating each data point independently, we ignore useful relationships between points that can be leveraged for jointly computing such sparse representations. Consequently, we motivate a multi-task learning-based framework for learning coupled sparse representations leading to a segmentation pipeline that is both robust against correlation structure and tailored to generate an optimal affinity matrix. Theoretical analysis and empirical tests are provided to support these claims.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/wangc15.pdf",
        "supp": "",
        "pdf_size": 166259,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11397688305215802927&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Computer Laboratory, University of Cambridge, Cambridge, UK; Microsoft Research, Beijing, China; University of Science and Technology of China, Hefei, Anhui, China; Computer Laboratory, University of Cambridge, Cambridge, UK; Computer Laboratory, University of Cambridge, Cambridge, UK",
        "aff_domain": "CAM.AC.UK;MICROSOFT.COM;MAIL.USTC.EDU.CN;CAM.AC.UK;CAM.AC.UK",
        "email": "CAM.AC.UK;MICROSOFT.COM;MAIL.USTC.EDU.CN;CAM.AC.UK;CAM.AC.UK",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0;0",
        "aff_unique_norm": "University of Cambridge;Microsoft;University of Science and Technology of China",
        "aff_unique_dep": "Computer Laboratory;Microsoft Research;",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.microsoft.com/en-us/research/group/microsoft-research-asia;http://www.ustc.edu.cn",
        "aff_unique_abbr": "Cambridge;MSR;USTC",
        "aff_campus_unique_index": "0;1;2;0;0",
        "aff_campus_unique": "Cambridge;Beijing;Hefei",
        "aff_country_unique_index": "0;1;1;0;0",
        "aff_country_unique": "United Kingdom;China"
    },
    {
        "id": "4b0acbe753",
        "title": "Multi-instance multi-label learning in the presence of novel class instances",
        "site": "https://proceedings.mlr.press/v37/pham15.html",
        "author": "Anh Pham; Raviv Raich; Xiaoli Fern; Jes\u00fas P\u00e9rez Arriaga",
        "abstract": "Multi-instance multi-label learning (MIML) is a framework for learning in the presence of label ambiguity. In MIML, experts provide labels for groups of instances (bags), instead of directly providing a label for every instance. When labeling efforts are focused on a set of target classes, instances outside this set will not be appropriately modeled. For example, ornithologists label bird audio recordings with a list of species present. Other additional sound instances, e.g., a rain drop or a moving vehicle sound, are not labeled. The challenge is due to the fact that for a given bag, the presence or absence of novel instances is latent. In this paper, this problem is addressed using a discriminative probabilistic model that accounts for novel instances. We propose an exact and efficient implementation of the maximum likelihood approach to determine the model parameters and consequently learn an instance-level classifier for all classes including the novel class. Experiments on both synthetic and real datasets illustrate the effectiveness of the proposed approach.",
        "bibtex": "@InProceedings{pmlr-v37-pham15,\n  title = \t {Multi-instance multi-label learning in the presence of novel class instances},\n  author = \t {Pham, Anh and Raich, Raviv and Fern, Xiaoli and Arriaga, Jes\u00fas P\u00e9rez},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2427--2435},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/pham15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/pham15.html},\n  abstract = \t {Multi-instance multi-label learning (MIML) is a framework for learning in the presence of label ambiguity. In MIML, experts provide labels for groups of instances (bags), instead of directly providing a label for every instance. When labeling efforts are focused on a set of target classes, instances outside this set will not be appropriately modeled. For example, ornithologists label bird audio recordings with a list of species present. Other additional sound instances, e.g., a rain drop or a moving vehicle sound, are not labeled. The challenge is due to the fact that for a given bag, the presence or absence of novel instances is latent. In this paper, this problem is addressed using a discriminative probabilistic model that accounts for novel instances. We propose an exact and efficient implementation of the maximum likelihood approach to determine the model parameters and consequently learn an instance-level classifier for all classes including the novel class. Experiments on both synthetic and real datasets illustrate the effectiveness of the proposed approach.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/pham15.pdf",
        "supp": "",
        "pdf_size": 3641676,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14614990033974075999&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "School of Electrical Engineering and Computer Science, Corvallis, OR 97330-5501 USA; School of Electrical Engineering and Computer Science, Corvallis, OR 97330-5501 USA; School of Electrical Engineering and Computer Science, Corvallis, OR 97330-5501 USA; Departamento de Ingenier \u00b4\u0131a de Comunicaciones, Universidad de Cantabria, 39005 Santander, Spain",
        "aff_domain": "EECS.OREGONSTATE.EDU;EECS.OREGONSTATE.EDU;EECS.OREGONSTATE.EDU;GTAS.DICOM.UNICAN.ES",
        "email": "EECS.OREGONSTATE.EDU;EECS.OREGONSTATE.EDU;EECS.OREGONSTATE.EDU;GTAS.DICOM.UNICAN.ES",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Oregon State University;Universidad de Cantabria",
        "aff_unique_dep": "School of Electrical Engineering and Computer Science;Departamento de Ingenierxeda de Comunicaciones",
        "aff_unique_url": "https://ee.cs.oregonstate.edu;https://www.unican.es",
        "aff_unique_abbr": "OSU;",
        "aff_campus_unique_index": "0;0;0;1",
        "aff_campus_unique": "Corvallis;Santander",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "United States;Spain"
    },
    {
        "id": "344926295b",
        "title": "Multi-view Sparse Co-clustering via Proximal Alternating Linearized Minimization",
        "site": "https://proceedings.mlr.press/v37/sunb15.html",
        "author": "Jiangwen Sun; Jin Lu; Tingyang Xu; Jinbo Bi",
        "abstract": "When multiple views of data are available for a set of subjects, co-clustering aims to identify subject clusters that agree across the different views. We explore the problem of co-clustering when the underlying clusters exist in different subspaces of each view. We propose a proximal alternating linearized minimization algorithm that simultaneously decomposes multiple data matrices into sparse row and columns vectors. This approach is able to group subjects consistently across the views and simultaneously identify the subset of features in each view that are associated with the clusters. The proposed algorithm can globally converge to a critical point of the problem. A simulation study validates that the proposed algorithm can identify the hypothesized clusters and their associated features. Comparison with several latest multi-view co-clustering methods on benchmark datasets demonstrates the superior performance of the proposed approach.",
        "bibtex": "@InProceedings{pmlr-v37-sunb15,\n  title = \t {Multi-view Sparse Co-clustering via Proximal Alternating Linearized Minimization},\n  author = \t {Sun, Jiangwen and Lu, Jin and Xu, Tingyang and Bi, Jinbo},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {757--766},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/sunb15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/sunb15.html},\n  abstract = \t {When multiple views of data are available for a set of subjects, co-clustering aims to identify subject clusters that agree across the different views. We explore the problem of co-clustering when the underlying clusters exist in different subspaces of each view. We propose a proximal alternating linearized minimization algorithm that simultaneously decomposes multiple data matrices into sparse row and columns vectors. This approach is able to group subjects consistently across the views and simultaneously identify the subset of features in each view that are associated with the clusters. The proposed algorithm can globally converge to a critical point of the problem. A simulation study validates that the proposed algorithm can identify the hypothesized clusters and their associated features. Comparison with several latest multi-view co-clustering methods on benchmark datasets demonstrates the superior performance of the proposed approach.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/sunb15.pdf",
        "supp": "",
        "pdf_size": 453427,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14962307695814826724&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science and Engineering, University of Connecticut, Storrs, CT 06269 USA; Department of Computer Science and Engineering, University of Connecticut, Storrs, CT 06269 USA; Department of Computer Science and Engineering, University of Connecticut, Storrs, CT 06269 USA; Department of Computer Science and Engineering, University of Connecticut, Storrs, CT 06269 USA",
        "aff_domain": "engr.uconn.edu;engr.uconn.edu;engr.uconn.edu;engr.uconn.edu",
        "email": "engr.uconn.edu;engr.uconn.edu;engr.uconn.edu;engr.uconn.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Connecticut",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.uconn.edu",
        "aff_unique_abbr": "UConn",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Storrs",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0c6626db17",
        "title": "Multiview Triplet Embedding: Learning Attributes in Multiple Maps",
        "site": "https://proceedings.mlr.press/v37/amid15.html",
        "author": "Ehsan Amid; Antti Ukkonen",
        "abstract": "For humans, it is usually easier to make statements about the similarity of objects in relative, rather than absolute terms. Moreover, subjective comparisons of objects can be based on a number of different and independent attributes. For example, objects can be compared based on their shape, color, etc. In this paper, we consider the problem of uncovering these hidden attributes given a set of relative distance judgments in the form of triplets. The attribute that was used to generate a particular triplet in this set is unknown. Such data occurs, e.g., in crowdsourcing applications where the triplets are collected from a large group of workers. We propose the Multiview Triplet Embedding (MVTE) algorithm that produces a number of low-dimensional maps, each corresponding to one of the hidden attributes. The method can be used to assess how many different attributes were used to create the triplets, as well as to assess the difficulty of a distance comparison task, and find objects that have multiple interpretations in relation to the other objects.",
        "bibtex": "@InProceedings{pmlr-v37-amid15,\n  title = \t {Multiview Triplet Embedding: Learning Attributes in Multiple Maps},\n  author = \t {Amid, Ehsan and Ukkonen, Antti},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1472--1480},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/amid15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/amid15.html},\n  abstract = \t {For humans, it is usually easier to make statements about the similarity of objects in relative, rather than absolute terms. Moreover, subjective comparisons of objects can be based on a number of different and independent attributes. For example, objects can be compared based on their shape, color, etc. In this paper, we consider the problem of uncovering these hidden attributes given a set of relative distance judgments in the form of triplets. The attribute that was used to generate a particular triplet in this set is unknown. Such data occurs, e.g., in crowdsourcing applications where the triplets are collected from a large group of workers. We propose the Multiview Triplet Embedding (MVTE) algorithm that produces a number of low-dimensional maps, each corresponding to one of the hidden attributes. The method can be used to assess how many different attributes were used to create the triplets, as well as to assess the difficulty of a distance comparison task, and find objects that have multiple interpretations in relation to the other objects.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/amid15.pdf",
        "supp": "",
        "pdf_size": 5474269,
        "gs_citation": 88,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9516235243666562177&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Aalto University and Helsinki Institute for Information Technology HIIT, Finland; Finnish Institute of Occupational Health, Helsinki, Finland",
        "aff_domain": "AALTO.FI;TTL.FI",
        "email": "AALTO.FI;TTL.FI",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Aalto University;Finnish Institute of Occupational Health",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.aalto.fi;https://www.fioh.fi",
        "aff_unique_abbr": "Aalto;FIOH",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Helsinki",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Finland"
    },
    {
        "id": "9afa3a595f",
        "title": "Nested Sequential Monte Carlo Methods",
        "site": "https://proceedings.mlr.press/v37/naesseth15.html",
        "author": "Christian Naesseth; Fredrik Lindsten; Thomas Schon",
        "abstract": "We propose nested sequential Monte Carlo (NSMC), a methodology to sample from sequences of probability distributions, even where the random variables are high-dimensional. NSMC generalises the SMC framework by requiring only approximate, properly weighted, samples from the SMC proposal distribution, while still resulting in a correct SMC algorithm. Furthermore, NSMC can in itself be used to produce such properly weighted samples. Consequently, one NSMC sampler can be used to construct an efficient high-dimensional proposal distribution for another NSMC sampler, and this nesting of the algorithm can be done to an arbitrary degree. This allows us to consider complex and high-dimensional models using SMC. We show results that motivate the efficacy of our approach on several filtering problems with dimensions in the order of 100 to 1000.",
        "bibtex": "@InProceedings{pmlr-v37-naesseth15,\n  title = \t {Nested Sequential Monte Carlo Methods},\n  author = \t {Naesseth, Christian and Lindsten, Fredrik and Schon, Thomas},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1292--1301},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/naesseth15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/naesseth15.html},\n  abstract = \t {We propose nested sequential Monte Carlo (NSMC), a methodology to sample from sequences of probability distributions, even where the random variables are high-dimensional. NSMC generalises the SMC framework by requiring only approximate, properly weighted, samples from the SMC proposal distribution, while still resulting in a correct SMC algorithm. Furthermore, NSMC can in itself be used to produce such properly weighted samples. Consequently, one NSMC sampler can be used to construct an efficient high-dimensional proposal distribution for another NSMC sampler, and this nesting of the algorithm can be done to an arbitrary degree. This allows us to consider complex and high-dimensional models using SMC. We show results that motivate the efficacy of our approach on several filtering problems with dimensions in the order of 100 to 1000.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/naesseth15.pdf",
        "supp": "",
        "pdf_size": 1437015,
        "gs_citation": 91,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1661002681786425457&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 13,
        "aff": "Link\u00f6ping University, Link\u00f6ping, Sweden; The University of Cambridge, Cambridge, United Kingdom; Uppsala University, Uppsala, Sweden",
        "aff_domain": "liu.se;eng.cam.ac.uk;it.uu.se",
        "email": "liu.se;eng.cam.ac.uk;it.uu.se",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Link\u00f6ping University;University of Cambridge;Uppsala University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.liu.se;https://www.cam.ac.uk;https://www.uu.se",
        "aff_unique_abbr": "LiU;Cambridge;UU",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "Link\u00f6ping;Cambridge;Uppsala",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Sweden;United Kingdom"
    },
    {
        "id": "708e7b66ed",
        "title": "Non-Gaussian Discriminative Factor Models via the Max-Margin Rank-Likelihood",
        "site": "https://proceedings.mlr.press/v37/yuan15.html",
        "author": "Xin Yuan; Ricardo Henao; Ephraim Tsalik; Raymond Langley; Lawrence Carin",
        "abstract": "We consider the problem of discriminative factor analysis for data that are in general non-Gaussian. A Bayesian model based on the ranks of the data is proposed. We first introduce a max-margin version of the rank-likelihood. A discriminative factor model is then developed, integrating the new max-margin rank-likelihood and (linear) Bayesian support vector machines, which are also built on the max-margin principle. The discriminative factor model is further extended to the nonlinear case through mixtures of local linear classifiers, via Dirichlet processes. Fully local conjugacy of the model yields efficient inference with both Markov Chain Monte Carlo and variational Bayes approaches. Extensive experiments on benchmark and real data demonstrate superior performance of the proposed model and its potential for applications in computational biology.",
        "bibtex": "@InProceedings{pmlr-v37-yuan15,\n  title = \t {Non-Gaussian Discriminative Factor Models via the Max-Margin Rank-Likelihood},\n  author = \t {Yuan, Xin and Henao, Ricardo and Tsalik, Ephraim and Langley, Raymond and Carin, Lawrence},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1254--1263},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/yuan15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/yuan15.html},\n  abstract = \t {We consider the problem of discriminative factor analysis for data that are in general non-Gaussian. A Bayesian model based on the ranks of the data is proposed. We first introduce a max-margin version of the rank-likelihood. A discriminative factor model is then developed, integrating the new max-margin rank-likelihood and (linear) Bayesian support vector machines, which are also built on the max-margin principle. The discriminative factor model is further extended to the nonlinear case through mixtures of local linear classifiers, via Dirichlet processes. Fully local conjugacy of the model yields efficient inference with both Markov Chain Monte Carlo and variational Bayes approaches. Extensive experiments on benchmark and real data demonstrate superior performance of the proposed model and its potential for applications in computational biology.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/yuan15.pdf",
        "supp": "",
        "pdf_size": 558400,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11722902348802846486&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Duke University; Duke University; Duke University; Department of Immunology, Lovelace Respiratory Research Institute; Duke University",
        "aff_domain": "GMAIL.COM;DUKE.EDU;DUKE.EDU;LRRI.ORG;DUKE.EDU",
        "email": "GMAIL.COM;DUKE.EDU;DUKE.EDU;LRRI.ORG;DUKE.EDU",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "Duke University;Lovelace Respiratory Research Institute",
        "aff_unique_dep": ";Department of Immunology",
        "aff_unique_url": "https://www.duke.edu;https://www.lovelace.edu",
        "aff_unique_abbr": "Duke;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "097aea93d1",
        "title": "Non-Linear Cross-Domain Collaborative Filtering via Hyper-Structure Transfer",
        "site": "https://proceedings.mlr.press/v37/liua15.html",
        "author": "Yan-Fu Liu; Cheng-Yu Hsu; Shan-Hung Wu",
        "abstract": "The Cross Domain Collaborative Filtering (CDCF) exploits the rating matrices from multiple domains to make better recommendations. Existing CDCF methods adopt the sub-structure sharing technique that can only transfer linearly correlated knowledge between domains. In this paper, we propose the notion of Hyper-Structure Transfer (HST) that requires the rating matrices to be explained by the projections of some more complex structure, called the hyper-structure, shared by all domains, and thus allows the non-linearly correlated knowledge between domains to be identified and transferred. Extensive experiments are conducted and the results demonstrate the effectiveness of our HST models empirically.",
        "bibtex": "@InProceedings{pmlr-v37-liua15,\n  title = \t {Non-Linear Cross-Domain Collaborative Filtering via Hyper-Structure Transfer},\n  author = \t {Liu, Yan-Fu and Hsu, Cheng-Yu and Wu, Shan-Hung},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1190--1198},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/liua15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/liua15.html},\n  abstract = \t {The Cross Domain Collaborative Filtering (CDCF) exploits the rating matrices from multiple domains to make better recommendations. Existing CDCF methods adopt the sub-structure sharing technique that can only transfer linearly correlated knowledge between domains. In this paper, we propose the notion of Hyper-Structure Transfer (HST) that requires the rating matrices to be explained by the projections of some more complex structure, called the hyper-structure, shared by all domains, and thus allows the non-linearly correlated knowledge between domains to be identified and transferred. Extensive experiments are conducted and the results demonstrate the effectiveness of our HST models empirically.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/liua15.pdf",
        "supp": "",
        "pdf_size": 555254,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17307358632302323163&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "National Tsing Hua University, ROC; National Tsing Hua University, ROC; National Tsing Hua University, ROC",
        "aff_domain": "NETDB.CS.NTHU.EDU.TW;NETDB.CS.NTHU.EDU.TW;CS.NTHU.EDU.TW",
        "email": "NETDB.CS.NTHU.EDU.TW;NETDB.CS.NTHU.EDU.TW;CS.NTHU.EDU.TW",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "National Tsing Hua University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nthu.edu.tw",
        "aff_unique_abbr": "NTHU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Taiwan",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "f4482805d8",
        "title": "Non-Stationary Approximate Modified Policy Iteration",
        "site": "https://proceedings.mlr.press/v37/lesner15.html",
        "author": "Boris Lesner; Bruno Scherrer",
        "abstract": "We consider the infinite-horizon \u03b3-discounted optimal control problem formalized by Markov Decision Processes. Running any instance of Modified Policy Iteration\u2014a family of algorithms that can interpolate between Value and Policy Iteration\u2014with an error \u03b5at each iteration is known to lead to stationary policies that are at least \\frac2\u03b3\u03b5(1-\u03b3)^2-optimal. Variations of Value and Policy Iteration, that build \\ell-periodic non-stationary policies, have recently been shown to display a better \\frac2\u03b3\u03b5(1-\u03b3)(1-\u03b3^\\ell)-optimality guarantee. Our first contribution is to describe a new algorithmic scheme, Non-Stationary Modified Policy Iteration, a family of algorithms parameterized by two integers m \\ge 0 and \\ell \\ge 1 that generalizes all the above mentionned algorithms. While m allows to interpolate between Value-Iteration-style and Policy-Iteration-style updates, \\ell specifies the period of the non-stationary policy that is output. We show that this new family of algorithms also enjoys the improved \\frac2\u03b3\u03b5(1-\u03b3)(1-\u03b3^\\ell)-optimality guarantee. Perhaps more importantly, we show, by exhibiting an original problem instance, that this guarantee is tight for all m and \\ell; this tightness was to our knowledge only proved two specific cases, Value Iteration (m=0,\\ell=1) and Policy Iteration (m=\u221e,\\ell=1).",
        "bibtex": "@InProceedings{pmlr-v37-lesner15,\n  title = \t {Non-Stationary Approximate Modified Policy Iteration},\n  author = \t {Lesner, Boris and Scherrer, Bruno},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1567--1575},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/lesner15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/lesner15.html},\n  abstract = \t {We consider the infinite-horizon \u03b3-discounted optimal control problem formalized by Markov Decision Processes. Running any instance of Modified Policy Iteration\u2014a family of algorithms that can interpolate between Value and Policy Iteration\u2014with an error \u03b5at each iteration is known to lead to stationary policies that are at least \\frac2\u03b3\u03b5(1-\u03b3)^2-optimal. Variations of Value and Policy Iteration, that build \\ell-periodic non-stationary policies, have recently been shown to display a better \\frac2\u03b3\u03b5(1-\u03b3)(1-\u03b3^\\ell)-optimality guarantee. Our first contribution is to describe a new algorithmic scheme, Non-Stationary Modified Policy Iteration, a family of algorithms parameterized by two integers m \\ge 0 and \\ell \\ge 1 that generalizes all the above mentionned algorithms. While m allows to interpolate between Value-Iteration-style and Policy-Iteration-style updates, \\ell specifies the period of the non-stationary policy that is output. We show that this new family of algorithms also enjoys the improved \\frac2\u03b3\u03b5(1-\u03b3)(1-\u03b3^\\ell)-optimality guarantee. Perhaps more importantly, we show, by exhibiting an original problem instance, that this guarantee is tight for all m and \\ell; this tightness was to our knowledge only proved two specific cases, Value Iteration (m=0,\\ell=1) and Policy Iteration (m=\u221e,\\ell=1).}\n}",
        "pdf": "http://proceedings.mlr.press/v37/lesner15.pdf",
        "supp": "",
        "pdf_size": 493034,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2117657172553364097&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Inria, Villers-ls-Nancy, F-54600, France + Universit de Lorraine, LORIA, UMR 7503, Vanduvre-ls-Nancy, F-54506, France; Inria, Villers-ls-Nancy, F-54600, France + Universit de Lorraine, LORIA, UMR 7503, Vanduvre-ls-Nancy, F-54506, France",
        "aff_domain": "GMAIL.COM;INRIA.FR",
        "email": "GMAIL.COM;INRIA.FR",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "INRIA;Universit de Lorraine",
        "aff_unique_dep": ";LORIA, UMR 7503",
        "aff_unique_url": "https://www.inria.fr;",
        "aff_unique_abbr": "Inria;",
        "aff_campus_unique_index": "0+1;0+1",
        "aff_campus_unique": "Villers-l\u00e8s-Nancy;Vanduvre-ls-Nancy",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "France"
    },
    {
        "id": "a85e8d4f9c",
        "title": "Off-policy Model-based Learning under Unknown Factored Dynamics",
        "site": "https://proceedings.mlr.press/v37/hallak15.html",
        "author": "Assaf Hallak; Francois Schnitzler; Timothy Mann; Shie Mannor",
        "abstract": "Off-policy learning in dynamic decision problems is essential for providing strong evidence that a new policy is better than the one in use. But how can we prove superiority without testing the new policy? To answer this question, we introduce the G-SCOPE algorithm that evaluates a new policy based on data generated by the existing policy. Our algorithm is both computationally and sample efficient because it greedily learns to exploit factored structure in the dynamics of the environment. We present a finite sample analysis of our approach and show through experiments that the algorithm scales well on high-dimensional problems with few samples.",
        "bibtex": "@InProceedings{pmlr-v37-hallak15,\n  title = \t {Off-policy Model-based Learning under Unknown Factored Dynamics},\n  author = \t {Hallak, Assaf and Schnitzler, Francois and Mann, Timothy and Mannor, Shie},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {711--719},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/hallak15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/hallak15.html},\n  abstract = \t {Off-policy learning in dynamic decision problems is essential for providing strong evidence that a new policy is better than the one in use. But how can we prove superiority without testing the new policy? To answer this question, we introduce the G-SCOPE algorithm that evaluates a new policy based on data generated by the existing policy. Our algorithm is both computationally and sample efficient because it greedily learns to exploit factored structure in the dynamics of the environment. We present a finite sample analysis of our approach and show through experiments that the algorithm scales well on high-dimensional problems with few samples.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/hallak15.pdf",
        "supp": "",
        "pdf_size": 446596,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13717478405247787903&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Technion, Haifa, Israel; Technion, Haifa, Israel; Technion, Haifa, Israel; Technion, Haifa, Israel",
        "aff_domain": "ifogph;ee.technion.ac.il;ee.technion.ac.il;ee.technion.ac.il",
        "email": "ifogph;ee.technion.ac.il;ee.technion.ac.il;ee.technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Technion - Israel Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.technion.ac.il/en/",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Haifa",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "efa46afe9f",
        "title": "On Deep Multi-View Representation Learning",
        "site": "https://proceedings.mlr.press/v37/wangb15.html",
        "author": "Weiran Wang; Raman Arora; Karen Livescu; Jeff Bilmes",
        "abstract": "We consider learning representations (features) in the setting in which we have access to multiple unlabeled views of the data for representation learning while only one view is available at test time. Previous work on this problem has proposed several techniques based on deep neural networks, typically involving either autoencoder-like networks with a reconstruction objective or paired feedforward networks with a correlation-based objective. We analyze several techniques based on prior work, as well as new variants, and compare them experimentally on visual, speech, and language domains. To our knowledge this is the first head-to-head comparison of a variety of such techniques on multiple tasks. We find an advantage for correlation-based representation learning, while the best results on most tasks are obtained with our new variant, deep canonically correlated autoencoders (DCCAE).",
        "bibtex": "@InProceedings{pmlr-v37-wangb15,\n  title = \t {On Deep Multi-View Representation Learning},\n  author = \t {Wang, Weiran and Arora, Raman and Livescu, Karen and Bilmes, Jeff},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1083--1092},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/wangb15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/wangb15.html},\n  abstract = \t {We consider learning representations (features) in the setting in which we have access to multiple unlabeled views of the data for representation learning while only one view is available at test time. Previous work on this problem has proposed several techniques based on deep neural networks, typically involving either autoencoder-like networks with a reconstruction objective or paired feedforward networks with a correlation-based objective. We analyze several techniques based on prior work, as well as new variants, and compare them experimentally on visual, speech, and language domains. To our knowledge this is the first head-to-head comparison of a variety of such techniques on multiple tasks. We find an advantage for correlation-based representation learning, while the best results on most tasks are obtained with our new variant, deep canonically correlated autoencoders (DCCAE).}\n}",
        "pdf": "http://proceedings.mlr.press/v37/wangb15.pdf",
        "supp": "",
        "pdf_size": 6547856,
        "gs_citation": 1235,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9056558950588383944&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Toyota Technological Institute at Chicago; Johns Hopkins University; Toyota Technological Institute at Chicago; University of Washington, Seattle",
        "aff_domain": "ttic.edu;cs.jhu.edu;ttic.edu;ee.washington.edu",
        "email": "ttic.edu;cs.jhu.edu;ttic.edu;ee.washington.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "Toyota Technological Institute at Chicago;Johns Hopkins University;University of Washington",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.tti-chicago.org;https://www.jhu.edu;https://www.washington.edu",
        "aff_unique_abbr": "TTI Chicago;JHU;UW",
        "aff_campus_unique_index": "0;0;2",
        "aff_campus_unique": "Chicago;;Seattle",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "86527ca4d1",
        "title": "On Greedy Maximization of Entropy",
        "site": "https://proceedings.mlr.press/v37/sharma15.html",
        "author": "Dravyansh Sharma; Ashish Kapoor; Amit Deshpande",
        "abstract": "Submodular function maximization is one of the key problems that arise in many machine learning tasks. Greedy selection algorithms are the proven choice to solve such problems, where prior theoretical work guarantees (1 - 1/e) approximation ratio. However, it has been empirically observed that greedy selection provides almost optimal solutions in practice. The main goal of this paper is to explore and answer why the greedy selection does significantly better than the theoretical guarantee of (1 - 1/e). Applications include, but are not limited to, sensor selection tasks which use both entropy and mutual information as a maximization criteria. We give a theoretical justification for the nearly optimal approximation ratio via detailed analysis of the curvature of these objective functions for Gaussian RBF kernels.",
        "bibtex": "@InProceedings{pmlr-v37-sharma15,\n  title = \t {On Greedy Maximization of Entropy},\n  author = \t {Sharma, Dravyansh and Kapoor, Ashish and Deshpande, Amit},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1330--1338},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/sharma15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/sharma15.html},\n  abstract = \t {Submodular function maximization is one of the key problems that arise in many machine learning tasks. Greedy selection algorithms are the proven choice to solve such problems, where prior theoretical work guarantees (1 - 1/e) approximation ratio. However, it has been empirically observed that greedy selection provides almost optimal solutions in practice. The main goal of this paper is to explore and answer why the greedy selection does significantly better than the theoretical guarantee of (1 - 1/e). Applications include, but are not limited to, sensor selection tasks which use both entropy and mutual information as a maximization criteria. We give a theoretical justification for the nearly optimal approximation ratio via detailed analysis of the curvature of these objective functions for Gaussian RBF kernels.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/sharma15.pdf",
        "supp": "",
        "pdf_size": 330052,
        "gs_citation": 71,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16105225055708930531&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "IIT Delhi, New Delhi, India; Microsoft Research, Bangalore, India; Microsoft Research, Redmond, USA",
        "aff_domain": "CSE.IITD.AC.IN;MICROSOFT.COM;MICROSOFT.COM",
        "email": "CSE.IITD.AC.IN;MICROSOFT.COM;MICROSOFT.COM",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Indian Institute of Technology Delhi;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.iitd.ac.in;https://www.microsoft.com/en-us/research/group/microsoft-research-india",
        "aff_unique_abbr": "IIT Delhi;MSR",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "New Delhi;Bangalore;Redmond",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "India;United States"
    },
    {
        "id": "3866f605f7",
        "title": "On Identifying Good Options under Combinatorially Structured Feedback in Finite Noisy Environments",
        "site": "https://proceedings.mlr.press/v37/wub15.html",
        "author": "Yifan Wu; Andras Gyorgy; Csaba Szepesvari",
        "abstract": "We consider the problem of identifying a good option out of finite set of options under combinatorially structured, noisy feedback about the quality of the options in a sequential process: In each round, a subset of the options, from an available set of subsets, can be selected to receive noisy information about the quality of the options in the chosen subset. The goal is to identify the highest quality option, or a group of options of the highest quality, with a small error probability, while using the smallest number of measurements. The problem generalizes best-arm identification problems. By extending previous work, we design new algorithms that are shown to be able to exploit the combinatorial structure of the problem in a nontrivial fashion, while being unimprovable in special cases. The algorithms call a set multi-covering oracle, hence their performance and efficiency is strongly tied to whether the associated set multi-covering problem can be efficiently solved.",
        "bibtex": "@InProceedings{pmlr-v37-wub15,\n  title = \t {On Identifying Good Options under Combinatorially Structured Feedback in Finite Noisy Environments},\n  author = \t {Wu, Yifan and Gyorgy, Andras and Szepesvari, Csaba},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1283--1291},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/wub15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/wub15.html},\n  abstract = \t {We consider the problem of identifying a good option out of finite set of options under combinatorially structured, noisy feedback about the quality of the options in a sequential process: In each round, a subset of the options, from an available set of subsets, can be selected to receive noisy information about the quality of the options in the chosen subset. The goal is to identify the highest quality option, or a group of options of the highest quality, with a small error probability, while using the smallest number of measurements. The problem generalizes best-arm identification problems. By extending previous work, we design new algorithms that are shown to be able to exploit the combinatorial structure of the problem in a nontrivial fashion, while being unimprovable in special cases. The algorithms call a set multi-covering oracle, hence their performance and efficiency is strongly tied to whether the associated set multi-covering problem can be efficiently solved.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/wub15.pdf",
        "supp": "",
        "pdf_size": 367732,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10346706750086251314&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computing Science, University of Alberta, Edmonton, AB T6G 2E8 CANADA; Department of Computing Science, University of Alberta, Edmonton, AB T6G 2E8 CANADA; Department of Computing Science, University of Alberta, Edmonton, AB T6G 2E8 CANADA",
        "aff_domain": "UALBERTA.CA;UALBERTA.CA;UALBERTA.CA",
        "email": "UALBERTA.CA;UALBERTA.CA;UALBERTA.CA",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "Department of Computing Science",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Edmonton",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "ec8d40362a",
        "title": "On Symmetric and Asymmetric LSHs for Inner Product Search",
        "site": "https://proceedings.mlr.press/v37/neyshabur15.html",
        "author": "Behnam Neyshabur; Nathan Srebro",
        "abstract": "We consider the problem of designing locality sensitive hashes (LSH) for inner product similarity, and of the power of asymmetric hashes in this context. Shrivastava and Li (2014a) argue that there is no symmetric LSH for the problem and propose an asymmetric LSH based on different mappings for query and database points. However, we show there does exist a simple symmetric LSH that enjoys stronger guarantees and better empirical performance than the asymmetric LSH they suggest. We also show a variant of the settings where asymmetry is in-fact needed, but there a different asymmetric LSH is required.",
        "bibtex": "@InProceedings{pmlr-v37-neyshabur15,\n  title = \t {On Symmetric and Asymmetric LSHs for Inner Product Search},\n  author = \t {Neyshabur, Behnam and Srebro, Nathan},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1926--1934},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/neyshabur15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/neyshabur15.html},\n  abstract = \t {We consider the problem of designing locality sensitive hashes (LSH) for inner product similarity, and of the power of asymmetric hashes in this context. Shrivastava and Li (2014a) argue that there is no symmetric LSH for the problem and propose an asymmetric LSH based on different mappings for query and database points. However, we show there does exist a simple symmetric LSH that enjoys stronger guarantees and better empirical performance than the asymmetric LSH they suggest. We also show a variant of the settings where asymmetry is in-fact needed, but there a different asymmetric LSH is required.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/neyshabur15.pdf",
        "supp": "",
        "pdf_size": 656252,
        "gs_citation": 224,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7573098365541111759&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Toyota Technological Institute at Chicago; Toyota Technological Institute at Chicago",
        "aff_domain": "TTIC.EDU;TTIC.EDU",
        "email": "TTIC.EDU;TTIC.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Toyota Technological Institute at Chicago",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tti-chicago.org",
        "aff_unique_abbr": "TTI Chicago",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chicago",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9f7a5cf6a5",
        "title": "On TD(0) with function approximation: Concentration bounds and a centered variant with exponential convergence",
        "site": "https://proceedings.mlr.press/v37/korda15.html",
        "author": "Nathaniel Korda; Prashanth La",
        "abstract": "We provide non-asymptotic bounds for the well-known temporal difference learning algorithm TD(0) with linear function approximators. These include high-probability bounds as well as bounds in expectation. Our analysis suggests that a step-size inversely proportional to the number of iterations cannot guarantee optimal rate of convergence unless we assume (partial) knowledge of the stationary distribution for the Markov chain underlying the policy considered. We also provide bounds for the iterate averaged TD(0) variant, which gets rid of the step-size dependency while exhibiting the optimal rate of convergence. Furthermore, we propose a variant of TD(0) with linear approximators that incorporates a centering sequence, and establish that it exhibits an exponential rate of convergence in expectation. We demonstrate the usefulness of our bounds on two synthetic experimental settings.",
        "bibtex": "@InProceedings{pmlr-v37-korda15,\n  title = \t {On TD(0) with function approximation: Concentration bounds and a centered variant with exponential convergence},\n  author = \t {Korda, Nathaniel and La, Prashanth},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {626--634},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/korda15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/korda15.html},\n  abstract = \t {We provide non-asymptotic bounds for the well-known temporal difference learning algorithm TD(0) with linear function approximators. These include high-probability bounds as well as bounds in expectation. Our analysis suggests that a step-size inversely proportional to the number of iterations cannot guarantee optimal rate of convergence unless we assume (partial) knowledge of the stationary distribution for the Markov chain underlying the policy considered. We also provide bounds for the iterate averaged TD(0) variant, which gets rid of the step-size dependency while exhibiting the optimal rate of convergence. Furthermore, we propose a variant of TD(0) with linear approximators that incorporates a centering sequence, and establish that it exhibits an exponential rate of convergence in expectation. We demonstrate the usefulness of our bounds on two synthetic experimental settings.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/korda15.pdf",
        "supp": "",
        "pdf_size": 365643,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7288250899536911768&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "MLRG, University of Oxford, UK; INRIA Lille - Nord Europe, Team SequeL, FRANCE",
        "aff_domain": "ENG.OX.AC.UK;INRIA.FR",
        "email": "ENG.OX.AC.UK;INRIA.FR",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Oxford;INRIA Lille - Nord Europe",
        "aff_unique_dep": "Machine Learning Research Group;Team SequeL",
        "aff_unique_url": "https://www.ox.ac.uk;https://www.inria.fr/en",
        "aff_unique_abbr": "Oxford;INRIA",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Lille",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United Kingdom;France"
    },
    {
        "id": "5ef167e24f",
        "title": "On the Optimality of Multi-Label Classification under Subset Zero-One Loss for Distributions Satisfying the Composition Property",
        "site": "https://proceedings.mlr.press/v37/gasse15.html",
        "author": "Maxime Gasse; Alexandre Aussem; Haytham Elghazel",
        "abstract": "The benefit of exploiting label dependence in multi-label classification is known to be closely dependent on the type of loss to be minimized. In this paper, we show that the subsets of labels that appear as irreducible factors in the factorization of the conditional distribution of the label set given the input features play a pivotal role for multi-label classification in the context of subset Zero-One loss minimization, as they divide the learning task into simpler independent multi-class problems. We establish theoretical results to characterize and identify these irreducible label factors for any given probability distribution satisfying the Composition property. The analysis lays the foundation for generic multi-label classification and optimal feature subset selection procedures under this subclass of distributions. Our conclusions are supported by carefully designed experiments on synthetic and benchmark data.",
        "bibtex": "@InProceedings{pmlr-v37-gasse15,\n  title = \t {On the Optimality of Multi-Label Classification under Subset Zero-One Loss for Distributions Satisfying the Composition Property},\n  author = \t {Gasse, Maxime and Aussem, Alexandre and Elghazel, Haytham},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2531--2539},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/gasse15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/gasse15.html},\n  abstract = \t {The benefit of exploiting label dependence in multi-label classification is known to be closely dependent on the type of loss to be minimized. In this paper, we show that the subsets of labels that appear as irreducible factors in the factorization of the conditional distribution of the label set given the input features play a pivotal role for multi-label classification in the context of subset Zero-One loss minimization, as they divide the learning task into simpler independent multi-class problems. We establish theoretical results to characterize and identify these irreducible label factors for any given probability distribution satisfying the Composition property. The analysis lays the foundation for generic multi-label classification and optimal feature subset selection procedures under this subclass of distributions. Our conclusions are supported by carefully designed experiments on synthetic and benchmark data.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/gasse15.pdf",
        "supp": "",
        "pdf_size": 304008,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2075848940416597757&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "LIRIS, UMR 5205, University of Lyon 1, 69622 Lyon, France; LIRIS, UMR 5205, University of Lyon 1, 69622 Lyon, France; LIRIS, UMR 5205, University of Lyon 1, 69622 Lyon, France",
        "aff_domain": "liris.cnrs.fr;liris.cnrs.fr;liris.cnrs.fr",
        "email": "liris.cnrs.fr;liris.cnrs.fr;liris.cnrs.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Lyon 1",
        "aff_unique_dep": "LIRIS",
        "aff_unique_url": "https://www.universite-lyon1.fr",
        "aff_unique_abbr": "UCBL",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Lyon",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "f1d494acf3",
        "title": "On the Rate of Convergence and Error Bounds for LSTD(\u03bb)",
        "site": "https://proceedings.mlr.press/v37/tagorti15.html",
        "author": "Manel Tagorti; Bruno Scherrer",
        "abstract": "We consider LSTD(\u03bb), the least-squares temporal-difference algorithm with eligibility traces algorithm proposed by Boyan (2002). It computes a linear approximation of the value function of a fixed policy in a large Markov Decision Process. Under a \u03b2-mixing assumption, we derive, for any value of \u03bb\u2208(0,1), a high-probability bound on the rate of convergence of this algorithm to its limit. We deduce a high-probability bound on the error of this algorithm, that extends (and slightly improves) that derived by Lazaric et al. (2012) in the specific case where \u03bb=0. In the context of temporal-difference algorithms with value function approximation, this analysis is to our knowledge the first to provide insight on the choice of the eligibility-trace parameter \u03bbwith respect to the approximation quality of the space and the number of samples.",
        "bibtex": "@InProceedings{pmlr-v37-tagorti15,\n  title = \t {On the Rate of Convergence and Error Bounds for LSTD($\\lambda$)},\n  author = \t {Tagorti, Manel and Scherrer, Bruno},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1521--1529},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/tagorti15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/tagorti15.html},\n  abstract = \t {We consider LSTD(\u03bb), the least-squares temporal-difference algorithm with eligibility traces algorithm proposed by Boyan (2002). It computes a linear approximation of the value function of a fixed policy in a large Markov Decision Process. Under a \u03b2-mixing assumption, we derive, for any value of \u03bb\u2208(0,1), a high-probability bound on the rate of convergence of this algorithm to its limit. We deduce a high-probability bound on the error of this algorithm, that extends (and slightly improves) that derived by Lazaric et al. (2012) in the specific case where \u03bb=0. In the context of temporal-difference algorithms with value function approximation, this analysis is to our knowledge the first to provide insight on the choice of the eligibility-trace parameter \u03bbwith respect to the approximation quality of the space and the number of samples.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/tagorti15.pdf",
        "supp": "",
        "pdf_size": 378399,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8610270356617521580&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Inria, Villers-l `es-Nancy, F-54600, France + Universit \u00b4e de Lorraine, LORIA, UMR 7503, Vand\u0153uvre-l `es-Nancy, F-54506, France; Inria, Villers-l `es-Nancy, F-54600, France + Universit \u00b4e de Lorraine, LORIA, UMR 7503, Vand\u0153uvre-l `es-Nancy, F-54506, France",
        "aff_domain": "inria.fr;inria.fr",
        "email": "inria.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "INRIA;Universit\u00e9 de Lorraine",
        "aff_unique_dep": ";LORIA, UMR 7503",
        "aff_unique_url": "https://www.inria.fr;https://www.univ-lorraine.fr",
        "aff_unique_abbr": "Inria;",
        "aff_campus_unique_index": "0+1;0+1",
        "aff_campus_unique": "Villers-l\u00e8s-Nancy;Vand\u0153uvre-l\u00e8s-Nancy",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "France"
    },
    {
        "id": "133433f2ef",
        "title": "On the Relationship between Sum-Product Networks and Bayesian Networks",
        "site": "https://proceedings.mlr.press/v37/zhaoc15.html",
        "author": "Han Zhao; Mazen Melibari; Pascal Poupart",
        "abstract": "In this paper, we establish some theoretical connections between Sum-Product Networks (SPNs) and Bayesian Networks (BNs). We prove that every SPN can be converted into a BN in linear time and space in terms of the network size. The key insight is to use Algebraic Decision Diagrams (ADDs) to compactly represent the local conditional probability distributions at each node in the resulting BN by exploiting context-specific independence (CSI). The generated BN has a simple directed bipartite graphical structure. We show that by applying the Variable Elimination algorithm (VE) to the generated BN with ADD representations, we can recover the original SPN where the SPN can be viewed as a history record or caching of the VE inference process. To help state the proof clearly, we introduce the notion of \\em normal SPN and present a theoretical analysis of the consistency and decomposability properties. We conclude the paper with some discussion of the implications of the proof and establish a connection between the depth of an SPN and a lower bound of the tree-width of its corresponding BN.",
        "bibtex": "@InProceedings{pmlr-v37-zhaoc15,\n  title = \t {On the Relationship between Sum-Product Networks and Bayesian Networks},\n  author = \t {Zhao, Han and Melibari, Mazen and Poupart, Pascal},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {116--124},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/zhaoc15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/zhaoc15.html},\n  abstract = \t {In this paper, we establish some theoretical connections between Sum-Product Networks (SPNs) and Bayesian Networks (BNs). We prove that every SPN can be converted into a BN in linear time and space in terms of the network size. The key insight is to use Algebraic Decision Diagrams (ADDs) to compactly represent the local conditional probability distributions at each node in the resulting BN by exploiting context-specific independence (CSI). The generated BN has a simple directed bipartite graphical structure. We show that by applying the Variable Elimination algorithm (VE) to the generated BN with ADD representations, we can recover the original SPN where the SPN can be viewed as a history record or caching of the VE inference process. To help state the proof clearly, we introduce the notion of \\em normal SPN and present a theoretical analysis of the consistency and decomposability properties. We conclude the paper with some discussion of the implications of the proof and establish a connection between the depth of an SPN and a lower bound of the tree-width of its corresponding BN.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/zhaoc15.pdf",
        "supp": "",
        "pdf_size": 339091,
        "gs_citation": 117,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14794186202397455996&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 23,
        "aff": "David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, ON, Canada; David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, ON, Canada; David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, ON, Canada",
        "aff_domain": "UWATERLOO.CA;UWATERLOO.CA;UWATERLOO.CA",
        "email": "UWATERLOO.CA;UWATERLOO.CA;UWATERLOO.CA",
        "github": "",
        "project": "http://spn.cs.washington.edu/faq.shtml",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Waterloo",
        "aff_unique_dep": "David R. Cheriton School of Computer Science",
        "aff_unique_url": "https://uwaterloo.ca",
        "aff_unique_abbr": "UWaterloo",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Waterloo",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "43319c656d",
        "title": "Online Learning of Eigenvectors",
        "site": "https://proceedings.mlr.press/v37/garberb15.html",
        "author": "Dan Garber; Elad Hazan; Tengyu Ma",
        "abstract": "Computing the leading eigenvector of a symmetric real matrix is a fundamental primitive of numerical linear algebra with numerous applications. We consider a natural online extension of the leading eigenvector problem: a sequence of matrices is presented and the goal is to predict for each matrix a unit vector, with the overall goal of competing with the leading eigenvector of the cumulative matrix. Existing regret-minimization algorithms for this problem either require to compute an \\textiteigen decompostion every iteration, or suffer from a large dependency of the regret bound on the dimension. In both cases the algorithms are not practical for large scale applications. In this paper we present new algorithms that avoid both issues. On one hand they do not require any expensive matrix decompositions and on the other, they guarantee regret rates with a mild dependence on the dimension at most. In contrast to previous algorithms, our algorithms also admit implementations that enable to leverage sparsity in the data to further reduce computation. We extend our results to also handle non-symmetric matrices.",
        "bibtex": "@InProceedings{pmlr-v37-garberb15,\n  title = \t {Online Learning of Eigenvectors},\n  author = \t {Garber, Dan and Hazan, Elad and Ma, Tengyu},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {560--568},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/garberb15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/garberb15.html},\n  abstract = \t {Computing the leading eigenvector of a symmetric real matrix is a fundamental primitive of numerical linear algebra with numerous applications. We consider a natural online extension of the leading eigenvector problem: a sequence of matrices is presented and the goal is to predict for each matrix a unit vector, with the overall goal of competing with the leading eigenvector of the cumulative matrix. Existing regret-minimization algorithms for this problem either require to compute an \\textiteigen decompostion every iteration, or suffer from a large dependency of the regret bound on the dimension. In both cases the algorithms are not practical for large scale applications. In this paper we present new algorithms that avoid both issues. On one hand they do not require any expensive matrix decompositions and on the other, they guarantee regret rates with a mild dependence on the dimension at most. In contrast to previous algorithms, our algorithms also admit implementations that enable to leverage sparsity in the data to further reduce computation. We extend our results to also handle non-symmetric matrices.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/garberb15.pdf",
        "supp": "",
        "pdf_size": 451656,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1169390758141584919&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Technion - Israel Institute of Technology; Princeton University; Princeton University",
        "aff_domain": "TX.TECHNION.AC.IL;CS.PRINCETON.EDU;CS.PRINCETON.EDU",
        "email": "TX.TECHNION.AC.IL;CS.PRINCETON.EDU;CS.PRINCETON.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Technion - Israel Institute of Technology;Princeton University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.technion.ac.il/en/;https://www.princeton.edu",
        "aff_unique_abbr": "Technion;Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Israel;United States"
    },
    {
        "id": "2096d5a5dc",
        "title": "Online Time Series Prediction with Missing Data",
        "site": "https://proceedings.mlr.press/v37/anava15.html",
        "author": "Oren Anava; Elad Hazan; Assaf Zeevi",
        "abstract": "We consider the problem of time series prediction in the presence of missing data. We cast the problem as an online learning problem in which the goal of the learner is to minimize prediction error. We then devise an efficient algorithm for the problem, which is based on autoregressive model, and does not assume any structure on the missing data nor on the mechanism that generates the time series. We show that our algorithm\u2019s performance asymptotically approaches the performance of the best AR predictor in hindsight, and corroborate the theoretic results with an empirical study on synthetic and real-world data.",
        "bibtex": "@InProceedings{pmlr-v37-anava15,\n  title = \t {Online Time Series Prediction with Missing Data},\n  author = \t {Anava, Oren and Hazan, Elad and Zeevi, Assaf},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2191--2199},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/anava15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/anava15.html},\n  abstract = \t {We consider the problem of time series prediction in the presence of missing data. We cast the problem as an online learning problem in which the goal of the learner is to minimize prediction error. We then devise an efficient algorithm for the problem, which is based on autoregressive model, and does not assume any structure on the missing data nor on the mechanism that generates the time series. We show that our algorithm\u2019s performance asymptotically approaches the performance of the best AR predictor in hindsight, and corroborate the theoretic results with an empirical study on synthetic and real-world data.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/anava15.pdf",
        "supp": "",
        "pdf_size": 380719,
        "gs_citation": 100,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10345055486065082199&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Technion, Haifa, Israel; Princeton University, NY, USA; Columbia University, NY, USA",
        "aff_domain": "tx.technion.ac.il;cs.princeton.edu;gsb.columbia.edu",
        "email": "tx.technion.ac.il;cs.princeton.edu;gsb.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Technion - Israel Institute of Technology;Princeton University;Columbia University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.technion.ac.il/en/;https://www.princeton.edu;https://www.columbia.edu",
        "aff_unique_abbr": "Technion;Princeton;Columbia",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "Haifa;Princeton;New York",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Israel;United States"
    },
    {
        "id": "7f745e4e4d",
        "title": "Online Tracking by Learning Discriminative Saliency Map with Convolutional Neural Network",
        "site": "https://proceedings.mlr.press/v37/hong15.html",
        "author": "Seunghoon Hong; Tackgeun You; Suha Kwak; Bohyung Han",
        "abstract": "We propose an online visual tracking algorithm by learning discriminative saliency map using Convolutional Neural Network (CNN). Given a CNN pre-trained on a large-scale image repository in offline, our algorithm takes outputs from hidden layers of the network as feature descriptors since they show excellent representation performance in various general visual recognition problems. The features are used to learn discriminative target appearance models using an online Support Vector Machine (SVM). In addition, we construct target-specific saliency map by back-projecting CNN features with guidance of the SVM, and obtain the final tracking result in each frame based on the appearance model generatively constructed with the saliency map. Since the saliency map reveals spatial configuration of target effectively, it improves target localization accuracy and enables us to achieve pixel-level target segmentation. We verify the effectiveness of our tracking algorithm through extensive experiment on a challenging benchmark, where our method illustrates outstanding performance compared to the state-of-the-art tracking algorithms.",
        "bibtex": "@InProceedings{pmlr-v37-hong15,\n  title = \t {Online Tracking by Learning Discriminative Saliency Map with Convolutional Neural Network},\n  author = \t {Hong, Seunghoon and You, Tackgeun and Kwak, Suha and Han, Bohyung},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {597--606},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/hong15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/hong15.html},\n  abstract = \t {We propose an online visual tracking algorithm by learning discriminative saliency map using Convolutional Neural Network (CNN). Given a CNN pre-trained on a large-scale image repository in offline, our algorithm takes outputs from hidden layers of the network as feature descriptors since they show excellent representation performance in various general visual recognition problems. The features are used to learn discriminative target appearance models using an online Support Vector Machine (SVM). In addition, we construct target-specific saliency map by back-projecting CNN features with guidance of the SVM, and obtain the final tracking result in each frame based on the appearance model generatively constructed with the saliency map. Since the saliency map reveals spatial configuration of target effectively, it improves target localization accuracy and enables us to achieve pixel-level target segmentation. We verify the effectiveness of our tracking algorithm through extensive experiment on a challenging benchmark, where our method illustrates outstanding performance compared to the state-of-the-art tracking algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/hong15.pdf",
        "supp": "",
        "pdf_size": 3774729,
        "gs_citation": 1028,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8452657516733607725&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Dept. of Computer Science and Engineering, POSTECH, Pohang, Korea; Dept. of Computer Science and Engineering, POSTECH, Pohang, Korea; Inria\u2013WILLOW Project, Paris, France; Dept. of Computer Science and Engineering, POSTECH, Pohang, Korea",
        "aff_domain": "POSTECH.AC.KR;POSTECH.AC.KR;INRIA.FR;POSTECH.AC.KR",
        "email": "POSTECH.AC.KR;POSTECH.AC.KR;INRIA.FR;POSTECH.AC.KR",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "POSTECH;INRIA",
        "aff_unique_dep": "Dept. of Computer Science and Engineering;WILLOW Project",
        "aff_unique_url": "https://www.postech.ac.kr;https://www.inria.fr",
        "aff_unique_abbr": "POSTECH;Inria",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Pohang;Paris",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "South Korea;France"
    },
    {
        "id": "65592f2b4d",
        "title": "Optimal Regret Analysis of Thompson Sampling in Stochastic Multi-armed Bandit Problem with Multiple Plays",
        "site": "https://proceedings.mlr.press/v37/komiyama15.html",
        "author": "Junpei Komiyama; Junya Honda; Hiroshi Nakagawa",
        "abstract": "We discuss a multiple-play multi-armed bandit (MAB) problem in which several arms are selected at each round. Recently, Thompson sampling (TS), a randomized algorithm with a Bayesian spirit, has attracted much attention for its empirically excellent performance, and it is revealed to have an optimal regret bound in the standard single-play MAB problem. In this paper, we propose the multiple-play Thompson sampling (MP-TS) algorithm, an extension of TS to the multiple-play MAB problem, and discuss its regret analysis. We prove that MP-TS has the optimal regret upper bound that matches the regret lower bound provided by Anantharam et al.\\,(1987). Therefore, MP-TS is the first computationally efficient algorithm with optimal regret. A set of computer simulations was also conducted, which compared MP-TS with state-of-the-art algorithms. We also propose a modification of MP-TS, which is shown to have better empirical performance.",
        "bibtex": "@InProceedings{pmlr-v37-komiyama15,\n  title = \t {Optimal Regret Analysis of Thompson Sampling in Stochastic Multi-armed Bandit Problem with Multiple Plays},\n  author = \t {Komiyama, Junpei and Honda, Junya and Nakagawa, Hiroshi},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1152--1161},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/komiyama15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/komiyama15.html},\n  abstract = \t {We discuss a multiple-play multi-armed bandit (MAB) problem in which several arms are selected at each round. Recently, Thompson sampling (TS), a randomized algorithm with a Bayesian spirit, has attracted much attention for its empirically excellent performance, and it is revealed to have an optimal regret bound in the standard single-play MAB problem. In this paper, we propose the multiple-play Thompson sampling (MP-TS) algorithm, an extension of TS to the multiple-play MAB problem, and discuss its regret analysis. We prove that MP-TS has the optimal regret upper bound that matches the regret lower bound provided by Anantharam et al.\\,(1987). Therefore, MP-TS is the first computationally efficient algorithm with optimal regret. A set of computer simulations was also conducted, which compared MP-TS with state-of-the-art algorithms. We also propose a modification of MP-TS, which is shown to have better empirical performance.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/komiyama15.pdf",
        "supp": "",
        "pdf_size": 557839,
        "gs_citation": 202,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15265376460067949735&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "The University of Tokyo, Japan; The University of Tokyo, Japan; The University of Tokyo, Japan",
        "aff_domain": "KOMIYAMA.INFO;STAT.T.U-TOKYO.AC.JP;DL.ITC.U-TOKYO.AC.JP",
        "email": "KOMIYAMA.INFO;STAT.T.U-TOKYO.AC.JP;DL.ITC.U-TOKYO.AC.JP",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Tokyo",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "UTokyo",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "1f9089ac43",
        "title": "Optimal and Adaptive Algorithms for Online Boosting",
        "site": "https://proceedings.mlr.press/v37/beygelzimer15.html",
        "author": "Alina Beygelzimer; Satyen Kale; Haipeng Luo",
        "abstract": "We study online boosting, the task of converting any weak online learner into a strong online learner. Based on a novel and natural definition of weak online learnability, we develop two online boosting algorithms. The first algorithm is an online version of boost-by-majority. By proving a matching lower bound, we show that this algorithm is essentially optimal in terms of the number of weak learners and the sample complexity needed to achieve a specified accuracy. The second algorithm is adaptive and parameter-free, albeit not optimal.",
        "bibtex": "@InProceedings{pmlr-v37-beygelzimer15,\n  title = \t {Optimal and Adaptive Algorithms for Online Boosting},\n  author = \t {Beygelzimer, Alina and Kale, Satyen and Luo, Haipeng},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2323--2331},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/beygelzimer15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/beygelzimer15.html},\n  abstract = \t {We study online boosting, the task of converting any weak online learner into a strong online learner. Based on a novel and natural definition of weak online learnability, we develop two online boosting algorithms. The first algorithm is an online version of boost-by-majority. By proving a matching lower bound, we show that this algorithm is essentially optimal in terms of the number of weak learners and the sample complexity needed to achieve a specified accuracy. The second algorithm is adaptive and parameter-free, albeit not optimal.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/beygelzimer15.pdf",
        "supp": "",
        "pdf_size": 322720,
        "gs_citation": 98,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5133164394294404950&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Yahoo Labs, New York, NY 10036; Yahoo Labs, New York, NY 10036; Department of Computer Science, Princeton University, Princeton, NJ 08540",
        "aff_domain": "yahoo-inc.com;yahoo-inc.com;cs.princeton.edu",
        "email": "yahoo-inc.com;yahoo-inc.com;cs.princeton.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Yahoo Labs;Princeton University",
        "aff_unique_dep": ";Department of Computer Science",
        "aff_unique_url": "https://labs.yahoo.com;https://www.princeton.edu",
        "aff_unique_abbr": "Yahoo Labs;Princeton",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "New York;Princeton",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "df06b8c2c8",
        "title": "Optimizing Neural Networks with Kronecker-factored Approximate Curvature",
        "site": "https://proceedings.mlr.press/v37/martens15.html",
        "author": "James Martens; Roger Grosse",
        "abstract": "We propose an efficient method for approximating natural gradient descent in neural networks which we call Kronecker-factored Approximate Curvature (K-FAC). K-FAC is based on an efficiently invertible approximation of a neural network\u2019s Fisher information matrix which is neither diagonal nor low-rank, and in some cases is completely non-sparse. It is derived by approximating various large blocks of the Fisher (corresponding to entire layers) as being the Kronecker product of two much smaller matrices. While only several times more expensive to compute than the plain stochastic gradient, the updates produced by K-FAC make much more progress optimizing the objective, which results in an algorithm that can be much faster than stochastic gradient descent with momentum in practice. And unlike some previously proposed approximate natural-gradient/Newton methods which use high-quality non-diagonal curvature matrices (such as Hessian-free optimization), K-FAC works very well in highly stochastic optimization regimes. This is because the cost of storing and inverting K-FAC\u2019s approximation to the curvature matrix does not depend on the amount of data used to estimate it, which is a feature typically associated only with diagonal or low-rank approximations to the curvature matrix.",
        "bibtex": "@InProceedings{pmlr-v37-martens15,\n  title = \t {Optimizing Neural Networks with Kronecker-factored Approximate Curvature},\n  author = \t {Martens, James and Grosse, Roger},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2408--2417},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/martens15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/martens15.html},\n  abstract = \t {We propose an efficient method for approximating natural gradient descent in neural networks which we call Kronecker-factored Approximate Curvature (K-FAC). K-FAC is based on an efficiently invertible approximation of a neural network\u2019s Fisher information matrix which is neither diagonal nor low-rank, and in some cases is completely non-sparse. It is derived by approximating various large blocks of the Fisher (corresponding to entire layers) as being the Kronecker product of two much smaller matrices. While only several times more expensive to compute than the plain stochastic gradient, the updates produced by K-FAC make much more progress optimizing the objective, which results in an algorithm that can be much faster than stochastic gradient descent with momentum in practice. And unlike some previously proposed approximate natural-gradient/Newton methods which use high-quality non-diagonal curvature matrices (such as Hessian-free optimization), K-FAC works very well in highly stochastic optimization regimes. This is because the cost of storing and inverting K-FAC\u2019s approximation to the curvature matrix does not depend on the amount of data used to estimate it, which is a feature typically associated only with diagonal or low-rank approximations to the curvature matrix.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/martens15.pdf",
        "supp": "",
        "pdf_size": 866438,
        "gs_citation": 1238,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11551045348725818062&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto",
        "aff_domain": "CS.TORONTO.EDU;CS.TORONTO.EDU",
        "email": "CS.TORONTO.EDU;CS.TORONTO.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "920136a215",
        "title": "Optimizing Non-decomposable Performance Measures: A Tale of Two Classes",
        "site": "https://proceedings.mlr.press/v37/narasimhana15.html",
        "author": "Harikrishna Narasimhan; Purushottam Kar; Prateek Jain",
        "abstract": "Modern classification problems frequently present mild to severe label imbalance as well as specific requirements on classification characteristics, and require optimizing performance measures that are non-decomposable over the dataset, such as F-measure. Such measures have spurred much interest and pose specific challenges to learning algorithms since their non-additive nature precludes a direct application of well-studied large scale optimization methods such as stochastic gradient descent. In this paper we reveal that for two large families of performance measures that can be expressed as functions of true positive/negative rates, it is indeed possible to implement point stochastic updates. The families we consider are concave and pseudo-linear functions of TPR, TNR which cover several popularly used performance measures such as F-measure, G-mean and H-mean. Our core contribution is an adaptive linearization scheme for these families, using which we develop optimization techniques that enable truly point-based stochastic updates. For concave performance measures we propose SPADE, a stochastic primal dual solver; for pseudo-linear measures we propose STAMP, a stochastic alternate maximization procedure. Both methods have crisp convergence guarantees, demonstrate significant speedups over existing methods - often by an order of magnitude or more, and give similar or more accurate predictions on test data.",
        "bibtex": "@InProceedings{pmlr-v37-narasimhana15,\n  title = \t {Optimizing Non-decomposable Performance Measures: A Tale of Two Classes},\n  author = \t {Narasimhan, Harikrishna and Kar, Purushottam and Jain, Prateek},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {199--208},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/narasimhana15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/narasimhana15.html},\n  abstract = \t {Modern classification problems frequently present mild to severe label imbalance as well as specific requirements on classification characteristics, and require optimizing performance measures that are non-decomposable over the dataset, such as F-measure. Such measures have spurred much interest and pose specific challenges to learning algorithms since their non-additive nature precludes a direct application of well-studied large scale optimization methods such as stochastic gradient descent. In this paper we reveal that for two large families of performance measures that can be expressed as functions of true positive/negative rates, it is indeed possible to implement point stochastic updates. The families we consider are concave and pseudo-linear functions of TPR, TNR which cover several popularly used performance measures such as F-measure, G-mean and H-mean. Our core contribution is an adaptive linearization scheme for these families, using which we develop optimization techniques that enable truly point-based stochastic updates. For concave performance measures we propose SPADE, a stochastic primal dual solver; for pseudo-linear measures we propose STAMP, a stochastic alternate maximization procedure. Both methods have crisp convergence guarantees, demonstrate significant speedups over existing methods - often by an order of magnitude or more, and give similar or more accurate predictions on test data.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/narasimhana15.pdf",
        "supp": "",
        "pdf_size": 479417,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11916793243780571403&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Indian Institute of Science, Bangalore, INDIA; Microsoft Research, INDIA; Microsoft Research, INDIA",
        "aff_domain": "CSA.IISC.ERNET.IN;MICROSOFT.COM;MICROSOFT.COM",
        "email": "CSA.IISC.ERNET.IN;MICROSOFT.COM;MICROSOFT.COM",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Indian Institute of Science;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.iisc.ac.in;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "IISc;MSR",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Bangalore;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "d0733f77a6",
        "title": "Ordered Stick-Breaking Prior for Sequential MCMC Inference of Bayesian Nonparametric Models",
        "site": "https://proceedings.mlr.press/v37/das15.html",
        "author": "Mrinal Das; Trapit Bansal; Chiranjib Bhattacharyya",
        "abstract": "This paper introduces ordered stick-breaking process (OSBP), where the atoms in a stick-breaking process (SBP) appear in order. The choice of weights on the atoms of OSBP ensure that; (1) probability of adding new atoms exponentially decrease, and (2) OSBP, though non-exchangeable, admit predictive probability functions (PPFs). In a Bayesian nonparametric (BNP) setting, OSBP serves as a natural prior over sequential mini-batches, facilitating exchange of relevant statistical information by sharing the atoms of OSBP. One of the major contributions of this paper is SUMO, an MCMC algorithm, for solving the inference problem arising from applying OSBP to BNP models. SUMO uses the PPFs of OSBP to obtain a Gibbs-sampling based truncation-free algorithm which applies generally to BNP models. For large scale inference problems existing algorithms such as particle filtering (PF) are not practical and variational procedures such as TSVI (Wang & Blei, 2012) are the only alternative. For Dirichlet process mixture model (DPMM), SUMO outperforms TSVI on perplexity by 33% on 3 datasets with million data points, which are beyond the scope of PF, using only 3GB RAM.",
        "bibtex": "@InProceedings{pmlr-v37-das15,\n  title = \t {Ordered Stick-Breaking Prior for Sequential MCMC Inference of Bayesian Nonparametric Models},\n  author = \t {Das, Mrinal and Bansal, Trapit and Bhattacharyya, Chiranjib},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {550--559},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/das15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/das15.html},\n  abstract = \t {This paper introduces ordered stick-breaking process (OSBP), where the atoms in a stick-breaking process (SBP) appear in order. The choice of weights on the atoms of OSBP ensure that; (1) probability of adding new atoms exponentially decrease, and (2) OSBP, though non-exchangeable, admit predictive probability functions (PPFs). In a Bayesian nonparametric (BNP) setting, OSBP serves as a natural prior over sequential mini-batches, facilitating exchange of relevant statistical information by sharing the atoms of OSBP. One of the major contributions of this paper is SUMO, an MCMC algorithm, for solving the inference problem arising from applying OSBP to BNP models. SUMO uses the PPFs of OSBP to obtain a Gibbs-sampling based truncation-free algorithm which applies generally to BNP models. For large scale inference problems existing algorithms such as particle filtering (PF) are not practical and variational procedures such as TSVI (Wang & Blei, 2012) are the only alternative. For Dirichlet process mixture model (DPMM), SUMO outperforms TSVI on perplexity by 33% on 3 datasets with million data points, which are beyond the scope of PF, using only 3GB RAM.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/das15.pdf",
        "supp": "",
        "pdf_size": 422597,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11859523241368967725&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India; Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India; Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India",
        "aff_domain": "CSA.IISC.ERNET.IN;CSA.IISC.ERNET.IN;CSA.IISC.ERNET.IN",
        "email": "CSA.IISC.ERNET.IN;CSA.IISC.ERNET.IN;CSA.IISC.ERNET.IN",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indian Institute of Science",
        "aff_unique_dep": "Department of Computer Science and Automation",
        "aff_unique_url": "https://www.iisc.ac.in",
        "aff_unique_abbr": "IISc",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Bangalore",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "a2e0d842db",
        "title": "Ordinal Mixed Membership Models",
        "site": "https://proceedings.mlr.press/v37/virtanen15.html",
        "author": "Seppo Virtanen; Mark Girolami",
        "abstract": "We present a novel class of mixed membership models for joint distributions of groups of observations that co-occur with ordinal response variables for each group for learning statistical associations between the ordinal response variables and the observation groups. The class of proposed models addresses a requirement for predictive and diagnostic methods in a wide range of practical contemporary applications. In this work, by way of illustration, we apply the models to a collection of consumer-generated reviews of mobile software applications, where each review contains unstructured text data accompanied with an ordinal rating, and demonstrate that the models infer useful and meaningful recurring patterns of consumer feedback. We also compare the developed models to relevant existing works, which rely on improper statistical assumptions for ordinal variables, showing significant improvements both in predictive ability and knowledge extraction.",
        "bibtex": "@InProceedings{pmlr-v37-virtanen15,\n  title = \t {Ordinal Mixed Membership Models},\n  author = \t {Virtanen, Seppo and Girolami, Mark},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {588--596},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/virtanen15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/virtanen15.html},\n  abstract = \t {We present a novel class of mixed membership models for joint distributions of groups of observations that co-occur with ordinal response variables for each group for learning statistical associations between the ordinal response variables and the observation groups. The class of proposed models addresses a requirement for predictive and diagnostic methods in a wide range of practical contemporary applications. In this work, by way of illustration, we apply the models to a collection of consumer-generated reviews of mobile software applications, where each review contains unstructured text data accompanied with an ordinal rating, and demonstrate that the models infer useful and meaningful recurring patterns of consumer feedback. We also compare the developed models to relevant existing works, which rely on improper statistical assumptions for ordinal variables, showing significant improvements both in predictive ability and knowledge extraction.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/virtanen15.pdf",
        "supp": "",
        "pdf_size": 431293,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12209816338454815559&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Statistics, University of Warwick, CV4 7AL Coventry UK; Department of Statistics, University of Warwick, CV4 7AL Coventry UK",
        "aff_domain": "WARWICK.AC.UK;WARWICK.AC.UK",
        "email": "WARWICK.AC.UK;WARWICK.AC.UK",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Warwick",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://warwick.ac.uk",
        "aff_unique_abbr": "Warwick",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Coventry",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "441eb7bda5",
        "title": "PASSCoDe: Parallel ASynchronous Stochastic dual Co-ordinate Descent",
        "site": "https://proceedings.mlr.press/v37/hsieha15.html",
        "author": "Cho-Jui Hsieh; Hsiang-Fu Yu; Inderjit Dhillon",
        "abstract": "Stochastic Dual Coordinate Descent (DCD) is one of the most efficient ways to solve the family of L2-regularized empirical risk minimization problems, including linear SVM, logistic regression, and many others. The vanilla implementation of DCD is quite slow; however, by maintaining primal variables while updating dual variables, the time complexity of DCD can be significantly reduced. Such a strategy forms the core algorithm in the widely-used LIBLINEAR package. In this paper, we parallelize the DCD algorithms in LIBLINEAR. In recent research, several synchronized parallel DCD algorithms have been proposed, however, they fail to achieve good speedup in the shared memory multi-core setting. In this paper, we propose a family of parallel asynchronous stochastic dual coordinate descent algorithms (PASSCoDe). Each thread repeatedly selects a random dual variable and conducts coordinate updates using the primal variables that are stored in the shared memory. We analyze the convergence properties of DCD when different locking/atomic mechanisms are applied. For implementation with atomic operations, we show linear convergence under mild conditions. For implementation without any atomic operations or locking, we present a novel error analysis for PASSCoDe under the multi-core environment, showing that the converged solution is the exact solution for a primal problem with a perturbed regularizer. Experimental results show that our methods are much faster than previous parallel coordinate descent solvers.",
        "bibtex": "@InProceedings{pmlr-v37-hsieha15,\n  title = \t {PASSCoDe: Parallel ASynchronous Stochastic dual Co-ordinate Descent},\n  author = \t {Hsieh, Cho-Jui and Yu, Hsiang-Fu and Dhillon, Inderjit},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2370--2379},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/hsieha15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/hsieha15.html},\n  abstract = \t {Stochastic Dual Coordinate Descent (DCD) is one of the most efficient ways to solve the family of L2-regularized empirical risk minimization problems, including linear SVM, logistic regression, and many others. The vanilla implementation of DCD is quite slow; however, by maintaining primal variables while updating dual variables, the time complexity of DCD can be significantly reduced. Such a strategy forms the core algorithm in the widely-used LIBLINEAR package. In this paper, we parallelize the DCD algorithms in LIBLINEAR. In recent research, several synchronized parallel DCD algorithms have been proposed, however, they fail to achieve good speedup in the shared memory multi-core setting. In this paper, we propose a family of parallel asynchronous stochastic dual coordinate descent algorithms (PASSCoDe). Each thread repeatedly selects a random dual variable and conducts coordinate updates using the primal variables that are stored in the shared memory. We analyze the convergence properties of DCD when different locking/atomic mechanisms are applied. For implementation with atomic operations, we show linear convergence under mild conditions. For implementation without any atomic operations or locking, we present a novel error analysis for PASSCoDe under the multi-core environment, showing that the converged solution is the exact solution for a primal problem with a perturbed regularizer. Experimental results show that our methods are much faster than previous parallel coordinate descent solvers.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/hsieha15.pdf",
        "supp": "",
        "pdf_size": 858776,
        "gs_citation": 119,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17762595334428067007&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, The University of Texas, Austin, TX 78721, USA; Department of Computer Science, The University of Texas, Austin, TX 78721, USA; Department of Computer Science, The University of Texas, Austin, TX 78721, USA",
        "aff_domain": "CS.UTEXAS.EDU;CS.UTEXAS.EDU;CS.UTEXAS.EDU",
        "email": "CS.UTEXAS.EDU;CS.UTEXAS.EDU;CS.UTEXAS.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1d86c4bbd0",
        "title": "PU Learning for Matrix Completion",
        "site": "https://proceedings.mlr.press/v37/hsiehb15.html",
        "author": "Cho-Jui Hsieh; Nagarajan Natarajan; Inderjit Dhillon",
        "abstract": "In this paper, we consider the matrix completion problem when the observations are one-bit measurements of some underlying matrix M , and in particular the observed samples consist only of ones and no zeros. This problem is motivated by modern applications such as recommender systems and social networks where only \u201clikes\u201d or \u201cfriendships\u201d are observed. The problem is an instance of PU (positive-unlabeled) learning, i.e. learning from only positive and unlabeled examples that has been studied in the context of binary classification. Under the assumption that M has bounded nuclear norm, we provide recovery guarantees for two different observation models: 1) M parameterizes a distribution that generates a binary matrix, 2) M is thresholded to obtain a binary matrix. For the first case, we propose a \u201cshifted matrix completion\u201d method that recovers M using only a subset of indices corresponding to ones; for the second case, we propose a \u201cbiased matrix completion\u201d method that recovers the (thresholded) binary matrix. Both methods yield strong error bounds \u2014 if M \u2208R^n \\times n, the error is bounded as O(1-\u03c1) , where 1-\u03c1denotes the fraction of ones observed. This implies a sample complexity of O(n log n) ones to achieve a small error, when M is dense and n is large. We extend our analysis to the inductive matrix completion problem, where rows and columns of M have associated features. We develop efficient and scalable optimization procedures for both the proposed methods and demonstrate their effectiveness for link prediction (on real-world networks consisting of over 2 million nodes and 90 million links) and semi-supervised clustering tasks.",
        "bibtex": "@InProceedings{pmlr-v37-hsiehb15,\n  title = \t {PU Learning for Matrix Completion},\n  author = \t {Hsieh, Cho-Jui and Natarajan, Nagarajan and Dhillon, Inderjit},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2445--2453},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/hsiehb15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/hsiehb15.html},\n  abstract = \t {In this paper, we consider the matrix completion problem when the observations are one-bit measurements of some underlying matrix M , and in particular the observed samples consist only of ones and no zeros. This problem is motivated by modern applications such as recommender systems and social networks where only \u201clikes\u201d or \u201cfriendships\u201d are observed. The problem is an instance of PU (positive-unlabeled) learning, i.e. learning from only positive and unlabeled examples that has been studied in the context of binary classification. Under the assumption that M has bounded nuclear norm, we provide recovery guarantees for two different observation models: 1) M parameterizes a distribution that generates a binary matrix, 2) M is thresholded to obtain a binary matrix. For the first case, we propose a \u201cshifted matrix completion\u201d method that recovers M using only a subset of indices corresponding to ones; for the second case, we propose a \u201cbiased matrix completion\u201d method that recovers the (thresholded) binary matrix. Both methods yield strong error bounds \u2014 if M \u2208R^n \\times n, the error is bounded as O(1-\u03c1) , where 1-\u03c1denotes the fraction of ones observed. This implies a sample complexity of O(n log n) ones to achieve a small error, when M is dense and n is large. We extend our analysis to the inductive matrix completion problem, where rows and columns of M have associated features. We develop efficient and scalable optimization procedures for both the proposed methods and demonstrate their effectiveness for link prediction (on real-world networks consisting of over 2 million nodes and 90 million links) and semi-supervised clustering tasks.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/hsiehb15.pdf",
        "supp": "",
        "pdf_size": 302393,
        "gs_citation": 195,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16473991471831775504&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, The University of Texas, Austin, TX 78721, USA; Department of Computer Science, The University of Texas, Austin, TX 78721, USA; Department of Computer Science, The University of Texas, Austin, TX 78721, USA",
        "aff_domain": "CS.UTEXAS.EDU;CS.UTEXAS.EDU;CS.UTEXAS.EDU",
        "email": "CS.UTEXAS.EDU;CS.UTEXAS.EDU;CS.UTEXAS.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9570b6ae04",
        "title": "Paired-Dual Learning for Fast Training of Latent Variable Hinge-Loss MRFs",
        "site": "https://proceedings.mlr.press/v37/bach15.html",
        "author": "Stephen Bach; Bert Huang; Jordan Boyd-Graber; Lise Getoor",
        "abstract": "Latent variables allow probabilistic graphical models to capture nuance and structure in important domains such as network science, natural language processing, and computer vision. Naive approaches to learning such complex models can be prohibitively expensive\u2014because they require repeated inferences to update beliefs about latent variables\u2014so lifting this restriction for useful classes of models is an important problem. Hinge-loss Markov random fields (HL-MRFs) are graphical models that allow highly scalable inference and learning in structured domains, in part by representing structured problems with continuous variables. However, this representation leads to challenges when learning with latent variables. We introduce paired-dual learning, a framework that greatly speeds up training by using tractable entropy surrogates and avoiding repeated inferences. Paired-dual learning optimizes an objective with a pair of dual inference problems. This allows fast, joint optimization of parameters and dual variables. We evaluate on social-group detection, trust prediction in social networks, and image reconstruction, finding that paired-dual learning trains models as accurate as those trained by traditional methods in much less time, often before traditional methods make even a single parameter update.",
        "bibtex": "@InProceedings{pmlr-v37-bach15,\n  title = \t {Paired-Dual Learning for Fast Training of Latent Variable Hinge-Loss MRFs},\n  author = \t {Bach, Stephen and Huang, Bert and Boyd-Graber, Jordan and Getoor, Lise},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {381--390},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/bach15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/bach15.html},\n  abstract = \t {Latent variables allow probabilistic graphical models to capture nuance and structure in important domains such as network science, natural language processing, and computer vision. Naive approaches to learning such complex models can be prohibitively expensive\u2014because they require repeated inferences to update beliefs about latent variables\u2014so lifting this restriction for useful classes of models is an important problem. Hinge-loss Markov random fields (HL-MRFs) are graphical models that allow highly scalable inference and learning in structured domains, in part by representing structured problems with continuous variables. However, this representation leads to challenges when learning with latent variables. We introduce paired-dual learning, a framework that greatly speeds up training by using tractable entropy surrogates and avoiding repeated inferences. Paired-dual learning optimizes an objective with a pair of dual inference problems. This allows fast, joint optimization of parameters and dual variables. We evaluate on social-group detection, trust prediction in social networks, and image reconstruction, finding that paired-dual learning trains models as accurate as those trained by traditional methods in much less time, often before traditional methods make even a single parameter update.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/bach15.pdf",
        "supp": "",
        "pdf_size": 364998,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15299487160416424526&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "8428dc477f",
        "title": "PeakSeg: constrained optimal segmentation and supervised penalty learning for peak detection in count data",
        "site": "https://proceedings.mlr.press/v37/hocking15.html",
        "author": "Toby Hocking; Guillem Rigaill; Guillaume Bourque",
        "abstract": "Peak detection is a central problem in genomic data analysis, and current algorithms for this task are unsupervised and mostly effective for a single data type and pattern (e.g. H3K4me3 data with a sharp peak pattern). We propose PeakSeg, a new constrained maximum likelihood segmentation model for peak detection with an efficient inference algorithm: constrained dynamic programming. We investigate unsupervised and supervised learning of penalties for the critical model selection problem. We show that the supervised method has state-of-the-art peak detection across all data sets in a benchmark that includes both sharp H3K4me3 and broad H3K36me3 patterns.",
        "bibtex": "@InProceedings{pmlr-v37-hocking15,\n  title = \t {PeakSeg: constrained optimal segmentation and supervised penalty learning for peak detection in count data},\n  author = \t {Hocking, Toby and Rigaill, Guillem and Bourque, Guillaume},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {324--332},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/hocking15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/hocking15.html},\n  abstract = \t {Peak detection is a central problem in genomic data analysis, and current algorithms for this task are unsupervised and mostly effective for a single data type and pattern (e.g. H3K4me3 data with a sharp peak pattern). We propose PeakSeg, a new constrained maximum likelihood segmentation model for peak detection with an efficient inference algorithm: constrained dynamic programming. We investigate unsupervised and supervised learning of penalties for the critical model selection problem. We show that the supervised method has state-of-the-art peak detection across all data sets in a benchmark that includes both sharp H3K4me3 and broad H3K36me3 patterns.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/hocking15.pdf",
        "supp": "",
        "pdf_size": 663794,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6350294148276165118&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Human Genetics, McGill University, Montr \u00b4eal, Qu \u00b4ebec, Canada; Institute of Plant Sciences Paris-Saclay, UMR 9213/UMR1403, CNRS, INRA, Universit \u00b4e Paris-Sud, Universit \u00b4e d\u2019Evry, Universit \u00b4e Paris-Diderot, Sorbonne Paris-Cit \u00b4e, Orsay, France; Department of Human Genetics, McGill University, Montr \u00b4eal, Qu \u00b4ebec, Canada",
        "aff_domain": "mail.mcgill.ca;evry.inra.fr;mcgill.ca",
        "email": "mail.mcgill.ca;evry.inra.fr;mcgill.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "McGill University;Institute of Plant Sciences Paris-Saclay",
        "aff_unique_dep": "Department of Human Genetics;UMR 9213/UMR1403, CNRS, INRA",
        "aff_unique_url": "https://www.mcgill.ca;https://www.i2ps.eu",
        "aff_unique_abbr": "McGill;I2PS",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Montr\u00e9al;Orsay",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Canada;France"
    },
    {
        "id": "ddec421c03",
        "title": "Phrase-based Image Captioning",
        "site": "https://proceedings.mlr.press/v37/lebret15.html",
        "author": "Remi Lebret; Pedro Pinheiro; Ronan Collobert",
        "abstract": "Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely linear model to embed an image representation (generated from a previously trained Convolutional Neural Network) into a multimodal space that is common to the images and the phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on the sentence description statistics, we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-the-art models, achieves comparable results in two popular datasets for the task: Flickr30k and the recently proposed Microsoft COCO.",
        "bibtex": "@InProceedings{pmlr-v37-lebret15,\n  title = \t {Phrase-based Image Captioning},\n  author = \t {Lebret, Remi and Pinheiro, Pedro and Collobert, Ronan},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2085--2094},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/lebret15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/lebret15.html},\n  abstract = \t {Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely linear model to embed an image representation (generated from a previously trained Convolutional Neural Network) into a multimodal space that is common to the images and the phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on the sentence description statistics, we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-the-art models, achieves comparable results in two popular datasets for the task: Flickr30k and the recently proposed Microsoft COCO.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/lebret15.pdf",
        "supp": "",
        "pdf_size": 815439,
        "gs_citation": 160,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9868419265152877728&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Idiap Research Institute, Martigny, Switzerland + \u00b4Ecole Polytechnique F \u00b4ed\u00b4erale de Lausanne (EPFL), Lausanne, Switzerland; Idiap Research Institute, Martigny, Switzerland + \u00b4Ecole Polytechnique F \u00b4ed\u00b4erale de Lausanne (EPFL), Lausanne, Switzerland; Facebook AI Research, Menlo Park, CA, USA",
        "aff_domain": "lebret.ch;opinheiro.com;collobert.com",
        "email": "lebret.ch;opinheiro.com;collobert.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;2",
        "aff_unique_norm": "Idiap Research Institute;EPFL;Meta",
        "aff_unique_dep": ";;Facebook AI Research",
        "aff_unique_url": "https://www.idiap.ch;https://www.epfl.ch;https://research.facebook.com",
        "aff_unique_abbr": "Idiap;EPFL;FAIR",
        "aff_campus_unique_index": "0+1;0+1;2",
        "aff_campus_unique": "Martigny;Lausanne;Menlo Park",
        "aff_country_unique_index": "0+0;0+0;1",
        "aff_country_unique": "Switzerland;United States"
    },
    {
        "id": "4f671e8c81",
        "title": "Predictive Entropy Search for Bayesian Optimization with Unknown Constraints",
        "site": "https://proceedings.mlr.press/v37/hernandez-lobatob15.html",
        "author": "Jose Miguel Hernandez-Lobato; Michael Gelbart; Matthew Hoffman; Ryan Adams; Zoubin Ghahramani",
        "abstract": "Unknown constraints arise in many types of expensive black-box optimization problems. Several methods have been proposed recently for performing Bayesian optimization with constraints, based on the expected improvement (EI) heuristic. However, EI can lead to pathologies when used with constraints. For example, in the case of decoupled constraints\u2014i.e., when one can independently evaluate the objective or the constraints\u2014EI can encounter a pathology that prevents exploration. Additionally, computing EI requires a current best solution, which may not exist if none of the data collected so far satisfy the constraints. By contrast, information-based approaches do not suffer from these failure modes. In this paper, we present a new information-based method called Predictive Entropy Search with Constraints (PESC). We analyze the performance of PESC and show that it compares favorably to EI-based approaches on synthetic and benchmark problems, as well as several real-world examples. We demonstrate that PESC is an effective algorithm that provides a promising direction towards a unified solution for constrained Bayesian optimization.",
        "bibtex": "@InProceedings{pmlr-v37-hernandez-lobatob15,\n  title = \t {Predictive Entropy Search for Bayesian Optimization with Unknown Constraints},\n  author = \t {Hernandez-Lobato, Jose Miguel and Gelbart, Michael and Hoffman, Matthew and Adams, Ryan and Ghahramani, Zoubin},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1699--1707},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/hernandez-lobatob15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/hernandez-lobatob15.html},\n  abstract = \t {Unknown constraints arise in many types of expensive black-box optimization problems. Several methods have been proposed recently for performing Bayesian optimization with constraints, based on the expected improvement (EI) heuristic. However, EI can lead to pathologies when used with constraints. For example, in the case of decoupled constraints\u2014i.e., when one can independently evaluate the objective or the constraints\u2014EI can encounter a pathology that prevents exploration. Additionally, computing EI requires a current best solution, which may not exist if none of the data collected so far satisfy the constraints. By contrast, information-based approaches do not suffer from these failure modes. In this paper, we present a new information-based method called Predictive Entropy Search with Constraints (PESC). We analyze the performance of PESC and show that it compares favorably to EI-based approaches on synthetic and benchmark problems, as well as several real-world examples. We demonstrate that PESC is an effective algorithm that provides a promising direction towards a unified solution for constrained Bayesian optimization.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/hernandez-lobatob15.pdf",
        "supp": "",
        "pdf_size": 906915,
        "gs_citation": 202,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3796069130775453801&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Harvard University, Cambridge, MA 02138 USA; Harvard University, Cambridge, MA 02138 USA; University of Cambridge, Cambridge, CB2 1PZ, UK; Harvard University, Cambridge, MA 02138 USA; University of Cambridge, Cambridge, CB2 1PZ, UK",
        "aff_domain": "SEAS.HARVARD.EDU;SEAS.HARVARD.EDU;CAM.AC.UK;SEAS.HARVARD.EDU;ENG.CAM.AC.UK",
        "email": "SEAS.HARVARD.EDU;SEAS.HARVARD.EDU;CAM.AC.UK;SEAS.HARVARD.EDU;ENG.CAM.AC.UK",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0;1",
        "aff_unique_norm": "Harvard University;University of Cambridge",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.harvard.edu;https://www.cam.ac.uk",
        "aff_unique_abbr": "Harvard;Cambridge",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;1;0;1",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "4be768a8c4",
        "title": "Preference Completion: Large-scale Collaborative Ranking from Pairwise Comparisons",
        "site": "https://proceedings.mlr.press/v37/park15.html",
        "author": "Dohyung Park; Joe Neeman; Jin Zhang; Sujay Sanghavi; Inderjit Dhillon",
        "abstract": "In this paper we consider the collaborative ranking setting: a pool of users each provides a set of pairwise preferences over a small subset of the set of d possible items; from these we need to predict each user\u2019s preferences for items s/he has not yet seen. We do so via fitting a rank r score matrix to the pairwise data, and provide two main contributions: (a) We show that an algorithm based on convex optimization provides good generalization guarantees once each user provides as few as O(r \\log^2 d) pairwise comparisons \u2014 essentially matching the sample complexity required in the related matrix completion setting (which uses actual numerical as opposed to pairwise information), and also matching a lower bound we establish here. (b) We develop a large-scale non-convex implementation, which we call AltSVM, which trains a factored form of the matrix via alternating minimization (which we show reduces to alternating SVM problems), and scales and parallelizes very well to large problem settings. It also outperforms common baselines on many moderately large popular collaborative filtering datasets in both NDCG and other measures of ranking performance.",
        "bibtex": "@InProceedings{pmlr-v37-park15,\n  title = \t {Preference Completion: Large-scale Collaborative Ranking from Pairwise Comparisons},\n  author = \t {Park, Dohyung and Neeman, Joe and Zhang, Jin and Sanghavi, Sujay and Dhillon, Inderjit},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1907--1916},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/park15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/park15.html},\n  abstract = \t {In this paper we consider the collaborative ranking setting: a pool of users each provides a set of pairwise preferences over a small subset of the set of d possible items; from these we need to predict each user\u2019s preferences for items s/he has not yet seen. We do so via fitting a rank r score matrix to the pairwise data, and provide two main contributions: (a) We show that an algorithm based on convex optimization provides good generalization guarantees once each user provides as few as O(r \\log^2 d) pairwise comparisons \u2014 essentially matching the sample complexity required in the related matrix completion setting (which uses actual numerical as opposed to pairwise information), and also matching a lower bound we establish here. (b) We develop a large-scale non-convex implementation, which we call AltSVM, which trains a factored form of the matrix via alternating minimization (which we show reduces to alternating SVM problems), and scales and parallelizes very well to large problem settings. It also outperforms common baselines on many moderately large popular collaborative filtering datasets in both NDCG and other measures of ranking performance.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/park15.pdf",
        "supp": "",
        "pdf_size": 348857,
        "gs_citation": 99,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15179745083580684378&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "The University of Texas at Austin; The University of Texas at Austin; The University of Texas at Austin; The University of Texas at Austin; The University of Texas at Austin",
        "aff_domain": "UTEXAS.EDU;GMAIL.COM;UTEXAS.EDU;MAIL.UTEXAS.EDU;CS.UTEXAS.EDU",
        "email": "UTEXAS.EDU;GMAIL.COM;UTEXAS.EDU;MAIL.UTEXAS.EDU;CS.UTEXAS.EDU",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a42b2ad366",
        "title": "Privacy for Free: Posterior Sampling and Stochastic Gradient Monte Carlo",
        "site": "https://proceedings.mlr.press/v37/wangg15.html",
        "author": "Yu-Xiang Wang; Stephen Fienberg; Alex Smola",
        "abstract": "We consider the problem of Bayesian learning on sensitive datasets and present two simple but somewhat surprising results that connect Bayesian learning to \u201cdifferential privacy\u201d, a cryptographic approach to protect individual-level privacy while permitting database-level utility. Specifically, we show that under standard assumptions, getting one sample from a posterior distribution is differentially private \u201cfor free\u201d; and this sample as a statistical estimator is often consistent, near optimal, and computationally tractable. Similarly but separately, we show that a recent line of work that use stochastic gradient for Hybrid Monte Carlo (HMC) sampling also preserve differentially privacy with minor or no modifications of the algorithmic procedure at all, these observations lead to an \u201canytime\u201d algorithm for Bayesian learning under privacy constraint. We demonstrate that it performs much better than the state-of-the-art differential private methods on synthetic and real datasets.",
        "bibtex": "@InProceedings{pmlr-v37-wangg15,\n  title = \t {Privacy for Free: Posterior Sampling and Stochastic Gradient Monte Carlo},\n  author = \t {Wang, Yu-Xiang and Fienberg, Stephen and Smola, Alex},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2493--2502},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/wangg15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/wangg15.html},\n  abstract = \t {We consider the problem of Bayesian learning on sensitive datasets and present two simple but somewhat surprising results that connect Bayesian learning to \u201cdifferential privacy\u201d, a cryptographic approach to protect individual-level privacy while permitting database-level utility. Specifically, we show that under standard assumptions, getting one sample from a posterior distribution is differentially private \u201cfor free\u201d; and this sample as a statistical estimator is often consistent, near optimal, and computationally tractable. Similarly but separately, we show that a recent line of work that use stochastic gradient for Hybrid Monte Carlo (HMC) sampling also preserve differentially privacy with minor or no modifications of the algorithmic procedure at all, these observations lead to an \u201canytime\u201d algorithm for Bayesian learning under privacy constraint. We demonstrate that it performs much better than the state-of-the-art differential private methods on synthetic and real datasets.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/wangg15.pdf",
        "supp": "",
        "pdf_size": 567776,
        "gs_citation": 304,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15663411176409337089&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15213, USA+Marianas Labs Inc., Pittsburgh, PA 15213, USA; Department of Statistics, Carnegie Mellon University, Pittsburgh, PA 15213, USA+Marianas Labs Inc., Pittsburgh, PA 15213, USA; Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15213, USA+Marianas Labs Inc., Pittsburgh, PA 15213, USA",
        "aff_domain": "CS.CMU.EDU;STAT.CMU.EDU;SMOLA.ORG",
        "email": "CS.CMU.EDU;STAT.CMU.EDU;SMOLA.ORG",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "Carnegie Mellon University;Marianas Labs Inc.",
        "aff_unique_dep": "Machine Learning Department;",
        "aff_unique_url": "https://www.cmu.edu;",
        "aff_unique_abbr": "CMU;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Pittsburgh;",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ac97cc6f8e",
        "title": "Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks",
        "site": "https://proceedings.mlr.press/v37/hernandez-lobatoc15.html",
        "author": "Jose Miguel Hernandez-Lobato; Ryan Adams",
        "abstract": "Large multilayer neural networks trained with backpropagation have recently achieved state-of-the-art results in a wide range of problems. However, using backprop for neural net learning still has some disadvantages, e.g., having to tune a large number of hyperparameters to the data, lack of calibrated probabilistic predictions, and a tendency to overfit the training data. In principle, the Bayesian approach to learning neural networks does not have these problems. However, existing Bayesian techniques lack scalability to large dataset and network sizes. In this work we present a novel scalable method for learning Bayesian neural networks, called probabilistic backpropagation (PBP). Similar to classical backpropagation, PBP works by computing a forward propagation of probabilities through the network and then doing a backward computation of gradients. A series of experiments on ten real-world datasets show that PBP is significantly faster than other techniques, while offering competitive predictive abilities. Our experiments also show that PBP provides accurate estimates of the posterior variance on the network weights.",
        "bibtex": "@InProceedings{pmlr-v37-hernandez-lobatoc15,\n  title = \t {Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks},\n  author = \t {Hernandez-Lobato, Jose Miguel and Adams, Ryan},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1861--1869},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/hernandez-lobatoc15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/hernandez-lobatoc15.html},\n  abstract = \t {Large multilayer neural networks trained with backpropagation have recently achieved state-of-the-art results in a wide range of problems. However, using backprop for neural net learning still has some disadvantages, e.g., having to tune a large number of hyperparameters to the data, lack of calibrated probabilistic predictions, and a tendency to overfit the training data. In principle, the Bayesian approach to learning neural networks does not have these problems. However, existing Bayesian techniques lack scalability to large dataset and network sizes. In this work we present a novel scalable method for learning Bayesian neural networks, called probabilistic backpropagation (PBP). Similar to classical backpropagation, PBP works by computing a forward propagation of probabilities through the network and then doing a backward computation of gradients. A series of experiments on ten real-world datasets show that PBP is significantly faster than other techniques, while offering competitive predictive abilities. Our experiments also show that PBP provides accurate estimates of the posterior variance on the network weights.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/hernandez-lobatoc15.pdf",
        "supp": "",
        "pdf_size": 753355,
        "gs_citation": 1265,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7631378507206910182&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "School of Engineering and Applied Sciences, Harvard University, Cambridge, MA 02138 USA; School of Engineering and Applied Sciences, Harvard University, Cambridge, MA 02138 USA",
        "aff_domain": "SEAS.HARVARD.EDU;SEAS.HARVARD.EDU",
        "email": "SEAS.HARVARD.EDU;SEAS.HARVARD.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Harvard University",
        "aff_unique_dep": "School of Engineering and Applied Sciences",
        "aff_unique_url": "https://www.harvard.edu",
        "aff_unique_abbr": "Harvard",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "986ba07483",
        "title": "Proteins, Particles, and Pseudo-Max-Marginals: A Submodular Approach",
        "site": "https://proceedings.mlr.press/v37/pacheco15.html",
        "author": "Jason Pacheco; Erik Sudderth",
        "abstract": "Variants of max-product (MP) belief propagation effectively find modes of many complex graphical models, but are limited to discrete distributions. Diverse particle max-product (D-PMP) robustly approximates max-product updates in continuous MRFs using stochastically sampled particles, but previous work was specialized to tree-structured models. Motivated by the challenging problem of protein side chain prediction, we extend D-PMP in several key ways to create a generic MAP inference algorithm for loopy models. We define a modified diverse particle selection objective that is provably submodular, leading to an efficient greedy algorithm with rigorous optimality guarantees, and corresponding max-marginal error bounds. We further incorporate tree-reweighted variants of the MP algorithm to allow provable verification of global MAP recovery in many models. Our general-purpose Matlab library is applicable to a wide range of pairwise graphical models, and we validate our approach using optical flow benchmarks. We further demonstrate superior side chain prediction accuracy compared to baseline algorithms from the state-of-the-art Rosetta package.",
        "bibtex": "@InProceedings{pmlr-v37-pacheco15,\n  title = \t {Proteins, Particles, and Pseudo-Max-Marginals: A Submodular Approach},\n  author = \t {Pacheco, Jason and Sudderth, Erik},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2200--2208},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/pacheco15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/pacheco15.html},\n  abstract = \t {Variants of max-product (MP) belief propagation effectively find modes of many complex graphical models, but are limited to discrete distributions. Diverse particle max-product (D-PMP) robustly approximates max-product updates in continuous MRFs using stochastically sampled particles, but previous work was specialized to tree-structured models. Motivated by the challenging problem of protein side chain prediction, we extend D-PMP in several key ways to create a generic MAP inference algorithm for loopy models. We define a modified diverse particle selection objective that is provably submodular, leading to an efficient greedy algorithm with rigorous optimality guarantees, and corresponding max-marginal error bounds. We further incorporate tree-reweighted variants of the MP algorithm to allow provable verification of global MAP recovery in many models. Our general-purpose Matlab library is applicable to a wide range of pairwise graphical models, and we validate our approach using optical flow benchmarks. We further demonstrate superior side chain prediction accuracy compared to baseline algorithms from the state-of-the-art Rosetta package.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/pacheco15.pdf",
        "supp": "",
        "pdf_size": 4725465,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14856910350109115522&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "Department of Computer Science, Brown University, Providence, RI 02912, USA; Department of Computer Science, Brown University, Providence, RI 02912, USA",
        "aff_domain": "CS.BROWN.EDU;CS.BROWN.EDU",
        "email": "CS.BROWN.EDU;CS.BROWN.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Brown University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.brown.edu",
        "aff_unique_abbr": "Brown",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Providence",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ceef5a921d",
        "title": "Pushing the Limits of Affine Rank Minimization by Adapting Probabilistic PCA",
        "site": "https://proceedings.mlr.press/v37/xin15.html",
        "author": "Bo Xin; David Wipf",
        "abstract": "Many applications require recovering a matrix of minimal rank within an affine constraint set, with matrix completion a notable special case. Because the problem is NP-hard in general, it is common to replace the matrix rank with the nuclear norm, which acts as a convenient convex surrogate. While elegant theoretical conditions elucidate when this replacement is likely to be successful, they are highly restrictive and convex algorithms fail when the ambient rank is too high or when the constraint set is poorly structured. Non-convex alternatives fare somewhat better when carefully tuned; however, convergence to locally optimal solutions remains a continuing source of failure. Against this backdrop we derive a deceptively simple and parameter-free probabilistic PCA-like algorithm that is capable, over a wide battery of empirical tests, of successful recovery even at the theoretical limit where the number of measurements equals the degrees of freedom in the unknown low-rank matrix. Somewhat surprisingly, this is possible even when the affine constraint set is highly ill-conditioned. While proving general recovery guarantees remains evasive for non-convex algorithms, Bayesian-inspired or otherwise, we nonetheless show conditions whereby the underlying cost function has a unique stationary point located at the global optimum; no existing cost function we are aware of satisfies this property. The algorithm has also been successfully deployed on a computer vision application involving image rectification and a standard collaborative filtering benchmark.",
        "bibtex": "@InProceedings{pmlr-v37-xin15,\n  title = \t {Pushing the Limits of Affine Rank Minimization by Adapting Probabilistic PCA},\n  author = \t {Xin, Bo and Wipf, David},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {419--427},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/xin15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/xin15.html},\n  abstract = \t {Many applications require recovering a matrix of minimal rank within an affine constraint set, with matrix completion a notable special case. Because the problem is NP-hard in general, it is common to replace the matrix rank with the nuclear norm, which acts as a convenient convex surrogate. While elegant theoretical conditions elucidate when this replacement is likely to be successful, they are highly restrictive and convex algorithms fail when the ambient rank is too high or when the constraint set is poorly structured. Non-convex alternatives fare somewhat better when carefully tuned; however, convergence to locally optimal solutions remains a continuing source of failure. Against this backdrop we derive a deceptively simple and parameter-free probabilistic PCA-like algorithm that is capable, over a wide battery of empirical tests, of successful recovery even at the theoretical limit where the number of measurements equals the degrees of freedom in the unknown low-rank matrix. Somewhat surprisingly, this is possible even when the affine constraint set is highly ill-conditioned. While proving general recovery guarantees remains evasive for non-convex algorithms, Bayesian-inspired or otherwise, we nonetheless show conditions whereby the underlying cost function has a unique stationary point located at the global optimum; no existing cost function we are aware of satisfies this property. The algorithm has also been successfully deployed on a computer vision application involving image rectification and a standard collaborative filtering benchmark.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/xin15.pdf",
        "supp": "",
        "pdf_size": 382955,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7202204415451103&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 8,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "be32ae7a9e",
        "title": "Qualitative Multi-Armed Bandits: A Quantile-Based Approach",
        "site": "https://proceedings.mlr.press/v37/szorenyi15.html",
        "author": "Balazs Szorenyi; Robert Busa-Fekete; Paul Weng; Eyke H\u00fcllermeier",
        "abstract": "We formalize and study the multi-armed bandit (MAB) problem in a generalized stochastic setting, in which rewards are not assumed to be numerical. Instead, rewards are measured on a qualitative scale that allows for comparison but invalidates arithmetic operations such as averaging. Correspondingly, instead of characterizing an arm in terms of the mean of the underlying distribution, we opt for using a quantile of that distribution as a representative value. We address the problem of quantile-based online learning both for the case of a finite (pure exploration) and infinite time horizon (cumulative regret minimization). For both cases, we propose suitable algorithms and analyze their properties. These properties are also illustrated by means of first experimental studies.",
        "bibtex": "@InProceedings{pmlr-v37-szorenyi15,\n  title = \t {Qualitative Multi-Armed Bandits: A Quantile-Based Approach},\n  author = \t {Szorenyi, Balazs and Busa-Fekete, Robert and Weng, Paul and H\u00fcllermeier, Eyke},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1660--1668},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/szorenyi15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/szorenyi15.html},\n  abstract = \t {We formalize and study the multi-armed bandit (MAB) problem in a generalized stochastic setting, in which rewards are not assumed to be numerical. Instead, rewards are measured on a qualitative scale that allows for comparison but invalidates arithmetic operations such as averaging. Correspondingly, instead of characterizing an arm in terms of the mean of the underlying distribution, we opt for using a quantile of that distribution as a representative value. We address the problem of quantile-based online learning both for the case of a finite (pure exploration) and infinite time horizon (cumulative regret minimization). For both cases, we propose suitable algorithms and analyze their properties. These properties are also illustrated by means of first experimental studies.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/szorenyi15.pdf",
        "supp": "",
        "pdf_size": 449391,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6460722531347221287&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "INRIA Lille - Nord Europe, SequeL project, 40 avenue Halley, 59650 Villeneuve d\u2019Ascq, France + MTA-SZTE Research Group on Arti\ufb01cial Intelligence, Tisza Lajos krt. 103., H-6720 Szeged, Hungary + Department of Electrical Engineering, The Technion - Israel Institute of Technology, Haifa, Israel 32000; Department of Computer Science, University of Paderborn, Warburger Str. 100, 33098 Paderborn, Germany; SYSU-CMU Joint Institute of Engineering, 132 East Waihuan Road, Guangzhou, 510006, China + SYSU-CMU Shunde International Joint Research Institute, 9 Eastern Nanguo Road, Shunde, 528300, China; Department of Computer Science, University of Paderborn, Warburger Str. 100, 33098 Paderborn, Germany",
        "aff_domain": "INF.U-SZEGED.HU;UPB.DE;CMU.EDU;UPB.DE",
        "email": "INF.U-SZEGED.HU;UPB.DE;CMU.EDU;UPB.DE",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1+2;3;4+5;3",
        "aff_unique_norm": "INRIA Lille - Nord Europe;Hungarian Academy of Sciences - University of Szeged;Technion - Israel Institute of Technology;University of Paderborn;SYSU-CMU Joint Institute of Engineering;SYSU-CMU Shunde International Joint Research Institute",
        "aff_unique_dep": "SequeL project;Research Group on Arti\ufb01cial Intelligence;Department of Electrical Engineering;Department of Computer Science;;",
        "aff_unique_url": "https://www.inria.fr/lille-nord-europe;https://www.mta.hu/english, https://www.szte.hu/english;https://www.technion.ac.il;https://www.uni-paderborn.de;;",
        "aff_unique_abbr": "INRIA;MTA-SZTE AI RG;Technion;UPB;;",
        "aff_campus_unique_index": "0+1+2;3;4+5;3",
        "aff_campus_unique": "Lille;Szeged;Haifa;Paderborn;Guangzhou;Shunde",
        "aff_country_unique_index": "0+1+2;3;4+4;3",
        "aff_country_unique": "France;Hungary;Israel;Germany;China"
    },
    {
        "id": "55f1071ac5",
        "title": "Rademacher Observations, Private Data, and Boosting",
        "site": "https://proceedings.mlr.press/v37/nock15.html",
        "author": "Richard Nock; Giorgio Patrini; Arik Friedman",
        "abstract": "The minimization of the logistic loss is a popular approach to batch supervised learning. Our paper starts from the surprising observation that, when fitting linear classifiers, the minimization of the logistic loss is \\textitequivalent to the minimization of an exponential \\textitrado-loss computed (i) over transformed data that we call Rademacher observations (rados), and (ii) over the \\textitsame classifier as the one of the logistic loss. Thus, a classifier learnt from rados can be \\textitdirectly used to classify \\textitobservations. We provide a learning algorithm over rados with boosting-compliant convergence rates on the \\textitlogistic loss (computed over examples). Experiments on domains with up to millions of examples, backed up by theoretical arguments, display that learning over a small set of random rados can challenge the state of the art that learns over the \\textitcomplete set of examples. We show that rados comply with various privacy requirements that make them good candidates for machine learning in a privacy framework. We give several algebraic, geometric and computational hardness results on reconstructing examples from rados. We also show how it is possible to craft, and efficiently learn from, rados in a differential privacy framework. Tests reveal that learning from differentially private rados brings non-trivial privacy vs accuracy tradeoffs.",
        "bibtex": "@InProceedings{pmlr-v37-nock15,\n  title = \t {Rademacher Observations, Private Data, and Boosting},\n  author = \t {Nock, Richard and Patrini, Giorgio and Friedman, Arik},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {948--956},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/nock15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/nock15.html},\n  abstract = \t {The minimization of the logistic loss is a popular approach to batch supervised learning. Our paper starts from the surprising observation that, when fitting linear classifiers, the minimization of the logistic loss is \\textitequivalent to the minimization of an exponential \\textitrado-loss computed (i) over transformed data that we call Rademacher observations (rados), and (ii) over the \\textitsame classifier as the one of the logistic loss. Thus, a classifier learnt from rados can be \\textitdirectly used to classify \\textitobservations. We provide a learning algorithm over rados with boosting-compliant convergence rates on the \\textitlogistic loss (computed over examples). Experiments on domains with up to millions of examples, backed up by theoretical arguments, display that learning over a small set of random rados can challenge the state of the art that learns over the \\textitcomplete set of examples. We show that rados comply with various privacy requirements that make them good candidates for machine learning in a privacy framework. We give several algebraic, geometric and computational hardness results on reconstructing examples from rados. We also show how it is possible to craft, and efficiently learn from, rados in a differential privacy framework. Tests reveal that learning from differentially private rados brings non-trivial privacy vs accuracy tradeoffs.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/nock15.pdf",
        "supp": "",
        "pdf_size": 250413,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11830087121326213228&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "NICTA\u2020The Australian National University; NICTA\u2020The Australian National University; NICTA\u2021The University of New South Wales",
        "aff_domain": "NICTA.COM.AU;NICTA.COM.AU;NICTA.COM.AU",
        "email": "NICTA.COM.AU;NICTA.COM.AU;NICTA.COM.AU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Australian National University;University of New South Wales",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.anu.edu.au;https://www.unsw.edu.au",
        "aff_unique_abbr": "ANU;UNSW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "e0ba19bab1",
        "title": "Random Coordinate Descent Methods for Minimizing Decomposable Submodular Functions",
        "site": "https://proceedings.mlr.press/v37/ene15.html",
        "author": "Alina Ene; Huy Nguyen",
        "abstract": "Submodular function minimization is a fundamental optimization problem that arises in several applications in machine learning and computer vision. The problem is known to be solvable in polynomial time, but general purpose algorithms have high running times and are unsuitable for large-scale problems. Recent work have used convex optimization techniques to obtain very practical algorithms for minimizing functions that are sums of \u201csimple\u201d functions. In this paper, we use random coordinate descent methods to obtain algorithms with faster \\emphlinear convergence rates and cheaper iteration costs. Compared to alternating projection methods, our algorithms do not rely on full-dimensional vector operations and they converge in significantly fewer iterations.",
        "bibtex": "@InProceedings{pmlr-v37-ene15,\n  title = \t {Random Coordinate Descent Methods for Minimizing Decomposable Submodular Functions},\n  author = \t {Ene, Alina and Nguyen, Huy},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {787--795},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/ene15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/ene15.html},\n  abstract = \t {Submodular function minimization is a fundamental optimization problem that arises in several applications in machine learning and computer vision. The problem is known to be solvable in polynomial time, but general purpose algorithms have high running times and are unsuitable for large-scale problems. Recent work have used convex optimization techniques to obtain very practical algorithms for minimizing functions that are sums of \u201csimple\u201d functions. In this paper, we use random coordinate descent methods to obtain algorithms with faster \\emphlinear convergence rates and cheaper iteration costs. Compared to alternating projection methods, our algorithms do not rely on full-dimensional vector operations and they converge in significantly fewer iterations.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/ene15.pdf",
        "supp": "",
        "pdf_size": 1103323,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7198881155984190342&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Computer Science and DIMAP, University of Warwick; Simons Institute, University of California, Berkeley",
        "aff_domain": "DCS.WARWICK.AC.UK;CS.PRINCETON.EDU",
        "email": "DCS.WARWICK.AC.UK;CS.PRINCETON.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Warwick;University of California, Berkeley",
        "aff_unique_dep": "Department of Computer Science and DIMAP;Simons Institute",
        "aff_unique_url": "https://www.warwick.ac.uk;https://simons.berkeley.edu",
        "aff_unique_abbr": "Warwick;UC Berkeley",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "ffc521d03c",
        "title": "Ranking from Stochastic Pairwise Preferences: Recovering Condorcet Winners and Tournament Solution Sets at the Top",
        "site": "https://proceedings.mlr.press/v37/rajkumar15.html",
        "author": "Arun Rajkumar; Suprovat Ghoshal; Lek-Heng Lim; Shivani Agarwal",
        "abstract": "We consider the problem of ranking n items from stochastically sampled pairwise preferences. It was shown recently that when the underlying pairwise preferences are acyclic, several algorithms including the Rank Centrality algorithm, the Matrix Borda algorithm, and the SVM-RankAggregation algorithm succeed in recovering a ranking that minimizes a global pairwise disagreement error (Rajkumar and Agarwal, 2014). In this paper, we consider settings where pairwise preferences can contain cycles. In such settings, one may still like to be able to recover \u2018good\u2019 items at the top of the ranking. For example, if a Condorcet winner exists that beats every other item, it is natural to ask that this be ranked at the top. More generally, several tournament solution concepts such as the top cycle, Copeland set, Markov set and others have been proposed in the social choice literature for choosing a set of winners in the presence of cycles. We show that existing algorithms can fail to perform well in terms of ranking Condorcet winners and various natural tournament solution sets at the top. We then give alternative ranking algorithms that provably rank Condorcet winners, top cycles, and other tournament solution sets of interest at the top. In all cases, we give finite sample complexity bounds for our algorithms to recover such winners. As a by-product of our analysis, we also obtain an improved sample complexity bound for the Rank Centrality algorithm to recover an optimal ranking under a Bradley-Terry-Luce (BTL) condition, which answers an open question of Rajkumar and Agarwal (2014).",
        "bibtex": "@InProceedings{pmlr-v37-rajkumar15,\n  title = \t {Ranking from Stochastic Pairwise Preferences: Recovering Condorcet Winners and Tournament Solution Sets at the Top},\n  author = \t {Rajkumar, Arun and Ghoshal, Suprovat and Lim, Lek-Heng and Agarwal, Shivani},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {665--673},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/rajkumar15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/rajkumar15.html},\n  abstract = \t {We consider the problem of ranking n items from stochastically sampled pairwise preferences. It was shown recently that when the underlying pairwise preferences are acyclic, several algorithms including the Rank Centrality algorithm, the Matrix Borda algorithm, and the SVM-RankAggregation algorithm succeed in recovering a ranking that minimizes a global pairwise disagreement error (Rajkumar and Agarwal, 2014). In this paper, we consider settings where pairwise preferences can contain cycles. In such settings, one may still like to be able to recover \u2018good\u2019 items at the top of the ranking. For example, if a Condorcet winner exists that beats every other item, it is natural to ask that this be ranked at the top. More generally, several tournament solution concepts such as the top cycle, Copeland set, Markov set and others have been proposed in the social choice literature for choosing a set of winners in the presence of cycles. We show that existing algorithms can fail to perform well in terms of ranking Condorcet winners and various natural tournament solution sets at the top. We then give alternative ranking algorithms that provably rank Condorcet winners, top cycles, and other tournament solution sets of interest at the top. In all cases, we give finite sample complexity bounds for our algorithms to recover such winners. As a by-product of our analysis, we also obtain an improved sample complexity bound for the Rank Centrality algorithm to recover an optimal ranking under a Bradley-Terry-Luce (BTL) condition, which answers an open question of Rajkumar and Agarwal (2014).}\n}",
        "pdf": "http://proceedings.mlr.press/v37/rajkumar15.pdf",
        "supp": "",
        "pdf_size": 1129928,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12455188726914437081&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Indian Institute of Science; Indian Institute of Science; University of Chicago; Indian Institute of Science",
        "aff_domain": "CSA.IISC.ERNET.IN;CSA.IISC.ERNET.IN;GALTON.UCHICAGO.EDU;CSA.IISC.ERNET.IN",
        "email": "CSA.IISC.ERNET.IN;CSA.IISC.ERNET.IN;GALTON.UCHICAGO.EDU;CSA.IISC.ERNET.IN",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Indian Institute of Science;University of Chicago",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iisc.ac.in;https://www.uchicago.edu",
        "aff_unique_abbr": "IISc;UChicago",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "India;United States"
    },
    {
        "id": "cc5f5bfa8d",
        "title": "Rebuilding Factorized Information Criterion: Asymptotically Accurate Marginal Likelihood",
        "site": "https://proceedings.mlr.press/v37/hayashi15.html",
        "author": "Kohei Hayashi; Shin-ichi Maeda; Ryohei Fujimaki",
        "abstract": "Factorized information criterion (FIC) is a recently developed approximation technique for the marginal log-likelihood, which provides an automatic model selection framework for a few latent variable models (LVMs) with tractable inference algorithms. This paper reconsiders FIC and fills theoretical gaps of previous FIC studies. First, we reveal the core idea of FIC that allows generalization for a broader class of LVMs, including continuous LVMs, in contrast to previous FICs, which are applicable only to binary LVMs. Second, we investigate the model selection mechanism of the generalized FIC. Our analysis provides a formal justification of FIC as a model selection criterion for LVMs and also a systematic procedure for pruning redundant latent variables that have been removed heuristically in previous studies. Third, we provide an interpretation of FIC as a variational free energy and uncover previously-unknown their relationship. A demonstrative study on Bayesian principal component analysis is provided and numerical experiments support our theoretical results.",
        "bibtex": "@InProceedings{pmlr-v37-hayashi15,\n  title = \t {Rebuilding Factorized Information Criterion: Asymptotically Accurate Marginal Likelihood},\n  author = \t {Hayashi, Kohei and Maeda, Shin-ichi and Fujimaki, Ryohei},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1358--1366},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/hayashi15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/hayashi15.html},\n  abstract = \t {Factorized information criterion (FIC) is a recently developed approximation technique for the marginal log-likelihood, which provides an automatic model selection framework for a few latent variable models (LVMs) with tractable inference algorithms. This paper reconsiders FIC and fills theoretical gaps of previous FIC studies. First, we reveal the core idea of FIC that allows generalization for a broader class of LVMs, including continuous LVMs, in contrast to previous FICs, which are applicable only to binary LVMs. Second, we investigate the model selection mechanism of the generalized FIC. Our analysis provides a formal justification of FIC as a model selection criterion for LVMs and also a systematic procedure for pruning redundant latent variables that have been removed heuristically in previous studies. Third, we provide an interpretation of FIC as a variational free energy and uncover previously-unknown their relationship. A demonstrative study on Bayesian principal component analysis is provided and numerical experiments support our theoretical results.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/hayashi15.pdf",
        "supp": "",
        "pdf_size": 197836,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5250147362129301479&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Global Research Center for Big Data Mathematics, National Institute of Informatics + Kawarabayashi Large Graph Project, ERATO, JST; Graduate School of Informatics, Kyoto University; NEC Knowledge Discovery Research Laboratories",
        "aff_domain": "GMAIL.COM;SYS.I.KYOTO-U.AC.JP;NEC-LABS.COM",
        "email": "GMAIL.COM;SYS.I.KYOTO-U.AC.JP;NEC-LABS.COM",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;3",
        "aff_unique_norm": "National Institute of Informatics;Japan Science and Technology Agency;Kyoto University;NEC",
        "aff_unique_dep": "Global Research Center for Big Data Mathematics;Kawarabayashi Large Graph Project;Graduate School of Informatics;Knowledge Discovery Research Laboratories",
        "aff_unique_url": "https://www.nii.ac.jp;https://www.jst.go.jp;https://www.kyoto-u.ac.jp;https://www.nec.com",
        "aff_unique_abbr": "NII;JST;Kyoto U;NEC",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Kyoto",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "27cd9e1372",
        "title": "Reified Context Models",
        "site": "https://proceedings.mlr.press/v37/steinhardta15.html",
        "author": "Jacob Steinhardt; Percy Liang",
        "abstract": "A classic tension exists between exact inference in a simple model and approximate inference in a complex model. The latter offers expressivity and thus accuracy, but the former provides coverage of the space, an important property for confidence estimation and learning with indirect supervision. In this work, we introduce a new approach, reified context models, to reconcile this tension. Specifically, we let the choice of factors in a graphical model (the contexts) be random variables inside the model itself. In this sense, the contexts are reified and can be chosen in a data-dependent way. Empirically, we show that our approach obtains expressivity and coverage on three sequence modeling tasks.",
        "bibtex": "@InProceedings{pmlr-v37-steinhardta15,\n  title = \t {Reified Context Models},\n  author = \t {Steinhardt, Jacob and Liang, Percy},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1043--1052},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/steinhardta15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/steinhardta15.html},\n  abstract = \t {A classic tension exists between exact inference in a simple model and approximate inference in a complex model. The latter offers expressivity and thus accuracy, but the former provides coverage of the space, an important property for confidence estimation and learning with indirect supervision. In this work, we introduce a new approach, reified context models, to reconcile this tension. Specifically, we let the choice of factors in a graphical model (the contexts) be random variables inside the model itself. In this sense, the contexts are reified and can be chosen in a data-dependent way. Empirically, we show that our approach obtains expressivity and coverage on three sequence modeling tasks.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/steinhardta15.pdf",
        "supp": "",
        "pdf_size": 3757550,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17522940104584586732&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Stanford University; Stanford University",
        "aff_domain": "CS.STANFORD.EDU;CS.STANFORD.EDU",
        "email": "CS.STANFORD.EDU;CS.STANFORD.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "db2b38d242",
        "title": "Removing systematic errors for exoplanet search via latent causes",
        "site": "https://proceedings.mlr.press/v37/scholkopf15.html",
        "author": "Bernhard Sch\u00f6lkopf; David Hogg; Dun Wang; Dan Foreman-Mackey; Dominik Janzing; Carl-Johann Simon-Gabriel; Jonas Peters",
        "abstract": "We describe a method for removing the effect of confounders in order to reconstruct a latent quantity of interest. The method, referred to as",
        "bibtex": "@InProceedings{pmlr-v37-scholkopf15,\n  title = \t {Removing systematic errors for exoplanet search via latent causes},\n  author = \t {Sch\u00f6lkopf, Bernhard and Hogg, David and Wang, Dun and Foreman-Mackey, Dan and Janzing, Dominik and Simon-Gabriel, Carl-Johann and Peters, Jonas},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2218--2226},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/scholkopf15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/scholkopf15.html},\n  abstract = \t {We describe a method for removing the effect of confounders in order to reconstruct a latent quantity of interest. The method, referred to as",
        "pdf": "http://proceedings.mlr.press/v37/scholkopf15.pdf",
        "supp": "",
        "pdf_size": 1371768,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11768165421845046384&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 10,
        "aff": "Max Planck Institute for Intelligent Systems, 72076 T\u00fcbingen, GERMANY; Center for Cosmology and Particle Physics, New York University, New York, NY 10003, USA; Center for Cosmology and Particle Physics, New York University, New York, NY 10003, USA; Center for Cosmology and Particle Physics, New York University, New York, NY 10003, USA; Max Planck Institute for Intelligent Systems, 72076 T\u00fcbingen, GERMANY; Max Planck Institute for Intelligent Systems, 72076 T\u00fcbingen, GERMANY; Max Planck Institute for Intelligent Systems, 72076 T\u00fcbingen, GERMANY",
        "aff_domain": "TUEBINGEN.MPG.DE;NYU.EDU;NYU.EDU;GMAIL.COM;TUEBINGEN.MPG.DE;TUEBINGEN.MPG.DE;TUEBINGEN.MPG.DE",
        "email": "TUEBINGEN.MPG.DE;NYU.EDU;NYU.EDU;GMAIL.COM;TUEBINGEN.MPG.DE;TUEBINGEN.MPG.DE;TUEBINGEN.MPG.DE",
        "github": "",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1;0;0;0",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems;New York University",
        "aff_unique_dep": ";Center for Cosmology and Particle Physics",
        "aff_unique_url": "https://www.mpi-is.mpg.de;https://www.nyu.edu",
        "aff_unique_abbr": "MPI-IS;NYU",
        "aff_campus_unique_index": "0;1;1;1;0;0;0",
        "aff_campus_unique": "T\u00fcbingen;New York",
        "aff_country_unique_index": "0;1;1;1;0;0;0",
        "aff_country_unique": "Germany;United States"
    },
    {
        "id": "8ff2740774",
        "title": "Risk and Regret of Hierarchical Bayesian Learners",
        "site": "https://proceedings.mlr.press/v37/hugginsb15.html",
        "author": "Jonathan Huggins; Josh Tenenbaum",
        "abstract": "Common statistical practice has shown that the full power of Bayesian methods is not realized until hierarchical priors are used, as these allow for greater \u201crobustness\u201d and the ability to \u201cshare statistical strength.\u201d Yet it is an ongoing challenge to provide a learning-theoretically sound formalism of such notions that: offers practical guidance concerning when and how best to utilize hierarchical models; provides insights into what makes for a good hierarchical prior; and, when the form of the prior has been chosen, can guide the choice of hyperparameter settings. We present a set of analytical tools for understanding hierarchical priors in both the online and batch learning settings. We provide regret bounds under log-loss, which show how certain hierarchical models compare, in retrospect, to the best single model in the model class. We also show how to convert a Bayesian log-loss regret bound into a Bayesian risk bound for any bounded loss, a result which may be of independent interest. Risk and regret bounds for Student\u2019s t and hierarchical Gaussian priors allow us to formalize the concepts of \u201crobustness\u201d and \u201csharing statistical strength.\u201d Priors for feature selection are investigated as well. Our results suggest that the learning-theoretic benefits of using hierarchical priors can often come at little cost on practical problems.",
        "bibtex": "@InProceedings{pmlr-v37-hugginsb15,\n  title = \t {Risk and Regret of Hierarchical Bayesian Learners},\n  author = \t {Huggins, Jonathan and Tenenbaum, Josh},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1442--1451},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/hugginsb15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/hugginsb15.html},\n  abstract = \t {Common statistical practice has shown that the full power of Bayesian methods is not realized until hierarchical priors are used, as these allow for greater \u201crobustness\u201d and the ability to \u201cshare statistical strength.\u201d Yet it is an ongoing challenge to provide a learning-theoretically sound formalism of such notions that: offers practical guidance concerning when and how best to utilize hierarchical models; provides insights into what makes for a good hierarchical prior; and, when the form of the prior has been chosen, can guide the choice of hyperparameter settings. We present a set of analytical tools for understanding hierarchical priors in both the online and batch learning settings. We provide regret bounds under log-loss, which show how certain hierarchical models compare, in retrospect, to the best single model in the model class. We also show how to convert a Bayesian log-loss regret bound into a Bayesian risk bound for any bounded loss, a result which may be of independent interest. Risk and regret bounds for Student\u2019s t and hierarchical Gaussian priors allow us to formalize the concepts of \u201crobustness\u201d and \u201csharing statistical strength.\u201d Priors for feature selection are investigated as well. Our results suggest that the learning-theoretic benefits of using hierarchical priors can often come at little cost on practical problems.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/hugginsb15.pdf",
        "supp": "",
        "pdf_size": 3493016,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16684417842946871551&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Computer Science and Artificial Intelligence Laboratory, MIT; Brain and Cognitive Science Department, MIT",
        "aff_domain": "MIT.EDU;MIT.EDU",
        "email": "MIT.EDU;MIT.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.csail.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0d965a594a",
        "title": "Robust Estimation of Transition Matrices in High Dimensional Heavy-tailed Vector Autoregressive Processes",
        "site": "https://proceedings.mlr.press/v37/qiu15.html",
        "author": "Huitong Qiu; Sheng Xu; Fang Han; Han Liu; Brian Caffo",
        "abstract": "Gaussian vector autoregressive (VAR) processes have been extensively studied in the literature. However, Gaussian assumptions are stringent for heavy-tailed time series that frequently arises in finance and economics. In this paper, we develop a unified framework for modeling and estimating heavy-tailed VAR processes. In particular, we generalize the Gaussian VAR model by an elliptical VAR model that naturally accommodates heavy-tailed time series. Under this model, we develop a quantile-based robust estimator for the transition matrix of the VAR process. We show that the proposed estimator achieves parametric rates of convergence in high dimensions. This is the first work in analyzing heavy-tailed high dimensional VAR processes. As an application of the proposed framework, we investigate Granger causality in the elliptical VAR process, and show that the robust transition matrix estimator induces sign-consistent estimators of Granger causality. The empirical performance of the proposed methodology is demonstrated by both synthetic and real data. We show that the proposed estimator is robust to heavy tails, and exhibit superior performance in stock price prediction.",
        "bibtex": "@InProceedings{pmlr-v37-qiu15,\n  title = \t {Robust Estimation of Transition Matrices in High Dimensional Heavy-tailed Vector Autoregressive Processes},\n  author = \t {Qiu, Huitong and Xu, Sheng and Han, Fang and Liu, Han and Caffo, Brian},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1843--1851},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/qiu15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/qiu15.html},\n  abstract = \t {Gaussian vector autoregressive (VAR) processes have been extensively studied in the literature. However, Gaussian assumptions are stringent for heavy-tailed time series that frequently arises in finance and economics. In this paper, we develop a unified framework for modeling and estimating heavy-tailed VAR processes. In particular, we generalize the Gaussian VAR model by an elliptical VAR model that naturally accommodates heavy-tailed time series. Under this model, we develop a quantile-based robust estimator for the transition matrix of the VAR process. We show that the proposed estimator achieves parametric rates of convergence in high dimensions. This is the first work in analyzing heavy-tailed high dimensional VAR processes. As an application of the proposed framework, we investigate Granger causality in the elliptical VAR process, and show that the robust transition matrix estimator induces sign-consistent estimators of Granger causality. The empirical performance of the proposed methodology is demonstrated by both synthetic and real data. We show that the proposed estimator is robust to heavy tails, and exhibit superior performance in stock price prediction.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/qiu15.pdf",
        "supp": "",
        "pdf_size": 371182,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1888958163676172725&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Johns Hopkins University, 615 N. Wolfe St., Baltimore, MD 21210 USA; Johns Hopkins University, 615 N. Wolfe St., Baltimore, MD 21210 USA; Johns Hopkins University, 615 N. Wolfe St., Baltimore, MD 21210 USA; Princeton University, 98 Charlton Street, Princeton, NJ 08544 USA; Johns Hopkins University, 615 N. Wolfe St., Baltimore, MD 21210 USA",
        "aff_domain": "JHU.EDU;JHU.EDU;JHU.EDU;PRINCETON.EDU;JHU.EDU",
        "email": "JHU.EDU;JHU.EDU;JHU.EDU;PRINCETON.EDU;JHU.EDU",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "Johns Hopkins University;Princeton University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.jhu.edu;https://www.princeton.edu",
        "aff_unique_abbr": "JHU;Princeton",
        "aff_campus_unique_index": "0;0;0;1;0",
        "aff_campus_unique": "Baltimore;Princeton",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ac9a6d52d5",
        "title": "Robust partially observable Markov decision process",
        "site": "https://proceedings.mlr.press/v37/osogami15.html",
        "author": "Takayuki Osogami",
        "abstract": "We seek to find the robust policy that maximizes the expected cumulative reward for the worst case when a partially observable Markov decision process (POMDP) has uncertain parameters whose values are only known to be in a given region. We prove that the robust value function, which represents the expected cumulative reward that can be obtained with the robust policy, is convex with respect to the belief state. Based on the convexity, we design a value-iteration algorithm for finding the robust policy. We prove that our value iteration converges for an infinite horizon. We also design point-based value iteration for fining the robust policy more efficiency possibly with approximation. Numerical experiments show that our point-based value iteration can adequately find robust policies.",
        "bibtex": "@InProceedings{pmlr-v37-osogami15,\n  title = \t {Robust partially observable Markov decision process},\n  author = \t {Osogami, Takayuki},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {106--115},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/osogami15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/osogami15.html},\n  abstract = \t {We seek to find the robust policy that maximizes the expected cumulative reward for the worst case when a partially observable Markov decision process (POMDP) has uncertain parameters whose values are only known to be in a given region. We prove that the robust value function, which represents the expected cumulative reward that can be obtained with the robust policy, is convex with respect to the belief state. Based on the convexity, we design a value-iteration algorithm for finding the robust policy. We prove that our value iteration converges for an infinite horizon. We also design point-based value iteration for fining the robust policy more efficiency possibly with approximation. Numerical experiments show that our point-based value iteration can adequately find robust policies.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/osogami15.pdf",
        "supp": "",
        "pdf_size": 367622,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15821828505064637974&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "IBM Research - Tokyo, Tokyo, Japan",
        "aff_domain": "JP.IBM.COM",
        "email": "JP.IBM.COM",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "IBM",
        "aff_unique_dep": "Research",
        "aff_unique_url": "https://www.ibm.com/research",
        "aff_unique_abbr": "IBM",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Tokyo",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "589a4ac4f0",
        "title": "Safe Exploration for Optimization with Gaussian Processes",
        "site": "https://proceedings.mlr.press/v37/sui15.html",
        "author": "Yanan Sui; Alkis Gotovos; Joel Burdick; Andreas Krause",
        "abstract": "We consider sequential decision problems under uncertainty, where we seek to optimize an unknown function from noisy samples. This requires balancing exploration (learning about the objective) and exploitation (localizing the maximum), a problem well-studied in the multi-armed bandit literature. In many applications, however, we require that the sampled function values exceed some prespecified \"safety\" threshold, a requirement that existing algorithms fail to meet. Examples include medical applications where patient comfort must be guaranteed, recommender systems aiming to avoid user dissatisfaction, and robotic control, where one seeks to avoid controls causing physical harm to the platform. We tackle this novel, yet rich, set of problems under the assumption that the unknown function satisfies regularity conditions expressed via a Gaussian process prior. We develop an efficient algorithm called SafeOpt, and theoretically guarantee its convergence to a natural notion of optimum reachable under safety constraints. We evaluate SafeOpt on synthetic data, as well as two real applications: movie recommendation, and therapeutic spinal cord stimulation.",
        "bibtex": "@InProceedings{pmlr-v37-sui15,\n  title = \t {Safe Exploration for Optimization with Gaussian Processes},\n  author = \t {Sui, Yanan and Gotovos, Alkis and Burdick, Joel and Krause, Andreas},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {997--1005},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/sui15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/sui15.html},\n  abstract = \t {We consider sequential decision problems under uncertainty, where we seek to optimize an unknown function from noisy samples. This requires balancing exploration (learning about the objective) and exploitation (localizing the maximum), a problem well-studied in the multi-armed bandit literature. In many applications, however, we require that the sampled function values exceed some prespecified \"safety\" threshold, a requirement that existing algorithms fail to meet. Examples include medical applications where patient comfort must be guaranteed, recommender systems aiming to avoid user dissatisfaction, and robotic control, where one seeks to avoid controls causing physical harm to the platform. We tackle this novel, yet rich, set of problems under the assumption that the unknown function satisfies regularity conditions expressed via a Gaussian process prior. We develop an efficient algorithm called SafeOpt, and theoretically guarantee its convergence to a natural notion of optimum reachable under safety constraints. We evaluate SafeOpt on synthetic data, as well as two real applications: movie recommendation, and therapeutic spinal cord stimulation.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/sui15.pdf",
        "supp": "",
        "pdf_size": 369748,
        "gs_citation": 498,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9013171973487763232&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "California Institute of Technology, Pasadena, CA, USA; ETH Zurich, Zurich, Switzerland; California Institute of Technology, Pasadena, CA, USA; ETH Zurich, Zurich, Switzerland",
        "aff_domain": "caltech.edu;inf.ethz.ch;robotics.caltech.edu;ethz.ch",
        "email": "caltech.edu;inf.ethz.ch;robotics.caltech.edu;ethz.ch",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "California Institute of Technology;ETH Zurich",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.caltech.edu;https://www.ethz.ch",
        "aff_unique_abbr": "Caltech;ETHZ",
        "aff_campus_unique_index": "0;1;0;1",
        "aff_campus_unique": "Pasadena;Zurich",
        "aff_country_unique_index": "0;1;0;1",
        "aff_country_unique": "United States;Switzerland"
    },
    {
        "id": "7bfa1d1640",
        "title": "Safe Policy Search for Lifelong Reinforcement Learning with Sublinear Regret",
        "site": "https://proceedings.mlr.press/v37/ammar15.html",
        "author": "Haitham Bou Ammar; Rasul Tutunov; Eric Eaton",
        "abstract": "Lifelong reinforcement learning provides a promising framework for developing versatile agents that can accumulate knowledge over a lifetime of experience and rapidly learn new tasks by building upon prior knowledge. However, current lifelong learning methods exhibit non-vanishing regret as the amount of experience increases, and include limitations that can lead to suboptimal or unsafe control policies. To address these issues, we develop a lifelong policy gradient learner that operates in an adversarial setting to learn multiple tasks online while enforcing safety constraints on the learned policies. We demonstrate, for the first time, sublinear regret for lifelong policy search, and validate our algorithm on several benchmark dynamical systems and an application to quadrotor control.",
        "bibtex": "@InProceedings{pmlr-v37-ammar15,\n  title = \t {Safe Policy Search for Lifelong Reinforcement Learning with Sublinear Regret},\n  author = \t {Ammar, Haitham Bou and Tutunov, Rasul and Eaton, Eric},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2361--2369},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/ammar15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/ammar15.html},\n  abstract = \t {Lifelong reinforcement learning provides a promising framework for developing versatile agents that can accumulate knowledge over a lifetime of experience and rapidly learn new tasks by building upon prior knowledge. However, current lifelong learning methods exhibit non-vanishing regret as the amount of experience increases, and include limitations that can lead to suboptimal or unsafe control policies. To address these issues, we develop a lifelong policy gradient learner that operates in an adversarial setting to learn multiple tasks online while enforcing safety constraints on the learned policies. We demonstrate, for the first time, sublinear regret for lifelong policy search, and validate our algorithm on several benchmark dynamical systems and an application to quadrotor control.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/ammar15.pdf",
        "supp": "",
        "pdf_size": 747426,
        "gs_citation": 88,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18163488271384936437&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "University of Pennsylvania, Computer and Information Science Department, Philadelphia, PA 19104 USA; University of Pennsylvania, Computer and Information Science Department, Philadelphia, PA 19104 USA; University of Pennsylvania, Computer and Information Science Department, Philadelphia, PA 19104 USA",
        "aff_domain": "SEAS.UPENN.EDU;SEAS.UPENN.EDU;CIS.UPENN.EDU",
        "email": "SEAS.UPENN.EDU;SEAS.UPENN.EDU;CIS.UPENN.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "Computer and Information Science Department",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Philadelphia",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "34cedf90ce",
        "title": "Safe Screening for Multi-Task Feature Learning with Multiple Data Matrices",
        "site": "https://proceedings.mlr.press/v37/wangf15.html",
        "author": "Jie Wang; Jieping Ye",
        "abstract": "Multi-task feature learning (MTFL) is a powerful technique in boosting the predictive performance by learning multiple related classification/regression/clustering tasks simultaneously. However, solving the MTFL problem remains challenging when the feature dimension is extremely large. In this paper, we propose a novel screening rule\u2014that is based on the dual projection onto convex sets (DPC)\u2014to quickly identify the inactive features\u2014that have zero coefficients in the solution vectors across all tasks. One of the appealing features of DPC is that: it is safe in the sense that the detected inactive features are guaranteed to have zero coefficients in the solution vectors across all tasks. Thus, by removing the inactive features from the training phase, we may have substantial savings in the computational cost and memory usage without sacrificing accuracy. To the best of our knowledge, it is the first screening rule that is applicable to sparse models with multiple data matrices. A key challenge in deriving DPC is to solve a nonconvex problem. We show that we can solve for the global optimum efficiently via a properly chosen parametrization of the constraint set. Moreover, DPC has very low computational cost and can be integrated with any existing solvers. We have evaluated the proposed DPC rule on both synthetic and real data sets. The experiments indicate that DPC is very effective in identifying the inactive features\u2014especially for high dimensional data\u2014which leads to a speedup up to several orders of magnitude.",
        "bibtex": "@InProceedings{pmlr-v37-wangf15,\n  title = \t {Safe Screening for Multi-Task Feature Learning with Multiple Data Matrices},\n  author = \t {Wang, Jie and Ye, Jieping},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1747--1756},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/wangf15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/wangf15.html},\n  abstract = \t {Multi-task feature learning (MTFL) is a powerful technique in boosting the predictive performance by learning multiple related classification/regression/clustering tasks simultaneously. However, solving the MTFL problem remains challenging when the feature dimension is extremely large. In this paper, we propose a novel screening rule\u2014that is based on the dual projection onto convex sets (DPC)\u2014to quickly identify the inactive features\u2014that have zero coefficients in the solution vectors across all tasks. One of the appealing features of DPC is that: it is safe in the sense that the detected inactive features are guaranteed to have zero coefficients in the solution vectors across all tasks. Thus, by removing the inactive features from the training phase, we may have substantial savings in the computational cost and memory usage without sacrificing accuracy. To the best of our knowledge, it is the first screening rule that is applicable to sparse models with multiple data matrices. A key challenge in deriving DPC is to solve a nonconvex problem. We show that we can solve for the global optimum efficiently via a properly chosen parametrization of the constraint set. Moreover, DPC has very low computational cost and can be integrated with any existing solvers. We have evaluated the proposed DPC rule on both synthetic and real data sets. The experiments indicate that DPC is very effective in identifying the inactive features\u2014especially for high dimensional data\u2014which leads to a speedup up to several orders of magnitude.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/wangf15.pdf",
        "supp": "",
        "pdf_size": 453158,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9358575369179918568&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computational Medicine and Bioinformatics, University of Michigan, MI 48109 USA+Department of Electrical Engineering and Computer Science, University of Michigan, MI 48109 USA; Department of Computational Medicine and Bioinformatics, University of Michigan, MI 48109 USA+Department of Electrical Engineering and Computer Science, University of Michigan, MI 48109 USA",
        "aff_domain": "umich.edu;umich.edu",
        "email": "umich.edu;umich.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+0;0+0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Department of Computational Medicine and Bioinformatics",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0+0;0+0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "babb1034fb",
        "title": "Safe Subspace Screening for Nuclear Norm Regularized Least Squares Problems",
        "site": "https://proceedings.mlr.press/v37/zhoua15.html",
        "author": "Qiang Zhou; Qi Zhao",
        "abstract": "Nuclear norm regularization has been shown very promising for pursing a low rank matrix solution in various machine learning problems. Many efforts have been devoted to develop efficient algorithms for solving the optimization problem in nuclear norm regularization. Solving it for large-scale matrix variables, however, is still a challenging task since the complexity grows fast with the size of matrix variable. In this work, we propose a novel method called safe subspace screening (SSS), to improve the efficiency of the solver for nuclear norm regularized least squares problems. Motivated by the fact that the low rank solution can be represented by a few subspaces, the proposed method accurately discards a predominant percentage of inactive subspaces prior to solving the problem to reduce problem size. Consequently, a much smaller problem is required to solve, making it more efficient than optimizing the original problem. The proposed SSS is safe, in that its solution is identical to the solution from the solver. In addition, the proposed SSS can be used together with any existing nuclear norm solver since it is independent of the solver. Extensive results on several synthetic and real data sets show that the proposed SSS is very effective in inactive subspace screening.",
        "bibtex": "@InProceedings{pmlr-v37-zhoua15,\n  title = \t {Safe Subspace Screening for Nuclear Norm Regularized Least Squares Problems},\n  author = \t {Zhou, Qiang and Zhao, Qi},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1103--1112},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/zhoua15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/zhoua15.html},\n  abstract = \t {Nuclear norm regularization has been shown very promising for pursing a low rank matrix solution in various machine learning problems. Many efforts have been devoted to develop efficient algorithms for solving the optimization problem in nuclear norm regularization. Solving it for large-scale matrix variables, however, is still a challenging task since the complexity grows fast with the size of matrix variable. In this work, we propose a novel method called safe subspace screening (SSS), to improve the efficiency of the solver for nuclear norm regularized least squares problems. Motivated by the fact that the low rank solution can be represented by a few subspaces, the proposed method accurately discards a predominant percentage of inactive subspaces prior to solving the problem to reduce problem size. Consequently, a much smaller problem is required to solve, making it more efficient than optimizing the original problem. The proposed SSS is safe, in that its solution is identical to the solution from the solver. In addition, the proposed SSS can be used together with any existing nuclear norm solver since it is independent of the solver. Extensive results on several synthetic and real data sets show that the proposed SSS is very effective in inactive subspace screening.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/zhoua15.pdf",
        "supp": "",
        "pdf_size": 675411,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11931438949364458929&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Electrical and Computer Engineering, National University of Singapore, Singapore 117583; Department of Electrical and Computer Engineering, National University of Singapore, Singapore 117583",
        "aff_domain": "u.nus.edu;nus.edu.sg",
        "email": "u.nus.edu;nus.edu.sg",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "National University of Singapore",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.nus.edu.sg",
        "aff_unique_abbr": "NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "7da7a76ab8",
        "title": "Scalable Bayesian Optimization Using Deep Neural Networks",
        "site": "https://proceedings.mlr.press/v37/snoek15.html",
        "author": "Jasper Snoek; Oren Rippel; Kevin Swersky; Ryan Kiros; Nadathur Satish; Narayanan Sundaram; Mostofa Patwary; Mr Prabhat; Ryan Adams",
        "abstract": "Bayesian optimization is an effective methodology for the global optimization of functions with expensive evaluations. It relies on querying a distribution over functions defined by a relatively cheap surrogate model. An accurate model for this distribution over functions is critical to the effectiveness of the approach, and is typically fit using Gaussian processes (GPs). However, since GPs scale cubically with the number of observations, it has been challenging to handle objectives whose optimization requires many evaluations, and as such, massively parallelizing the optimization. In this work, we explore the use of neural networks as an alternative to GPs to model distributions over functions. We show that performing adaptive basis function regression with a neural network as the parametric form performs competitively with state-of-the-art GP-based approaches, but scales linearly with the number of data rather than cubically. This allows us to achieve a previously intractable degree of parallelism, which we apply to large scale hyperparameter optimization, rapidly finding competitive models on benchmark object recognition tasks using convolutional networks, and image caption generation using neural language models.",
        "bibtex": "@InProceedings{pmlr-v37-snoek15,\n  title = \t {Scalable Bayesian Optimization Using Deep Neural Networks},\n  author = \t {Snoek, Jasper and Rippel, Oren and Swersky, Kevin and Kiros, Ryan and Satish, Nadathur and Sundaram, Narayanan and Patwary, Mostofa and Prabhat, Mr and Adams, Ryan},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2171--2180},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/snoek15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/snoek15.html},\n  abstract = \t {Bayesian optimization is an effective methodology for the global optimization of functions with expensive evaluations. It relies on querying a distribution over functions defined by a relatively cheap surrogate model. An accurate model for this distribution over functions is critical to the effectiveness of the approach, and is typically fit using Gaussian processes (GPs). However, since GPs scale cubically with the number of observations, it has been challenging to handle objectives whose optimization requires many evaluations, and as such, massively parallelizing the optimization. In this work, we explore the use of neural networks as an alternative to GPs to model distributions over functions. We show that performing adaptive basis function regression with a neural network as the parametric form performs competitively with state-of-the-art GP-based approaches, but scales linearly with the number of data rather than cubically. This allows us to achieve a previously intractable degree of parallelism, which we apply to large scale hyperparameter optimization, rapidly finding competitive models on benchmark object recognition tasks using convolutional networks, and image caption generation using neural language models.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/snoek15.pdf",
        "supp": "",
        "pdf_size": 1223015,
        "gs_citation": 1406,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7151760993960049125&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 21,
        "aff": "Harvard University, School of Engineering and Applied Sciences; Massachusetts Institute of Technology, Department of Mathematics; University of Toronto, Department of Computer Science; Intel Labs, Parallel Computing Lab; NERSC, Lawrence Berkeley National Laboratory; Intel Labs, Parallel Computing Lab; Intel Labs, Parallel Computing Lab; NERSC, Lawrence Berkeley National Laboratory; Harvard University, School of Engineering and Applied Sciences",
        "aff_domain": "SEAS.HARVARD.EDU;MATH.MIT.EDU;CS.TORONTO.EDU;CS.TORONTO.EDU;INTEL.COM;INTEL.COM;INTEL.COM;LBL.GOV;SEAS.HARVARD.EDU",
        "email": "SEAS.HARVARD.EDU;MATH.MIT.EDU;CS.TORONTO.EDU;CS.TORONTO.EDU;INTEL.COM;INTEL.COM;INTEL.COM;LBL.GOV;SEAS.HARVARD.EDU",
        "github": "",
        "project": "",
        "author_num": 9,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3;4;3;3;4;0",
        "aff_unique_norm": "Harvard University;Massachusetts Institute of Technology;University of Toronto;Intel;Lawrence Berkeley National Laboratory",
        "aff_unique_dep": "School of Engineering and Applied Sciences;Department of Mathematics;Department of Computer Science;Parallel Computing Lab;NERSC",
        "aff_unique_url": "https://www.harvard.edu;https://web.mit.edu;https://www.utoronto.ca;https://www.intel.com/research;https://www.nersc.gov",
        "aff_unique_abbr": "Harvard;MIT;U of T;Intel;LBNL",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;0;1;0;0;0;0;0;0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "4a7f6eeca0",
        "title": "Scalable Deep Poisson Factor Analysis for Topic Modeling",
        "site": "https://proceedings.mlr.press/v37/gan15.html",
        "author": "Zhe Gan; Changyou Chen; Ricardo Henao; David Carlson; Lawrence Carin",
        "abstract": "A new framework for topic modeling is developed, based on deep graphical models, where interactions between topics are inferred through deep latent binary hierarchies. The proposed multi-layer model employs a deep sigmoid belief network or restricted Boltzmann machine, the bottom binary layer of which selects topics for use in a Poisson factor analysis model. Under this setting, topics live on the bottom layer of the model, while the deep specification serves as a flexible prior for revealing topic structure. Scalable inference algorithms are derived by applying Bayesian conditional density filtering algorithm, in addition to extending recently proposed work on stochastic gradient thermostats. Experimental results on several corpora show that the proposed approach readily handles very large collections of text documents, infers structured topic representations, and obtains superior test perplexities when compared with related models.",
        "bibtex": "@InProceedings{pmlr-v37-gan15,\n  title = \t {Scalable Deep Poisson Factor Analysis for Topic Modeling},\n  author = \t {Gan, Zhe and Chen, Changyou and Henao, Ricardo and Carlson, David and Carin, Lawrence},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1823--1832},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/gan15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/gan15.html},\n  abstract = \t {A new framework for topic modeling is developed, based on deep graphical models, where interactions between topics are inferred through deep latent binary hierarchies. The proposed multi-layer model employs a deep sigmoid belief network or restricted Boltzmann machine, the bottom binary layer of which selects topics for use in a Poisson factor analysis model. Under this setting, topics live on the bottom layer of the model, while the deep specification serves as a flexible prior for revealing topic structure. Scalable inference algorithms are derived by applying Bayesian conditional density filtering algorithm, in addition to extending recently proposed work on stochastic gradient thermostats. Experimental results on several corpora show that the proposed approach readily handles very large collections of text documents, infers structured topic representations, and obtains superior test perplexities when compared with related models.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/gan15.pdf",
        "supp": "",
        "pdf_size": 533759,
        "gs_citation": 111,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15913064328439417262&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Electrical and Computer Engineering, Duke University, Durham, NC 27708, USA; Department of Electrical and Computer Engineering, Duke University, Durham, NC 27708, USA; Department of Electrical and Computer Engineering, Duke University, Durham, NC 27708, USA; Department of Electrical and Computer Engineering, Duke University, Durham, NC 27708, USA; Department of Electrical and Computer Engineering, Duke University, Durham, NC 27708, USA",
        "aff_domain": "DUKE.EDU;DUKE.EDU;DUKE.EDU;DUKE.EDU;DUKE.EDU",
        "email": "DUKE.EDU;DUKE.EDU;DUKE.EDU;DUKE.EDU;DUKE.EDU",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Duke University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.duke.edu",
        "aff_unique_abbr": "Duke",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Durham",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a1864672b1",
        "title": "Scalable Model Selection for Large-Scale Factorial Relational Models",
        "site": "https://proceedings.mlr.press/v37/liub15.html",
        "author": "Chunchen Liu; Lu Feng; Ryohei Fujimaki; Yusuke Muraoka",
        "abstract": "With a growing need to understand large-scale networks, factorial relational models, such as binary matrix factorization models (BMFs), have become important in many applications. Although BMFs have a natural capability to uncover overlapping group structures behind network data, existing inference techniques have issues of either high computational cost or lack of model selection capability, and this limits their applicability. For scalable model selection of BMFs, this paper proposes stochastic factorized asymptotic Bayesian (sFAB) inference that combines concepts in two recently-developed techniques: stochastic variational inference (SVI) and FAB inference. sFAB is a highly-efficient algorithm, having both scalability and an inherent model selection capability in a single inference framework. Empirical results show the superiority of sFAB/BMF in both accuracy and scalability over state-of-the-art inference methods for overlapping relational models.",
        "bibtex": "@InProceedings{pmlr-v37-liub15,\n  title = \t {Scalable Model Selection for Large-Scale Factorial Relational Models},\n  author = \t {Liu, Chunchen and Feng, Lu and Fujimaki, Ryohei and Muraoka, Yusuke},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1227--1235},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/liub15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/liub15.html},\n  abstract = \t {With a growing need to understand large-scale networks, factorial relational models, such as binary matrix factorization models (BMFs), have become important in many applications. Although BMFs have a natural capability to uncover overlapping group structures behind network data, existing inference techniques have issues of either high computational cost or lack of model selection capability, and this limits their applicability. For scalable model selection of BMFs, this paper proposes stochastic factorized asymptotic Bayesian (sFAB) inference that combines concepts in two recently-developed techniques: stochastic variational inference (SVI) and FAB inference. sFAB is a highly-efficient algorithm, having both scalability and an inherent model selection capability in a single inference framework. Empirical results show the superiority of sFAB/BMF in both accuracy and scalability over state-of-the-art inference methods for overlapping relational models.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/liub15.pdf",
        "supp": "",
        "pdf_size": 486502,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3111335713197667931&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "NEC Laboratories China; NEC Laboratories China; NEC Knowledge Discovery Research Laboratories; NEC Knowledge Discovery Research Laboratories",
        "aff_domain": "nec.cn;nec.cn;nec-labs.com;nec-labs.com",
        "email": "nec.cn;nec.cn;nec-labs.com;nec-labs.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;1",
        "aff_unique_norm": "NEC Laboratories;NEC",
        "aff_unique_dep": "China;Knowledge Discovery Research Laboratories",
        "aff_unique_url": "https://www.nec-labs.com/;https://www.nec.com",
        "aff_unique_abbr": "NEC Labs China;NEC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;1",
        "aff_country_unique": "China;Japan"
    },
    {
        "id": "3684f9814b",
        "title": "Scalable Nonparametric Bayesian Inference on Point Processes with Gaussian Processes",
        "site": "https://proceedings.mlr.press/v37/samo15.html",
        "author": "Yves-Laurent Kom Samo; Stephen Roberts",
        "abstract": "In this paper we propose an efficient, scalable non-parametric Gaussian process model for inference on Poisson point processes. Our model does not resort to gridding the domain or to introducing latent thinning points. Unlike competing models that scale as O(n^3) over n data points, our model has a complexity O(nk^2) where k << n. We propose a MCMC sampler and show that the model obtained is faster, more accurate and generates less correlated samples than competing approaches on both synthetic and real-life data. Finally, we show that our model easily handles data sizes not considered thus far by alternate approaches.",
        "bibtex": "@InProceedings{pmlr-v37-samo15,\n  title = \t {Scalable Nonparametric Bayesian Inference on Point Processes with Gaussian Processes},\n  author = \t {Samo, Yves-Laurent Kom and Roberts, Stephen},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2227--2236},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/samo15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/samo15.html},\n  abstract = \t {In this paper we propose an efficient, scalable non-parametric Gaussian process model for inference on Poisson point processes. Our model does not resort to gridding the domain or to introducing latent thinning points. Unlike competing models that scale as O(n^3) over n data points, our model has a complexity O(nk^2) where k << n. We propose a MCMC sampler and show that the model obtained is faster, more accurate and generates less correlated samples than competing approaches on both synthetic and real-life data. Finally, we show that our model easily handles data sizes not considered thus far by alternate approaches.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/samo15.pdf",
        "supp": "",
        "pdf_size": 859362,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2856091358054514445&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Deparment of Engineering Science and Oxford-Man Institute, University of Oxford; Deparment of Engineering Science and Oxford-Man Institute, University of Oxford",
        "aff_domain": "ROBOTS.OX.AC.UK;ROBOTS.OX.AC.UK",
        "email": "ROBOTS.OX.AC.UK;ROBOTS.OX.AC.UK",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Oxford",
        "aff_unique_dep": "Department of Engineering Science",
        "aff_unique_url": "https://www.ox.ac.uk",
        "aff_unique_abbr": "Oxford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Oxford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "7d37787131",
        "title": "Scalable Variational Inference in Log-supermodular Models",
        "site": "https://proceedings.mlr.press/v37/djolonga15.html",
        "author": "Josip Djolonga; Andreas Krause",
        "abstract": "We consider the problem of approximate Bayesian inference in log-supermodular models. These models encompass regular pairwise MRFs with binary variables, but allow to capture high order interactions, which are intractable for existing approximate inference techniques such as belief propagation, mean field and variants. We show that a recently proposed variational approach to inference in log-supermodular models \u2013 L-Field \u2013 reduces to the widely studied minimum norm problem for submodular minimization. This insight allows to leverage powerful existing tools, and allows solving the variational problem orders of magnitude more efficiently than previously possible. We then provide another natural interpretation of L-Field, demonstrating that it exactly minimizes a specific type of Renyi divergence measure. This insight sheds light on the nature of the variational approximations produced by L-Field. Furthermore, we show how to perform parallel inference as message passing in a suitable factor graph at a linear convergence rate, without having to sum up over all the configurations of the factor. Finally, we apply our approach to a challenging image segmentation task. Our experiments confirm scalability of our approach, high quality of the marginals and the benefit of incorporating higher order potentials.",
        "bibtex": "@InProceedings{pmlr-v37-djolonga15,\n  title = \t {Scalable Variational Inference in Log-supermodular Models},\n  author = \t {Djolonga, Josip and Krause, Andreas},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1804--1813},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/djolonga15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/djolonga15.html},\n  abstract = \t {We consider the problem of approximate Bayesian inference in log-supermodular models. These models encompass regular pairwise MRFs with binary variables, but allow to capture high order interactions, which are intractable for existing approximate inference techniques such as belief propagation, mean field and variants. We show that a recently proposed variational approach to inference in log-supermodular models \u2013 L-Field \u2013 reduces to the widely studied minimum norm problem for submodular minimization. This insight allows to leverage powerful existing tools, and allows solving the variational problem orders of magnitude more efficiently than previously possible. We then provide another natural interpretation of L-Field, demonstrating that it exactly minimizes a specific type of Renyi divergence measure. This insight sheds light on the nature of the variational approximations produced by L-Field. Furthermore, we show how to perform parallel inference as message passing in a suitable factor graph at a linear convergence rate, without having to sum up over all the configurations of the factor. Finally, we apply our approach to a challenging image segmentation task. Our experiments confirm scalability of our approach, high quality of the marginals and the benefit of incorporating higher order potentials.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/djolonga15.pdf",
        "supp": "",
        "pdf_size": 858113,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1225172830333184597&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Science, ETH Zurich; Department of Computer Science, ETH Zurich",
        "aff_domain": "INF.ETHZ.CH;ETHZ.CH",
        "email": "INF.ETHZ.CH;ETHZ.CH",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "342fb2325a",
        "title": "Scaling up Natural Gradient by Sparsely Factorizing the Inverse Fisher Matrix",
        "site": "https://proceedings.mlr.press/v37/grosse15.html",
        "author": "Roger Grosse; Ruslan Salakhudinov",
        "abstract": "Second-order optimization methods, such as natural gradient, are difficult to apply to high-dimensional problems, because they require approximately solving large linear systems. We present FActorized Natural Gradient (FANG), an approximation to natural gradient descent where the Fisher matrix is approximated with a Gaussian graphical model whose precision matrix can be computed efficiently. We analyze the Fisher matrix for a small RBM and derive an extremely sparse graphical model which is a good match to the covariance of the sufficient statistics. Our experiments indicate that FANG allows RBMs to be trained more efficiently compared with stochastic gradient descent. Additionally, our analysis yields insight into the surprisingly good performance of the \u201ccentering trick\u201d for training RBMs.",
        "bibtex": "@InProceedings{pmlr-v37-grosse15,\n  title = \t {Scaling up Natural Gradient by Sparsely Factorizing the Inverse Fisher Matrix},\n  author = \t {Grosse, Roger and Salakhudinov, Ruslan},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2304--2313},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/grosse15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/grosse15.html},\n  abstract = \t {Second-order optimization methods, such as natural gradient, are difficult to apply to high-dimensional problems, because they require approximately solving large linear systems. We present FActorized Natural Gradient (FANG), an approximation to natural gradient descent where the Fisher matrix is approximated with a Gaussian graphical model whose precision matrix can be computed efficiently. We analyze the Fisher matrix for a small RBM and derive an extremely sparse graphical model which is a good match to the covariance of the sufficient statistics. Our experiments indicate that FANG allows RBMs to be trained more efficiently compared with stochastic gradient descent. Additionally, our analysis yields insight into the surprisingly good performance of the \u201ccentering trick\u201d for training RBMs.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/grosse15.pdf",
        "supp": "",
        "pdf_size": 533261,
        "gs_citation": 73,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11333774912139078065&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto",
        "aff_domain": "CS.TORONTO.EDU;CS.TORONTO.EDU",
        "email": "CS.TORONTO.EDU;CS.TORONTO.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "b6163c98ed",
        "title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention",
        "site": "https://proceedings.mlr.press/v37/xuc15.html",
        "author": "Kelvin Xu; Jimmy Ba; Ryan Kiros; Kyunghyun Cho; Aaron Courville; Ruslan Salakhudinov; Rich Zemel; Yoshua Bengio",
        "abstract": "Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.",
        "bibtex": "@InProceedings{pmlr-v37-xuc15,\n  title = \t {Show, Attend and Tell: Neural Image Caption Generation with Visual Attention},\n  author = \t {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2048--2057},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/xuc15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/xuc15.html},\n  abstract = \t {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/xuc15.pdf",
        "supp": "",
        "pdf_size": 3126761,
        "gs_citation": 13573,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9471583366007765258&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 24,
        "aff": "Universit\u00e9 de Montr\u00e9al; University of Toronto; University of Toronto; Universit\u00e9 de Montr\u00e9al; Universit\u00e9 de Montr\u00e9al; University of Toronto; University of Toronto; Universit\u00e9 de Montr\u00e9al",
        "aff_domain": "UMONTREAL.CA;PSI.UTORONTO.CA;CS.TORONTO.EDU;UMONTREAL.CA;UMONTREAL.CA;CS.TORONTO.EDU;CS.TORONTO.EDU;UMONTREAL.CA",
        "email": "UMONTREAL.CA;PSI.UTORONTO.CA;CS.TORONTO.EDU;UMONTREAL.CA;UMONTREAL.CA;CS.TORONTO.EDU;CS.TORONTO.EDU;UMONTREAL.CA",
        "github": "",
        "project": "",
        "author_num": 8,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0;0;1;1;0",
        "aff_unique_norm": "Universit\u00e9 de Montr\u00e9al;University of Toronto",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.umontreal.ca;https://www.utoronto.ca",
        "aff_unique_abbr": "UdeM;U of T",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "382cb8f183",
        "title": "Simple regret for infinitely many armed bandits",
        "site": "https://proceedings.mlr.press/v37/carpentier15.html",
        "author": "Alexandra Carpentier; Michal Valko",
        "abstract": "We consider a stochastic bandit problem with infinitely many arms. In this setting, the learner has no chance of trying all the arms even once and has to dedicate its limited number of samples only to a certain number of arms. All previous algorithms for this setting were designed for minimizing the cumulative regret of the learner. In this paper, we propose an algorithm aiming at minimizing the simple regret. As in the cumulative regret setting of infinitely many armed bandits, the rate of the simple regret will depend on a parameter \u03b2characterizing the distribution of the near-optimal arms. We prove that depending on \u03b2, our algorithm is minimax optimal either up to a multiplicative constant or up to a \\log(n) factor. We also provide extensions to several important cases: when \u03b2is unknown, in a natural setting where the near-optimal arms have a small variance, and in the case of unknown time horizon.",
        "bibtex": "@InProceedings{pmlr-v37-carpentier15,\n  title = \t {Simple regret for infinitely many armed bandits},\n  author = \t {Carpentier, Alexandra and Valko, Michal},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1133--1141},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/carpentier15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/carpentier15.html},\n  abstract = \t {We consider a stochastic bandit problem with infinitely many arms. In this setting, the learner has no chance of trying all the arms even once and has to dedicate its limited number of samples only to a certain number of arms. All previous algorithms for this setting were designed for minimizing the cumulative regret of the learner. In this paper, we propose an algorithm aiming at minimizing the simple regret. As in the cumulative regret setting of infinitely many armed bandits, the rate of the simple regret will depend on a parameter \u03b2characterizing the distribution of the near-optimal arms. We prove that depending on \u03b2, our algorithm is minimax optimal either up to a multiplicative constant or up to a \\log(n) factor. We also provide extensions to several important cases: when \u03b2is unknown, in a natural setting where the near-optimal arms have a small variance, and in the case of unknown time horizon.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/carpentier15.pdf",
        "supp": "",
        "pdf_size": 352114,
        "gs_citation": 116,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1631802698519095405&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 20,
        "aff": "Statistical Laboratory, CMS, Wilberforce Road, CB3 0WB, University of Cambridge, United Kingdom; INRIA Lille - Nord Europe, SequeL team, 40 avenue Halley 59650, Villeneuve d\u2019Ascq, France",
        "aff_domain": "STATSLAB.CAM.AC.UK;INRIA.FR",
        "email": "STATSLAB.CAM.AC.UK;INRIA.FR",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Cambridge;INRIA Lille - Nord Europe",
        "aff_unique_dep": "Statistical Laboratory;SequeL team",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.inria.fr/lille-nord-europe",
        "aff_unique_abbr": "Cambridge;INRIA",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Cambridge;Lille",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United Kingdom;France"
    },
    {
        "id": "eefe55e256",
        "title": "Sparse Subspace Clustering with Missing Entries",
        "site": "https://proceedings.mlr.press/v37/yangf15.html",
        "author": "Congyuan Yang; Daniel Robinson; Rene Vidal",
        "abstract": "We consider the problem of clustering incomplete data drawn from a union of subspaces. Classical subspace clustering methods are not applicable to this problem because the data are incomplete, while classical low-rank matrix completion methods may not be applicable because data in multiple subspaces may not be low rank. This paper proposes and evaluates two new approaches for subspace clustering and completion. The first one generalizes the sparse subspace clustering algorithm so that it can obtain a sparse representation of the data using only the observed entries. The second one estimates a suitable kernel matrix by assuming a random model for the missing entries and obtains the sparse representation from this kernel. Experiments on synthetic and real data show the advantages and disadvantages of the proposed methods, which all outperform the natural approach (low-rank matrix completion followed by sparse subspace clustering) when the data matrix is high-rank or the percentage of missing entries is large.",
        "bibtex": "@InProceedings{pmlr-v37-yangf15,\n  title = \t {Sparse Subspace Clustering with Missing Entries},\n  author = \t {Yang, Congyuan and Robinson, Daniel and Vidal, Rene},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2463--2472},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/yangf15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/yangf15.html},\n  abstract = \t {We consider the problem of clustering incomplete data drawn from a union of subspaces. Classical subspace clustering methods are not applicable to this problem because the data are incomplete, while classical low-rank matrix completion methods may not be applicable because data in multiple subspaces may not be low rank. This paper proposes and evaluates two new approaches for subspace clustering and completion. The first one generalizes the sparse subspace clustering algorithm so that it can obtain a sparse representation of the data using only the observed entries. The second one estimates a suitable kernel matrix by assuming a random model for the missing entries and obtains the sparse representation from this kernel. Experiments on synthetic and real data show the advantages and disadvantages of the proposed methods, which all outperform the natural approach (low-rank matrix completion followed by sparse subspace clustering) when the data matrix is high-rank or the percentage of missing entries is large.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/yangf15.pdf",
        "supp": "",
        "pdf_size": 347815,
        "gs_citation": 112,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15649000881416094122&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Johns Hopkins University; Johns Hopkins University; Johns Hopkins University",
        "aff_domain": "JHU.EDU;JHU.EDU;CIS.JHU.EDU",
        "email": "JHU.EDU;JHU.EDU;CIS.JHU.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Johns Hopkins University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.jhu.edu",
        "aff_unique_abbr": "JHU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1b92015eab",
        "title": "Sparse Variational Inference for Generalized GP Models",
        "site": "https://proceedings.mlr.press/v37/sheth15.html",
        "author": "Rishit Sheth; Yuyang Wang; Roni Khardon",
        "abstract": "Gaussian processes (GP) provide an attractive machine learning model due to their non-parametric form, their flexibility to capture many types of observation data, and their generic inference procedures. Sparse GP inference algorithms address the cubic complexity of GPs by focusing on a small set of pseudo-samples. To date, such approaches have focused on the simple case of Gaussian observation likelihoods. This paper develops a variational sparse solution for GPs under general likelihoods by providing a new characterization of the gradients required for inference in terms of individual observation likelihood terms. In addition, we propose a simple new approach for optimizing the sparse variational approximation using a fixed point computation. We demonstrate experimentally that the fixed point operator acts as a contraction in many cases and therefore leads to fast convergence. An experimental evaluation for count regression, classification, and ordinal regression illustrates the generality and advantages of the new approach.",
        "bibtex": "@InProceedings{pmlr-v37-sheth15,\n  title = \t {Sparse Variational Inference for Generalized GP Models},\n  author = \t {Sheth, Rishit and Wang, Yuyang and Khardon, Roni},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1302--1311},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/sheth15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/sheth15.html},\n  abstract = \t {Gaussian processes (GP) provide an attractive machine learning model due to their non-parametric form, their flexibility to capture many types of observation data, and their generic inference procedures. Sparse GP inference algorithms address the cubic complexity of GPs by focusing on a small set of pseudo-samples. To date, such approaches have focused on the simple case of Gaussian observation likelihoods. This paper develops a variational sparse solution for GPs under general likelihoods by providing a new characterization of the gradients required for inference in terms of individual observation likelihood terms. In addition, we propose a simple new approach for optimizing the sparse variational approximation using a fixed point computation. We demonstrate experimentally that the fixed point operator acts as a contraction in many cases and therefore leads to fast convergence. An experimental evaluation for count regression, classification, and ordinal regression illustrates the generality and advantages of the new approach.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/sheth15.pdf",
        "supp": "",
        "pdf_size": 790131,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7478064282599831397&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, Tufts University, Medford, MA 02155, USA; Amazon, 500 9th Ave N, Seattle, WA, USA; Department of Computer Science, Tufts University, Medford, MA 02155, USA",
        "aff_domain": "TUFTS.EDU;GMAIL.COM;CS.TUFTS.EDU",
        "email": "TUFTS.EDU;GMAIL.COM;CS.TUFTS.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Tufts University;Amazon",
        "aff_unique_dep": "Department of Computer Science;Amazon",
        "aff_unique_url": "https://www.tufts.edu;https://www.amazon.com",
        "aff_unique_abbr": "Tufts;Amazon",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Medford;Seattle",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "903b80ebde",
        "title": "Spectral Clustering via the Power Method - Provably",
        "site": "https://proceedings.mlr.press/v37/boutsidis15.html",
        "author": "Christos Boutsidis; Prabhanjan Kambadur; Alex Gittens",
        "abstract": "Spectral clustering is one of the most important algorithms in data mining and machine intelligence; however, its computational complexity limits its application to truly large scale data analysis. The computational bottleneck in spectral clustering is computing a few of the top eigenvectors of the (normalized) Laplacian matrix corresponding to the graph representing the data to be clustered. One way to speed up the computation of these eigenvectors is to use the \u201cpower method\u201d from the numerical linear algebra literature. Although the power method has been empirically used to speed up spectral clustering, the theory behind this approach, to the best of our knowledge, remains unexplored. This paper provides the first such rigorous theoretical justification, arguing that a small number of power iterations suffices to obtain near-optimal partitionings using the approximate eigenvectors. Specifically, we prove that solving the k-means clustering problem on the approximate eigenvectors obtained via the power method gives an additive-error approximation to solving the k-means problem on the optimal eigenvectors.",
        "bibtex": "@InProceedings{pmlr-v37-boutsidis15,\n  title = \t {Spectral Clustering via the Power Method - Provably},\n  author = \t {Boutsidis, Christos and Kambadur, Prabhanjan and Gittens, Alex},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {40--48},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/boutsidis15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/boutsidis15.html},\n  abstract = \t {Spectral clustering is one of the most important algorithms in data mining and machine intelligence; however, its computational complexity limits its application to truly large scale data analysis. The computational bottleneck in spectral clustering is computing a few of the top eigenvectors of the (normalized) Laplacian matrix corresponding to the graph representing the data to be clustered. One way to speed up the computation of these eigenvectors is to use the \u201cpower method\u201d from the numerical linear algebra literature. Although the power method has been empirically used to speed up spectral clustering, the theory behind this approach, to the best of our knowledge, remains unexplored. This paper provides the first such rigorous theoretical justification, arguing that a small number of power iterations suffices to obtain near-optimal partitionings using the approximate eigenvectors. Specifically, we prove that solving the k-means clustering problem on the approximate eigenvectors obtained via the power method gives an additive-error approximation to solving the k-means problem on the optimal eigenvectors.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/boutsidis15.pdf",
        "supp": "",
        "pdf_size": 621129,
        "gs_citation": 81,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4958950872087018252&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Yahoo, 229 West 43rd Street, New York, NY, USA; International Computer Science Institute, Berkeley, CA, USA; Bloomberg L.P., 731 Lexington Avenue, New York, 10022, USA",
        "aff_domain": "YAHOO-INC.COM;ICSI.BERKELEY.EDU;BLOOMBERG.NET",
        "email": "YAHOO-INC.COM;ICSI.BERKELEY.EDU;BLOOMBERG.NET",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Yahoo;International Computer Science Institute;Bloomberg L.P.",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.yahoo.com;https://www.icsi.berkeley.edu/;https://www.bloomberg.com",
        "aff_unique_abbr": "Yahoo;ICSI;Bloomberg",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4fa72933cb",
        "title": "Spectral MLE: Top-K Rank Aggregation from Pairwise Comparisons",
        "site": "https://proceedings.mlr.press/v37/chena15.html",
        "author": "Yuxin Chen; Changho Suh",
        "abstract": "This paper explores the preference-based top-K rank aggregation problem. Suppose that a collection of items is repeatedly compared in pairs, and one wishes to recover a consistent ordering that emphasizes the top-K ranked items, based on partially revealed preferences. We focus on the Bradley-Terry-Luce (BTL) model that postulates a set of latent preference scores underlying all items, where the odds of paired comparisons depend only on the relative scores of the items involved. We characterize the minimax limits on identifiability of top-K ranked items, in the presence of random and non-adaptive sampling. Our results highlight a separation measure that quantifies the gap of preference scores between the K-th and (K+1)-th ranked items. The minimum sample complexity required for reliable top-K ranking scales inversely with the separation measure irrespective of other preference distribution metrics. To approach this minimax limit, we propose a nearly linear-time ranking scheme, called Spectral MLE, that returns the indices of the top-K items in accordance to a careful score estimate. In a nutshell, Spectral MLE starts with an initial score estimate with minimal squared loss (obtained via a spectral method), and then successively refines each component with the assistance of coordinate-wise MLEs. Encouragingly, Spectral MLE allows perfect top-K item identification under minimal sample complexity. The practical applicability of Spectral MLE is further corroborated by numerical experiments.",
        "bibtex": "@InProceedings{pmlr-v37-chena15,\n  title = \t {Spectral MLE: Top-K Rank Aggregation from Pairwise Comparisons},\n  author = \t {Chen, Yuxin and Suh, Changho},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {371--380},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/chena15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/chena15.html},\n  abstract = \t {This paper explores the preference-based top-K rank aggregation problem. Suppose that a collection of items is repeatedly compared in pairs, and one wishes to recover a consistent ordering that emphasizes the top-K ranked items, based on partially revealed preferences. We focus on the Bradley-Terry-Luce (BTL) model that postulates a set of latent preference scores underlying all items, where the odds of paired comparisons depend only on the relative scores of the items involved. We characterize the minimax limits on identifiability of top-K ranked items, in the presence of random and non-adaptive sampling. Our results highlight a separation measure that quantifies the gap of preference scores between the K-th and (K+1)-th ranked items. The minimum sample complexity required for reliable top-K ranking scales inversely with the separation measure irrespective of other preference distribution metrics. To approach this minimax limit, we propose a nearly linear-time ranking scheme, called Spectral MLE, that returns the indices of the top-K items in accordance to a careful score estimate. In a nutshell, Spectral MLE starts with an initial score estimate with minimal squared loss (obtained via a spectral method), and then successively refines each component with the assistance of coordinate-wise MLEs. Encouragingly, Spectral MLE allows perfect top-K item identification under minimal sample complexity. The practical applicability of Spectral MLE is further corroborated by numerical experiments.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/chena15.pdf",
        "supp": "",
        "pdf_size": 363679,
        "gs_citation": 182,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4002674174484746441&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Statistics, Stanford University, Stanford, CA 94305, USA; Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Korea",
        "aff_domain": "STANFORD.EDU;KAIST.AC.KR",
        "email": "STANFORD.EDU;KAIST.AC.KR",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Stanford University;Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": "Department of Statistics;Department of Electrical Engineering",
        "aff_unique_url": "https://www.stanford.edu;https://www.kaist.ac.kr",
        "aff_unique_abbr": "Stanford;KAIST",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;South Korea"
    },
    {
        "id": "7a0845a2b2",
        "title": "Statistical and Algorithmic Perspectives on Randomized Sketching for Ordinary Least-Squares",
        "site": "https://proceedings.mlr.press/v37/raskutti15.html",
        "author": "Garvesh Raskutti; Michael Mahoney",
        "abstract": "We consider statistical and algorithmic aspects of solving large-scale least-squares (LS) problems using randomized sketching algorithms. Prior results show that, from an \\emphalgorithmic perspective, when using sketching matrices constructed from random projections and leverage-score sampling, if the number of samples r much smaller than the original sample size n, then the worst-case (WC) error is the same as solving the original problem, up to a very small relative error. From a \\emphstatistical perspective, one typically considers the mean-squared error performance of randomized sketching algorithms, when data are generated according to a statistical linear model. In this paper, we provide a rigorous comparison of both perspectives leading to insights on how they differ. To do this, we first develop a framework for assessing, in a unified manner, algorithmic and statistical aspects of randomized sketching methods. We then consider the statistical prediction efficiency (PE) and the statistical residual efficiency (RE) of the sketched LS estimator; and we use our framework to provide upper bounds for several types of random projection and random sampling algorithms. Among other results, we show that the RE can be upper bounded when r is much smaller than n, while the PE typically requires the number of samples r to be substantially larger. Lower bounds developed in subsequent work show that our upper bounds on PE can not be improved.",
        "bibtex": "@InProceedings{pmlr-v37-raskutti15,\n  title = \t {Statistical and Algorithmic Perspectives on Randomized Sketching for Ordinary Least-Squares},\n  author = \t {Raskutti, Garvesh and Mahoney, Michael},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {617--625},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/raskutti15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/raskutti15.html},\n  abstract = \t {We consider statistical and algorithmic aspects of solving large-scale least-squares (LS) problems using randomized sketching algorithms. Prior results show that, from an \\emphalgorithmic perspective, when using sketching matrices constructed from random projections and leverage-score sampling, if the number of samples r much smaller than the original sample size n, then the worst-case (WC) error is the same as solving the original problem, up to a very small relative error. From a \\emphstatistical perspective, one typically considers the mean-squared error performance of randomized sketching algorithms, when data are generated according to a statistical linear model. In this paper, we provide a rigorous comparison of both perspectives leading to insights on how they differ. To do this, we first develop a framework for assessing, in a unified manner, algorithmic and statistical aspects of randomized sketching methods. We then consider the statistical prediction efficiency (PE) and the statistical residual efficiency (RE) of the sketched LS estimator; and we use our framework to provide upper bounds for several types of random projection and random sampling algorithms. Among other results, we show that the RE can be upper bounded when r is much smaller than n, while the PE typically requires the number of samples r to be substantially larger. Lower bounds developed in subsequent work show that our upper bounds on PE can not be improved.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/raskutti15.pdf",
        "supp": "",
        "pdf_size": 280891,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17594074231621988847&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "University of Wisconsin, Madison, Department of Statistics, Department of Computer Science, Wisconsin Institute for Discovery, Madison, WI 53706 USA; University of California, Berkeley, ICSI, Department of Statistics, Berkeley, CA 94720 USA",
        "aff_domain": "STAT.WISC.EDU;STAT.BERKELEY.EDU",
        "email": "STAT.WISC.EDU;STAT.BERKELEY.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Wisconsin\u2013Madison;University of California, Berkeley",
        "aff_unique_dep": "Department of Statistics;Department of Statistics",
        "aff_unique_url": "https://www.wisc.edu;https://www.berkeley.edu",
        "aff_unique_abbr": "UW\u2013Madison;UC Berkeley",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Madison;Berkeley",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9b2d50cd46",
        "title": "Stay on path: PCA along graph paths",
        "site": "https://proceedings.mlr.press/v37/asteris15.html",
        "author": "Megasthenis Asteris; Anastasios Kyrillidis; Alex Dimakis; Han-Gyol Yi; Bharath Chandrasekaran",
        "abstract": "We introduce a variant of (sparse) PCA in which the set of feasible support sets is determined by a graph. In particular, we consider the following setting: given a directed acyclic graph G on p vertices corresponding to variables, the non-zero entries of the extracted principal component must coincide with vertices lying along a path in G. From a statistical perspective, information on the underlying network may potentially reduce the number of observations required to recover the population principal component. We consider the canonical estimator which optimally exploits the prior knowledge by solving a non-convex quadratic maximization on the empirical covariance. We introduce a simple network and analyze the estimator under the spiked covariance model for sparse PCA. We show that side information potentially improves the statistical complexity. We propose two algorithms to approximate the solution of the constrained quadratic maximization, and recover a component with the desired properties. We empirically evaluate our schemes on synthetic and real datasets.",
        "bibtex": "@InProceedings{pmlr-v37-asteris15,\n  title = \t {Stay on path: PCA along graph paths},\n  author = \t {Asteris, Megasthenis and Kyrillidis, Anastasios and Dimakis, Alex and Yi, Han-Gyol and Chandrasekaran, Bharath},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1728--1736},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/asteris15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/asteris15.html},\n  abstract = \t {We introduce a variant of (sparse) PCA in which the set of feasible support sets is determined by a graph. In particular, we consider the following setting: given a directed acyclic graph G on p vertices corresponding to variables, the non-zero entries of the extracted principal component must coincide with vertices lying along a path in G. From a statistical perspective, information on the underlying network may potentially reduce the number of observations required to recover the population principal component. We consider the canonical estimator which optimally exploits the prior knowledge by solving a non-convex quadratic maximization on the empirical covariance. We introduce a simple network and analyze the estimator under the spiked covariance model for sparse PCA. We show that side information potentially improves the statistical complexity. We propose two algorithms to approximate the solution of the constrained quadratic maximization, and recover a component with the desired properties. We empirically evaluate our schemes on synthetic and real datasets.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/asteris15.pdf",
        "supp": "",
        "pdf_size": 3004996,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7718210955080169429&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Electrical and Computer Engineering, The University of Texas at Austin; Department of Electrical and Computer Engineering, The University of Texas at Austin; Department of Electrical and Computer Engineering, The University of Texas at Austin; Department of Electrical and Computer Engineering, The University of Texas at Austin; Department of Communication Sciences & Disorders, The University of Texas at Austin",
        "aff_domain": "UTEXAS.EDU;UTEXAS.EDU;AUSTIN.UTEXAS.EDU;UTEXAS.EDU;AUSTIN.UTEXAS.EDU",
        "email": "UTEXAS.EDU;UTEXAS.EDU;AUSTIN.UTEXAS.EDU;UTEXAS.EDU;AUSTIN.UTEXAS.EDU",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "22b72a1468",
        "title": "Stochastic Dual Coordinate Ascent with Adaptive Probabilities",
        "site": "https://proceedings.mlr.press/v37/csiba15.html",
        "author": "Dominik Csiba; Zheng Qu; Peter Richtarik",
        "abstract": "This paper introduces AdaSDCA: an adaptive variant of stochastic dual coordinate ascent (SDCA) for solving the regularized empirical risk minimization problems. Our modification consists in allowing the method adaptively change the probability distribution over the dual variables throughout the iterative process. AdaSDCA achieves provably better complexity bound than SDCA with the best fixed probability distribution, known as importance sampling. However, it is of a theoretical character as it is expensive to implement. We also propose AdaSDCA+: a practical variant which in our experiments outperforms existing non-adaptive methods.",
        "bibtex": "@InProceedings{pmlr-v37-csiba15,\n  title = \t {Stochastic Dual Coordinate Ascent with Adaptive Probabilities},\n  author = \t {Csiba, Dominik and Qu, Zheng and Richtarik, Peter},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {674--683},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/csiba15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/csiba15.html},\n  abstract = \t {This paper introduces AdaSDCA: an adaptive variant of stochastic dual coordinate ascent (SDCA) for solving the regularized empirical risk minimization problems. Our modification consists in allowing the method adaptively change the probability distribution over the dual variables throughout the iterative process. AdaSDCA achieves provably better complexity bound than SDCA with the best fixed probability distribution, known as importance sampling. However, it is of a theoretical character as it is expensive to implement. We also propose AdaSDCA+: a practical variant which in our experiments outperforms existing non-adaptive methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/csiba15.pdf",
        "supp": "",
        "pdf_size": 533241,
        "gs_citation": 107,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14684377702427829889&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "University of Edinburgh; University of Edinburgh; University of Edinburgh",
        "aff_domain": "gmail.com;ed.ac.uk;ed.ac.uk",
        "email": "gmail.com;ed.ac.uk;ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Edinburgh",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ed.ac.uk",
        "aff_unique_abbr": "Edinburgh",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "11996f2072",
        "title": "Stochastic Optimization with Importance Sampling for Regularized Loss Minimization",
        "site": "https://proceedings.mlr.press/v37/zhaoa15.html",
        "author": "Peilin Zhao; Tong Zhang",
        "abstract": "Uniform sampling of training data has been commonly used in traditional stochastic optimization algorithms such as Proximal Stochastic Mirror Descent (prox-SMD) and Proximal Stochastic Dual Coordinate Ascent (prox-SDCA). Although uniform sampling can guarantee that the sampled stochastic quantity is an unbiased estimate of the corresponding true quantity, the resulting estimator may have a rather high variance, which negatively affects the convergence of the underlying optimization procedure. In this paper we study stochastic optimization, including prox-SMD and prox-SDCA, with importance sampling, which improves the convergence rate by reducing the stochastic variance. We theoretically analyze the algorithms and empirically validate their effectiveness.",
        "bibtex": "@InProceedings{pmlr-v37-zhaoa15,\n  title = \t {Stochastic Optimization with Importance Sampling for Regularized Loss Minimization},\n  author = \t {Zhao, Peilin and Zhang, Tong},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1--9},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/zhaoa15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/zhaoa15.html},\n  abstract = \t {Uniform sampling of training data has been commonly used in traditional stochastic optimization algorithms such as Proximal Stochastic Mirror Descent (prox-SMD) and Proximal Stochastic Dual Coordinate Ascent (prox-SDCA). Although uniform sampling can guarantee that the sampled stochastic quantity is an unbiased estimate of the corresponding true quantity, the resulting estimator may have a rather high variance, which negatively affects the convergence of the underlying optimization procedure. In this paper we study stochastic optimization, including prox-SMD and prox-SDCA, with importance sampling, which improves the convergence rate by reducing the stochastic variance. We theoretically analyze the algorithms and empirically validate their effectiveness.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/zhaoa15.pdf",
        "supp": "",
        "pdf_size": 186188,
        "gs_citation": 502,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7983301616188342677&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Data Analytics Department, Institute for Infocomm Research, A*STAR, Singapore+Department of Statistics & Biostatistics, Rutgers University, USA; and Big Data Lab, Baidu Research, China",
        "aff_domain": "I2R.A-STAR.EDU.SG;STAT.RUTGERS.EDU",
        "email": "I2R.A-STAR.EDU.SG;STAT.RUTGERS.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "Institute for Infocomm Research;Rutgers University;Baidu",
        "aff_unique_dep": "Data Analytics Department;Department of Statistics & Biostatistics;Big Data Lab",
        "aff_unique_url": "https://www.i2r.a-star.edu.sg;https://www.rutgers.edu;https://baidu.com",
        "aff_unique_abbr": "I2R;Rutgers;Baidu",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;2",
        "aff_country_unique": "Singapore;United States;China"
    },
    {
        "id": "4d99b71bbe",
        "title": "Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization",
        "site": "https://proceedings.mlr.press/v37/zhanga15.html",
        "author": "Yuchen Zhang; Xiao Lin",
        "abstract": "We consider a generic convex optimization problem associated with regularized empirical risk minimization of linear predictors. The problem structure allows us to reformulate it as a convex-concave saddle point problem. We propose a stochastic primal-dual coordinate method, which alternates between maximizing over one (or more) randomly chosen dual variable and minimizing over the primal variable. We also develop an extension to non-smooth and non-strongly convex loss functions, and an extension with better convergence rate on unnormalized data. Both theoretically and empirically, we show that the SPDC method has comparable or better performance than several state-of-the-art optimization methods.",
        "bibtex": "@InProceedings{pmlr-v37-zhanga15,\n  title = \t {Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization},\n  author = \t {Zhang, Yuchen and Lin, Xiao},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {353--361},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/zhanga15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/zhanga15.html},\n  abstract = \t {We consider a generic convex optimization problem associated with regularized empirical risk minimization of linear predictors. The problem structure allows us to reformulate it as a convex-concave saddle point problem. We propose a stochastic primal-dual coordinate method, which alternates between maximizing over one (or more) randomly chosen dual variable and minimizing over the primal variable. We also develop an extension to non-smooth and non-strongly convex loss functions, and an extension with better convergence rate on unnormalized data. Both theoretically and empirically, we show that the SPDC method has comparable or better performance than several state-of-the-art optimization methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/zhanga15.pdf",
        "supp": "",
        "pdf_size": 435931,
        "gs_citation": 310,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14588707342837179622&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "University of California Berkeley, Berkeley, CA 94720, USA; Microsoft Research, Redmond, WA 98053, USA",
        "aff_domain": "EECS.BERKELEY.EDU;MICROSOFT.COM",
        "email": "EECS.BERKELEY.EDU;MICROSOFT.COM",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of California, Berkeley;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.berkeley.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "UC Berkeley;MSR",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Berkeley;Redmond",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "81524bbb88",
        "title": "Streaming Sparse Principal Component Analysis",
        "site": "https://proceedings.mlr.press/v37/yangd15.html",
        "author": "Wenzhuo Yang; Huan Xu",
        "abstract": "This paper considers estimating the leading k principal components with at most s non-zero attributes from p-dimensional samples collected sequentially in memory limited environments. We develop and analyze two memory and computational efficient algorithms called streaming sparse PCA and streaming sparse ECA for analyzing data generated according to the spike model and the elliptical model respectively. In particular, the proposed algorithms have memory complexity O(pk), computational complexity O(pk mink,slogp) and sample complexity \u0398(s \\log p). We provide their finite sample performance guarantees, which implies statistical consistency in the high dimensional regime. Numerical experiments on synthetic and real-world datasets demonstrate good empirical performance of the proposed algorithms.",
        "bibtex": "@InProceedings{pmlr-v37-yangd15,\n  title = \t {Streaming Sparse Principal Component Analysis},\n  author = \t {Yang, Wenzhuo and Xu, Huan},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {494--503},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/yangd15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/yangd15.html},\n  abstract = \t {This paper considers estimating the leading k principal components with at most s non-zero attributes from p-dimensional samples collected sequentially in memory limited environments. We develop and analyze two memory and computational efficient algorithms called streaming sparse PCA and streaming sparse ECA for analyzing data generated according to the spike model and the elliptical model respectively. In particular, the proposed algorithms have memory complexity O(pk), computational complexity O(pk mink,slogp) and sample complexity \u0398(s \\log p). We provide their finite sample performance guarantees, which implies statistical consistency in the high dimensional regime. Numerical experiments on synthetic and real-world datasets demonstrate good empirical performance of the proposed algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/yangd15.pdf",
        "supp": "",
        "pdf_size": 237140,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9990282404247021689&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Mechanical Engineering, National University of Singapore, Singapore 117576; Department of Mechanical Engineering, National University of Singapore, Singapore 117576",
        "aff_domain": "NUS.EDU.SG;NUS.EDU.SG",
        "email": "NUS.EDU.SG;NUS.EDU.SG",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "National University of Singapore",
        "aff_unique_dep": "Department of Mechanical Engineering",
        "aff_unique_url": "https://www.nus.edu.sg",
        "aff_unique_abbr": "NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "d5a3b41b6d",
        "title": "Strongly Adaptive Online Learning",
        "site": "https://proceedings.mlr.press/v37/daniely15.html",
        "author": "Amit Daniely; Alon Gonen; Shai Shalev-Shwartz",
        "abstract": "Strongly adaptive algorithms are algorithms whose performance on every time interval is close to optimal. We present a reduction that can transform standard low-regret algorithms to strongly adaptive. As a consequence, we derive simple, yet efficient, strongly adaptive algorithms for a handful of problems.",
        "bibtex": "@InProceedings{pmlr-v37-daniely15,\n  title = \t {Strongly Adaptive Online Learning},\n  author = \t {Daniely, Amit and Gonen, Alon and Shalev-Shwartz, Shai},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1405--1411},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/daniely15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/daniely15.html},\n  abstract = \t {Strongly adaptive algorithms are algorithms whose performance on every time interval is close to optimal. We present a reduction that can transform standard low-regret algorithms to strongly adaptive. As a consequence, we derive simple, yet efficient, strongly adaptive algorithms for a handful of problems.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/daniely15.pdf",
        "supp": "",
        "pdf_size": 313672,
        "gs_citation": 205,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3543359477551667492&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "The Hebrew University; The Hebrew University; The Hebrew University",
        "aff_domain": "mail.huji.ac.il;cs.huji.ac.il;cs.huji.ac.il",
        "email": "mail.huji.ac.il;cs.huji.ac.il;cs.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Hebrew University of Jerusalem",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.huji.ac.il",
        "aff_unique_abbr": "HUJI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "c7048a8aea",
        "title": "Structural Maxent Models",
        "site": "https://proceedings.mlr.press/v37/cortes15.html",
        "author": "Corinna Cortes; Vitaly Kuznetsov; Mehryar Mohri; Umar Syed",
        "abstract": "We present a new class of density estimation models, Structural Maxent models, with feature functions selected from possibly very complex families. The design of our models is motivated by data-dependent convergence bounds and benefits from new data-dependent learning bounds expressed in terms of the Rademacher complexities of the sub-families composing the family of features considered. We prove a duality theorem, which we use to derive our Structural Maxent algorithm. We give a full description of our algorithm, including the details of its derivation and report the results of several experiments demonstrating that its performance compares favorably to that of existing regularized Maxent. We further similarly define conditional Structural Maxent models for multi-class classification problems. These are conditional probability models making use of possibly complex feature families. We also prove a duality theorem for these models which shows the connection between these models and existing binary and multi-class deep boosting algorithms.",
        "bibtex": "@InProceedings{pmlr-v37-cortes15,\n  title = \t {Structural Maxent Models},\n  author = \t {Cortes, Corinna and Kuznetsov, Vitaly and Mohri, Mehryar and Syed, Umar},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {391--399},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/cortes15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/cortes15.html},\n  abstract = \t {We present a new class of density estimation models, Structural Maxent models, with feature functions selected from possibly very complex families. The design of our models is motivated by data-dependent convergence bounds and benefits from new data-dependent learning bounds expressed in terms of the Rademacher complexities of the sub-families composing the family of features considered. We prove a duality theorem, which we use to derive our Structural Maxent algorithm. We give a full description of our algorithm, including the details of its derivation and report the results of several experiments demonstrating that its performance compares favorably to that of existing regularized Maxent. We further similarly define conditional Structural Maxent models for multi-class classification problems. These are conditional probability models making use of possibly complex feature families. We also prove a duality theorem for these models which shows the connection between these models and existing binary and multi-class deep boosting algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/cortes15.pdf",
        "supp": "",
        "pdf_size": 425540,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6410153826036870711&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Google Research; Courant Institute of Mathematical Sciences; Courant Institute and Google Research; Google Research",
        "aff_domain": "GOOGLE.COM;CIMS.NYU.EDU;CIMS.NYU.EDU;GOOGLE.COM",
        "email": "GOOGLE.COM;CIMS.NYU.EDU;CIMS.NYU.EDU;GOOGLE.COM",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Google;Courant Institute of Mathematical Sciences;Courant Institute",
        "aff_unique_dep": "Google Research;Mathematical Sciences;Courant Institute",
        "aff_unique_url": "https://research.google;https://cims.nyu.edu;https://courant.nyu.edu",
        "aff_unique_abbr": "Google Research;CIMS;Courant",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f36776b694",
        "title": "Submodularity in Data Subset Selection and Active Learning",
        "site": "https://proceedings.mlr.press/v37/wei15.html",
        "author": "Kai Wei; Rishabh Iyer; Jeff Bilmes",
        "abstract": "We study the problem of selecting a subset of big data to train a classifier while incurring minimal performance loss. We show the connection of submodularity to the data likelihood functions for Naive Bayes (NB) and Nearest Neighbor (NN) classifiers, and formulate the data subset selection problems for these classifiers as constrained submodular maximization. Furthermore, we apply this framework to active learning and propose a novel scheme filtering active submodular selection (FASS), where we combine the uncertainty sampling method with a submodular data subset selection framework. We extensively evaluate the proposed framework on text categorization and handwritten digit recognition tasks with four different classifiers, including Deep Neural Network (DNN) based classifiers. Empirical results indicate that the proposed framework yields significant improvement over the state-of-the-art algorithms on all classifiers.",
        "bibtex": "@InProceedings{pmlr-v37-wei15,\n  title = \t {Submodularity in Data Subset Selection and Active Learning},\n  author = \t {Wei, Kai and Iyer, Rishabh and Bilmes, Jeff},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1954--1963},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/wei15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/wei15.html},\n  abstract = \t {We study the problem of selecting a subset of big data to train a classifier while incurring minimal performance loss. We show the connection of submodularity to the data likelihood functions for Naive Bayes (NB) and Nearest Neighbor (NN) classifiers, and formulate the data subset selection problems for these classifiers as constrained submodular maximization. Furthermore, we apply this framework to active learning and propose a novel scheme filtering active submodular selection (FASS), where we combine the uncertainty sampling method with a submodular data subset selection framework. We extensively evaluate the proposed framework on text categorization and handwritten digit recognition tasks with four different classifiers, including Deep Neural Network (DNN) based classifiers. Empirical results indicate that the proposed framework yields significant improvement over the state-of-the-art algorithms on all classifiers.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/wei15.pdf",
        "supp": "",
        "pdf_size": 407117,
        "gs_citation": 501,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4791736610090089126&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Washington, Seattle, WA 98195, USA; University of Washington, Seattle, WA 98195, USA; University of Washington, Seattle, WA 98195, USA",
        "aff_domain": "U.WASHINGTON.EDU;U.WASHINGTON.EDU;U.WASHINGTON.EDU",
        "email": "U.WASHINGTON.EDU;U.WASHINGTON.EDU;U.WASHINGTON.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "71cfe6d2cf",
        "title": "Subsampling Methods for Persistent Homology",
        "site": "https://proceedings.mlr.press/v37/chazal15.html",
        "author": "Frederic Chazal; Brittany Fasy; Fabrizio Lecci; Bertrand Michel; Alessandro Rinaldo; Larry Wasserman",
        "abstract": "Persistent homology is a multiscale method for analyzing the shape of sets and functions from point cloud data arising from an unknown distribution supported on those sets. When the size of the sample is large, direct computation of the persistent homology is prohibitive due to the combinatorial nature of the existing algorithms. We propose to compute the persistent homology of several subsamples of the data and then combine the resulting estimates. We study the risk of two estimators and we prove that the subsampling approach carries stable topological information while achieving a great reduction in computational complexity.",
        "bibtex": "@InProceedings{pmlr-v37-chazal15,\n  title = \t {Subsampling Methods for Persistent Homology},\n  author = \t {Chazal, Frederic and Fasy, Brittany and Lecci, Fabrizio and Michel, Bertrand and Rinaldo, Alessandro and Wasserman, Larry},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2143--2151},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/chazal15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/chazal15.html},\n  abstract = \t {Persistent homology is a multiscale method for analyzing the shape of sets and functions from point cloud data arising from an unknown distribution supported on those sets. When the size of the sample is large, direct computation of the persistent homology is prohibitive due to the combinatorial nature of the existing algorithms. We propose to compute the persistent homology of several subsamples of the data and then combine the resulting estimates. We study the risk of two estimators and we prove that the subsampling approach carries stable topological information while achieving a great reduction in computational complexity.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/chazal15.pdf",
        "supp": "",
        "pdf_size": 1198757,
        "gs_citation": 151,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3514087802765062985&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "INRIA Saclay, Palaiseau, 91120, France; Computer Science Department, Tulane University, New Orleans, LA 70118; Department of Statistics, Carnegie Mellon University, Pittsburgh, PA 15213; LSTA, Universit\u00e9 Pierre et Marie Curie (UPMC), Paris, 75005, France; Department of Statistics, Carnegie Mellon University, Pittsburgh, PA 15213; Department of Statistics, Carnegie Mellon University, Pittsburgh, PA 15213",
        "aff_domain": "INRIA.FR;FASY.US;CMU.EDU;UPMC.FR;CMU.EDU;CMU.EDU",
        "email": "INRIA.FR;FASY.US;CMU.EDU;UPMC.FR;CMU.EDU;CMU.EDU",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3;2;2",
        "aff_unique_norm": "INRIA;Tulane University;Carnegie Mellon University;Universit\u00e9 Pierre et Marie Curie",
        "aff_unique_dep": ";Computer Science Department;Department of Statistics;LSTA",
        "aff_unique_url": "https://www.inria.fr;https://www.tulane.edu;https://www.cmu.edu;https://www.upmc.fr",
        "aff_unique_abbr": "INRIA;Tulane;CMU;UPMC",
        "aff_campus_unique_index": "0;1;2;3;2;2",
        "aff_campus_unique": "Saclay;New Orleans;Pittsburgh;Paris",
        "aff_country_unique_index": "0;1;1;0;1;1",
        "aff_country_unique": "France;United States"
    },
    {
        "id": "2392dcc029",
        "title": "Support Matrix Machines",
        "site": "https://proceedings.mlr.press/v37/luo15.html",
        "author": "Luo Luo; Yubo Xie; Zhihua Zhang; Wu-Jun Li",
        "abstract": "In many classification problems such as electroencephalogram (EEG) classification and image classification, the input features are naturally represented as matrices rather than vectors or scalars. In general, the structure information of the original feature matrix is useful and informative for data analysis tasks such as classification. One typical structure information is the correlation between columns or rows in the feature matrix. To leverage this kind of structure information, we propose a new classification method that we call support matrix machine (SMM). Specifically, SMM is defined as a hinge loss plus a so-called spectral elastic net penalty which is a spectral extension of the conventional elastic net over a matrix. The spectral elastic net enjoys a property of grouping effect, i.e., strongly correlated columns or rows tend to be selected altogether or not. Since the optimization problem for SMM is convex, this encourages us to devise an alternating direction method of multipliers algorithm for solving the problem. Experimental results on EEG and face image classification data show that our model is more robust and efficient than the state-of-the-art methods.",
        "bibtex": "@InProceedings{pmlr-v37-luo15,\n  title = \t {Support Matrix Machines},\n  author = \t {Luo, Luo and Xie, Yubo and Zhang, Zhihua and Li, Wu-Jun},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {938--947},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/luo15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/luo15.html},\n  abstract = \t {In many classification problems such as electroencephalogram (EEG) classification and image classification, the input features are naturally represented as matrices rather than vectors or scalars. In general, the structure information of the original feature matrix is useful and informative for data analysis tasks such as classification. One typical structure information is the correlation between columns or rows in the feature matrix. To leverage this kind of structure information, we propose a new classification method that we call support matrix machine (SMM). Specifically, SMM is defined as a hinge loss plus a so-called spectral elastic net penalty which is a spectral extension of the conventional elastic net over a matrix. The spectral elastic net enjoys a property of grouping effect, i.e., strongly correlated columns or rows tend to be selected altogether or not. Since the optimization problem for SMM is convex, this encourages us to devise an alternating direction method of multipliers algorithm for solving the problem. Experimental results on EEG and face image classification data show that our model is more robust and efficient than the state-of-the-art methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/luo15.pdf",
        "supp": "",
        "pdf_size": 448198,
        "gs_citation": 144,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5859218659478013565&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, China; Institute of Data Science, Department of Computer Science and Engineering, Shanghai Jiao Tong University, China; National Key Laboratory for Novel Software Technology, Collaborative Innovation Center of Novel Software Technology and Industrialization, Department of Computer Science and Technology, Nanjing University, China",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;nju.edu.cn",
        "email": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;nju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Shanghai Jiao Tong University;Nanjing University",
        "aff_unique_dep": "Department of Computer Science and Engineering;Department of Computer Science and Technology",
        "aff_unique_url": "https://www.sjtu.edu.cn;http://www.nju.edu.cn",
        "aff_unique_abbr": "SJTU;Nanjing U",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Shanghai;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "30ac6fe512",
        "title": "Surrogate Functions for Maximizing Precision at the Top",
        "site": "https://proceedings.mlr.press/v37/kar15.html",
        "author": "Purushottam Kar; Harikrishna Narasimhan; Prateek Jain",
        "abstract": "The problem of maximizing precision at the top of a ranked list, often dubbed Precision@k (prec@k), finds relevance in myriad learning applications such as ranking, multi-label classification, and learning with severe label imbalance. However, despite its popularity, there exist significant gaps in our understanding of this problem and its associated performance measure. The most notable of these is the lack of a convex upper bounding surrogate for prec@k. We also lack scalable perceptron and stochastic gradient descent algorithms for optimizing this performance measure. In this paper we make key contributions in these directions. At the heart of our results is a family of truly upper bounding surrogates for prec@k. These surrogates are motivated in a principled manner and enjoy attractive properties such as consistency to prec@k under various natural margin/noise conditions. These surrogates are then used to design a class of novel perceptron algorithms for optimizing prec@k with provable mistake bounds. We also devise scalable stochastic gradient descent style methods for this problem with provable convergence bounds. Our proofs rely on novel uniform convergence bounds which require an in-depth analysis of the structural properties of prec@k and its surrogates. We conclude with experimental results comparing our algorithms with state-of-the-art cutting plane and stochastic gradient algorithms for maximizing prec@k.",
        "bibtex": "@InProceedings{pmlr-v37-kar15,\n  title = \t {Surrogate Functions for Maximizing Precision at the Top},\n  author = \t {Kar, Purushottam and Narasimhan, Harikrishna and Jain, Prateek},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {189--198},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/kar15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/kar15.html},\n  abstract = \t {The problem of maximizing precision at the top of a ranked list, often dubbed Precision@k (prec@k), finds relevance in myriad learning applications such as ranking, multi-label classification, and learning with severe label imbalance. However, despite its popularity, there exist significant gaps in our understanding of this problem and its associated performance measure. The most notable of these is the lack of a convex upper bounding surrogate for prec@k. We also lack scalable perceptron and stochastic gradient descent algorithms for optimizing this performance measure. In this paper we make key contributions in these directions. At the heart of our results is a family of truly upper bounding surrogates for prec@k. These surrogates are motivated in a principled manner and enjoy attractive properties such as consistency to prec@k under various natural margin/noise conditions. These surrogates are then used to design a class of novel perceptron algorithms for optimizing prec@k with provable mistake bounds. We also devise scalable stochastic gradient descent style methods for this problem with provable convergence bounds. Our proofs rely on novel uniform convergence bounds which require an in-depth analysis of the structural properties of prec@k and its surrogates. We conclude with experimental results comparing our algorithms with state-of-the-art cutting plane and stochastic gradient algorithms for maximizing prec@k.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/kar15.pdf",
        "supp": "",
        "pdf_size": 444825,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14805076092166553217&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Microsoft Research, INDIA; Indian Institute of Science, Bangalore, INDIA + Microsoft Research, INDIA; Microsoft Research, INDIA",
        "aff_domain": "MICROSOFT.COM;CSA.IISC.ERNET.IN;MICROSOFT.COM",
        "email": "MICROSOFT.COM;CSA.IISC.ERNET.IN;MICROSOFT.COM",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;0",
        "aff_unique_norm": "Microsoft;Indian Institute of Science",
        "aff_unique_dep": "Microsoft Research;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://www.iisc.ac.in",
        "aff_unique_abbr": "MSR;IISc",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Bangalore",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "4d278cd576",
        "title": "Swept Approximate Message Passing for Sparse Estimation",
        "site": "https://proceedings.mlr.press/v37/manoel15.html",
        "author": "Andre Manoel; Florent Krzakala; Eric Tramel; Lenka Zdeborov\u00e0",
        "abstract": "Approximate Message Passing (AMP) has been shown to be a superior method for inference problems, such as the recovery of signals from sets of noisy, lower-dimensionality measurements, both in terms of reconstruction accuracy and in computational efficiency. However, AMP suffers from serious convergence issues in contexts that do not exactly match its assumptions. We propose a new approach to stabilizing AMP in these contexts by applying AMP updates to individual coefficients rather than in parallel. Our results show that this change to the AMP iteration can provide theoretically expected, but hitherto unobtainable, performance for problems on which the standard AMP iteration diverges. Additionally, we find that the computational costs of this swept coefficient update scheme is not unduly burdensome, allowing it to be applied efficiently to signals of large dimensionality.",
        "bibtex": "@InProceedings{pmlr-v37-manoel15,\n  title = \t {Swept Approximate Message Passing for Sparse Estimation},\n  author = \t {Manoel, Andre and Krzakala, Florent and Tramel, Eric and Zdeborov\u00e0, Lenka},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1123--1132},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/manoel15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/manoel15.html},\n  abstract = \t {Approximate Message Passing (AMP) has been shown to be a superior method for inference problems, such as the recovery of signals from sets of noisy, lower-dimensionality measurements, both in terms of reconstruction accuracy and in computational efficiency. However, AMP suffers from serious convergence issues in contexts that do not exactly match its assumptions. We propose a new approach to stabilizing AMP in these contexts by applying AMP updates to individual coefficients rather than in parallel. Our results show that this change to the AMP iteration can provide theoretically expected, but hitherto unobtainable, performance for problems on which the standard AMP iteration diverges. Additionally, we find that the computational costs of this swept coefficient update scheme is not unduly burdensome, allowing it to be applied efficiently to signals of large dimensionality.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/manoel15.pdf",
        "supp": "",
        "pdf_size": 534317,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4851264077296563587&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Institute of Physics, University of S \u02dcao Paulo; Universit \u00b4e Pierre et Marie Curie and \u00b4Ecole Normale Sup \u00b4erieure; \u00b4Ecole Normale Sup \u00b4erieure; Institut de Physique Th \u00b4eorique, CEA Saclay and CNRS URA 2306",
        "aff_domain": "IF.USP.BR;ENS.FR;LPS.ENS.FR;GMAIL.COM",
        "email": "IF.USP.BR;ENS.FR;LPS.ENS.FR;GMAIL.COM",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "University of S Paulo;Universit\u00e9 Pierre et Marie Curie;Ecole Normale Sup\u00e9rieure;CEA Saclay",
        "aff_unique_dep": "Institute of Physics;;;Institut de Physique Th\u00e9orique",
        "aff_unique_url": "https://www.usp.br;https://www.upmc.fr;https://www.ens.fr;https://www.cea.fr",
        "aff_unique_abbr": "USP;UPMC;ENS;CEA",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Sao Paulo;",
        "aff_country_unique_index": "0;1;1;1",
        "aff_country_unique": "Brazil;France"
    },
    {
        "id": "c4e41ecb29",
        "title": "Telling cause from effect in deterministic linear dynamical systems",
        "site": "https://proceedings.mlr.press/v37/shajarisales15.html",
        "author": "Naji Shajarisales; Dominik Janzing; Bernhard Schoelkopf; Michel Besserve",
        "abstract": "Telling a cause from its effect using observed time series data is a major challenge in natural and social sciences. Assuming the effect is generated by the cause through a linear system, we propose a new approach based on the hypothesis that nature chooses the \u201ccause\u201d and the \u201cmechanism generating the effect from the cause\u201d independently of each other. Specifically we postulate that the power spectrum of the \u201ccause\u201d time series is uncorrelated with the square of the frequency response of the linear filter (system) generating the effect. While most causal discovery methods for time series mainly rely on the noise, our method relies on asymmetries of the power spectral density properties that exist even in deterministic systems. We describe mathematical assumptions in a deterministic model under which the causal direction is identifiable. In particular, we show a scenario where the method works but Granger causality fails. Experiments show encouraging results on synthetic as well as real-world data. Overall, this suggests that the postulate of Independence of Cause and Mechanism is a promising principle for causal inference on observed time series.",
        "bibtex": "@InProceedings{pmlr-v37-shajarisales15,\n  title = \t {Telling cause from effect in deterministic linear dynamical systems},\n  author = \t {Shajarisales, Naji and Janzing, Dominik and Schoelkopf, Bernhard and Besserve, Michel},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {285--294},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/shajarisales15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/shajarisales15.html},\n  abstract = \t {Telling a cause from its effect using observed time series data is a major challenge in natural and social sciences. Assuming the effect is generated by the cause through a linear system, we propose a new approach based on the hypothesis that nature chooses the \u201ccause\u201d and the \u201cmechanism generating the effect from the cause\u201d independently of each other. Specifically we postulate that the power spectrum of the \u201ccause\u201d time series is uncorrelated with the square of the frequency response of the linear filter (system) generating the effect. While most causal discovery methods for time series mainly rely on the noise, our method relies on asymmetries of the power spectral density properties that exist even in deterministic systems. We describe mathematical assumptions in a deterministic model under which the causal direction is identifiable. In particular, we show a scenario where the method works but Granger causality fails. Experiments show encouraging results on synthetic as well as real-world data. Overall, this suggests that the postulate of Independence of Cause and Mechanism is a promising principle for causal inference on observed time series.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/shajarisales15.pdf",
        "supp": "",
        "pdf_size": 832577,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3691344447344898558&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "MPI for Intelligent Systems, Tuebingen, Germany; MPI for Intelligent Systems, Tuebingen, Germany; MPI for Intelligent Systems, Tuebingen, Germany; MPI for Intelligent Systems, Tuebingen, Germany+MPI for Biological Cybernetics, Tuebingen, Germany",
        "aff_domain": "TUEBINGEN.MPG.DE;TUEBINGEN.MPG.DE;TUEBINGEN.MPG.DE;TUEBINGEN.MPG.DE",
        "email": "TUEBINGEN.MPG.DE;TUEBINGEN.MPG.DE;TUEBINGEN.MPG.DE;TUEBINGEN.MPG.DE",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0+1",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems;Max Planck Institute for Biological Cybernetics",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.mpituebingen.mpg.de;https://www.biological-cybernetics.de",
        "aff_unique_abbr": "MPI-IS;MPIBC",
        "aff_campus_unique_index": "0;0;0;0+0",
        "aff_campus_unique": "Tuebingen",
        "aff_country_unique_index": "0;0;0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "e526b9d681",
        "title": "The Benefits of Learning with Strongly Convex Approximate Inference",
        "site": "https://proceedings.mlr.press/v37/london15.html",
        "author": "Ben London; Bert Huang; Lise Getoor",
        "abstract": "We explore the benefits of strongly convex free energies in variational inference, providing both theoretical motivation and a new meta-algorithm. Using the duality between strong convexity and stability, we prove a high-probability bound on the error of learned marginals that is inversely proportional to the modulus of convexity of the free energy, thereby motivating free energies whose moduli are constant with respect to the size of the graph. We identify sufficient conditions for \u03a9(1)-strong convexity in two popular variational techniques: tree-reweighted and counting number entropies. Our insights for the latter suggest a novel counting number optimization framework, which guarantees strong convexity for any given modulus. Our experiments demonstrate that learning with a strongly convex free energy, using our optimization framework to guarantee a given modulus, results in substantially more accurate marginal probabilities, thereby validating our theoretical claims and the effectiveness of our framework.",
        "bibtex": "@InProceedings{pmlr-v37-london15,\n  title = \t {The Benefits of Learning with Strongly Convex Approximate Inference},\n  author = \t {London, Ben and Huang, Bert and Getoor, Lise},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {410--418},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/london15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/london15.html},\n  abstract = \t {We explore the benefits of strongly convex free energies in variational inference, providing both theoretical motivation and a new meta-algorithm. Using the duality between strong convexity and stability, we prove a high-probability bound on the error of learned marginals that is inversely proportional to the modulus of convexity of the free energy, thereby motivating free energies whose moduli are constant with respect to the size of the graph. We identify sufficient conditions for \u03a9(1)-strong convexity in two popular variational techniques: tree-reweighted and counting number entropies. Our insights for the latter suggest a novel counting number optimization framework, which guarantees strong convexity for any given modulus. Our experiments demonstrate that learning with a strongly convex free energy, using our optimization framework to guarantee a given modulus, results in substantially more accurate marginal probabilities, thereby validating our theoretical claims and the effectiveness of our framework.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/london15.pdf",
        "supp": "",
        "pdf_size": 343156,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8980616768077171300&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "University of Maryland, College Park, MD 20742 USA; Virginia Tech, Blacksburg, VA 24061 USA; University of California, Santa Cruz, CA 95064 USA",
        "aff_domain": "CS.UMD.EDU;VT.EDU;SOE.UCSC.EDU",
        "email": "CS.UMD.EDU;VT.EDU;SOE.UCSC.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Maryland;Virginia Tech;University of California, Santa Cruz",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www/umd.edu;https://www.vt.edu;https://www.ucsc.edu",
        "aff_unique_abbr": "UMD;VT;UCSC",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "College Park;Blacksburg;Santa Cruz",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c579c5cc20",
        "title": "The Composition Theorem for Differential Privacy",
        "site": "https://proceedings.mlr.press/v37/kairouz15.html",
        "author": "Peter Kairouz; Sewoong Oh; Pramod Viswanath",
        "abstract": "Interactive querying of a database degrades the privacy level. In this paper we answer the fundamental question of characterizing the level of privacy degradation as a function of the number of adaptive interactions and the differential privacy levels maintained by the individual queries. Our solution is complete: the privacy degradation guarantee is true for every privacy mechanism, and further, we demonstrate a sequence of privacy mechanisms that do degrade in the characterized manner. The key innovation is the introduction of an operational interpretation (involving hypothesis testing) to differential privacy and the use of the corresponding data processing inequalities. Our result improves over the state of the art and has immediate applications to several problems studied in the literature.",
        "bibtex": "@InProceedings{pmlr-v37-kairouz15,\n  title = \t {The Composition Theorem for Differential Privacy},\n  author = \t {Kairouz, Peter and Oh, Sewoong and Viswanath, Pramod},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1376--1385},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/kairouz15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/kairouz15.html},\n  abstract = \t {Interactive querying of a database degrades the privacy level. In this paper we answer the fundamental question of characterizing the level of privacy degradation as a function of the number of adaptive interactions and the differential privacy levels maintained by the individual queries. Our solution is complete: the privacy degradation guarantee is true for every privacy mechanism, and further, we demonstrate a sequence of privacy mechanisms that do degrade in the characterized manner. The key innovation is the introduction of an operational interpretation (involving hypothesis testing) to differential privacy and the use of the corresponding data processing inequalities. Our result improves over the state of the art and has immediate applications to several problems studied in the literature.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/kairouz15.pdf",
        "supp": "",
        "pdf_size": 501140,
        "gs_citation": 890,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8105748252377660223&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 23,
        "aff": "ECE Department, University of Illinois at Urbana-Champaign; IESE Department, University of Illinois at Urbana-Champaign; ECE Department, University of Illinois at Urbana-Champaign",
        "aff_domain": "ILLINOIS.EDU;ILLINOIS.EDU;ILLINOIS.EDU",
        "email": "ILLINOIS.EDU;ILLINOIS.EDU;ILLINOIS.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign",
        "aff_unique_dep": "ECE Department",
        "aff_unique_url": "https://illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Urbana-Champaign",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "bb17d526a9",
        "title": "The Fundamental Incompatibility of Scalable Hamiltonian Monte Carlo and Naive Data Subsampling",
        "site": "https://proceedings.mlr.press/v37/betancourt15.html",
        "author": "Michael Betancourt",
        "abstract": "Leveraging the coherent exploration of Hamiltonian flow, Hamiltonian Monte Carlo produces computationally efficient Monte Carlo estimators, even with respect to complex and high-dimensional target distributions. When confronted with data-intensive applications, however, the algorithm may be too expensive to implement, leaving us to consider the utility of approximations such as data subsampling. In this paper I demonstrate how data subsampling fundamentally compromises the scalability of Hamiltonian Monte Carlo.",
        "bibtex": "@InProceedings{pmlr-v37-betancourt15,\n  title = \t {The Fundamental Incompatibility of Scalable Hamiltonian Monte Carlo and Naive Data Subsampling},\n  author = \t {Betancourt, Michael},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {533--540},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/betancourt15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/betancourt15.html},\n  abstract = \t {Leveraging the coherent exploration of Hamiltonian flow, Hamiltonian Monte Carlo produces computationally efficient Monte Carlo estimators, even with respect to complex and high-dimensional target distributions. When confronted with data-intensive applications, however, the algorithm may be too expensive to implement, leaving us to consider the utility of approximations such as data subsampling. In this paper I demonstrate how data subsampling fundamentally compromises the scalability of Hamiltonian Monte Carlo.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/betancourt15.pdf",
        "supp": "",
        "pdf_size": 290122,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15276273057304603476&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Statistics, University of Warwick, Coventry, UK CV4 7AL",
        "aff_domain": "betanalphabetagmail.com",
        "email": "betanalphabetagmail.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Warwick",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://warwick.ac.uk",
        "aff_unique_abbr": "Warwick",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Coventry",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "54c61cc2ec",
        "title": "The Hedge Algorithm on a Continuum",
        "site": "https://proceedings.mlr.press/v37/krichene15.html",
        "author": "Walid Krichene; Maximilian Balandat; Claire Tomlin; Alexandre Bayen",
        "abstract": "We consider an online optimization problem on a subset S of R^n (not necessarily convex), in which a decision maker chooses, at each iteration t, a probability distribution x^(t) over S, and seeks to minimize a cumulative expected loss, where each loss is a Lipschitz function revealed at the end of iteration t. Building on previous work, we propose a generalized Hedge algorithm and show a O(\\sqrtt \\log t) bound on the regret when the losses are uniformly Lipschitz and S is uniformly fat (a weaker condition than convexity). Finally, we propose a generalization to the dual averaging method on the set of Lebesgue-continuous distributions over S.",
        "bibtex": "@InProceedings{pmlr-v37-krichene15,\n  title = \t {The Hedge Algorithm on a Continuum},\n  author = \t {Krichene, Walid and Balandat, Maximilian and Tomlin, Claire and Bayen, Alexandre},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {824--832},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/krichene15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/krichene15.html},\n  abstract = \t {We consider an online optimization problem on a subset S of R^n (not necessarily convex), in which a decision maker chooses, at each iteration t, a probability distribution x^(t) over S, and seeks to minimize a cumulative expected loss, where each loss is a Lipschitz function revealed at the end of iteration t. Building on previous work, we propose a generalized Hedge algorithm and show a O(\\sqrtt \\log t) bound on the regret when the losses are uniformly Lipschitz and S is uniformly fat (a weaker condition than convexity). Finally, we propose a generalization to the dual averaging method on the set of Lebesgue-continuous distributions over S.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/krichene15.pdf",
        "supp": "",
        "pdf_size": 726272,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2502573298843724387&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "University of California, 652 Sutardja Dai Hall, Berkeley, CA 94720 USA; University of California, 736 Sutardja Dai Hall, Berkeley, CA 94720 USA; University of California, 721 Sutardja Dai Hall, Berkeley, CA 94720 USA; University of California, 642 Sutardja Dai Hall, Berkeley, CA 94720 USA",
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu;berkeley.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "14d692bf23",
        "title": "The Kendall and Mallows Kernels for Permutations",
        "site": "https://proceedings.mlr.press/v37/jiao15.html",
        "author": "Yunlong Jiao; Jean-Philippe Vert",
        "abstract": "We show that the widely used Kendall tau correlation coefficient is a positive definite kernel for permutations. It offers a computationally attractive alternative to more complex kernels on the symmetric group to learn from rankings, or to learn to rank. We show how to extend it to partial rankings or rankings with uncertainty, and demonstrate promising results on high-dimensional classification problems in biomedical applications.",
        "bibtex": "@InProceedings{pmlr-v37-jiao15,\n  title = \t {The Kendall and Mallows Kernels for Permutations},\n  author = \t {Jiao, Yunlong and Vert, Jean-Philippe},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1935--1944},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/jiao15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/jiao15.html},\n  abstract = \t {We show that the widely used Kendall tau correlation coefficient is a positive definite kernel for permutations. It offers a computationally attractive alternative to more complex kernels on the symmetric group to learn from rankings, or to learn to rank. We show how to extend it to partial rankings or rankings with uncertainty, and demonstrate promising results on high-dimensional classification problems in biomedical applications.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/jiao15.pdf",
        "supp": "",
        "pdf_size": 447611,
        "gs_citation": 81,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17755074249743699746&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff": "MINES ParisTech \u2013 CBIO, PSL Research University, Institut Curie, INSERM U900, Paris, France; MINES ParisTech \u2013 CBIO, PSL Research University, Institut Curie, INSERM U900, Paris, France",
        "aff_domain": "mines-paristech.fr;mines-paristech.fr",
        "email": "mines-paristech.fr;mines-paristech.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "MINES ParisTech",
        "aff_unique_dep": "CBIO",
        "aff_unique_url": "https://www.minesparistech.fr",
        "aff_unique_abbr": "MPT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Paris",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "c0cfdae628",
        "title": "The Ladder: A Reliable Leaderboard for Machine Learning Competitions",
        "site": "https://proceedings.mlr.press/v37/blum15.html",
        "author": "Avrim Blum; Moritz Hardt",
        "abstract": "The organizer of a machine learning competition faces the problem of maintaining an accurate leaderboard that faithfully represents the quality of the best submission of each competing team. What makes this estimation problem particularly challenging is its sequential and adaptive nature. As participants are allowed to repeatedly evaluate their submissions on the leaderboard, they may begin to overfit to the holdout data that supports the leaderboard. Few theoretical results give actionable advice on how to design a reliable leaderboard. Existing approaches therefore often resort to poorly understood heuristics such as limiting the bit precision of answers and the rate of re-submission. In this work, we introduce a notion of leaderboard accuracy tailored to the format of a competition. We introduce a natural algorithm called the Ladder and demonstrate that it simultaneously supports strong theoretical guarantees in a fully adaptive model of estimation, withstands practical adversarial attacks, and achieves high utility on real submission files from a Kaggle competition. Notably, we are able to sidestep a powerful recent hardness result for adaptive risk estimation that rules out algorithms such as ours under a seemingly very similar notion of accuracy. On a practical note, we provide a completely parameter-free variant of our algorithm that can be deployed in a real competition with no tuning required whatsoever.",
        "bibtex": "@InProceedings{pmlr-v37-blum15,\n  title = \t {The Ladder: A Reliable Leaderboard for Machine Learning Competitions},\n  author = \t {Blum, Avrim and Hardt, Moritz},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1006--1014},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/blum15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/blum15.html},\n  abstract = \t {The organizer of a machine learning competition faces the problem of maintaining an accurate leaderboard that faithfully represents the quality of the best submission of each competing team. What makes this estimation problem particularly challenging is its sequential and adaptive nature. As participants are allowed to repeatedly evaluate their submissions on the leaderboard, they may begin to overfit to the holdout data that supports the leaderboard. Few theoretical results give actionable advice on how to design a reliable leaderboard. Existing approaches therefore often resort to poorly understood heuristics such as limiting the bit precision of answers and the rate of re-submission. In this work, we introduce a notion of leaderboard accuracy tailored to the format of a competition. We introduce a natural algorithm called the Ladder and demonstrate that it simultaneously supports strong theoretical guarantees in a fully adaptive model of estimation, withstands practical adversarial attacks, and achieves high utility on real submission files from a Kaggle competition. Notably, we are able to sidestep a powerful recent hardness result for adaptive risk estimation that rules out algorithms such as ours under a seemingly very similar notion of accuracy. On a practical note, we provide a completely parameter-free variant of our algorithm that can be deployed in a real competition with no tuning required whatsoever.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/blum15.pdf",
        "supp": "",
        "pdf_size": 315863,
        "gs_citation": 154,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1707755351992340475&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": "MRTZ.ORG;CS.CMU.EDU",
        "email": "MRTZ.ORG;CS.CMU.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "964adc0648",
        "title": "The Power of Randomization: Distributed Submodular Maximization on Massive Datasets",
        "site": "https://proceedings.mlr.press/v37/barbosa15.html",
        "author": "Rafael Barbosa; Alina Ene; Huy Nguyen; Justin Ward",
        "abstract": "A wide variety of problems in machine learning, including exemplar clustering, document summarization, and sensor placement, can be cast as constrained submodular maximization problems. Unfortunately, the resulting submodular optimization problems are often too large to be solved on a single machine. We consider a distributed, greedy algorithm that combines previous approaches with randomization. The result is an algorithm that is embarrassingly parallel and achieves provable, constant factor, worst-case approximation guarantees. In our experiments, we demonstrate its efficiency in large problems with different kinds of constraints with objective values always close to what is achievable in the centralized setting.",
        "bibtex": "@InProceedings{pmlr-v37-barbosa15,\n  title = \t {The Power of Randomization: Distributed Submodular Maximization on Massive Datasets},\n  author = \t {Barbosa, Rafael and Ene, Alina and Nguyen, Huy and Ward, Justin},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1236--1244},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/barbosa15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/barbosa15.html},\n  abstract = \t {A wide variety of problems in machine learning, including exemplar clustering, document summarization, and sensor placement, can be cast as constrained submodular maximization problems. Unfortunately, the resulting submodular optimization problems are often too large to be solved on a single machine. We consider a distributed, greedy algorithm that combines previous approaches with randomization. The result is an algorithm that is embarrassingly parallel and achieves provable, constant factor, worst-case approximation guarantees. In our experiments, we demonstrate its efficiency in large problems with different kinds of constraints with objective values always close to what is achievable in the centralized setting.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/barbosa15.pdf",
        "supp": "",
        "pdf_size": 1516360,
        "gs_citation": 114,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2193782798441697344&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Department of Computer Science and DIMAP, University of Warwick; Department of Computer Science and DIMAP, University of Warwick; Simons Institute, University of California, Berkeley; Department of Computer Science and DIMAP, University of Warwick",
        "aff_domain": "DCS.WARWICK.AC.UK;DCS.WARWICK.AC.UK;CS.PRINCETON.EDU;DCS.WARWICK.AC.UK",
        "email": "DCS.WARWICK.AC.UK;DCS.WARWICK.AC.UK;CS.PRINCETON.EDU;DCS.WARWICK.AC.UK",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Warwick;University of California, Berkeley",
        "aff_unique_dep": "Department of Computer Science and DIMAP;Simons Institute",
        "aff_unique_url": "https://www.warwick.ac.uk;https://simons.berkeley.edu",
        "aff_unique_abbr": "Warwick;UC Berkeley",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "0143fc7a96",
        "title": "Theory of Dual-sparse Regularized Randomized Reduction",
        "site": "https://proceedings.mlr.press/v37/yangb15.html",
        "author": "Tianbao Yang; Lijun Zhang; Rong Jin; Shenghuo Zhu",
        "abstract": "In this paper, we study randomized reduction methods, which reduce high-dimensional features into low-dimensional space by randomized methods (e.g., random projection, random hashing), for large-scale high-dimensional classification. Previous theoretical results on randomized reduction methods hinge on strong assumptions about the data, e.g., low rank of the data matrix or a large separable margin of classification, which hinder their in broad domains. To address these limitations, we propose dual-sparse regularized randomized reduction methods that introduce a sparse regularizer into the reduced dual problem. Under a mild condition that the original dual solution is a (nearly) sparse vector, we show that the resulting dual solution is close to the original dual solution and concentrates on its support set. In numerical experiments, we present an empirical study to support the analysis and we also present a novel application of the dual-sparse randomized reduction methods to reducing the communication cost of distributed learning from large-scale high-dimensional data.",
        "bibtex": "@InProceedings{pmlr-v37-yangb15,\n  title = \t {Theory of Dual-sparse Regularized Randomized Reduction},\n  author = \t {Yang, Tianbao and Zhang, Lijun and Jin, Rong and Zhu, Shenghuo},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {305--314},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/yangb15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/yangb15.html},\n  abstract = \t {In this paper, we study randomized reduction methods, which reduce high-dimensional features into low-dimensional space by randomized methods (e.g., random projection, random hashing), for large-scale high-dimensional classification. Previous theoretical results on randomized reduction methods hinge on strong assumptions about the data, e.g., low rank of the data matrix or a large separable margin of classification, which hinder their in broad domains. To address these limitations, we propose dual-sparse regularized randomized reduction methods that introduce a sparse regularizer into the reduced dual problem. Under a mild condition that the original dual solution is a (nearly) sparse vector, we show that the resulting dual solution is close to the original dual solution and concentrates on its support set. In numerical experiments, we present an empirical study to support the analysis and we also present a novel application of the dual-sparse randomized reduction methods to reducing the communication cost of distributed learning from large-scale high-dimensional data.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/yangb15.pdf",
        "supp": "",
        "pdf_size": 420433,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18391440758923534593&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Science, the University of Iowa, Iowa City, USA; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Department of Computer Science and Engineering, Michigan State University, East Lansing, USA+Institute of Data Science and Technologies at Alibaba Group, Seattle, USA; Institute of Data Science and Technologies at Alibaba Group, Seattle, USA",
        "aff_domain": "uiowa.edu;lamda.nju.edu.cn;cse.msu.edu;gmail.com",
        "email": "uiowa.edu;lamda.nju.edu.cn;cse.msu.edu;gmail.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2+3;3",
        "aff_unique_norm": "University of Iowa;Nanjing University;Michigan State University;Alibaba Group",
        "aff_unique_dep": "Department of Computer Science;National Key Laboratory for Novel Software Technology;Department of Computer Science and Engineering;Institute of Data Science and Technologies",
        "aff_unique_url": "https://www.uiowa.edu;http://www.nju.edu.cn;https://www.msu.edu;https://www.alibaba.com",
        "aff_unique_abbr": "UIowa;Nanjing U;MSU;Alibaba",
        "aff_campus_unique_index": "0;1;2+3;3",
        "aff_campus_unique": "Iowa City;Nanjing;East Lansing;Seattle",
        "aff_country_unique_index": "0;1;0+0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "aea1f235cf",
        "title": "Threshold Influence Model for Allocating Advertising Budgets",
        "site": "https://proceedings.mlr.press/v37/miyauchi15.html",
        "author": "Atsushi Miyauchi; Yuni Iwamasa; Takuro Fukunaga; Naonori Kakimura",
        "abstract": "We propose a new influence model for allocating budgets to advertising channels. Our model captures customer\u2019s sensitivity to advertisements as a threshold behavior; a customer is expected to be influenced if the influence he receives exceeds his threshold. Over the threshold model, we discuss two optimization problems. The first one is the budget-constrained influence maximization. We propose two greedy algorithms based on different strategies, and analyze the performance when the influence is submodular. We then introduce a new characteristic to measure the cost-effectiveness of a marketing campaign, that is, the proportion of the resulting influence to the cost spent. We design an almost linear-time approximation algorithm to maximize the cost-effectiveness. Furthermore, we design a better-approximation algorithm based on linear programming for a special case. We conduct thorough experiments to confirm that our algorithms outperform baseline algorithms.",
        "bibtex": "@InProceedings{pmlr-v37-miyauchi15,\n  title = \t {Threshold Influence Model for Allocating Advertising Budgets},\n  author = \t {Miyauchi, Atsushi and Iwamasa, Yuni and Fukunaga, Takuro and Kakimura, Naonori},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1395--1404},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/miyauchi15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/miyauchi15.html},\n  abstract = \t {We propose a new influence model for allocating budgets to advertising channels. Our model captures customer\u2019s sensitivity to advertisements as a threshold behavior; a customer is expected to be influenced if the influence he receives exceeds his threshold. Over the threshold model, we discuss two optimization problems. The first one is the budget-constrained influence maximization. We propose two greedy algorithms based on different strategies, and analyze the performance when the influence is submodular. We then introduce a new characteristic to measure the cost-effectiveness of a marketing campaign, that is, the proportion of the resulting influence to the cost spent. We design an almost linear-time approximation algorithm to maximize the cost-effectiveness. Furthermore, we design a better-approximation algorithm based on linear programming for a special case. We conduct thorough experiments to confirm that our algorithms outperform baseline algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/miyauchi15.pdf",
        "supp": "",
        "pdf_size": 266318,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15819762748783453437&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Graduate School of Decision Science and Technology, Tokyo Institute of Technology, Japan; Graduate School of Information Science and Technology, University of Tokyo, Japan; National Institute of Informatics, JST, ERATO, Kawarabayashi Large Graph Project, Japan; Graduate School of Arts and Sciences, University of Tokyo, Japan",
        "aff_domain": "M.TITECH.AC.JP;MIST.I.U-TOKYO.AC.JP;NII.AC.JP;GLOBAL.C.U-TOKYO.AC.JP",
        "email": "M.TITECH.AC.JP;MIST.I.U-TOKYO.AC.JP;NII.AC.JP;GLOBAL.C.U-TOKYO.AC.JP",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;1",
        "aff_unique_norm": "Tokyo Institute of Technology;University of Tokyo;National Institute of Informatics",
        "aff_unique_dep": "Graduate School of Decision Science and Technology;Graduate School of Information Science and Technology;",
        "aff_unique_url": "https://www.titech.ac.jp;https://www.u-tokyo.ac.jp;https://www.nii.ac.jp",
        "aff_unique_abbr": "Titech;UTokyo;NII",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Tokyo",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "c189d5ae52",
        "title": "Towards a Learning Theory of Cause-Effect Inference",
        "site": "https://proceedings.mlr.press/v37/lopez-paz15.html",
        "author": "David Lopez-Paz; Krikamol Muandet; Bernhard Sch\u00f6lkopf; Iliya Tolstikhin",
        "abstract": "We pose causal inference as the problem of learning to classify probability distributions. In particular, we assume access to a collection {(S_i,l_i)}_i=1^n, where each S_i is a sample drawn from the probability distribution of X_i \\times Y_i, and l_i is a binary label indicating whether \u201cX_i \\to Y_i\u201d or \u201cX_i \u2190Y_i\u201d. Given these data, we build a causal inference rule in two steps. First, we featurize each S_i using the kernel mean embedding associated with some characteristic kernel. Second, we train a binary classifier on such embeddings to distinguish between causal directions. We present generalization bounds showing the statistical consistency and learning rates of the proposed approach, and provide a simple implementation that achieves state-of-the-art cause-effect inference. Furthermore, we extend our ideas to infer causal relationships between more than two variables.",
        "bibtex": "@InProceedings{pmlr-v37-lopez-paz15,\n  title = \t {Towards a Learning Theory of Cause-Effect Inference},\n  author = \t {Lopez-Paz, David and Muandet, Krikamol and Sch\u00f6lkopf, Bernhard and Tolstikhin, Iliya},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1452--1461},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/lopez-paz15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/lopez-paz15.html},\n  abstract = \t {We pose causal inference as the problem of learning to classify probability distributions. In particular, we assume access to a collection {(S_i,l_i)}_i=1^n, where each S_i is a sample drawn from the probability distribution of X_i \\times Y_i, and l_i is a binary label indicating whether \u201cX_i \\to Y_i\u201d or \u201cX_i \u2190Y_i\u201d. Given these data, we build a causal inference rule in two steps. First, we featurize each S_i using the kernel mean embedding associated with some characteristic kernel. Second, we train a binary classifier on such embeddings to distinguish between causal directions. We present generalization bounds showing the statistical consistency and learning rates of the proposed approach, and provide a simple implementation that achieves state-of-the-art cause-effect inference. Furthermore, we extend our ideas to infer causal relationships between more than two variables.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/lopez-paz15.pdf",
        "supp": "",
        "pdf_size": 390634,
        "gs_citation": 229,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11898881539328089290&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Max-Planck-Institute for Intelligent Systems+University of Cambridge; Max-Planck-Institute for Intelligent Systems; Max-Planck-Institute for Intelligent Systems; Max-Planck-Institute for Intelligent Systems",
        "aff_domain": "lopezpaz.org;tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de",
        "email": "lopezpaz.org;tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0;0",
        "aff_unique_norm": "Max-Planck-Institute for Intelligent Systems;University of Cambridge",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.mpi-is.mpg.de;https://www.cam.ac.uk",
        "aff_unique_abbr": "MPI-IS;Cambridge",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0+1;0;0;0",
        "aff_country_unique": "Germany;United Kingdom"
    },
    {
        "id": "66b0e8f44b",
        "title": "Towards a Lower Sample Complexity for Robust One-bit Compressed Sensing",
        "site": "https://proceedings.mlr.press/v37/zhua15.html",
        "author": "Rongda Zhu; Quanquan Gu",
        "abstract": "In this paper, we propose a novel algorithm based on nonconvex sparsity-inducing penalty for one-bit compressed sensing. We prove that our algorithm has a sample complexity of O(s/\u03b5^2) for strong signals, and O(s\\log d/\u03b5^2) for weak signals, where s is the number of nonzero entries in the signal vector, d is the signal dimension and \u03b5is the recovery error. For general signals, the sample complexity of our algorithm lies between O(s/\u03b5^2) and O(s\\log d/\u03b5^2). This is a remarkable improvement over the existing best sample complexity O(s\\log d/\u03b5^2). Furthermore, we show that our algorithm achieves exact support recovery with high probability for strong signals. Our theory is verified by extensive numerical experiments, which clearly illustrate the superiority of our algorithm for both approximate signal and support recovery in the noisy setting.",
        "bibtex": "@InProceedings{pmlr-v37-zhua15,\n  title = \t {Towards a Lower Sample Complexity for Robust One-bit Compressed Sensing},\n  author = \t {Zhu, Rongda and Gu, Quanquan},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {739--747},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/zhua15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/zhua15.html},\n  abstract = \t {In this paper, we propose a novel algorithm based on nonconvex sparsity-inducing penalty for one-bit compressed sensing. We prove that our algorithm has a sample complexity of O(s/\u03b5^2) for strong signals, and O(s\\log d/\u03b5^2) for weak signals, where s is the number of nonzero entries in the signal vector, d is the signal dimension and \u03b5is the recovery error. For general signals, the sample complexity of our algorithm lies between O(s/\u03b5^2) and O(s\\log d/\u03b5^2). This is a remarkable improvement over the existing best sample complexity O(s\\log d/\u03b5^2). Furthermore, we show that our algorithm achieves exact support recovery with high probability for strong signals. Our theory is verified by extensive numerical experiments, which clearly illustrate the superiority of our algorithm for both approximate signal and support recovery in the noisy setting.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/zhua15.pdf",
        "supp": "",
        "pdf_size": 319653,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15318160034734256163&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL 61801, USA; Department of Systems and Information Engineering, University of Virginia, Charlottesville, VA 22904, USA",
        "aff_domain": "ILLINOIS.EDU;VIRGINIA.EDU",
        "email": "ILLINOIS.EDU;VIRGINIA.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;University of Virginia",
        "aff_unique_dep": "Department of Computer Science;Department of Systems and Information Engineering",
        "aff_unique_url": "https://illinois.edu;https://www.virginia.edu",
        "aff_unique_abbr": "UIUC;UVA",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Urbana;Charlottesville",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4597833115",
        "title": "Tracking Approximate Solutions of Parameterized Optimization Problems over Multi-Dimensional (Hyper-)Parameter Domains",
        "site": "https://proceedings.mlr.press/v37/blechschmidt15.html",
        "author": "Katharina Blechschmidt; Joachim Giesen; Soeren Laue",
        "abstract": "Many machine learning methods are given as parameterized optimization problems. Important examples of such parameters are regularization- and kernel hyperparameters. These parameters have to be tuned carefully since the choice of their values can have a significant impact on the statistical performance of the learning methods. In most cases the parameter space does not carry much structure and parameter tuning essentially boils down to exploring the whole parameter space. The case when there is only one parameter received quite some attention over the years. First, algorithms for tracking an optimal solution for several machine learning optimization problems over regularization- and hyperparameter intervals had been developed, but since these algorithms can suffer from numerical problems more robust and efficient approximate path tracking algorithms have been devised and analyzed recently. By now approximate path tracking algorithms are known for regularization-and kernel hyperparameter paths with optimal path complexities that depend only on the prescribed approximation error. Here we extend the work on approximate path tracking algorithms with approximation guarantees to multi-dimensional parameter domains. We show a lower bound on the complexity of approximately exploring a multi-dimensional parameter domain that is the product of the corresponding path complexities. We also show a matching upper bound that can be turned into a theoretically and practically efficient algorithm. Experimental results for kernelized support vector machines and the elastic net confirm the theoretical complexity analysis.",
        "bibtex": "@InProceedings{pmlr-v37-blechschmidt15,\n  title = \t {Tracking Approximate Solutions of Parameterized Optimization Problems over Multi-Dimensional (Hyper-)Parameter Domains},\n  author = \t {Blechschmidt, Katharina and Giesen, Joachim and Laue, Soeren},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {438--447},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/blechschmidt15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/blechschmidt15.html},\n  abstract = \t {Many machine learning methods are given as parameterized optimization problems. Important examples of such parameters are regularization- and kernel hyperparameters. These parameters have to be tuned carefully since the choice of their values can have a significant impact on the statistical performance of the learning methods. In most cases the parameter space does not carry much structure and parameter tuning essentially boils down to exploring the whole parameter space. The case when there is only one parameter received quite some attention over the years. First, algorithms for tracking an optimal solution for several machine learning optimization problems over regularization- and hyperparameter intervals had been developed, but since these algorithms can suffer from numerical problems more robust and efficient approximate path tracking algorithms have been devised and analyzed recently. By now approximate path tracking algorithms are known for regularization-and kernel hyperparameter paths with optimal path complexities that depend only on the prescribed approximation error. Here we extend the work on approximate path tracking algorithms with approximation guarantees to multi-dimensional parameter domains. We show a lower bound on the complexity of approximately exploring a multi-dimensional parameter domain that is the product of the corresponding path complexities. We also show a matching upper bound that can be turned into a theoretically and practically efficient algorithm. Experimental results for kernelized support vector machines and the elastic net confirm the theoretical complexity analysis.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/blechschmidt15.pdf",
        "supp": "",
        "pdf_size": 7587049,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16726214441891502169&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Friedrich-Schiller-Universitat Jena, Germany; Friedrich-Schiller-Universitat Jena, Germany; Friedrich-Schiller-Universitat Jena, Germany",
        "aff_domain": "WEB.DE;UNI-JENA.DE;UNI-JENA.DE",
        "email": "WEB.DE;UNI-JENA.DE;UNI-JENA.DE",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Friedrich-Schiller-Universit\u00e4t Jena",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uni-jena.de/",
        "aff_unique_abbr": "FSU Jena",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "546bb9e7e7",
        "title": "Training Deep Convolutional Neural Networks to Play Go",
        "site": "https://proceedings.mlr.press/v37/clark15.html",
        "author": "Christopher Clark; Amos Storkey",
        "abstract": "Mastering the game of Go has remained a long-standing challenge to the field of AI. Modern computer Go programs rely on processing millions of possible future positions to play well, but intuitively a stronger and more \u2019humanlike\u2019 way to play the game would be to rely on pattern recognition rather than brute force computation. Following this sentiment, we train deep convolutional neural networks to play Go by training them to predict the moves made by expert Go players. To solve this problem we introduce a number of novel techniques, including a method of tying weights in the network to \u2019hard code\u2019 symmetries that are expected to exist in the target function, and demonstrate in an ablation study they considerably improve performance. Our final networks are able to achieve move prediction accuracies of 41.1% and 44.4% on two different Go datasets, surpassing previous state of the art on this task by significant margins. Additionally, while previous move prediction systems have not yielded strong Go playing programs, we show that the networks trained in this work acquired high levels of skill. Our convolutional neural networks can consistently defeat the well known Go program GNU Go and win some games against state of the art Go playing program Fuego while using a fraction of the play time.",
        "bibtex": "@InProceedings{pmlr-v37-clark15,\n  title = \t {Training Deep Convolutional Neural Networks to Play Go},\n  author = \t {Clark, Christopher and Storkey, Amos},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1766--1774},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/clark15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/clark15.html},\n  abstract = \t {Mastering the game of Go has remained a long-standing challenge to the field of AI. Modern computer Go programs rely on processing millions of possible future positions to play well, but intuitively a stronger and more \u2019humanlike\u2019 way to play the game would be to rely on pattern recognition rather than brute force computation. Following this sentiment, we train deep convolutional neural networks to play Go by training them to predict the moves made by expert Go players. To solve this problem we introduce a number of novel techniques, including a method of tying weights in the network to \u2019hard code\u2019 symmetries that are expected to exist in the target function, and demonstrate in an ablation study they considerably improve performance. Our final networks are able to achieve move prediction accuracies of 41.1% and 44.4% on two different Go datasets, surpassing previous state of the art on this task by significant margins. Additionally, while previous move prediction systems have not yielded strong Go playing programs, we show that the networks trained in this work acquired high levels of skill. Our convolutional neural networks can consistently defeat the well known Go program GNU Go and win some games against state of the art Go playing program Fuego while using a fraction of the play time.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/clark15.pdf",
        "supp": "",
        "pdf_size": 311315,
        "gs_citation": 193,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14097050053646758396&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Allen Institute for Artificial Intelligence*; School of Informatics, University of Edinburgh",
        "aff_domain": "allennai.org;ed.ac.uk",
        "email": "allennai.org;ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Allen Institute for Artificial Intelligence;University of Edinburgh",
        "aff_unique_dep": ";School of Informatics",
        "aff_unique_url": "https://allenai.org;https://www.ed.ac.uk",
        "aff_unique_abbr": "AI2;Edinburgh",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Edinburgh",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "a58e4ed772",
        "title": "Trust Region Policy Optimization",
        "site": "https://proceedings.mlr.press/v37/schulman15.html",
        "author": "John Schulman; Sergey Levine; Pieter Abbeel; Michael Jordan; Philipp Moritz",
        "abstract": "In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.",
        "bibtex": "@InProceedings{pmlr-v37-schulman15,\n  title = \t {Trust Region Policy Optimization},\n  author = \t {Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1889--1897},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/schulman15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/schulman15.html},\n  abstract = \t {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/schulman15.pdf",
        "supp": "",
        "pdf_size": 987863,
        "gs_citation": 9809,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4215501129336400677&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "University of California, Berkeley, Department of Electrical Engineering and Computer Sciences; University of California, Berkeley, Department of Electrical Engineering and Computer Sciences; University of California, Berkeley, Department of Electrical Engineering and Computer Sciences; University of California, Berkeley, Department of Electrical Engineering and Computer Sciences; University of California, Berkeley, Department of Electrical Engineering and Computer Sciences",
        "aff_domain": "EECS.BERKELEY.EDU;EECS.BERKELEY.EDU;EECS.BERKELEY.EDU;CS.BERKELEY.EDU;CS.BERKELEY.EDU",
        "email": "EECS.BERKELEY.EDU;EECS.BERKELEY.EDU;EECS.BERKELEY.EDU;CS.BERKELEY.EDU;CS.BERKELEY.EDU",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Sciences",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8b2673d829",
        "title": "Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization",
        "site": "https://proceedings.mlr.press/v37/frostig15.html",
        "author": "Roy Frostig; Rong Ge; Sham Kakade; Aaron Sidford",
        "abstract": "We develop a family of accelerated stochastic algorithms that optimize sums of convex functions. Our algorithms improve upon the fastest running time for empirical risk minimization (ERM), and in particular linear least-squares regression, across a wide range of problem settings. To achieve this, we establish a framework, based on the classical proximal point algorithm, useful for accelerating recent fast stochastic algorithms in a black-box fashion. Empirically, we demonstrate that the resulting algorithms exhibit notions of stability that are advantageous in practice. Both in theory and in practice, the provided algorithms reap the computational benefits of adding a large strongly convex regularization term, without incurring a corresponding bias to the original ERM problem.",
        "bibtex": "@InProceedings{pmlr-v37-frostig15,\n  title = \t {Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization},\n  author = \t {Frostig, Roy and Ge, Rong and Kakade, Sham and Sidford, Aaron},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2540--2548},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/frostig15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/frostig15.html},\n  abstract = \t {We develop a family of accelerated stochastic algorithms that optimize sums of convex functions. Our algorithms improve upon the fastest running time for empirical risk minimization (ERM), and in particular linear least-squares regression, across a wide range of problem settings. To achieve this, we establish a framework, based on the classical proximal point algorithm, useful for accelerating recent fast stochastic algorithms in a black-box fashion. Empirically, we demonstrate that the resulting algorithms exhibit notions of stability that are advantageous in practice. Both in theory and in practice, the provided algorithms reap the computational benefits of adding a large strongly convex regularization term, without incurring a corresponding bias to the original ERM problem.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/frostig15.pdf",
        "supp": "",
        "pdf_size": 406371,
        "gs_citation": 177,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=24149797568533124&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Stanford University; Microsoft Research, New England; Microsoft Research, New England; MIT",
        "aff_domain": "CS.STANFORD.EDU;MICROSOFT.COM;MICROSOFT.COM;MIT.EDU",
        "email": "CS.STANFORD.EDU;MICROSOFT.COM;MICROSOFT.COM;MIT.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;2",
        "aff_unique_norm": "Stanford University;Microsoft;Massachusetts Institute of Technology",
        "aff_unique_dep": ";Microsoft Research;",
        "aff_unique_url": "https://www.stanford.edu;https://www.microsoft.com/en-us/research/group/newengland;https://web.mit.edu",
        "aff_unique_abbr": "Stanford;MSR;MIT",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Stanford;New England;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1c0ebd1d3b",
        "title": "Universal Value Function Approximators",
        "site": "https://proceedings.mlr.press/v37/schaul15.html",
        "author": "Tom Schaul; Daniel Horgan; Karol Gregor; David Silver",
        "abstract": "Value functions are a core component of reinforcement learning. The main idea is to to construct a single function approximator V(s; theta) that estimates the long-term reward from any state s, using parameters \u03b8. In this paper we introduce universal value function approximators (UVFAs) V(s,g;theta) that generalise not just over states s but also over goals g. We develop an efficient technique for supervised learning of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a reinforcement learning algorithm that updates the UVFA solely from observed rewards. Finally, we demonstrate that a UVFA can successfully generalise to previously unseen goals.",
        "bibtex": "@InProceedings{pmlr-v37-schaul15,\n  title = \t {Universal Value Function Approximators},\n  author = \t {Schaul, Tom and Horgan, Daniel and Gregor, Karol and Silver, David},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1312--1320},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/schaul15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/schaul15.html},\n  abstract = \t {Value functions are a core component of reinforcement learning. The main idea is to to construct a single function approximator V(s; theta) that estimates the long-term reward from any state s, using parameters \u03b8. In this paper we introduce universal value function approximators (UVFAs) V(s,g;theta) that generalise not just over states s but also over goals g. We develop an efficient technique for supervised learning of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a reinforcement learning algorithm that updates the UVFA solely from observed rewards. Finally, we demonstrate that a UVFA can successfully generalise to previously unseen goals.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/schaul15.pdf",
        "supp": "",
        "pdf_size": 860963,
        "gs_citation": 1370,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=861364408827501200&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Google DeepMind, 5 New Street Square, EC4A 3TW London; Google DeepMind, 5 New Street Square, EC4A 3TW London; Google DeepMind, 5 New Street Square, EC4A 3TW London; Google DeepMind, 5 New Street Square, EC4A 3TW London",
        "aff_domain": "GOOGLE.COM;GOOGLE.COM;GOOGLE.COM;GOOGLE.COM",
        "email": "GOOGLE.COM;GOOGLE.COM;GOOGLE.COM;GOOGLE.COM",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google DeepMind",
        "aff_unique_url": "https://deepmind.com",
        "aff_unique_abbr": "DeepMind",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "94fa5960ed",
        "title": "Unsupervised Domain Adaptation by Backpropagation",
        "site": "https://proceedings.mlr.press/v37/ganin15.html",
        "author": "Yaroslav Ganin; Victor Lempitsky",
        "abstract": "Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of \"deep\" features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation. Overall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on Office datasets.",
        "bibtex": "@InProceedings{pmlr-v37-ganin15,\n  title = \t {Unsupervised Domain Adaptation by Backpropagation},\n  author = \t {Ganin, Yaroslav and Lempitsky, Victor},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1180--1189},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/ganin15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/ganin15.html},\n  abstract = \t {Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of \"deep\" features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation. Overall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on Office datasets.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/ganin15.pdf",
        "supp": "",
        "pdf_size": 3274846,
        "gs_citation": 8165,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17780020027228412435&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Skolkovo Institute of Science and Technology (Skoltech), Moscow Region, Russia; Skolkovo Institute of Science and Technology (Skoltech), Moscow Region, Russia",
        "aff_domain": "SKOLTECH.RU;SKOLTECH.RU",
        "email": "SKOLTECH.RU;SKOLTECH.RU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Skolkovo Institute of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.skoltech.ru",
        "aff_unique_abbr": "Skoltech",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Moscow Region",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Russian Federation"
    },
    {
        "id": "d8d1028c0f",
        "title": "Unsupervised Learning of Video Representations using LSTMs",
        "site": "https://proceedings.mlr.press/v37/srivastava15.html",
        "author": "Nitish Srivastava; Elman Mansimov; Ruslan Salakhudinov",
        "abstract": "We use Long Short Term Memory (LSTM) networks to learn representations of video sequences. Our model uses an encoder LSTM to map an input sequence into a fixed length representation. This representation is decoded using single or multiple decoder LSTMs to perform different tasks, such as reconstructing the input sequence, or predicting the future sequence. We experiment with two kinds of input sequences \u2013 patches of image pixels and high-level representations (\u201cpercepts\") of video frames extracted using a pretrained convolutional net. We explore different design choices such as whether the decoder LSTMs should condition on the generated output. We analyze the outputs of the model qualitatively to see how well the model can extrapolate the learned video representation into the future and into the past. We further evaluate the representations by finetuning them for a supervised learning problem \u2013 human action recognition on the UCF-101 and HMDB-51 datasets. We show that the representations help improve classification accuracy, especially when there are only few training examples. Even models pretrained on unrelated datasets (300 hours of YouTube videos) can help action recognition performance.",
        "bibtex": "@InProceedings{pmlr-v37-srivastava15,\n  title = \t {Unsupervised Learning of Video Representations using LSTMs},\n  author = \t {Srivastava, Nitish and Mansimov, Elman and Salakhudinov, Ruslan},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {843--852},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/srivastava15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/srivastava15.html},\n  abstract = \t {We use Long Short Term Memory (LSTM) networks to learn representations of video sequences. Our model uses an encoder LSTM to map an input sequence into a fixed length representation. This representation is decoded using single or multiple decoder LSTMs to perform different tasks, such as reconstructing the input sequence, or predicting the future sequence. We experiment with two kinds of input sequences \u2013 patches of image pixels and high-level representations (\u201cpercepts\") of video frames extracted using a pretrained convolutional net. We explore different design choices such as whether the decoder LSTMs should condition on the generated output. We analyze the outputs of the model qualitatively to see how well the model can extrapolate the learned video representation into the future and into the past. We further evaluate the representations by finetuning them for a supervised learning problem \u2013 human action recognition on the UCF-101 and HMDB-51 datasets. We show that the representations help improve classification accuracy, especially when there are only few training examples. Even models pretrained on unrelated datasets (300 hours of YouTube videos) can help action recognition performance.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/srivastava15.pdf",
        "supp": "",
        "pdf_size": 1005381,
        "gs_citation": 3471,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6890473943204323716&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 17,
        "aff": "University of Toronto; University of Toronto; University of Toronto",
        "aff_domain": "CS.TORONTO.EDU;CS.TORONTO.EDU;CS.TORONTO.EDU",
        "email": "CS.TORONTO.EDU;CS.TORONTO.EDU;CS.TORONTO.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "1a5913e6b9",
        "title": "Unsupervised Riemannian Metric Learning for Histograms Using Aitchison Transformations",
        "site": "https://proceedings.mlr.press/v37/le15.html",
        "author": "Tam Le; Marco Cuturi",
        "abstract": "Many applications in machine learning handle bags of features or histograms rather than simple vectors. In that context, defining a proper geometry to compare histograms can be crucial for many machine learning algorithms. While one might be tempted to use a default metric such as the Euclidean metric, empirical evidence shows this may not be the best choice when dealing with observations that lie in the probability simplex. Additionally, it might be desirable to choose a metric adaptively based on data. We consider in this paper the problem of learning a Riemannian metric on the simplex given unlabeled histogram data. We follow the approach of Lebanon(2006), who proposed to estimate such a metric within a parametric family by maximizing the inverse volume of a given data set of points under that metric. The metrics we consider on the multinomial simplex are pull-back metrics of the Fisher information parameterized by operations within the simplex known as Aitchison(1982) transformations. We propose an algorithmic approach to maximize inverse volumes using sampling and contrastive divergences. We provide experimental evidence that the metric obtained under our proposal outperforms alternative approaches.",
        "bibtex": "@InProceedings{pmlr-v37-le15,\n  title = \t {Unsupervised Riemannian Metric Learning for Histograms Using Aitchison Transformations},\n  author = \t {Le, Tam and Cuturi, Marco},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {2002--2011},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/le15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/le15.html},\n  abstract = \t {Many applications in machine learning handle bags of features or histograms rather than simple vectors. In that context, defining a proper geometry to compare histograms can be crucial for many machine learning algorithms. While one might be tempted to use a default metric such as the Euclidean metric, empirical evidence shows this may not be the best choice when dealing with observations that lie in the probability simplex. Additionally, it might be desirable to choose a metric adaptively based on data. We consider in this paper the problem of learning a Riemannian metric on the simplex given unlabeled histogram data. We follow the approach of Lebanon(2006), who proposed to estimate such a metric within a parametric family by maximizing the inverse volume of a given data set of points under that metric. The metrics we consider on the multinomial simplex are pull-back metrics of the Fisher information parameterized by operations within the simplex known as Aitchison(1982) transformations. We propose an algorithmic approach to maximize inverse volumes using sampling and contrastive divergences. We provide experimental evidence that the metric obtained under our proposal outperforms alternative approaches.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/le15.pdf",
        "supp": "",
        "pdf_size": 395400,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9879387739888944533&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Graduate School of Informatics, Kyoto University, Japan; Graduate School of Informatics, Kyoto University, Japan",
        "aff_domain": "IIP.IST.I.KYOTO-U.AC.JP;I.KYOTO-U.AC.JP",
        "email": "IIP.IST.I.KYOTO-U.AC.JP;I.KYOTO-U.AC.JP",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Kyoto University",
        "aff_unique_dep": "Graduate School of Informatics",
        "aff_unique_url": "https://www.kyoto-u.ac.jp",
        "aff_unique_abbr": "Kyoto U",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Kyoto",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "7b54cea200",
        "title": "Variational Generative Stochastic Networks with Collaborative Shaping",
        "site": "https://proceedings.mlr.press/v37/bachman15.html",
        "author": "Philip Bachman; Doina Precup",
        "abstract": "We develop an approach to training generative models based on unrolling a variational auto-encoder into a Markov chain, and shaping the chain\u2019s trajectories using a technique inspired by recent work in Approximate Bayesian computation. We show that the global minimizer of the resulting objective is achieved when the generative model reproduces the target distribution. To allow finer control over the behavior of the models, we add a regularization term inspired by techniques used for regularizing certain types of policy search in reinforcement learning. We present empirical results on the MNIST and TFD datasets which show that our approach offers state-of-the-art performance, both quantitatively and from a qualitative point of view.",
        "bibtex": "@InProceedings{pmlr-v37-bachman15,\n  title = \t {Variational Generative Stochastic Networks with Collaborative Shaping},\n  author = \t {Bachman, Philip and Precup, Doina},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1964--1972},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/bachman15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/bachman15.html},\n  abstract = \t {We develop an approach to training generative models based on unrolling a variational auto-encoder into a Markov chain, and shaping the chain\u2019s trajectories using a technique inspired by recent work in Approximate Bayesian computation. We show that the global minimizer of the resulting objective is achieved when the generative model reproduces the target distribution. To allow finer control over the behavior of the models, we add a regularization term inspired by techniques used for regularizing certain types of policy search in reinforcement learning. We present empirical results on the MNIST and TFD datasets which show that our approach offers state-of-the-art performance, both quantitatively and from a qualitative point of view.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/bachman15.pdf",
        "supp": "",
        "pdf_size": 7148450,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4181444647980956284&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "McGill University, School of Computer Science; McGill University, School of Computer Science",
        "aff_domain": "gmail.com;cs.mcgill.ca",
        "email": "gmail.com;cs.mcgill.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "McGill University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.mcgill.ca",
        "aff_unique_abbr": "McGill",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "1696ce00ed",
        "title": "Variational Inference for Gaussian Process Modulated Poisson Processes",
        "site": "https://proceedings.mlr.press/v37/lloyd15.html",
        "author": "Chris Lloyd; Tom Gunter; Michael Osborne; Stephen Roberts",
        "abstract": "We present the first fully variational Bayesian inference scheme for continuous Gaussian-process-modulated Poisson processes. Such point processes are used in a variety of domains, including neuroscience, geo-statistics and astronomy, but their use is hindered by the computational cost of existing inference schemes. Our scheme: requires no discretisation of the domain; scales linearly in the number of observed events; and is many orders of magnitude faster than previous sampling based approaches. The resulting algorithm is shown to outperform standard methods on synthetic examples, coal mining disaster data and in the prediction of Malaria incidences in Kenya.",
        "bibtex": "@InProceedings{pmlr-v37-lloyd15,\n  title = \t {Variational Inference for Gaussian Process Modulated Poisson Processes},\n  author = \t {Lloyd, Chris and Gunter, Tom and Osborne, Michael and Roberts, Stephen},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1814--1822},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/lloyd15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/lloyd15.html},\n  abstract = \t {We present the first fully variational Bayesian inference scheme for continuous Gaussian-process-modulated Poisson processes. Such point processes are used in a variety of domains, including neuroscience, geo-statistics and astronomy, but their use is hindered by the computational cost of existing inference schemes. Our scheme: requires no discretisation of the domain; scales linearly in the number of observed events; and is many orders of magnitude faster than previous sampling based approaches. The resulting algorithm is shown to outperform standard methods on synthetic examples, coal mining disaster data and in the prediction of Malaria incidences in Kenya.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/lloyd15.pdf",
        "supp": "",
        "pdf_size": 754364,
        "gs_citation": 146,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16515182331458748027&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Engineering Science, University of Oxford; Department of Engineering Science, University of Oxford; Department of Engineering Science, University of Oxford; Department of Engineering Science, University of Oxford",
        "aff_domain": "ROBOTS.OX.AC.UK;ROBOTS.OX.AC.UK;ROBOTS.OX.AC.UK;ROBOTS.OX.AC.UK",
        "email": "ROBOTS.OX.AC.UK;ROBOTS.OX.AC.UK;ROBOTS.OX.AC.UK;ROBOTS.OX.AC.UK",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Oxford",
        "aff_unique_dep": "Department of Engineering Science",
        "aff_unique_url": "https://www.ox.ac.uk",
        "aff_unique_abbr": "Oxford",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Oxford",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "154f71a289",
        "title": "Variational Inference with Normalizing Flows",
        "site": "https://proceedings.mlr.press/v37/rezende15.html",
        "author": "Danilo Rezende; Shakir Mohamed",
        "abstract": "The choice of the approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.",
        "bibtex": "@InProceedings{pmlr-v37-rezende15,\n  title = \t {Variational Inference with Normalizing Flows},\n  author = \t {Rezende, Danilo and Mohamed, Shakir},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1530--1538},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/rezende15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/rezende15.html},\n  abstract = \t {The choice of the approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/rezende15.pdf",
        "supp": "",
        "pdf_size": 2782087,
        "gs_citation": 5252,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6765826181105442087&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Google DeepMind, London; Google DeepMind, London",
        "aff_domain": "google.com;google.com",
        "email": "google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google DeepMind",
        "aff_unique_url": "https://deepmind.com",
        "aff_unique_abbr": "DeepMind",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "d3deef538c",
        "title": "Vector-Space Markov Random Fields via Exponential Families",
        "site": "https://proceedings.mlr.press/v37/tansey15.html",
        "author": "Wesley Tansey; Oscar Hernan Madrid Padilla; Arun Sai Suggala; Pradeep Ravikumar",
        "abstract": "We present Vector-Space Markov Random Fields (VS-MRFs), a novel class of undirected graphical models where each variable can belong to an arbitrary vector space. VS-MRFs generalize a recent line of work on scalar-valued, uni-parameter exponential family and mixed graphical models, thereby greatly broadening the class of exponential families available (e.g., allowing multinomial and Dirichlet distributions). Specifically, VS-MRFs are the joint graphical model distributions where the node-conditional distributions belong to generic exponential families with general vector space domains. We also present a sparsistent M-estimator for learning our class of MRFs that recovers the correct set of edges with high probability. We validate our approach via a set of synthetic data experiments as well as a real-world case study of over four million foods from the popular diet tracking app MyFitnessPal. Our results demonstrate that our algorithm performs well empirically and that VS-MRFs are capable of capturing and highlighting interesting structure in complex, real-world data. All code for our algorithm is open source and publicly available.",
        "bibtex": "@InProceedings{pmlr-v37-tansey15,\n  title = \t {Vector-Space Markov Random Fields via Exponential Families},\n  author = \t {Tansey, Wesley and Padilla, Oscar Hernan Madrid and Suggala, Arun Sai and Ravikumar, Pradeep},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {684--692},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/tansey15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/tansey15.html},\n  abstract = \t {We present Vector-Space Markov Random Fields (VS-MRFs), a novel class of undirected graphical models where each variable can belong to an arbitrary vector space. VS-MRFs generalize a recent line of work on scalar-valued, uni-parameter exponential family and mixed graphical models, thereby greatly broadening the class of exponential families available (e.g., allowing multinomial and Dirichlet distributions). Specifically, VS-MRFs are the joint graphical model distributions where the node-conditional distributions belong to generic exponential families with general vector space domains. We also present a sparsistent M-estimator for learning our class of MRFs that recovers the correct set of edges with high probability. We validate our approach via a set of synthetic data experiments as well as a real-world case study of over four million foods from the popular diet tracking app MyFitnessPal. Our results demonstrate that our algorithm performs well empirically and that VS-MRFs are capable of capturing and highlighting interesting structure in complex, real-world data. All code for our algorithm is open source and publicly available.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/tansey15.pdf",
        "supp": "",
        "pdf_size": 664759,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16540389875601178233&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, The University of Texas, Austin, TX 78712, USA; Department of Statistics and Data Sciences, The University of Texas, Austin, TX 78712, USA; Department of Computer Science, The University of Texas, Austin, TX 78712, USA; Department of Computer Science, The University of Texas, Austin, TX 78712, USA",
        "aff_domain": "cs.utexas.edu;utexas.edu;cs.utexas.edu;cs.utexas.edu",
        "email": "cs.utexas.edu;utexas.edu;cs.utexas.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ad059f3b0a",
        "title": "Weight Uncertainty in Neural Network",
        "site": "https://proceedings.mlr.press/v37/blundell15.html",
        "author": "Charles Blundell; Julien Cornebise; Koray Kavukcuoglu; Daan Wierstra",
        "abstract": "We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.",
        "bibtex": "@InProceedings{pmlr-v37-blundell15,\n  title = \t {Weight Uncertainty in Neural Network},\n  author = \t {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1613--1622},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/blundell15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/blundell15.html},\n  abstract = \t {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/blundell15.pdf",
        "supp": "",
        "pdf_size": 607681,
        "gs_citation": 4731,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6370453062994389837&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Google DeepMind; Google DeepMind; Google DeepMind; Google DeepMind",
        "aff_domain": "GOOGLE.COM;GOOGLE.COM;GOOGLE.COM;GOOGLE.COM",
        "email": "GOOGLE.COM;GOOGLE.COM;GOOGLE.COM;GOOGLE.COM",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google DeepMind",
        "aff_unique_url": "https://deepmind.com",
        "aff_unique_abbr": "DeepMind",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "c28d392230",
        "title": "Yinyang K-Means: A Drop-In Replacement of the Classic K-Means with Consistent Speedup",
        "site": "https://proceedings.mlr.press/v37/ding15.html",
        "author": "Yufei Ding; Yue Zhao; Xipeng Shen; Madanlal Musuvathi; Todd Mytkowicz",
        "abstract": "This paper presents Yinyang K-means, a new algorithm for K-means clustering. By clustering the centers in the initial stage, and leveraging efficiently maintained lower and upper bounds between a point and centers, it more effectively avoids unnecessary distance calculations than prior algorithms. It significantly outperforms classic K-means and prior alternative K-means algorithms consistently across all experimented data sets, cluster numbers, and machine configurations. The consistent, superior performance\u2014plus its simplicity, user-control of overheads, and guarantee in producing the same clustering results as the standard K-means does\u2014makes Yinyang K-means a drop-in replacement of the classic K-means with an order of magnitude higher performance.",
        "bibtex": "@InProceedings{pmlr-v37-ding15,\n  title = \t {Yinyang K-Means: A Drop-In Replacement of the Classic K-Means with Consistent Speedup},\n  author = \t {Ding, Yufei and Zhao, Yue and Shen, Xipeng and Musuvathi, Madanlal and Mytkowicz, Todd},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {579--587},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/ding15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/ding15.html},\n  abstract = \t {This paper presents Yinyang K-means, a new algorithm for K-means clustering. By clustering the centers in the initial stage, and leveraging efficiently maintained lower and upper bounds between a point and centers, it more effectively avoids unnecessary distance calculations than prior algorithms. It significantly outperforms classic K-means and prior alternative K-means algorithms consistently across all experimented data sets, cluster numbers, and machine configurations. The consistent, superior performance\u2014plus its simplicity, user-control of overheads, and guarantee in producing the same clustering results as the standard K-means does\u2014makes Yinyang K-means a drop-in replacement of the classic K-means with an order of magnitude higher performance.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/ding15.pdf",
        "supp": "",
        "pdf_size": 418054,
        "gs_citation": 194,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17809474603220384584&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, North Carolina State University; Department of Computer Science, North Carolina State University; Department of Computer Science, North Carolina State University; Microsoft Research; Microsoft Research",
        "aff_domain": "ncsu.edu;ncsu.edu;ncsu.edu;microsoft.com;microsoft.com",
        "email": "ncsu.edu;ncsu.edu;ncsu.edu;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;1",
        "aff_unique_norm": "North Carolina State University;Microsoft",
        "aff_unique_dep": "Department of Computer Science;Microsoft Research",
        "aff_unique_url": "https://www.ncsu.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "NCSU;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "850c695f6c",
        "title": "\\ell_1,p-Norm Regularization: Error Bounds and Convergence Rate Analysis of First-Order Methods",
        "site": "https://proceedings.mlr.press/v37/zhoub15.html",
        "author": "Zirui Zhou; Qi Zhang; Anthony Man-Cho So",
        "abstract": "Recently, \\ell_1,p-regularization has been widely used to induce structured sparsity in the solutions to various optimization problems. Motivated by the desire to analyze the convergence rate of first-order methods, we show that for a large class of \\ell_1,p-regularized problems, an error bound condition is satisfied when p\u2208[1,2] or p=\u221ebut fails to hold for any p\u2208(2,\u221e). Based on this result, we show that many first-order methods enjoy an asymptotic linear rate of convergence when applied to \\ell_1,p-regularized linear or logistic regression with p\u2208[1,2] or p=\u221e. By contrast, numerical experiments suggest that for the same class of problems with p\u2208(2,\u221e), the aforementioned methods may not converge linearly.",
        "bibtex": "@InProceedings{pmlr-v37-zhoub15,\n  title = \t {$\\ell_{1,p}$-Norm Regularization: Error Bounds and Convergence Rate Analysis of First-Order Methods},\n  author = \t {Zhou, Zirui and Zhang, Qi and So, Anthony Man-Cho},\n  booktitle = \t {Proceedings of the 32nd International Conference on Machine Learning},\n  pages = \t {1501--1510},\n  year = \t {2015},\n  editor = \t {Bach, Francis and Blei, David},\n  volume = \t {37},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Lille, France},\n  month = \t {07--09 Jul},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v37/zhoub15.pdf},\n  url = \t {https://proceedings.mlr.press/v37/zhoub15.html},\n  abstract = \t {Recently, \\ell_1,p-regularization has been widely used to induce structured sparsity in the solutions to various optimization problems. Motivated by the desire to analyze the convergence rate of first-order methods, we show that for a large class of \\ell_1,p-regularized problems, an error bound condition is satisfied when p\u2208[1,2] or p=\u221ebut fails to hold for any p\u2208(2,\u221e). Based on this result, we show that many first-order methods enjoy an asymptotic linear rate of convergence when applied to \\ell_1,p-regularized linear or logistic regression with p\u2208[1,2] or p=\u221e. By contrast, numerical experiments suggest that for the same class of problems with p\u2208(2,\u221e), the aforementioned methods may not converge linearly.}\n}",
        "pdf": "http://proceedings.mlr.press/v37/zhoub15.pdf",
        "supp": "",
        "pdf_size": 385260,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13813006446524710064&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong S.A.R., China; Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong S.A.R., China; Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong S.A.R., China",
        "aff_domain": "SE.CUHK.EDU.HK;SE.CUHK.EDU.HK;SE.CUHK.EDU.HK",
        "email": "SE.CUHK.EDU.HK;SE.CUHK.EDU.HK;SE.CUHK.EDU.HK",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Chinese University of Hong Kong",
        "aff_unique_dep": "Department of Systems Engineering and Engineering Management",
        "aff_unique_url": "https://www.cuhk.edu.hk",
        "aff_unique_abbr": "CUHK",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Shatin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    }
]