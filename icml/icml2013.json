[
    {
        "id": "2aae03df09",
        "title": "A Fast and Exact Energy Minimization Algorithm for Cycle MRFs",
        "site": "https://proceedings.mlr.press/v28/wang13f.html",
        "author": "Huayan Wang; Koller Daphne",
        "abstract": "The presence of cycles gives rise to the difficulty in performing inference for MRFs. Handling cycles efficiently would greatly enhance our ability to tackle general MRFs. In particular, for dual decomposition of energy minimization (MAP inference), using cycle subproblems leads   to a much tighter relaxation than using trees, but solving the cycle subproblems turns out to be the bottleneck.  In this paper, we present a fast and exact algorithm for energy minimization in cycle MRFs, which can be used as a subroutine in tackling general MRFs. Our method builds on junction-tree message passing, with a large portion of the message entries pruned for efficiency. The pruning conditions fully exploit the structure of a cycle. Experimental results show that our algorithm is more than an order of magnitude faster than other state-of-the-art fast inference methods, and it performs consistently well in several different real problems.",
        "bibtex": "@InProceedings{pmlr-v28-wang13f,\n  title = \t {A Fast and Exact Energy Minimization Algorithm for Cycle MRFs},\n  author = \t {Wang, Huayan and Daphne, Koller},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {190--198},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/wang13f.pdf},\n  url = \t {https://proceedings.mlr.press/v28/wang13f.html},\n  abstract = \t {The presence of cycles gives rise to the difficulty in performing inference for MRFs. Handling cycles efficiently would greatly enhance our ability to tackle general MRFs. In particular, for dual decomposition of energy minimization (MAP inference), using cycle subproblems leads   to a much tighter relaxation than using trees, but solving the cycle subproblems turns out to be the bottleneck.  In this paper, we present a fast and exact algorithm for energy minimization in cycle MRFs, which can be used as a subroutine in tackling general MRFs. Our method builds on junction-tree message passing, with a large portion of the message entries pruned for efficiency. The pruning conditions fully exploit the structure of a cycle. Experimental results show that our algorithm is more than an order of magnitude faster than other state-of-the-art fast inference methods, and it performs consistently well in several different real problems.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/wang13f.pdf",
        "supp": "",
        "pdf_size": 457707,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3705451354324747051&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Computer Science Department, Stanford University, Palo Alto, CA 94305 USA; Computer Science Department, Stanford University, Palo Alto, CA 94305 USA",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Palo Alto",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f3b84e90b4",
        "title": "A General Iterative Shrinkage and Thresholding Algorithm for Non-convex Regularized Optimization Problems",
        "site": "https://proceedings.mlr.press/v28/gong13a.html",
        "author": "Pinghua Gong; Changshui Zhang; Zhaosong Lu; Jianhua Huang; Jieping Ye",
        "abstract": "Non-convex sparsity-inducing penalties have recently received considerable attentions in sparse learning. Recent theoretical investigations have demonstrated their superiority over the convex counterparts in several sparse learning settings. However, solving the non-convex optimization problems associated with non-convex penalties remains a big challenge. A commonly used approach is the Multi-Stage (MS) convex relaxation (or DC programming), which relaxes the original non-convex problem to a sequence of convex problems. This approach is usually not very practical for large-scale problems because its computational cost is a multiple of solving a single convex problem. In this paper, we propose a General Iterative Shrinkage and Thresholding (GIST) algorithm to solve the nonconvex optimization problem for a large class of non-convex penalties. The GIST algorithm iteratively solves a proximal operator problem, which in turn has a closed-form solution for many commonly used penalties. At each outer iteration of the algorithm, we use a line search initialized by the Barzilai-Borwein (BB) rule that allows finding an appropriate step size quickly. The paper also presents a detailed convergence analysis of the GIST algorithm. The efficiency of the proposed algorithm is demonstrated by extensive experiments on large-scale data sets.",
        "bibtex": "@InProceedings{pmlr-v28-gong13a,\n  title = \t {A General Iterative Shrinkage and Thresholding Algorithm for Non-convex Regularized Optimization Problems},\n  author = \t {Gong, Pinghua and Zhang, Changshui and Lu, Zhaosong and Huang, Jianhua and Ye, Jieping},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {37--45},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/gong13a.pdf},\n  url = \t {https://proceedings.mlr.press/v28/gong13a.html},\n  abstract = \t {Non-convex sparsity-inducing penalties have recently received considerable attentions in sparse learning. Recent theoretical investigations have demonstrated their superiority over the convex counterparts in several sparse learning settings. However, solving the non-convex optimization problems associated with non-convex penalties remains a big challenge. A commonly used approach is the Multi-Stage (MS) convex relaxation (or DC programming), which relaxes the original non-convex problem to a sequence of convex problems. This approach is usually not very practical for large-scale problems because its computational cost is a multiple of solving a single convex problem. In this paper, we propose a General Iterative Shrinkage and Thresholding (GIST) algorithm to solve the nonconvex optimization problem for a large class of non-convex penalties. The GIST algorithm iteratively solves a proximal operator problem, which in turn has a closed-form solution for many commonly used penalties. At each outer iteration of the algorithm, we use a line search initialized by the Barzilai-Borwein (BB) rule that allows finding an appropriate step size quickly. The paper also presents a detailed convergence analysis of the GIST algorithm. The efficiency of the proposed algorithm is demonstrated by extensive experiments on large-scale data sets.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/gong13a.pdf",
        "supp": "",
        "pdf_size": 290522,
        "gs_citation": 421,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1480843261454713259&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "State Key Laboratory on Intelligent Technology and Systems+Tsinghua National Laboratory for Information Science and Technology (TNList)+Department of Automation, Tsinghua University, Beijing 100084, China; State Key Laboratory on Intelligent Technology and Systems+Tsinghua National Laboratory for Information Science and Technology (TNList)+Department of Automation, Tsinghua University, Beijing 100084, China; Department of Mathematics, Simon Fraser University, Burnaby, BC, V5A 1S6, Canada; Department of Statistics, Texas A&M University, TX 77843, USA; Computer Science and Engineering, Arizona State University, Tempe, AZ 85287, USA",
        "aff_domain": "mails.tsinghua.edu.cn;mail.tsinghua.edu.cn;sfu.ca;stat.tamu.edu;asu.edu",
        "email": "mails.tsinghua.edu.cn;mail.tsinghua.edu.cn;sfu.ca;stat.tamu.edu;asu.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1+1;0+1+1;2;3;4",
        "aff_unique_norm": "State Key Laboratory on Intelligent Technology and Systems;Tsinghua University;Simon Fraser University;Texas A&M University;Arizona State University",
        "aff_unique_dep": ";National Laboratory for Information Science and Technology;Department of Mathematics;Department of Statistics;Computer Science and Engineering",
        "aff_unique_url": ";http://www.tnlist.org/;https://www.sfu.ca;https://www.tamu.edu;https://www.asu.edu",
        "aff_unique_abbr": ";TNList;SFU;TAMU;ASU",
        "aff_campus_unique_index": "1;1;2;3;4",
        "aff_campus_unique": ";Beijing;Burnaby;College Station;Tempe",
        "aff_country_unique_index": "0+0+0;0+0+0;1;2;2",
        "aff_country_unique": "China;Canada;United States"
    },
    {
        "id": "ab5136c6ee",
        "title": "A Generalized Kernel Approach to Structured Output Learning",
        "site": "https://proceedings.mlr.press/v28/kadri13.html",
        "author": "Hachem Kadri; Mohammad Ghavamzadeh; Philippe Preux",
        "abstract": "We study the problem of structured output learning from a regression perspective. We first provide a general formulation of the kernel dependency estimation (KDE) approach to this problem using operator-valued kernels. Our formulation overcomes the two main limitations of the original KDE approach, namely the decoupling between outputs  in the image space and the inability to use a joint feature space. We then propose a covariance-based operator-valued kernel that allows us to take into account the structure of the kernel feature space. This kernel operates on the output space and only encodes the interactions between the outputs without any reference to the input space. To address this issue, we introduce a variant of our KDE method based on the conditional covariance operator that in addition to the correlation between the outputs takes into account the effects of the input variables. Finally, we evaluate the performance of our KDE approach using both covariance and conditional covariance kernels on three structured output problems, and compare it to the state-of-the art kernel-based structured output regression methods.",
        "bibtex": "@InProceedings{pmlr-v28-kadri13,\n  title = \t {A Generalized Kernel Approach to Structured Output Learning},\n  author = \t {Kadri, Hachem and Ghavamzadeh, Mohammad and Preux, Philippe},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {471--479},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/kadri13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/kadri13.html},\n  abstract = \t {We study the problem of structured output learning from a regression perspective. We first provide a general formulation of the kernel dependency estimation (KDE) approach to this problem using operator-valued kernels. Our formulation overcomes the two main limitations of the original KDE approach, namely the decoupling between outputs  in the image space and the inability to use a joint feature space. We then propose a covariance-based operator-valued kernel that allows us to take into account the structure of the kernel feature space. This kernel operates on the output space and only encodes the interactions between the outputs without any reference to the input space. To address this issue, we introduce a variant of our KDE method based on the conditional covariance operator that in addition to the correlation between the outputs takes into account the effects of the input variables. Finally, we evaluate the performance of our KDE approach using both covariance and conditional covariance kernels on three structured output problems, and compare it to the state-of-the art kernel-based structured output regression methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/kadri13.pdf",
        "supp": "",
        "pdf_size": 423985,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1034792660806196182&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Universit\u00b4 e d\u2019Aix-Marseille, QARMA - LIF/CNRS, FRANCE; INRIA Lille - Nord Europe, Team SequeL, FRANCE; Universit\u00b4 e de Lille, LIFL/CNRS, INRIA, FRANCE",
        "aff_domain": "lif.univ-mrs.fr;inria.fr;inria.fr",
        "email": "lif.univ-mrs.fr;inria.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Universit\u00e9 d\u2019Aix-Marseille;INRIA Lille - Nord Europe;Universit\u00e9 de Lille",
        "aff_unique_dep": "QARMA - LIF/CNRS;Team SequeL;LIFL/CNRS, INRIA",
        "aff_unique_url": "https://www.univ-amu.fr;https://www.inria.fr/en;https://www.univ-lille.fr",
        "aff_unique_abbr": ";INRIA;",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Lille",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "30776861c3",
        "title": "A Local Algorithm for Finding Well-Connected Clusters",
        "site": "https://proceedings.mlr.press/v28/allenzhu13.html",
        "author": "Zeyuan Allen Zhu; Silvio Lattanzi; Vahab Mirrokni",
        "abstract": "Motivated by applications of large-scale graph clustering, we study random-walk-based LOCAL algorithms whose running times depend only on the size of the output cluster, rather than the entire graph. In particular, we develop a method with better theoretical guarantee compared to all previous work, both in terms of the clustering accuracy and the conductance of the output set. We also prove that our analysis is tight, and perform empirical evaluation to support our theory on both synthetic and real data.    More specifically, our method outperforms prior work when the cluster is WELL-CONNECTED. In fact, the better it is well-connected inside, the more significant improvement we can obtain. Our results shed light on why in practice some random-walk-based algorithms perform better than its previous theory, and help guide future research about local clustering.",
        "bibtex": "@InProceedings{pmlr-v28-allenzhu13,\n  title = \t {A Local Algorithm for Finding Well-Connected Clusters},\n  author = \t {Allen Zhu, Zeyuan and Lattanzi, Silvio and Mirrokni, Vahab},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {396--404},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/allenzhu13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/allenzhu13.html},\n  abstract = \t {Motivated by applications of large-scale graph clustering, we study random-walk-based LOCAL algorithms whose running times depend only on the size of the output cluster, rather than the entire graph. In particular, we develop a method with better theoretical guarantee compared to all previous work, both in terms of the clustering accuracy and the conductance of the output set. We also prove that our analysis is tight, and perform empirical evaluation to support our theory on both synthetic and real data.    More specifically, our method outperforms prior work when the cluster is WELL-CONNECTED. In fact, the better it is well-connected inside, the more significant improvement we can obtain. Our results shed light on why in practice some random-walk-based algorithms perform better than its previous theory, and help guide future research about local clustering.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/allenzhu13.pdf",
        "supp": "",
        "pdf_size": 769310,
        "gs_citation": 114,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=117664481395517266&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "MIT CSAIL, 32 Vassar St., Cambridge, MA 02139 USA; Google Research, 111 8th Ave., 4th floor, New York, NY 10011 USA; Google Research, 111 8th Ave., 4th floor, New York, NY 10011 USA",
        "aff_domain": "csail.mit.edu;google.com;google.com",
        "email": "csail.mit.edu;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;Google",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory;Google Research",
        "aff_unique_url": "https://www.csail.mit.edu;https://research.google",
        "aff_unique_abbr": "MIT CSAIL;Google",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Cambridge;New York",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "894cb958e9",
        "title": "A Machine Learning Framework for Programming by Example",
        "site": "https://proceedings.mlr.press/v28/menon13.html",
        "author": "Aditya Menon; Omer Tamuz; Sumit Gulwani; Butler Lampson; Adam Kalai",
        "abstract": "Learning programs is a timely and interesting challenge. In Programming by Example (PBE), a system attempts to infer a program from input and output examples alone, by searching for a composition of some set of base functions. We show how machine learning can be used to speed up this seemingly hopeless search problem, by learning weights that relate textual features describing the provided input-output examples to plausible sub-components of a program. This generic learning framework lets us address problems beyond the scope of earlier PBE systems. Experiments on a prototype implementation show that learning improves search and ranking on a variety of text processing tasks found on help forums.",
        "bibtex": "@InProceedings{pmlr-v28-menon13,\n  title = \t {A Machine Learning Framework for Programming by Example},\n  author = \t {Menon, Aditya and Tamuz, Omer and Gulwani, Sumit and Lampson, Butler and Kalai, Adam},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {187--195},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/menon13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/menon13.html},\n  abstract = \t {Learning programs is a timely and interesting challenge. In Programming by Example (PBE), a system attempts to infer a program from input and output examples alone, by searching for a composition of some set of base functions. We show how machine learning can be used to speed up this seemingly hopeless search problem, by learning weights that relate textual features describing the provided input-output examples to plausible sub-components of a program. This generic learning framework lets us address problems beyond the scope of earlier PBE systems. Experiments on a prototype implementation show that learning improves search and ranking on a variety of text processing tasks found on help forums.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/menon13.pdf",
        "supp": "",
        "pdf_size": 451552,
        "gs_citation": 210,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15043939789527519390&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "University of California, San Diego; Faculty of Mathematics and Computer Science,The Weizmann Institute of Science; Microsoft Research, One Microsoft Way, Redmond, WA 98052; Microsoft Research, One Memorial Drive, Cambridge MA 02142; Microsoft Research, One Memorial Drive, Cambridge MA 02142",
        "aff_domain": "ucsd.edu;weizmann.ac.il;microsoft.com;microsoft.com;microsoft.com",
        "email": "ucsd.edu;weizmann.ac.il;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;2;2",
        "aff_unique_norm": "University of California, San Diego;Weizmann Institute of Science;Microsoft",
        "aff_unique_dep": ";Faculty of Mathematics and Computer Science;Microsoft Research",
        "aff_unique_url": "https://www.ucsd.edu;https://www.weizmann.ac.il;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "UCSD;Weizmann;MSR",
        "aff_campus_unique_index": "0;2;3;3",
        "aff_campus_unique": "San Diego;;Redmond;Cambridge",
        "aff_country_unique_index": "0;1;0;0;0",
        "aff_country_unique": "United States;Israel"
    },
    {
        "id": "3ed0976358",
        "title": "A New Frontier of Kernel Design for Structured Data",
        "site": "https://proceedings.mlr.press/v28/shin13.html",
        "author": "Kilho Shin",
        "abstract": "Many kernels for discretely structured data in the literature are designed within the framework of the convolution kernel and its generalization, the mapping kernel. The two most important advantages to use this framework is an easy-to-check criteria of positive definiteness and efficient computation based on the dynamic programming methodology of the resulting kernels.  On the other hand, the recent theory of partitionable kernels reveals that the known kernels only take advantage of a very small portion of the potential of the framework.  In fact, we have good opportunities to find novel and important kernels in the unexplored area.  In this paper, we shed light on a novel important class of kernels within the framework: We give a mathematical characterization of the class, show a parametric method to optimize kernels of the class to specific problems, based on this characterization, and present some experimental results, which show the new kernels are promising in both accuracy and efficiency.",
        "bibtex": "@InProceedings{pmlr-v28-shin13,\n  title = \t {A New Frontier of Kernel Design for Structured Data},\n  author = \t {Shin, Kilho},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {401--409},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/shin13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/shin13.html},\n  abstract = \t {Many kernels for discretely structured data in the literature are designed within the framework of the convolution kernel and its generalization, the mapping kernel. The two most important advantages to use this framework is an easy-to-check criteria of positive definiteness and efficient computation based on the dynamic programming methodology of the resulting kernels.  On the other hand, the recent theory of partitionable kernels reveals that the known kernels only take advantage of a very small portion of the potential of the framework.  In fact, we have good opportunities to find novel and important kernels in the unexplored area.  In this paper, we shed light on a novel important class of kernels within the framework: We give a mathematical characterization of the class, show a parametric method to optimize kernels of the class to specific problems, based on this characterization, and present some experimental results, which show the new kernels are promising in both accuracy and efficiency.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/shin13.pdf",
        "supp": "",
        "pdf_size": 141577,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13901229480224648215&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Hyogo, 7-1-28 Minatojima-Minami, Kobe 6500047, Japan",
        "aff_domain": "ai.u-hyogo.ac.jp",
        "email": "ai.u-hyogo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Hyogo",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.u-hyogo.ac.jp",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Kobe",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "7a4c038593",
        "title": "A PAC-Bayesian Approach for Domain Adaptation with Specialization to Linear Classifiers",
        "site": "https://proceedings.mlr.press/v28/germain13.html",
        "author": "Pascal Germain; Amaury Habrard; Fran\u00e7ois Laviolette; Emilie Morvant",
        "abstract": "We provide a first PAC-Bayesian analysis for domain adaptation (DA) which arises when the learning and test distributions differ. It relies on a novel distribution pseudodistance based on a disagreement averaging. Using this measure, we derive a PAC-Bayesian DA bound for the stochastic Gibbs classifier. This bound has the advantage of being directly optimizable for any hypothesis space. We specialize it to linear classifiers, and design a learning algorithm which shows interesting results on a synthetic problem and on a popular sentiment annotation task. This opens the door to tackling DA tasks by making use of all the PAC-Bayesian tools.",
        "bibtex": "@InProceedings{pmlr-v28-germain13,\n  title = \t {A PAC-Bayesian Approach for Domain Adaptation with Specialization to Linear Classifiers},\n  author = \t {Germain, Pascal and Habrard, Amaury and Laviolette, Fran\u00e7ois and Morvant, Emilie},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {738--746},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/germain13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/germain13.html},\n  abstract = \t {We provide a first PAC-Bayesian analysis for domain adaptation (DA) which arises when the learning and test distributions differ. It relies on a novel distribution pseudodistance based on a disagreement averaging. Using this measure, we derive a PAC-Bayesian DA bound for the stochastic Gibbs classifier. This bound has the advantage of being directly optimizable for any hypothesis space. We specialize it to linear classifiers, and design a learning algorithm which shows interesting results on a synthetic problem and on a popular sentiment annotation task. This opens the door to tackling DA tasks by making use of all the PAC-Bayesian tools.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/germain13.pdf",
        "supp": "",
        "pdf_size": 1073454,
        "gs_citation": 147,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10254872646618380986&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "D\u00e9partement d\u2019informatique et de g\u00e9nie logiciel, Universit\u00e9 Laval, Qu\u00e9bec, Canada; Laboratoire Hubert Curien UMR CNRS 5516, Universit\u00e9 Jean Monnet, 42000 St-Etienne, France; D\u00e9partement d\u2019informatique et de g\u00e9nie logiciel, Universit\u00e9 Laval, Qu\u00e9bec, Canada; Aix-Marseille Univ., LIF-QARMA, CNRS, UMR 7279, 13013, Marseille, France",
        "aff_domain": "ift.ulaval.ca;univ-st-etienne.fr;ift.ulaval.ca;lif.univ-mrs.fr",
        "email": "ift.ulaval.ca;univ-st-etienne.fr;ift.ulaval.ca;lif.univ-mrs.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "Universit\u00e9 Laval;Universit\u00e9 Jean Monnet;Aix-Marseille University",
        "aff_unique_dep": "D\u00e9partement d\u2019informatique et de g\u00e9nie logiciel;Laboratoire Hubert Curien UMR CNRS 5516;LIF-QARMA",
        "aff_unique_url": "https://www.universite-laval.ca;https://www.univ-jean-monnet.fr;https://www.univ-amu.fr",
        "aff_unique_abbr": "UL;;AMU",
        "aff_campus_unique_index": "0;1;0;2",
        "aff_campus_unique": "Qu\u00e9bec;St-Etienne;Marseille",
        "aff_country_unique_index": "0;1;0;1",
        "aff_country_unique": "Canada;France"
    },
    {
        "id": "2a309ef70b",
        "title": "A Practical Algorithm for Topic Modeling with Provable Guarantees",
        "site": "https://proceedings.mlr.press/v28/arora13.html",
        "author": "Sanjeev Arora; Rong Ge; Yonatan Halpern; David Mimno; Ankur Moitra; David Sontag; Yichen Wu; Michael Zhu",
        "abstract": "Topic models provide a useful method for dimensionality reduction and exploratory data analysis in large text corpora. Most approaches to topic model learning have been based on a maximum likelihood objective. Efficient algorithms exist that attempt to approximate this objective, but they have no provable guarantees. Recently, algorithms have been introduced that provide provable bounds, but these algorithms are not practical because they are inefficient and not robust to violations of model assumptions. In this paper we present an algorithm for learning topic models that is both provable and practical. The algorithm produces results comparable to the best MCMC implementations while running orders of magnitude faster.",
        "bibtex": "@InProceedings{pmlr-v28-arora13,\n  title = \t {A Practical Algorithm for Topic Modeling with Provable Guarantees},\n  author = \t {Arora, Sanjeev and Ge, Rong and Halpern, Yonatan and Mimno, David and Moitra, Ankur and Sontag, David and Wu, Yichen and Zhu, Michael},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {280--288},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/arora13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/arora13.html},\n  abstract = \t {Topic models provide a useful method for dimensionality reduction and exploratory data analysis in large text corpora. Most approaches to topic model learning have been based on a maximum likelihood objective. Efficient algorithms exist that attempt to approximate this objective, but they have no provable guarantees. Recently, algorithms have been introduced that provide provable bounds, but these algorithms are not practical because they are inefficient and not robust to violations of model assumptions. In this paper we present an algorithm for learning topic models that is both provable and practical. The algorithm produces results comparable to the best MCMC implementations while running orders of magnitude faster.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/arora13.pdf",
        "supp": "",
        "pdf_size": 385414,
        "gs_citation": 574,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2951957172627437084&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "Princeton University; Princeton University; New York University; Princeton University; Institute for Advanced Study; New York University; Princeton University; Princeton University",
        "aff_domain": "cs.princeton.edu;cs.princeton.edu;cs.nyu.edu;cs.princeton.edu;ias.edu;cs.nyu.edu;princeton.edu;princeton.edu",
        "email": "cs.princeton.edu;cs.princeton.edu;cs.nyu.edu;cs.princeton.edu;ias.edu;cs.nyu.edu;princeton.edu;princeton.edu",
        "github": "",
        "project": "",
        "author_num": 8,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0;2;1;0;0",
        "aff_unique_norm": "Princeton University;New York University;Institute for Advanced Study",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.princeton.edu;https://www.nyu.edu;https://ias.edu",
        "aff_unique_abbr": "Princeton;NYU;IAS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "61ddbd9b7d",
        "title": "A Randomized Mirror Descent Algorithm for Large Scale Multiple Kernel Learning",
        "site": "https://proceedings.mlr.press/v28/afkanpour13.html",
        "author": "Arash Afkanpour; Andr\u00e1s Gy\u00f6rgy; Csaba Szepesvari; Michael Bowling",
        "abstract": "We consider the problem of simultaneously learning to linearly combine a very large number of kernels and learn a good predictor based on the learnt kernel. When the number of kernels d to be combined is very large, multiple kernel learning methods whose computational cost scales linearly in d are intractable. We propose a randomized version of the mirror descent algorithm to overcome this issue, under the objective of minimizing the group p-norm penalized empirical risk. The key to achieve the required exponential speed-up is the computationally efficient construction of low-variance estimates of the gradient. We propose importance sampling based estimates, and find that the ideal distribution samples a coordinate with a probability proportional to the magnitude of the corresponding gradient. We show that in the case of learning the coefficients of a polynomial kernel, the combinatorial structure of the base kernels to be combined allows sampling from this distribution in O(\\log(d)) time, making the total computational cost of the method to achieve an epsilon-optimal solution to be O(\\log(d)/epsilon^2), thereby allowing our method to operate for very large values of d. Experiments with simulated and real data confirm that the new algorithm is computationally more efficient than its state-of-the-art alternatives.",
        "bibtex": "@InProceedings{pmlr-v28-afkanpour13,\n  title = \t {A Randomized Mirror Descent Algorithm for Large Scale Multiple Kernel Learning},\n  author = \t {Afkanpour, Arash and Gy\u00f6rgy, Andr\u00e1s and Szepesvari, Csaba and Bowling, Michael},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {374--382},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/afkanpour13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/afkanpour13.html},\n  abstract = \t {We consider the problem of simultaneously learning to linearly combine a very large number of kernels and learn a good predictor based on the learnt kernel. When the number of kernels d to be combined is very large, multiple kernel learning methods whose computational cost scales linearly in d are intractable. We propose a randomized version of the mirror descent algorithm to overcome this issue, under the objective of minimizing the group p-norm penalized empirical risk. The key to achieve the required exponential speed-up is the computationally efficient construction of low-variance estimates of the gradient. We propose importance sampling based estimates, and find that the ideal distribution samples a coordinate with a probability proportional to the magnitude of the corresponding gradient. We show that in the case of learning the coefficients of a polynomial kernel, the combinatorial structure of the base kernels to be combined allows sampling from this distribution in O(\\log(d)) time, making the total computational cost of the method to achieve an epsilon-optimal solution to be O(\\log(d)/epsilon^2), thereby allowing our method to operate for very large values of d. Experiments with simulated and real data confirm that the new algorithm is computationally more efficient than its state-of-the-art alternatives.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/afkanpour13.pdf",
        "supp": "",
        "pdf_size": 401274,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11338247149827148615&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Computing Science, University of Alberta, Edmonton, AB, T6G 2E8 Canada; Department of Computing Science, University of Alberta, Edmonton, AB, T6G 2E8 Canada; Department of Computing Science, University of Alberta, Edmonton, AB, T6G 2E8 Canada; Department of Computing Science, University of Alberta, Edmonton, AB, T6G 2E8 Canada",
        "aff_domain": "ualberta.ca;ualberta.ca;ualberta.ca;cs.ualberta.ca",
        "email": "ualberta.ca;ualberta.ca;ualberta.ca;cs.ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "Department of Computing Science",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Edmonton",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "adf62169a2",
        "title": "A Spectral Learning Approach to Range-Only SLAM",
        "site": "https://proceedings.mlr.press/v28/boots13.html",
        "author": "Byron Boots; Geoff Gordon",
        "abstract": "We present a novel spectral learning algorithm for simultaneous localization and mapping (SLAM) from range data with known correspondences.  This algorithm is an instance of a general spectral system identification framework, from which it inherits several desirable properties, including statistical consistency and no local optima. Compared with popular batch optimization or multiple-hypothesis tracking (MHT) methods for range-only SLAM, our spectral approach offers guaranteed low computational requirements and good tracking performance. Compared with MHT and with popular extended Kalman filter (EKF) or extended information filter (EIF) approaches, our approach does not need to linearize a transition or measurement model. We provide a theoretical analysis of our method, including finite-sample error bounds.  Finally, we demonstrate on a real-world robotic SLAM problem that our algorithm is not only theoretically justified, but works well in practice: in a comparison of multiple methods, the lowest errors come from a combination of our algorithm with batch optimization, but our method alone produces nearly as good a result at far lower computational cost.",
        "bibtex": "@InProceedings{pmlr-v28-boots13,\n  title = \t {A Spectral Learning Approach to Range-Only {SLAM}},\n  author = \t {Boots, Byron and Gordon, Geoff},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {19--26},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/boots13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/boots13.html},\n  abstract = \t {We present a novel spectral learning algorithm for simultaneous localization and mapping (SLAM) from range data with known correspondences.  This algorithm is an instance of a general spectral system identification framework, from which it inherits several desirable properties, including statistical consistency and no local optima. Compared with popular batch optimization or multiple-hypothesis tracking (MHT) methods for range-only SLAM, our spectral approach offers guaranteed low computational requirements and good tracking performance. Compared with MHT and with popular extended Kalman filter (EKF) or extended information filter (EIF) approaches, our approach does not need to linearize a transition or measurement model. We provide a theoretical analysis of our method, including finite-sample error bounds.  Finally, we demonstrate on a real-world robotic SLAM problem that our algorithm is not only theoretically justified, but works well in practice: in a comparison of multiple methods, the lowest errors come from a combination of our algorithm with batch optimization, but our method alone produces nearly as good a result at far lower computational cost.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/boots13.pdf",
        "supp": "",
        "pdf_size": 1249394,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2834200868094966073&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "Department of Computer Science and Engineering, University of Washington, Seattle, WA; Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15213",
        "aff_domain": "cs.washington.edu;cs.cmu.edu",
        "email": "cs.washington.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Washington;Carnegie Mellon University",
        "aff_unique_dep": "Department of Computer Science and Engineering;Machine Learning Department",
        "aff_unique_url": "https://www.washington.edu;https://www.cmu.edu",
        "aff_unique_abbr": "UW;CMU",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Seattle;Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b9e1138055",
        "title": "A Structural SVM Based Approach for Optimizing Partial AUC",
        "site": "https://proceedings.mlr.press/v28/narasimhan13.html",
        "author": "Harikrishna Narasimhan; Shivani Agarwal",
        "abstract": "The area under the ROC curve (AUC) is a widely used performance measure in machine learning. Increasingly, however, in several applications, ranging from ranking and biometric screening to medical diagnosis, performance is measured not in terms of the full area under the ROC curve, but instead, in terms of the partial area under the ROC curve between two specified false positive rates. In this paper, we develop a structural SVM framework for directly optimizing the partial AUC between any two false positive rates. Our approach makes use of a cutting plane solver along the lines of the structural SVM based approach for optimizing the full AUC developed by Joachims (2005). Unlike the full AUC, where the combinatorial optimization problem needed to find the most violated constraint in the cutting plane solver can be decomposed easily to yield an efficient algorithm, the corresponding optimization problem in the case of partial AUC is harder to decompose. One of our key technical contributions is an efficient algorithm for solving this combinatorial optimization problem that has the same computational complexity as Joachims\u2019 algorithm for optimizing the usual AUC. This allows us to efficiently optimize the partial AUC in any desired false positive range. We demonstrate the approach on a variety of real-world tasks.",
        "bibtex": "@InProceedings{pmlr-v28-narasimhan13,\n  title = \t {A Structural {SVM} Based Approach for Optimizing Partial AUC},\n  author = \t {Narasimhan, Harikrishna and Agarwal, Shivani},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {516--524},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/narasimhan13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/narasimhan13.html},\n  abstract = \t {The area under the ROC curve (AUC) is a widely used performance measure in machine learning. Increasingly, however, in several applications, ranging from ranking and biometric screening to medical diagnosis, performance is measured not in terms of the full area under the ROC curve, but instead, in terms of the partial area under the ROC curve between two specified false positive rates. In this paper, we develop a structural SVM framework for directly optimizing the partial AUC between any two false positive rates. Our approach makes use of a cutting plane solver along the lines of the structural SVM based approach for optimizing the full AUC developed by Joachims (2005). Unlike the full AUC, where the combinatorial optimization problem needed to find the most violated constraint in the cutting plane solver can be decomposed easily to yield an efficient algorithm, the corresponding optimization problem in the case of partial AUC is harder to decompose. One of our key technical contributions is an efficient algorithm for solving this combinatorial optimization problem that has the same computational complexity as Joachims\u2019 algorithm for optimizing the usual AUC. This allows us to efficiently optimize the partial AUC in any desired false positive range. We demonstrate the approach on a variety of real-world tasks.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/narasimhan13.pdf",
        "supp": "",
        "pdf_size": 258814,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India; Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India",
        "aff_domain": "csa.iisc.ernet.in;csa.iisc.ernet.in",
        "email": "csa.iisc.ernet.in;csa.iisc.ernet.in",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indian Institute of Science",
        "aff_unique_dep": "Department of Computer Science and Automation",
        "aff_unique_url": "https://www.iisc.ac.in",
        "aff_unique_abbr": "IISc",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Bangalore",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "4b5a4d358a",
        "title": "A Unified Robust Regression Model for Lasso-like Algorithms",
        "site": "https://proceedings.mlr.press/v28/yang13e.html",
        "author": "Wenzhuo Yang; Huan Xu",
        "abstract": "We develop a unified robust linear regression model and show that it is equivalent to a general regularization framework to encourage sparse-like structure that contains group Lasso and fused Lasso as specific examples. This provides a robustness interpretation of these widely applied Lasso-like algorithms, and allows us to construct novel generalizations of Lasso-like algorithms by considering different uncertainty sets. Using this robustness interpretation, we present new sparsity results, and establish the statistical consistency of the proposed regularized linear regression. This work extends a classical result from Xu et al. (2010) that relates standard Lasso with robust linear regression to learning problems with more general sparse-like structures, and provides new robustness-based tools to to understand learning problems with sparse-like structures.",
        "bibtex": "@InProceedings{pmlr-v28-yang13e,\n  title = \t {A Unified Robust Regression Model for Lasso-like Algorithms},\n  author = \t {Yang, Wenzhuo and Xu, Huan},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {585--593},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/yang13e.pdf},\n  url = \t {https://proceedings.mlr.press/v28/yang13e.html},\n  abstract = \t {We develop a unified robust linear regression model and show that it is equivalent to a general regularization framework to encourage sparse-like structure that contains group Lasso and fused Lasso as specific examples. This provides a robustness interpretation of these widely applied Lasso-like algorithms, and allows us to construct novel generalizations of Lasso-like algorithms by considering different uncertainty sets. Using this robustness interpretation, we present new sparsity results, and establish the statistical consistency of the proposed regularized linear regression. This work extends a classical result from Xu et al. (2010) that relates standard Lasso with robust linear regression to learning problems with more general sparse-like structures, and provides new robustness-based tools to to understand learning problems with sparse-like structures.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/yang13e.pdf",
        "supp": "",
        "pdf_size": 418764,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3176730322163714818&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Mechanical Engineering, National University of Singapore, Singapore 117576; Department of Mechanical Engineering, National University of Singapore, Singapore 117576",
        "aff_domain": "nus.edu.sg;nus.edu.sg",
        "email": "nus.edu.sg;nus.edu.sg",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "National University of Singapore",
        "aff_unique_dep": "Department of Mechanical Engineering",
        "aff_unique_url": "https://www.nus.edu.sg",
        "aff_unique_abbr": "NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "cf11c3e3d8",
        "title": "A Variational Approximation for Topic Modeling of Hierarchical Corpora",
        "site": "https://proceedings.mlr.press/v28/kim13.html",
        "author": "Do-kyum Kim; Geoffrey Voelker; Lawrence Saul",
        "abstract": "We study the problem of topic modeling in corpora whose documents are organized in a multi-level hierarchy.  We explore a parametric approach to this problem, assuming that the number of topics is known or can be estimated by cross-validation.  The models we consider can be viewed as special (finite-dimensional) instances of hierarchical Dirichlet processes (HDPs).  For these models we show that there exists a simple variational approximation for probabilistic inference.  The approximation relies on a previously unexploited inequality that handles the conditional dependence between Dirichlet latent variables in adjacent levels of the model\u2019s hierarchy.  We compare our approach to existing implementations of nonparametric HDPs.  On several benchmarks we find that our approach is faster than Gibbs sampling and able to learn more predictive models than existing variational methods.  Finally, we demonstrate the large-scale viability of our approach on two newly available corpora from researchers in computer security\u2013one with 350,000 documents and over 6,000 internal subcategories, the other with a five-level deep hierarchy.",
        "bibtex": "@InProceedings{pmlr-v28-kim13,\n  title = \t {A Variational Approximation for Topic Modeling of Hierarchical Corpora},\n  author = \t {Kim, Do-kyum and Voelker, Geoffrey and Saul, Lawrence},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {55--63},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/kim13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/kim13.html},\n  abstract = \t {We study the problem of topic modeling in corpora whose documents are organized in a multi-level hierarchy.  We explore a parametric approach to this problem, assuming that the number of topics is known or can be estimated by cross-validation.  The models we consider can be viewed as special (finite-dimensional) instances of hierarchical Dirichlet processes (HDPs).  For these models we show that there exists a simple variational approximation for probabilistic inference.  The approximation relies on a previously unexploited inequality that handles the conditional dependence between Dirichlet latent variables in adjacent levels of the model\u2019s hierarchy.  We compare our approach to existing implementations of nonparametric HDPs.  On several benchmarks we find that our approach is faster than Gibbs sampling and able to learn more predictive models than existing variational methods.  Finally, we demonstrate the large-scale viability of our approach on two newly available corpora from researchers in computer security\u2013one with 350,000 documents and over 6,000 internal subcategories, the other with a five-level deep hierarchy.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/kim13.pdf",
        "supp": "",
        "pdf_size": 540180,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14147369669479180968&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Computer Science and Engineering, University of California, San Diego; Department of Computer Science and Engineering, University of California, San Diego; Department of Computer Science and Engineering, University of California, San Diego",
        "aff_domain": "cs.ucsd.edu;cs.ucsd.edu;cs.ucsd.edu",
        "email": "cs.ucsd.edu;cs.ucsd.edu;cs.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "05a2b7b14c",
        "title": "A non-IID Framework for Collaborative Filtering with Restricted Boltzmann Machines",
        "site": "https://proceedings.mlr.press/v28/georgiev13.html",
        "author": "Kostadin Georgiev; Preslav Nakov",
        "abstract": "We propose a framework for collaborative filtering based on Restricted Boltzmann Machines (RBM), which extends previous RBM-based approaches in several important directions. First, while previous RBM research has focused on modeling the correlation between item ratings, we model both user-user and item-item correlations in a unified hybrid non-IID framework. We further use real values in the visible layer as opposed to multinomial variables, thus taking advantage of the natural order between user-item ratings. Finally, we explore the potential of combining the original training data with data generated by the RBM-based model itself in a bootstrapping fashion. The evaluation on two MovieLens datasets (with 100K and 1M user-item ratings, respectively), shows that our RBM model rivals the best previously-proposed approaches.",
        "bibtex": "@InProceedings{pmlr-v28-georgiev13,\n  title = \t {A non-IID Framework for Collaborative Filtering with Restricted Boltzmann Machines},\n  author = \t {Georgiev, Kostadin and Nakov, Preslav},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1148--1156},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/georgiev13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/georgiev13.html},\n  abstract = \t {We propose a framework for collaborative filtering based on Restricted Boltzmann Machines (RBM), which extends previous RBM-based approaches in several important directions. First, while previous RBM research has focused on modeling the correlation between item ratings, we model both user-user and item-item correlations in a unified hybrid non-IID framework. We further use real values in the visible layer as opposed to multinomial variables, thus taking advantage of the natural order between user-item ratings. Finally, we explore the potential of combining the original training data with data generated by the RBM-based model itself in a bootstrapping fashion. The evaluation on two MovieLens datasets (with 100K and 1M user-item ratings, respectively), shows that our RBM model rivals the best previously-proposed approaches.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/georgiev13.pdf",
        "supp": "",
        "pdf_size": 437439,
        "gs_citation": 217,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12407742541340980821&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "VMware Bulgaria EOOD, 16A G.M. Dimitrov Blvd, 1797 So\ufb01a, Bulgaria; Qatar Computing Research Institute, Qatar Foundation, Tornado Tower 10th \ufb02oor, PO box 5825, Doha, Qatar",
        "aff_domain": "vmware.com;qf.org.qa",
        "email": "vmware.com;qf.org.qa",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "VMware Bulgaria EOOD;Qatar Computing Research Institute",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.vmware.com/bulgaria.html;https://www.qcri.org",
        "aff_unique_abbr": ";Q-CRI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Bulgaria;Qatar"
    },
    {
        "id": "9ab92e8175",
        "title": "A proximal Newton framework for composite minimization: Graph learning without Cholesky decompositions and matrix inversions",
        "site": "https://proceedings.mlr.press/v28/trandinh13.html",
        "author": "Quoc Tran Dinh; Anastasios Kyrillidis; Volkan Cevher",
        "abstract": "We propose an algorithmic framework for convex minimization problems of composite functions with two terms:  a self-concordant part and a possibly nonsmooth regularization part.  Our method is a new proximal Newton algorithm with local quadratic convergence rate. As a specific problem instance, we consider sparse precision matrix estimation problems in graph learning. Via a careful dual formulation and a novel analytic step-size selection, we instantiate an algorithm within our framework for graph learning that avoids Cholesky decompositions and matrix inversions, making it attractive for parallel and distributed implementations.",
        "bibtex": "@InProceedings{pmlr-v28-trandinh13,\n  title = \t {A proximal {N}ewton framework for composite minimization: Graph learning without {C}holesky decompositions and matrix inversions},\n  author = \t {Tran Dinh, Quoc and Kyrillidis, Anastasios and Cevher, Volkan},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {271--279},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/trandinh13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/trandinh13.html},\n  abstract = \t {We propose an algorithmic framework for convex minimization problems of composite functions with two terms:  a self-concordant part and a possibly nonsmooth regularization part.  Our method is a new proximal Newton algorithm with local quadratic convergence rate. As a specific problem instance, we consider sparse precision matrix estimation problems in graph learning. Via a careful dual formulation and a novel analytic step-size selection, we instantiate an algorithm within our framework for graph learning that avoids Cholesky decompositions and matrix inversions, making it attractive for parallel and distributed implementations. }\n}",
        "pdf": "http://proceedings.mlr.press/v28/trandinh13.pdf",
        "supp": "",
        "pdf_size": 421253,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17474958080197792703&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "LIONS, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne, Switzerland; LIONS, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne, Switzerland; LIONS, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne, Switzerland",
        "aff_domain": "epfl.ch;epfl.ch;epfl.ch",
        "email": "epfl.ch;epfl.ch;epfl.ch",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "EPFL",
        "aff_unique_dep": "LIONS",
        "aff_unique_url": "https://www.epfl.ch",
        "aff_unique_abbr": "EPFL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "083947b5c2",
        "title": "A unifying framework for vector-valued manifold regularization and multi-view learning",
        "site": "https://proceedings.mlr.press/v28/haquang13.html",
        "author": "Minh H\u00e0 Quang; Loris Bazzani; Vittorio Murino",
        "abstract": "This paper presents a general vector-valued reproducing kernel Hilbert spaces (RKHS) formulation for the problem of learning an unknown functional dependency  between a structured input space and a structured output space, in the Semi-Supervised Learning setting. Our formulation includes as special cases Vector-valued Manifold Regularization and Multi-view Learning, thus provides in particular a unifying framework linking these two important learning approaches.  In the case of least square loss function, we provide a closed form solution with an efficient implementation. Numerical experiments on challenging multi-class categorization problems show that our multi-view learning formulation achieves results which are comparable with state of the art and are significantly better than single-view learning.",
        "bibtex": "@InProceedings{pmlr-v28-haquang13,\n  title = \t {A unifying framework for vector-valued manifold regularization and multi-view learning},\n  author = \t {H\u00e0 Quang, Minh and Bazzani, Loris and Murino, Vittorio},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {100--108},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/haquang13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/haquang13.html},\n  abstract = \t {This paper presents a general vector-valued reproducing kernel Hilbert spaces (RKHS) formulation for the problem of learning an unknown functional dependency  between a structured input space and a structured output space, in the Semi-Supervised Learning setting. Our formulation includes as special cases Vector-valued Manifold Regularization and Multi-view Learning, thus provides in particular a unifying framework linking these two important learning approaches.  In the case of least square loss function, we provide a closed form solution with an efficient implementation. Numerical experiments on challenging multi-class categorization problems show that our multi-view learning formulation achieves results which are comparable with state of the art and are significantly better than single-view learning.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/haquang13.pdf",
        "supp": "",
        "pdf_size": 655814,
        "gs_citation": 72,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4490253866086831249&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Istituto Italiano di Tecnologia; Istituto Italiano di Tecnologia; Istituto Italiano di Tecnologia",
        "aff_domain": "iit.it;iit.it;iit.it",
        "email": "iit.it;iit.it;iit.it",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Istituto Italiano di Tecnologia",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iit.it",
        "aff_unique_abbr": "IIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "20ce2b49da",
        "title": "ABC Reinforcement Learning",
        "site": "https://proceedings.mlr.press/v28/dimitrakakis13.html",
        "author": "Christos Dimitrakakis; Nikolaos Tziortziotis",
        "abstract": "We introduce a simple, general framework for likelihood-free Bayesian reinforcement learning, through Approximate Bayesian Computation (ABC). The advantage is that we only require a prior distribution on a class of simulators. This is useful when a probabilistic model of the underlying process is too complex to formulate, but where detailed simulation models are available. ABC-RL allows  the use of any Bayesian reinforcement learning technique in this case. It can be seen as an extension of simulation methods to both planning and inference.  We experimentally demonstrate the potential of this approach in a comparison with LSPI. Finally, we introduce a theorem showing that ABC is sound.",
        "bibtex": "@InProceedings{pmlr-v28-dimitrakakis13,\n  title = \t {ABC Reinforcement Learning},\n  author = \t {Dimitrakakis, Christos and Tziortziotis, Nikolaos},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {684--692},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/dimitrakakis13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/dimitrakakis13.html},\n  abstract = \t {We introduce a simple, general framework for likelihood-free Bayesian reinforcement learning, through Approximate Bayesian Computation (ABC). The advantage is that we only require a prior distribution on a class of simulators. This is useful when a probabilistic model of the underlying process is too complex to formulate, but where detailed simulation models are available. ABC-RL allows  the use of any Bayesian reinforcement learning technique in this case. It can be seen as an extension of simulation methods to both planning and inference.  We experimentally demonstrate the potential of this approach in a comparison with LSPI. Finally, we introduce a theorem showing that ABC is sound.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/dimitrakakis13.pdf",
        "supp": "",
        "pdf_size": 146981,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16796415227407507205&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "EPFL, Lausanne, Switzerland; University of Ioannina, Greece",
        "aff_domain": "gmail.com;gmail.com",
        "email": "gmail.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "EPFL;University of Ioannina",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.epfl.ch;https://www.uoi.gr",
        "aff_unique_abbr": "EPFL;UOI",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Lausanne;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Switzerland;Greece"
    },
    {
        "id": "7ae47a51d2",
        "title": "Active Learning for Multi-Objective Optimization",
        "site": "https://proceedings.mlr.press/v28/zuluaga13.html",
        "author": "Marcela Zuluaga; Guillaume Sergent; Andreas Krause; Markus P\u00fcschel",
        "abstract": "In many fields one encounters the challenge of identifying, out of a pool of possible designs, those that simultaneously optimize multiple objectives. This means that usually there is not one optimal design but an entire set of Pareto-optimal ones with optimal tradeoffs in the objectives. In many applications, evaluating one design is expensive; thus, an exhaustive search for the Pareto-optimal set is unfeasible. To address this challenge, we propose the Pareto Active Learning (PAL) algorithm, which intelligently samples the design space to predict the Pareto-optimal set. Key features of PAL include (1) modeling the objectives as samples from a Gaussian process distribution to capture structure and accommodate noisy evaluation; (2) a method to carefully choose the next design to evaluate to maximize progress; and (3) the ability to control prediction accuracy and sampling cost. We provide theoretical bounds on PAL\u2019s sampling cost required to achieve a desired accuracy. Further, we show an experimental evaluation on three real-world data sets. The results show PAL\u2019s effectiveness; in particular it improves significantly over a state-of-the-art evolutionary algorithm, saving in many cases about 33%.",
        "bibtex": "@InProceedings{pmlr-v28-zuluaga13,\n  title = \t {Active Learning for Multi-Objective Optimization},\n  author = \t {Zuluaga, Marcela and Sergent, Guillaume and Krause, Andreas and P\u00fcschel, Markus},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {462--470},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/zuluaga13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/zuluaga13.html},\n  abstract = \t {In many fields one encounters the challenge of identifying, out of a pool of possible designs, those that simultaneously optimize multiple objectives. This means that usually there is not one optimal design but an entire set of Pareto-optimal ones with optimal tradeoffs in the objectives. In many applications, evaluating one design is expensive; thus, an exhaustive search for the Pareto-optimal set is unfeasible. To address this challenge, we propose the Pareto Active Learning (PAL) algorithm, which intelligently samples the design space to predict the Pareto-optimal set. Key features of PAL include (1) modeling the objectives as samples from a Gaussian process distribution to capture structure and accommodate noisy evaluation; (2) a method to carefully choose the next design to evaluate to maximize progress; and (3) the ability to control prediction accuracy and sampling cost. We provide theoretical bounds on PAL\u2019s sampling cost required to achieve a desired accuracy. Further, we show an experimental evaluation on three real-world data sets. The results show PAL\u2019s effectiveness; in particular it improves significantly over a state-of-the-art evolutionary algorithm, saving in many cases about 33%.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/zuluaga13.pdf",
        "supp": "",
        "pdf_size": 2881944,
        "gs_citation": 223,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10326315225841650317&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "ETH Zurich, Clausiusstrasse 59, 8092 Zurich, Switzerland; ETH Zurich, Universit\u00a8 atstrasse 6, 8092 Zurich, Switzerland; ENS de Lyon, 46 All\u00b4 ee d\u2019Italie, 69007 Lyon, France; ETH Zurich, Clausiusstrasse 59, 8092 Zurich, Switzerland",
        "aff_domain": "inf.ethz.ch;ethz.ch;ens-lyon.fr;inf.ethz.ch",
        "email": "inf.ethz.ch;ethz.ch;ens-lyon.fr;inf.ethz.ch",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "ETH Zurich;Ecole Normale Sup\u00e9rieure de Lyon",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ethz.ch;https://www.ens-lyon.fr",
        "aff_unique_abbr": "ETHZ;ENS de Lyon",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Zurich;Lyon",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "Switzerland;France"
    },
    {
        "id": "2b44fa93f0",
        "title": "Activized Learning with Uniform Classification Noise",
        "site": "https://proceedings.mlr.press/v28/yang13c.html",
        "author": "Liu Yang; Steve Hanneke",
        "abstract": "We prove that for any VC class, it is possible to transform any passive learning algorithm into an active learning algorithm with strong asymptotic improvements in label complexity  for every nontrivial distribution satisfying a uniform classification noise condition.  This generalizes a similar result proven by  (Hanneke, 2009;2012) for the realizable case,  and is the first result establishing that such general improvement guarantees are possible  in the presence of restricted types of  classification noise.",
        "bibtex": "@InProceedings{pmlr-v28-yang13c,\n  title = \t {Activized Learning with Uniform Classification Noise},\n  author = \t {Yang, Liu and Hanneke, Steve},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {370--378},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/yang13c.pdf},\n  url = \t {https://proceedings.mlr.press/v28/yang13c.html},\n  abstract = \t {We prove that for any VC class, it is possible to transform any passive learning algorithm into an active learning algorithm with strong asymptotic improvements in label complexity  for every nontrivial distribution satisfying a uniform classification noise condition.  This generalizes a similar result proven by  (Hanneke, 2009;2012) for the realizable case,  and is the first result establishing that such general improvement guarantees are possible  in the presence of restricted types of  classification noise.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/yang13c.pdf",
        "supp": "",
        "pdf_size": 203005,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3593023333419467338&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Machine Learning Department, Carnegie Mellon University; ",
        "aff_domain": "cs.cmu.edu;gmail.com",
        "email": "cs.cmu.edu;gmail.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Machine Learning Department",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "45aa02fad9",
        "title": "Adaptive Hamiltonian and Riemann Manifold Monte Carlo",
        "site": "https://proceedings.mlr.press/v28/wang13e.html",
        "author": "Ziyu Wang; Shakir Mohamed; Nando Freitas",
        "abstract": "In this paper we address the widely-experienced difficulty in tuning Hamiltonian-based Monte Carlo samplers. We develop an algorithm that allows for the adaptation of Hamiltonian and Riemann manifold Hamiltonian Monte Carlo samplers using Bayesian optimization that allows for infinite adaptation of the parameters of these samplers. We show that the resulting sampling algorithms are ergodic, and demonstrate on several models and data sets that the use of our adaptive algorithms makes it is easy to obtain more efficient samplers, in some precluding the need for more complex models. Hamiltonian-based Monte Carlo samplers are widely known to be an excellent choice of MCMC method, and we aim with this paper to remove a key obstacle towards the more widespread use of these samplers in practice.",
        "bibtex": "@InProceedings{pmlr-v28-wang13e,\n  title = \t {Adaptive Hamiltonian and Riemann Manifold Monte Carlo},\n  author = \t {Wang, Ziyu and Mohamed, Shakir and Freitas, Nando},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1462--1470},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/wang13e.pdf},\n  url = \t {https://proceedings.mlr.press/v28/wang13e.html},\n  abstract = \t {In this paper we address the widely-experienced difficulty in tuning Hamiltonian-based Monte Carlo samplers. We develop an algorithm that allows for the adaptation of Hamiltonian and Riemann manifold Hamiltonian Monte Carlo samplers using Bayesian optimization that allows for infinite adaptation of the parameters of these samplers. We show that the resulting sampling algorithms are ergodic, and demonstrate on several models and data sets that the use of our adaptive algorithms makes it is easy to obtain more efficient samplers, in some precluding the need for more complex models. Hamiltonian-based Monte Carlo samplers are widely known to be an excellent choice of MCMC method, and we aim with this paper to remove a key obstacle towards the more widespread use of these samplers in practice.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/wang13e.pdf",
        "supp": "",
        "pdf_size": 712022,
        "gs_citation": 153,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9907425465412323963&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, University of British Columbia, Vancouver, BC, Canada; Department of Computer Science, University of British Columbia, Vancouver, BC, Canada; Department of Computer Science, University of British Columbia, Vancouver, BC, Canada",
        "aff_domain": "cs.ubc.ca;cs.ubc.ca;cs.ubc.ca",
        "email": "cs.ubc.ca;cs.ubc.ca;cs.ubc.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of British Columbia",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ubc.ca",
        "aff_unique_abbr": "UBC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Vancouver",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "62be0d1ce2",
        "title": "Adaptive Sparsity in Gaussian Graphical Models",
        "site": "https://proceedings.mlr.press/v28/wong13.html",
        "author": "Eleanor Wong; Suyash Awate; P. Thomas Fletcher",
        "abstract": "An effective approach to structure learning and parameter estimation for Gaussian graphical models is to impose a sparsity prior, such as a Laplace prior, on the entries of the precision matrix. Such an approach involves a hyperparameter that must be tuned to control the amount of sparsity. In this paper, we introduce a parameter-free method for estimating a precision matrix with sparsity that adapts to the data automatically. We achieve this by formulating a hierarchical Bayesian model of the precision matrix with a non-informative Jeffreys\u2019 hyperprior. We also naturally enforce the symmetry and positive-definiteness constraints on the precision matrix by parameterizing it with the Cholesky decomposition. Experiments on simulated and real (cell signaling) data demonstrate that the proposed approach not only automatically adapts the sparsity of the model, but it also results in improved estimates of the precision matrix compared to the Laplace prior model with sparsity parameter chosen by cross-validation.",
        "bibtex": "@InProceedings{pmlr-v28-wong13,\n  title = \t {Adaptive Sparsity in {G}aussian Graphical Models},\n  author = \t {Wong, Eleanor and Awate, Suyash and Fletcher, P. Thomas},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {311--319},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/wong13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/wong13.html},\n  abstract = \t {An effective approach to structure learning and parameter estimation for Gaussian graphical models is to impose a sparsity prior, such as a Laplace prior, on the entries of the precision matrix. Such an approach involves a hyperparameter that must be tuned to control the amount of sparsity. In this paper, we introduce a parameter-free method for estimating a precision matrix with sparsity that adapts to the data automatically. We achieve this by formulating a hierarchical Bayesian model of the precision matrix with a non-informative Jeffreys\u2019 hyperprior. We also naturally enforce the symmetry and positive-definiteness constraints on the precision matrix by parameterizing it with the Cholesky decomposition. Experiments on simulated and real (cell signaling) data demonstrate that the proposed approach not only automatically adapts the sparsity of the model, but it also results in improved estimates of the precision matrix compared to the Laplace prior model with sparsity parameter chosen by cross-validation.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/wong13.pdf",
        "supp": "",
        "pdf_size": 293289,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7192687514097467131&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Scientific Computing and Imaging Institute; Scientific Computing and Imaging Institute; Scientific Computing and Imaging Institute",
        "aff_domain": "sci.utah.edu;sci.utah.edu;sci.utah.edu",
        "email": "sci.utah.edu;sci.utah.edu;sci.utah.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Utah",
        "aff_unique_dep": "Scientific Computing and Imaging Institute",
        "aff_unique_url": "https://www.sci.utah.edu",
        "aff_unique_abbr": "SCI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "586174300a",
        "title": "Adaptive Task Assignment for Crowdsourced Classification",
        "site": "https://proceedings.mlr.press/v28/ho13.html",
        "author": "Chien-Ju Ho; Shahin Jabbari; Jennifer Wortman Vaughan",
        "abstract": "Crowdsourcing markets have gained popularity as a tool for inexpensively collecting data from diverse populations of workers. Classification tasks, in which workers provide labels (such as \u201coffensive\u201d or \u201cnot offensive\u201d) for instances (such as websites), are among the most common tasks posted, but due to a mix of human error and the overwhelming prevalence of spam, the labels collected are often noisy. This problem is typically addressed by collecting labels for each instance from multiple workers and combining them in a clever way. However, the question of how to choose which tasks to assign to each worker is often overlooked. We investigate the problem of task assignment and label inference for heterogeneous classification tasks. By applying online primal-dual techniques, we derive a provably near-optimal adaptive assignment algorithm. We show that adaptively assigning workers to tasks can lead to more accurate predictions at a lower cost when the available workers are diverse.",
        "bibtex": "@InProceedings{pmlr-v28-ho13,\n  title = \t {Adaptive Task Assignment for Crowdsourced Classification},\n  author = \t {Ho, Chien-Ju and Jabbari, Shahin and Vaughan, Jennifer Wortman},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {534--542},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/ho13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/ho13.html},\n  abstract = \t {Crowdsourcing markets have gained popularity as a tool for inexpensively collecting data from diverse populations of workers. Classification tasks, in which workers provide labels (such as \u201coffensive\u201d or \u201cnot offensive\u201d) for instances (such as websites), are among the most common tasks posted, but due to a mix of human error and the overwhelming prevalence of spam, the labels collected are often noisy. This problem is typically addressed by collecting labels for each instance from multiple workers and combining them in a clever way. However, the question of how to choose which tasks to assign to each worker is often overlooked. We investigate the problem of task assignment and label inference for heterogeneous classification tasks. By applying online primal-dual techniques, we derive a provably near-optimal adaptive assignment algorithm. We show that adaptively assigning workers to tasks can lead to more accurate predictions at a lower cost when the available workers are diverse.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/ho13.pdf",
        "supp": "",
        "pdf_size": 249541,
        "gs_citation": 342,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14492464870318055950&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of California, Los Angeles; University of California, Los Angeles; Microsoft Research, New York City + University of California, Los Angeles",
        "aff_domain": "cs.ucla.edu;cs.ucla.edu;microsoft.com",
        "email": "cs.ucla.edu;cs.ucla.edu;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1+0",
        "aff_unique_norm": "University of California, Los Angeles;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.ucla.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "UCLA;MSR",
        "aff_campus_unique_index": "0;0;1+0",
        "aff_campus_unique": "Los Angeles;New York City",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1b3232420e",
        "title": "Algebraic classifiers: a generic approach to fast cross-validation, online training, and parallel training",
        "site": "https://proceedings.mlr.press/v28/izbicki13.html",
        "author": "Michael Izbicki",
        "abstract": "We use abstract algebra to derive new algorithms for fast cross-validation, online learning, and parallel learning.  To use these algorithms on a classification model, we must show that the model has appropriate algebraic structure.  It is easy to give algebraic structure to some models, and we do this explicitly for Bayesian classifiers and a novel variation of decision stumps called HomStumps.  But not all classifiers have an obvious structure, so we introduce the Free HomTrainer.  This can be used to give a \u201cgeneric\u201d algebraic structure to any classifier.  We use the Free HomTrainer to give algebraic structure to bagging and boosting.  In so doing, we derive novel online and parallel algorithms, and present the first fast cross-validation schemes for these classifiers.",
        "bibtex": "@InProceedings{pmlr-v28-izbicki13,\n  title = \t {Algebraic classifiers: a generic approach to fast cross-validation, online training, and parallel training},\n  author = \t {Izbicki, Michael},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {648--656},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/izbicki13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/izbicki13.html},\n  abstract = \t {We use abstract algebra to derive new algorithms for fast cross-validation, online learning, and parallel learning.  To use these algorithms on a classification model, we must show that the model has appropriate algebraic structure.  It is easy to give algebraic structure to some models, and we do this explicitly for Bayesian classifiers and a novel variation of decision stumps called HomStumps.  But not all classifiers have an obvious structure, so we introduce the Free HomTrainer.  This can be used to give a \u201cgeneric\u201d algebraic structure to any classifier.  We use the Free HomTrainer to give algebraic structure to bagging and boosting.  In so doing, we derive novel online and parallel algorithms, and present the first fast cross-validation schemes for these classifiers.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/izbicki13.pdf",
        "supp": "",
        "pdf_size": 391244,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17426225609142111194&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "UC Riverside",
        "aff_domain": "izbicki.me",
        "email": "izbicki.me",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of California, Riverside",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucr.edu",
        "aff_unique_abbr": "UCR",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Riverside",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "41a16c12f3",
        "title": "Algorithms for Direct 0\u20131 Loss Optimization in Binary Classification",
        "site": "https://proceedings.mlr.press/v28/nguyen13a.html",
        "author": "Tan Nguyen; Scott Sanner",
        "abstract": "While convex losses for binary classification are attractive due to the existence of numerous (provably) efficient methods for finding their global optima, they are sensitive to outliers.  On the other hand, while the non-convex 0\u20131 loss is robust to outliers, it is NP-hard to optimize and thus rarely directly optimized in practice.  In this paper, however, we do just that: we explore a variety of practical methods for direct (approximate) optimization of the 0\u20131 loss based on branch and bound search, combinatorial search, and coordinate descent on smooth, differentiable relaxations of 0\u20131 loss. Empirically, we compare our proposed algorithms to logistic regression, SVM, and the Bayes point machine showing that the proposed 0\u20131 loss optimization algorithms perform at least as well and offer a clear advantage in the presence of outliers.  To this end, we believe this work reiterates the importance of 0\u20131 loss and its robustness properties while challenging the notion that it is difficult to directly optimize.",
        "bibtex": "@InProceedings{pmlr-v28-nguyen13a,\n  title = \t {Algorithms for Direct 0--1 Loss Optimization in Binary Classification},\n  author = \t {Nguyen, Tan and Sanner, Scott},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1085--1093},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/nguyen13a.pdf},\n  url = \t {https://proceedings.mlr.press/v28/nguyen13a.html},\n  abstract = \t {While convex losses for binary classification are attractive due to the existence of numerous (provably) efficient methods for finding their global optima, they are sensitive to outliers.  On the other hand, while the non-convex 0\u20131 loss is robust to outliers, it is NP-hard to optimize and thus rarely directly optimized in practice.  In this paper, however, we do just that: we explore a variety of practical methods for direct (approximate) optimization of the 0\u20131 loss based on branch and bound search, combinatorial search, and coordinate descent on smooth, differentiable relaxations of 0\u20131 loss. Empirically, we compare our proposed algorithms to logistic regression, SVM, and the Bayes point machine showing that the proposed 0\u20131 loss optimization algorithms perform at least as well and offer a clear advantage in the presence of outliers.  To this end, we believe this work reiterates the importance of 0\u20131 loss and its robustness properties while challenging the notion that it is difficult to directly optimize.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/nguyen13a.pdf",
        "supp": "",
        "pdf_size": 496164,
        "gs_citation": 156,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=649268883182829207&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "QUT, Brisbane, QLD 4001, Australia; NICTA & ANU, Canberra, ACT 2601, Australia",
        "aff_domain": "qut.edu.au;nicta.com.au",
        "email": "qut.edu.au;nicta.com.au",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Queensland University of Technology;Australian National University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.qut.edu.au;https://www.anu.edu.au",
        "aff_unique_abbr": "QUT;ANU",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Brisbane;Canberra",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "ff22da37da",
        "title": "Almost Optimal Exploration in Multi-Armed Bandits",
        "site": "https://proceedings.mlr.press/v28/karnin13.html",
        "author": "Zohar Karnin; Tomer Koren; Oren Somekh",
        "abstract": "We study the problem of exploration in stochastic Multi-Armed Bandits. Even in the simplest setting of identifying the best arm, there remains a logarithmic multiplicative gap between the known lower and upper bounds for the number of arm pulls required for the task. This extra logarithmic factor is quite meaningful in nowadays large-scale applications. We present two novel, parameter-free algorithms for identifying the best arm, in two different settings: given a target confidence and given a target budget of arm pulls, for which we prove upper bounds whose gap from the lower bound is only doubly-logarithmic in the problem parameters. We corroborate our theoretical results with experiments demonstrating that our algorithm outperforms the state-of-the-art and scales better as the size of the problem increases.",
        "bibtex": "@InProceedings{pmlr-v28-karnin13,\n  title = \t {Almost Optimal Exploration in Multi-Armed Bandits},\n  author = \t {Karnin, Zohar and Koren, Tomer and Somekh, Oren},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1238--1246},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/karnin13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/karnin13.html},\n  abstract = \t {We study the problem of exploration in stochastic Multi-Armed Bandits. Even in the simplest setting of identifying the best arm, there remains a logarithmic multiplicative gap between the known lower and upper bounds for the number of arm pulls required for the task. This extra logarithmic factor is quite meaningful in nowadays large-scale applications. We present two novel, parameter-free algorithms for identifying the best arm, in two different settings: given a target confidence and given a target budget of arm pulls, for which we prove upper bounds whose gap from the lower bound is only doubly-logarithmic in the problem parameters. We corroborate our theoretical results with experiments demonstrating that our algorithm outperforms the state-of-the-art and scales better as the size of the problem increases.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/karnin13.pdf",
        "supp": "",
        "pdf_size": 319512,
        "gs_citation": 636,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=304147300452456216&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Yahoo! Labs, Haifa, Israel; Technion\u2014Israel Institute of Technology, Haifa, Israel + Yahoo! Labs, Haifa, Israel; Yahoo! Labs, Haifa, Israel",
        "aff_domain": "ymail.com;technion.ac.il;yahoo-inc.com",
        "email": "ymail.com;technion.ac.il;yahoo-inc.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;0",
        "aff_unique_norm": "Yahoo! Labs;Technion\u2014Israel Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://labs.yahoo.com;https://www.technion.ac.il/en/",
        "aff_unique_abbr": "Yahoo! Labs;Technion",
        "aff_campus_unique_index": "0;0+0;0",
        "aff_campus_unique": "Haifa",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "b683bca710",
        "title": "An Adaptive Learning Rate for Stochastic Variational Inference",
        "site": "https://proceedings.mlr.press/v28/ranganath13.html",
        "author": "Rajesh Ranganath; Chong Wang; Blei David; Eric Xing",
        "abstract": "Stochastic variational inference finds good posterior approximations of probabilistic models with very large data sets.  It optimizes the variational objective with stochastic optimization, following noisy estimates of the natural gradient.  Operationally, stochastic inference iteratively subsamples from the data, analyzes the subsample, and updates parameters with a decreasing learning rate. However, the algorithm is sensitive to that rate, which usually requires hand-tuning to each application. We solve this problem by developing an adaptive learning rate for stochastic inference.  Our method requires no tuning and is easily implemented with computations already made in the algorithm.  We demonstrate our approach with latent Dirichlet allocation applied to three large text corpora.  Inference with the adaptive learning rate converges faster and to a better approximation than the best settings of hand-tuned rates.",
        "bibtex": "@InProceedings{pmlr-v28-ranganath13,\n  title = \t {An Adaptive Learning Rate for Stochastic Variational Inference},\n  author = \t {Ranganath, Rajesh and Wang, Chong and David, Blei and Xing, Eric},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {298--306},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/ranganath13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/ranganath13.html},\n  abstract = \t {Stochastic variational inference finds good posterior approximations of probabilistic models with very large data sets.  It optimizes the variational objective with stochastic optimization, following noisy estimates of the natural gradient.  Operationally, stochastic inference iteratively subsamples from the data, analyzes the subsample, and updates parameters with a decreasing learning rate. However, the algorithm is sensitive to that rate, which usually requires hand-tuning to each application. We solve this problem by developing an adaptive learning rate for stochastic inference.  Our method requires no tuning and is easily implemented with computations already made in the algorithm.  We demonstrate our approach with latent Dirichlet allocation applied to three large text corpora.  Inference with the adaptive learning rate converges faster and to a better approximation than the best settings of hand-tuned rates.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/ranganath13.pdf",
        "supp": "",
        "pdf_size": 1207143,
        "gs_citation": 119,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15317927237685024513&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 21,
        "aff": "Princeton University; Carnegie Mellon University; Princeton University; Carnegie Mellon University",
        "aff_domain": "cs.princeton.edu;cs.cmu.edu;cs.princeton.edu;cs.cmu.edu",
        "email": "cs.princeton.edu;cs.cmu.edu;cs.princeton.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "Princeton University;Carnegie Mellon University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.princeton.edu;https://www.cmu.edu",
        "aff_unique_abbr": "Princeton;CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a7c269454f",
        "title": "An Efficient Posterior Regularized Latent Variable Model for Interactive Sound Source Separation",
        "site": "https://proceedings.mlr.press/v28/bryan13.html",
        "author": "Nicholas Bryan; Gautham Mysore",
        "abstract": "In applications such as audio denoising, music transcription, music remixing, and audio-based forensics, it is desirable to decompose a single-channel recording into its respective sources.  One of the current most effective class of methods to do so is based on non-negative matrix factorization and related latent variable models.  Such techniques, however, typically perform poorly when no isolated training data is given and do not allow user feedback to correct for poor results. To overcome these issues, we allow a user to interactively constrain a latent variable model by painting on a time-frequency display of sound to guide the learning process.  The annotations are used within the framework of posterior regularization to impose linear grouping constraints that would otherwise be difficult to achieve via standard priors.  For the constraints considered, an efficient expectation-maximization algorithm is derived with closed-form multiplicative updates, drawing connections to non-negative matrix factorization methods, and allowing for high-quality interactive-rate separation without explicit training data.",
        "bibtex": "@InProceedings{pmlr-v28-bryan13,\n  title = \t {An Efficient Posterior Regularized Latent Variable Model for Interactive Sound Source Separation},\n  author = \t {Bryan, Nicholas and Mysore, Gautham},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {208--216},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/bryan13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/bryan13.html},\n  abstract = \t {In applications such as audio denoising, music transcription, music remixing, and audio-based forensics, it is desirable to decompose a single-channel recording into its respective sources.  One of the current most effective class of methods to do so is based on non-negative matrix factorization and related latent variable models.  Such techniques, however, typically perform poorly when no isolated training data is given and do not allow user feedback to correct for poor results. To overcome these issues, we allow a user to interactively constrain a latent variable model by painting on a time-frequency display of sound to guide the learning process.  The annotations are used within the framework of posterior regularization to impose linear grouping constraints that would otherwise be difficult to achieve via standard priors.  For the constraints considered, an efficient expectation-maximization algorithm is derived with closed-form multiplicative updates, drawing connections to non-negative matrix factorization methods, and allowing for high-quality interactive-rate separation without explicit training data.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/bryan13.pdf",
        "supp": "",
        "pdf_size": 2241548,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2676298344338491524&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Center for Computer Research in Music and Acoustics, Stanford University; Adobe Research",
        "aff_domain": "ccrma.stanford.edu;adobe.com",
        "email": "ccrma.stanford.edu;adobe.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Stanford University;Adobe",
        "aff_unique_dep": "Center for Computer Research in Music and Acoustics;Adobe Research",
        "aff_unique_url": "https://www.stanford.edu;https://research.adobe.com",
        "aff_unique_abbr": "Stanford;Adobe",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2987dc19cb",
        "title": "An Optimal Policy for Target Localization with Application to Electron Microscopy",
        "site": "https://proceedings.mlr.press/v28/sznitman13.html",
        "author": "Raphael Sznitman; Aurelien Lucchi; Peter Frazier; Bruno Jedynak; Pascal Fua",
        "abstract": "This paper considers the task of finding a target location by making a limited number of sequential observations.  Each observation results from evaluating an imperfect classifier of a chosen cost and accuracy on an interval of chosen length and position.  Within a Bayesian framework, we study the problem of minimizing an objective that combines the entropy of the posterior distribution with the cost of the questions asked.  In this problem, we show that the one-step lookahead policy is Bayes-optimal for any arbitrary time horizon.  Moreover, this one-step lookahead policy is easy to compute and implement. We then use this policy in the context of localizing mitochondria in electron microscope images, and experimentally show that significant speed ups in acquisition can be gained, while maintaining near equal image quality at target locations, when compared to current policies.",
        "bibtex": "@InProceedings{pmlr-v28-sznitman13,\n  title = \t {An Optimal Policy for Target Localization with Application to Electron Microscopy},\n  author = \t {Sznitman, Raphael and Lucchi, Aurelien and Frazier, Peter and Jedynak, Bruno and Fua, Pascal},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1--9},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/sznitman13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/sznitman13.html},\n  abstract = \t {This paper considers the task of finding a target location by making a limited number of sequential observations.  Each observation results from evaluating an imperfect classifier of a chosen cost and accuracy on an interval of chosen length and position.  Within a Bayesian framework, we study the problem of minimizing an objective that combines the entropy of the posterior distribution with the cost of the questions asked.  In this problem, we show that the one-step lookahead policy is Bayes-optimal for any arbitrary time horizon.  Moreover, this one-step lookahead policy is easy to compute and implement. We then use this policy in the context of localizing mitochondria in electron microscope images, and experimentally show that significant speed ups in acquisition can be gained, while maintaining near equal image quality at target locations, when compared to current policies.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/sznitman13.pdf",
        "supp": "",
        "pdf_size": 2529829,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6878408530579035425&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Ecole Polytechnique Federale de Lausanne, Switzerland; Ecole Polytechnique Federale de Lausanne, Switzerland; Cornell University, Ithaca, NY 14850, USA; Johns Hopkins University, Baltimore, MD 21218, USA; Ecole Polytechnique Federale de Lausanne, Switzerland",
        "aff_domain": "epfl.ch;epfl.ch;cornell.edu;jhu.edu;epfl.ch",
        "email": "epfl.ch;epfl.ch;cornell.edu;jhu.edu;epfl.ch",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "Ecole Polytechnique Federale de Lausanne;Cornell University;Johns Hopkins University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.epfl.ch;https://www.cornell.edu;https://www.jhu.edu",
        "aff_unique_abbr": "EPFL;Cornell;JHU",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Ithaca;Baltimore",
        "aff_country_unique_index": "0;0;1;1;0",
        "aff_country_unique": "Switzerland;United States"
    },
    {
        "id": "2cb97c404d",
        "title": "Analogy-preserving Semantic Embedding for Visual Object Categorization",
        "site": "https://proceedings.mlr.press/v28/juhwang13.html",
        "author": "Sung Ju Hwang; Kristen Grauman; Fei Sha",
        "abstract": "In multi-class categorization tasks, knowledge about the classes\u2019 semantic relationships can provide valuable information beyond the class labels themselves.  However, existing techniques focus on preserving the semantic distances between classes (e.g., according to a given object taxonomy for visual recognition), limiting the influence to pairwise structures.  We propose to model \\emphanalogies that reflect the relationships between multiple pairs of classes simultaneously, in the form \u201cp is to q, as r is to s\"\".  We translate semantic analogies into higher-order geometric constraints called \\emphanalogical parallelograms, and use them in a novel convex regularizer for a discriminatively learned label embedding.   Furthermore, we show how to discover analogies from attribute-based class descriptions, and how to prioritize those likely to reduce inter-class confusion.  Evaluating our Analogy-preserving Semantic Embedding (ASE) on two visual recognition datasets, we demonstrate clear improvements over existing approaches, both in terms of recognition accuracy and analogy completion.",
        "bibtex": "@InProceedings{pmlr-v28-juhwang13,\n  title = \t {Analogy-preserving Semantic Embedding for Visual Object Categorization},\n  author = \t {Ju Hwang, Sung and Grauman, Kristen and Sha, Fei},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {639--647},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/juhwang13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/juhwang13.html},\n  abstract = \t {In multi-class categorization tasks, knowledge about the classes\u2019 semantic relationships can provide valuable information beyond the class labels themselves.  However, existing techniques focus on preserving the semantic distances between classes (e.g., according to a given object taxonomy for visual recognition), limiting the influence to pairwise structures.  We propose to model \\emphanalogies that reflect the relationships between multiple pairs of classes simultaneously, in the form \u201cp is to q, as r is to s\"\".  We translate semantic analogies into higher-order geometric constraints called \\emphanalogical parallelograms, and use them in a novel convex regularizer for a discriminatively learned label embedding.   Furthermore, we show how to discover analogies from attribute-based class descriptions, and how to prioritize those likely to reduce inter-class confusion.  Evaluating our Analogy-preserving Semantic Embedding (ASE) on two visual recognition datasets, we demonstrate clear improvements over existing approaches, both in terms of recognition accuracy and analogy completion.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/juhwang13.pdf",
        "supp": "",
        "pdf_size": 351865,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9332855910734484101&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, University of Texas, Austin, TX, 78701; Department of Computer Science, University of Texas, Austin, TX, 78701; Department of Computer Science, University of Southern California, Los Angeles, CA, 90089",
        "aff_domain": "cs.utexas.edu;cs.utexas.edu;usc.edu",
        "email": "cs.utexas.edu;cs.utexas.edu;usc.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Texas at Austin;University of Southern California",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.utexas.edu;https://www.usc.edu",
        "aff_unique_abbr": "UT Austin;USC",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Austin;Los Angeles",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8cadbcee45",
        "title": "Anytime Representation Learning",
        "site": "https://proceedings.mlr.press/v28/xu13b.html",
        "author": "Zhixiang Xu; Matt Kusner; Gao Huang; Kilian Weinberger",
        "abstract": "Evaluation cost during test-time is becoming increasingly important as many real-world applications need fast evaluation (e.g. web search engines, email spam filtering) or use expensive features (e.g. medical diagnosis). We introduce Anytime Feature Representations (AFR), a novel algorithm that explicitly addresses this trade-off in the data representation rather than in the classifier. This enables us to turn conventional classifiers, in particular Support Vector Machines, into test-time cost sensitive",
        "bibtex": "@InProceedings{pmlr-v28-xu13b,\n  title = \t {Anytime Representation Learning},\n  author = \t {Xu, Zhixiang and Kusner, Matt and Huang, Gao and Weinberger, Kilian},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1076--1084},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/xu13b.pdf},\n  url = \t {https://proceedings.mlr.press/v28/xu13b.html},\n  abstract = \t {Evaluation cost during test-time is becoming increasingly important as many real-world applications need fast evaluation (e.g. web search engines, email spam filtering) or use expensive features (e.g. medical diagnosis). We introduce Anytime Feature Representations (AFR), a novel algorithm that explicitly addresses this trade-off in the data representation rather than in the classifier. This enables us to turn conventional classifiers, in particular Support Vector Machines, into test-time cost sensitive",
        "pdf": "http://proceedings.mlr.press/v28/xu13b.pdf",
        "supp": "",
        "pdf_size": 1516854,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14104197939988803813&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Washington University; Washington University; Tsinghua University; Washington University",
        "aff_domain": "cse.wustl.edu;wustl.edu;mails.tsinghua.edu.cn;wustl.edu",
        "email": "cse.wustl.edu;wustl.edu;mails.tsinghua.edu.cn;wustl.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Washington University in St. Louis;Tsinghua University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://wustl.edu;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "WUSTL;THU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "St. Louis;",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "fa7cb8bc56",
        "title": "Approximate Inference in Collective Graphical Models",
        "site": "https://proceedings.mlr.press/v28/sheldon13.html",
        "author": "Daniel Sheldon; Tao Sun; Akshat Kumar; Tom Dietterich",
        "abstract": "We study the problem of approximate inference in collective graphical models (CGMs), which were recently introduced to model the problem of learning and inference with noisy aggregate observations. We first analyze the complexity of inference in CGMs: unlike inference in conventional graphical models, exact inference in CGMs is NP-hard even for tree-structured models. We then develop a tractable convex approximation to the NP-hard MAP inference problem in CGMs, and show how to use MAP inference for approximate marginal inference within the EM framework. We demonstrate empirically that these approximation techniques can reduce the computational cost of inference by two orders of magnitude and the cost of learning by at least an order of magnitude while providing solutions of equal or better quality.",
        "bibtex": "@InProceedings{pmlr-v28-sheldon13,\n  title = \t {Approximate Inference in Collective Graphical Models},\n  author = \t {Sheldon, Daniel and Sun, Tao and Kumar, Akshat and Dietterich, Tom},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1004--1012},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/sheldon13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/sheldon13.html},\n  abstract = \t {We study the problem of approximate inference in collective graphical models (CGMs), which were recently introduced to model the problem of learning and inference with noisy aggregate observations. We first analyze the complexity of inference in CGMs: unlike inference in conventional graphical models, exact inference in CGMs is NP-hard even for tree-structured models. We then develop a tractable convex approximation to the NP-hard MAP inference problem in CGMs, and show how to use MAP inference for approximate marginal inference within the EM framework. We demonstrate empirically that these approximation techniques can reduce the computational cost of inference by two orders of magnitude and the cost of learning by at least an order of magnitude while providing solutions of equal or better quality.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/sheldon13.pdf",
        "supp": "",
        "pdf_size": 401826,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17504770675851046970&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "University of Massachusetts, Amherst, MA 01002, USA; University of Massachusetts, Amherst, MA 01002, USA; IBM Research, New Delhi 110070, India; Oregon State University, Corvallis, OR 97331, USA",
        "aff_domain": "cs.umass.edu;cs.umass.edu;gmail.com;eecs.oregonstate.edu",
        "email": "cs.umass.edu;cs.umass.edu;gmail.com;eecs.oregonstate.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "University of Massachusetts Amherst;IBM;Oregon State University",
        "aff_unique_dep": ";IBM Research;",
        "aff_unique_url": "https://www.umass.edu;https://www.ibm.com/research;https://oregonstate.edu",
        "aff_unique_abbr": "UMass Amherst;IBM;OSU",
        "aff_campus_unique_index": "0;0;1;2",
        "aff_campus_unique": "Amherst;New Delhi;Corvallis",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "United States;India"
    },
    {
        "id": "75aa6dc2cf",
        "title": "Approximation properties of DBNs with binary hidden units and real-valued visible units",
        "site": "https://proceedings.mlr.press/v28/krause13.html",
        "author": "Oswin Krause; Asja Fischer; Tobias Glasmachers; Christian Igel",
        "abstract": "Deep belief networks (DBNs) can approximate any distribution over fixed-length binary vectors. However, DBNs are frequently applied to model real-valued data, and so far little is known about their representational power in this case.  We analyze the approximation properties of DBNs with two layers of binary hidden units and visible units with conditional distributions from the exponential family. It is shown that these DBNs can, under mild assumptions, model any additive mixture of distributions from the exponential family with independent variables. An arbitrarily good approximation in terms of Kullback-Leibler divergence of an m-dimensional mixture distribution with n components can be achieved by a DBN with m visible variables and n and n+1 hidden variables in the first and second hidden layer, respectively. Furthermore, relevant infinite mixtures can be approximated arbitrarily well by a DBN with a finite number of neurons. This includes the important special case of an infinite mixture of Gaussian distributions with fixed variance restricted to a compact domain, which in turn can approximate any strictly positive density over this domain.",
        "bibtex": "@InProceedings{pmlr-v28-krause13,\n  title = \t {Approximation properties of {DBNs} with binary hidden units and real-valued visible units},\n  author = \t {Krause, Oswin and Fischer, Asja and Glasmachers, Tobias and Igel, Christian},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {419--426},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/krause13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/krause13.html},\n  abstract = \t {Deep belief networks (DBNs) can approximate any distribution over fixed-length binary vectors. However, DBNs are frequently applied to model real-valued data, and so far little is known about their representational power in this case.  We analyze the approximation properties of DBNs with two layers of binary hidden units and visible units with conditional distributions from the exponential family. It is shown that these DBNs can, under mild assumptions, model any additive mixture of distributions from the exponential family with independent variables. An arbitrarily good approximation in terms of Kullback-Leibler divergence of an m-dimensional mixture distribution with n components can be achieved by a DBN with m visible variables and n and n+1 hidden variables in the first and second hidden layer, respectively. Furthermore, relevant infinite mixtures can be approximated arbitrarily well by a DBN with a finite number of neurons. This includes the important special case of an infinite mixture of Gaussian distributions with fixed variance restricted to a compact domain, which in turn can approximate any strictly positive density over this domain.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/krause13.pdf",
        "supp": "",
        "pdf_size": 347836,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5924000265448168292&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, University of Copenhagen, 2100 Copenhagen, Denmark; Institut f\u00a8 ur Neuroinformatik, Ruhr-Universit\u00a8 at Bochum, 44780 Bochum, Germany + Department of Computer Science, University of Copenhagen, 2100 Copenhagen, Denmark; Institut f\u00a8 ur Neuroinformatik, Ruhr-Universit\u00a8 at Bochum, 44780 Bochum, Germany; Department of Computer Science, University of Copenhagen, 2100 Copenhagen, Denmark",
        "aff_domain": "diku.dk;ini.rub.de;ini.rub.de;diku.dk",
        "email": "diku.dk;ini.rub.de;ini.rub.de;diku.dk",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;1;0",
        "aff_unique_norm": "University of Copenhagen;Ruhr-Universit\u00e4t Bochum",
        "aff_unique_dep": "Department of Computer Science;Institut f\u00fcr Neuroinformatik",
        "aff_unique_url": "https://www.ku.dk;https://www.ruhr-uni-bochum.de",
        "aff_unique_abbr": "UCPH;",
        "aff_campus_unique_index": "0;1+0;1;0",
        "aff_campus_unique": "Copenhagen;Bochum",
        "aff_country_unique_index": "0;1+0;1;0",
        "aff_country_unique": "Denmark;Germany"
    },
    {
        "id": "372a9eec26",
        "title": "Average Reward Optimization Objective In Partially Observable Domains",
        "site": "https://proceedings.mlr.press/v28/grinberg13.html",
        "author": "Yuri Grinberg; Doina Precup",
        "abstract": "We consider the problem of average reward optimization in domains with partial observability, within the modeling framework of linear predictive state representations (PSRs). The key to average-reward computation is to have a well-defined stationary behavior of a system, so the required averages can be computed. If, additionally, the stationary behavior varies smoothly with changes in policy parameters, average-reward control through policy search also becomes a possibility. In this paper, we show that PSRs have a well-behaved stationary distribution, which is a rational function of policy parameters.  Based on this result, we define a related reward process particularly suitable for average reward optimization, and analyze its properties. We show that in such a predictive state reward process, the average reward is a rational function of the policy parameters, whose complexity depends on the dimension of the underlying linear PSR. This result suggests that average reward-based policy search methods can be effective when the dimension of the system is small, even when the system representation in the POMDP framework requires many hidden states. We provide illustrative examples of this type.",
        "bibtex": "@InProceedings{pmlr-v28-grinberg13,\n  title = \t {Average Reward Optimization Objective In Partially Observable Domains},\n  author = \t {Grinberg, Yuri and Precup, Doina},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {320--328},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/grinberg13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/grinberg13.html},\n  abstract = \t {We consider the problem of average reward optimization in domains with partial observability, within the modeling framework of linear predictive state representations (PSRs). The key to average-reward computation is to have a well-defined stationary behavior of a system, so the required averages can be computed. If, additionally, the stationary behavior varies smoothly with changes in policy parameters, average-reward control through policy search also becomes a possibility. In this paper, we show that PSRs have a well-behaved stationary distribution, which is a rational function of policy parameters.  Based on this result, we define a related reward process particularly suitable for average reward optimization, and analyze its properties. We show that in such a predictive state reward process, the average reward is a rational function of the policy parameters, whose complexity depends on the dimension of the underlying linear PSR. This result suggests that average reward-based policy search methods can be effective when the dimension of the system is small, even when the system representation in the POMDP framework requires many hidden states. We provide illustrative examples of this type.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/grinberg13.pdf",
        "supp": "",
        "pdf_size": 692006,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12879616312700949774&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "School of Computer Science, McGill University, Canada; School of Computer Science, McGill University, Canada",
        "aff_domain": "cs.mcgill.ca;cs.mcgill.ca",
        "email": "cs.mcgill.ca;cs.mcgill.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "McGill University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.mcgill.ca",
        "aff_unique_abbr": "McGill",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "9c843ac74e",
        "title": "Bayesian Games for Adversarial Regression Problems",
        "site": "https://proceedings.mlr.press/v28/grosshans13.html",
        "author": "Michael Gro\u00dfhans; Christoph Sawade; Michael Br\u00fcckner; Tobias Scheffer",
        "abstract": "We study regression problems in which an adversary can exercise some control over the data generation process. Learner and adversary have conflicting but not necessarily perfectly antagonistic objectives. We study the case in which the learner is not fully informed about the adversary\u2019s objective; instead, any knowledge of the learner about parameters of the adversary\u2019s goal may be reflected in a Bayesian prior. We model this problem as a Bayesian game, and characterize conditions under which a unique Bayesian equilibrium point exists. We experimentally compare the Bayesian equilibrium strategy to the Nash equilibrium strategy, the minimax strategy, and regular linear regression.",
        "bibtex": "@InProceedings{pmlr-v28-grosshans13,\n  title = \t {Bayesian Games for Adversarial Regression Problems},\n  author = \t {Gro\u00dfhans, Michael and Sawade, Christoph and Br\u00fcckner, Michael and Scheffer, Tobias},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {55--63},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/grosshans13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/grosshans13.html},\n  abstract = \t {We study regression problems in which an adversary can exercise some control over the data generation process. Learner and adversary have conflicting but not necessarily perfectly antagonistic objectives. We study the case in which the learner is not fully informed about the adversary\u2019s objective; instead, any knowledge of the learner about parameters of the adversary\u2019s goal may be reflected in a Bayesian prior. We model this problem as a Bayesian game, and characterize conditions under which a unique Bayesian equilibrium point exists. We experimentally compare the Bayesian equilibrium strategy to the Nash equilibrium strategy, the minimax strategy, and regular linear regression.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/grosshans13.pdf",
        "supp": "",
        "pdf_size": 1748531,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1827159952436421699&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "University of Potsdam, Department of Computer Science; University of Potsdam, Department of Computer Science; SoundCloud Ltd.; University of Potsdam, Department of Computer Science",
        "aff_domain": "cs.uni-potsdam.de;cs.uni-potsdam.de;soundcloud.com;cs.uni-potsdam.de",
        "email": "cs.uni-potsdam.de;cs.uni-potsdam.de;soundcloud.com;cs.uni-potsdam.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Potsdam;SoundCloud",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.uni-potsdam.de;https://soundcloud.com",
        "aff_unique_abbr": ";SoundCloud",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "Germany;United Kingdom"
    },
    {
        "id": "a76decc91a",
        "title": "Bayesian Learning of Recursively Factored Environments",
        "site": "https://proceedings.mlr.press/v28/bellemare13.html",
        "author": "Marc Bellemare; Joel Veness; Michael Bowling",
        "abstract": "Model-based reinforcement learning techniques have historically encountered a number of difficulties scaling up to large observation spaces. One promising approach has been to decompose the model learning task into a number of smaller, more manageable sub-problems by factoring the observation space. Typically, many different factorizations are possible, which can make it difficult to select an appropriate factorization without extensive testing. In this paper we introduce the class of recursively decomposable factorizations, and show how exact Bayesian inference can be used to efficiently guarantee predictive performance close to the best factorization in this class. We demonstrate the strength of this approach by presenting a collection of empirical results for 20 different Atari 2600 games.",
        "bibtex": "@InProceedings{pmlr-v28-bellemare13,\n  title = \t {Bayesian Learning of Recursively Factored Environments},\n  author = \t {Bellemare, Marc and Veness, Joel and Bowling, Michael},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1211--1219},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/bellemare13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/bellemare13.html},\n  abstract = \t {Model-based reinforcement learning techniques have historically encountered a number of difficulties scaling up to large observation spaces. One promising approach has been to decompose the model learning task into a number of smaller, more manageable sub-problems by factoring the observation space. Typically, many different factorizations are possible, which can make it difficult to select an appropriate factorization without extensive testing. In this paper we introduce the class of recursively decomposable factorizations, and show how exact Bayesian inference can be used to efficiently guarantee predictive performance close to the best factorization in this class. We demonstrate the strength of this approach by presenting a collection of empirical results for 20 different Atari 2600 games.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/bellemare13.pdf",
        "supp": "",
        "pdf_size": 357636,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7006967271784757710&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "University of Alberta, Edmonton, Canada, T6G 2E8; University of Alberta, Edmonton, Canada, T6G 2E8; University of Alberta, Edmonton, Canada, T6G 2E8",
        "aff_domain": "cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca",
        "email": "cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Edmonton",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "e0aa2eb7cb",
        "title": "Better Mixing via Deep Representations",
        "site": "https://proceedings.mlr.press/v28/bengio13.html",
        "author": "Yoshua Bengio; Gregoire Mesnil; Yann Dauphin; Salah Rifai",
        "abstract": "It has been hypothesized, and supported with experimental evidence, that deeper representations, when well trained, tend to do a better job at disentangling the underlying factors of variation.  We study the following related conjecture: better representations, in the sense of better disentangling, can be exploited to produce Markov chains that mix faster between modes. Consequently, mixing between modes would be more efficient at higher levels of representation.  To better understand this, we propose a secondary conjecture: the higher-level samples fill more uniformly the space they occupy and the high-density manifolds tend to unfold when represented at higher levels.  The paper discusses these hypotheses and tests them experimentally through visualization and measurements of mixing between modes and interpolating between samples.",
        "bibtex": "@InProceedings{pmlr-v28-bengio13,\n  title = \t {Better Mixing via Deep Representations},\n  author = \t {Bengio, Yoshua and Mesnil, Gregoire and Dauphin, Yann and Rifai, Salah},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {552--560},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/bengio13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/bengio13.html},\n  abstract = \t {It has been hypothesized, and supported with experimental evidence, that deeper representations, when well trained, tend to do a better job at disentangling the underlying factors of variation.  We study the following related conjecture: better representations, in the sense of better disentangling, can be exploited to produce Markov chains that mix faster between modes. Consequently, mixing between modes would be more efficient at higher levels of representation.  To better understand this, we propose a secondary conjecture: the higher-level samples fill more uniformly the space they occupy and the high-density manifolds tend to unfold when represented at higher levels.  The paper discusses these hypotheses and tests them experimentally through visualization and measurements of mixing between modes and interpolating between samples.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/bengio13.pdf",
        "supp": "",
        "pdf_size": 652978,
        "gs_citation": 471,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6451747009255969438&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 15,
        "aff": "Dept. IRO, Universit\u00e9 de Montr\u00e9al. Montr\u00e9al (QC), H2C 3J7, Canada+LITIS EA 4108, Universit\u00e9 de Rouen. 768000 Saint Etienne du Rouvray, France; Dept. IRO, Universit\u00e9 de Montr\u00e9al. Montr\u00e9al (QC), H2C 3J7, Canada; ; Dept. IRO, Universit\u00e9 de Montr\u00e9al. Montr\u00e9al (QC), H2C 3J7, Canada",
        "aff_domain": "umontreal.ca;umontreal.ca;umontreal.ca;umontreal.ca",
        "email": "umontreal.ca;umontreal.ca;umontreal.ca;umontreal.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "Universit\u00e9 de Montr\u00e9al;Universit\u00e9 de Rouen",
        "aff_unique_dep": "Dept. IRO;LITIS EA 4108",
        "aff_unique_url": "https://www.umontreal.ca;https://www.univ-rouen.fr",
        "aff_unique_abbr": "UdeM;",
        "aff_campus_unique_index": "0+1;0;0",
        "aff_campus_unique": "Montr\u00e9al;Saint Etienne du Rouvray",
        "aff_country_unique_index": "0+1;0;0",
        "aff_country_unique": "Canada;France"
    },
    {
        "id": "f796b8402f",
        "title": "Better Rates for Any Adversarial Deterministic MDP",
        "site": "https://proceedings.mlr.press/v28/dekel13.html",
        "author": "Ofer Dekel; Elad Hazan",
        "abstract": "We consider regret minimization in adversarial deterministic Markov  Decision Processes (ADMDPs) with bandit feedback. We devise a new  algorithm that pushes the state-of-the-art forward in two ways: First,  it attains a regret of O(T^2/3) with respect to the best fixed  policy in hindsight, whereas the previous best regret bound was  O(T^3/4). Second, the algorithm and its analysis are compatible  with any feasible ADMDP graph topology, while all previous approaches  required additional restrictions on the graph topology.",
        "bibtex": "@InProceedings{pmlr-v28-dekel13,\n  title = \t {Better Rates for Any Adversarial Deterministic MDP},\n  author = \t {Dekel, Ofer and Hazan, Elad},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {675--683},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/dekel13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/dekel13.html},\n  abstract = \t {We consider regret minimization in adversarial deterministic Markov  Decision Processes (ADMDPs) with bandit feedback. We devise a new  algorithm that pushes the state-of-the-art forward in two ways: First,  it attains a regret of O(T^2/3) with respect to the best fixed  policy in hindsight, whereas the previous best regret bound was  O(T^3/4). Second, the algorithm and its analysis are compatible  with any feasible ADMDP graph topology, while all previous approaches  required additional restrictions on the graph topology.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/dekel13.pdf",
        "supp": "",
        "pdf_size": 316948,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16711868831912895538&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Microsoft Research, 1 Microsoft Way, Redmond, WA 98052, USA; Technion - Israel Inst. of Tech., Haifa 32000, Israel",
        "aff_domain": "microsoft.com;ie.technion.ac.il",
        "email": "microsoft.com;ie.technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Microsoft;Technion - Israel Institute of Technology",
        "aff_unique_dep": "Microsoft Research;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://www.technion.ac.il/en/",
        "aff_unique_abbr": "MSR;Technion",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Redmond;Haifa",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;Israel"
    },
    {
        "id": "fb5aae1fc7",
        "title": "Block-Coordinate Frank-Wolfe Optimization for Structural SVMs",
        "site": "https://proceedings.mlr.press/v28/lacoste-julien13.html",
        "author": "Simon Lacoste-Julien; Martin Jaggi; Mark Schmidt; Patrick Pletscher",
        "abstract": "We propose a randomized block-coordinate variant of the classic Frank-Wolfe algorithm for convex optimization with block-separable constraints. Despite its lower iteration cost, we show that it achieves a similar convergence rate in duality gap as the full Frank-Wolfe algorithm. We also show that, when applied to the dual structural support vector machine (SVM) objective, this yields an online algorithm that has the same low iteration complexity as primal stochastic subgradient methods. However, unlike stochastic subgradient methods, the block-coordinate Frank-Wolfe algorithm allows us to compute the optimal step-size and yields a computable duality gap guarantee. Our experiments indicate that this simple algorithm outperforms competing structural SVM solvers.",
        "bibtex": "@InProceedings{pmlr-v28-lacoste-julien13,\n  title = \t {Block-Coordinate {Frank-Wolfe} Optimization for Structural {SVMs}},\n  author = \t {Lacoste-Julien, Simon and Jaggi, Martin and Schmidt, Mark and Pletscher, Patrick},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {53--61},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/lacoste-julien13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/lacoste-julien13.html},\n  abstract = \t {We propose a randomized block-coordinate variant of the classic Frank-Wolfe algorithm for convex optimization with block-separable constraints. Despite its lower iteration cost, we show that it achieves a similar convergence rate in duality gap as the full Frank-Wolfe algorithm. We also show that, when applied to the dual structural support vector machine (SVM) objective, this yields an online algorithm that has the same low iteration complexity as primal stochastic subgradient methods. However, unlike stochastic subgradient methods, the block-coordinate Frank-Wolfe algorithm allows us to compute the optimal step-size and yields a computable duality gap guarantee. Our experiments indicate that this simple algorithm outperforms competing structural SVM solvers.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/lacoste-julien13.pdf",
        "supp": "",
        "pdf_size": 1101010,
        "gs_citation": 460,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1037461109655326854&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 13,
        "aff": "INRIA - SIERRA project-team, \u00b4Ecole Normale Sup\u00b4erieure, Paris, France+CMAP, \u00b4Ecole Polytechnique, Palaiseau, France; CMAP, \u00b4Ecole Polytechnique, Palaiseau, France; INRIA - SIERRA project-team, \u00b4Ecole Normale Sup\u00b4erieure, Paris, France; Machine Learning Laboratory, ETH Zurich, Switzerland",
        "aff_domain": "inria.fr;polytechnique.edu;inria.fr;inf.ethz.ch",
        "email": "inria.fr;polytechnique.edu;inria.fr;inf.ethz.ch",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;0;2",
        "aff_unique_norm": "Ecole Normale Sup\u00e9rieure;Ecole Polytechnique;ETH Zurich",
        "aff_unique_dep": "INRIA - SIERRA project-team;CMAP;Machine Learning Laboratory",
        "aff_unique_url": "https://www.ens.fr;https://www.ecolepolytechnique.fr;https://www.ethz.ch",
        "aff_unique_abbr": "ENS;Polytechnique;ETHZ",
        "aff_campus_unique_index": "0+1;1;0",
        "aff_campus_unique": "Paris;Palaiseau;",
        "aff_country_unique_index": "0+0;0;0;1",
        "aff_country_unique": "France;Switzerland"
    },
    {
        "id": "c67869ffc9",
        "title": "Breaking the Small Cluster Barrier of Graph Clustering",
        "site": "https://proceedings.mlr.press/v28/ailon13.html",
        "author": "Nir Ailon; Yudong Chen; Huan Xu",
        "abstract": "This paper investigates graph clustering in the planted cluster model in the   presence of  \\em small clusters. Traditional results dictate that for an   algorithm to provably correctly recover the clusters, \\em all clusters must be   sufficiently large (in particular, \\tilde\u03a9(\\sqrtn) where n is the number   of nodes of the graph). We show that this is not really a restriction: by a more refined   analysis of the trace-norm based matrix recovery approach proposed in (Jalali et al. 2011) and (Chen et al. 2012), we prove that small clusters, under certain mild assuptions, do not hinder recovery of large ones.  Based on this result, we further devise an iterative algorithm   to recover \\em almost all clusters via a \u201cpeeling strategy\u201d, i.e., recover large clusters   first, leading to a reduced problem, and repeat this procedure.   These results are extended to the    \\em partial observation setting, in which only a (chosen) part of the graph is observed.  The peeling strategy gives rise to an active learning algorithm, in which   edges adjacent to smaller clusters are queried more often as large clusters are learned  (and removed).  Our findings are supported by experiments.    From a high level, this paper sheds novel insights on high-dimesional statistics and   learning structured data, by presenting a structured matrix learning problem for which  a one shot convex relaxation approach necessarily fails, but a carefully constructed sequence of convex relaxations  does the job.",
        "bibtex": "@InProceedings{pmlr-v28-ailon13,\n  title = \t {Breaking the Small Cluster Barrier of Graph Clustering},\n  author = \t {Ailon, Nir and Chen, Yudong and Xu, Huan},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {995--1003},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/ailon13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/ailon13.html},\n  abstract = \t {This paper investigates graph clustering in the planted cluster model in the   presence of  \\em small clusters. Traditional results dictate that for an   algorithm to provably correctly recover the clusters, \\em all clusters must be   sufficiently large (in particular, \\tilde\u03a9(\\sqrtn) where n is the number   of nodes of the graph). We show that this is not really a restriction: by a more refined   analysis of the trace-norm based matrix recovery approach proposed in (Jalali et al. 2011) and (Chen et al. 2012), we prove that small clusters, under certain mild assuptions, do not hinder recovery of large ones.  Based on this result, we further devise an iterative algorithm   to recover \\em almost all clusters via a \u201cpeeling strategy\u201d, i.e., recover large clusters   first, leading to a reduced problem, and repeat this procedure.   These results are extended to the    \\em partial observation setting, in which only a (chosen) part of the graph is observed.  The peeling strategy gives rise to an active learning algorithm, in which   edges adjacent to smaller clusters are queried more often as large clusters are learned  (and removed).  Our findings are supported by experiments.    From a high level, this paper sheds novel insights on high-dimesional statistics and   learning structured data, by presenting a structured matrix learning problem for which  a one shot convex relaxation approach necessarily fails, but a carefully constructed sequence of convex relaxations  does the job.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/ailon13.pdf",
        "supp": "",
        "pdf_size": 410514,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14931860913513323543&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, Technion Israel Institute of Technology; Department of Electrical and Computer Engineering, The University of Texas at Austin; Department of Mechanical Engineering, National University of Singapore",
        "aff_domain": "cs.technion.ac.il;utexas.edu;nus.edu.sg",
        "email": "cs.technion.ac.il;utexas.edu;nus.edu.sg",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Technion Israel Institute of Technology;University of Texas at Austin;National University of Singapore",
        "aff_unique_dep": "Department of Computer Science;Department of Electrical and Computer Engineering;Department of Mechanical Engineering",
        "aff_unique_url": "https://www.technion.ac.il;https://www.utexas.edu;https://www.nus.edu.sg",
        "aff_unique_abbr": "Technion;UT Austin;NUS",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Austin",
        "aff_country_unique_index": "0;1;2",
        "aff_country_unique": "Israel;United States;Singapore"
    },
    {
        "id": "07ccf8e35c",
        "title": "Canonical Correlation Analysis based on Hilbert-Schmidt Independence Criterion and Centered Kernel Target Alignment",
        "site": "https://proceedings.mlr.press/v28/chang13.html",
        "author": "Billy Chang; Uwe Kruger; Rafal Kustra; Junping Zhang",
        "abstract": "Canonical correlation analysis (CCA) is a well established technique for identifying linear relationships among two variable sets.  Kernel CCA (KCCA) is the most notable nonlinear extension but it lacks interpretability and robustness against irrelevant features.  The aim of this article is to introduce two nonlinear CCA extensions that rely on the recently proposed Hilbert-Schmidt independence criterion and the centered kernel target alignment.  These extensions determine linear projections that provide maximally dependent projected data pairs.  The paper demonstrates that the use of linear projections allows removing irrelevant features, whilst extracting combinations of strongly associated features.  This is exemplified through a simulation and the analysis of recorded data that are available in the literature.",
        "bibtex": "@InProceedings{pmlr-v28-chang13,\n  title = \t {Canonical Correlation Analysis based on Hilbert-Schmidt Independence Criterion and Centered Kernel Target Alignment},\n  author = \t {Chang, Billy and Kruger, Uwe and Kustra, Rafal and Zhang, Junping},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {316--324},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/chang13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/chang13.html},\n  abstract = \t {Canonical correlation analysis (CCA) is a well established technique for identifying linear relationships among two variable sets.  Kernel CCA (KCCA) is the most notable nonlinear extension but it lacks interpretability and robustness against irrelevant features.  The aim of this article is to introduce two nonlinear CCA extensions that rely on the recently proposed Hilbert-Schmidt independence criterion and the centered kernel target alignment.  These extensions determine linear projections that provide maximally dependent projected data pairs.  The paper demonstrates that the use of linear projections allows removing irrelevant features, whilst extracting combinations of strongly associated features.  This is exemplified through a simulation and the analysis of recorded data that are available in the literature.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/chang13.pdf",
        "supp": "",
        "pdf_size": 453005,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12237145347209680198&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Dalla Lana School of Public Health, University of Toronto, M5T 3M7, Canada; Dept. Mechanical & Industrial Engineering, Sultan Qaboos University, Al Khoud, Sultanate of Oman; Dalla Lana School of Public Health, University of Toronto, M5T 3M7, Canada; Shanghai Key Lab of Intelligent Information Processing & School of Computer Science, Fudan University, China",
        "aff_domain": "mail.utoronto.ca;squ.edu.om;utoronto.ca;fudan.edu.cn",
        "email": "mail.utoronto.ca;squ.edu.om;utoronto.ca;fudan.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "University of Toronto;Sultan Qaboos University;Fudan University",
        "aff_unique_dep": "Dalla Lana School of Public Health;Mechanical & Industrial Engineering;School of Computer Science",
        "aff_unique_url": "https://www.utoronto.ca;https://www.squ.edu.om;https://www.fudan.edu.cn",
        "aff_unique_abbr": "U of T;SQU;Fudan",
        "aff_campus_unique_index": "0;1;0;2",
        "aff_campus_unique": "Toronto;Al Khoud;Shanghai",
        "aff_country_unique_index": "0;1;0;2",
        "aff_country_unique": "Canada;Oman;China"
    },
    {
        "id": "db05b46b5f",
        "title": "Characterizing the Representer Theorem",
        "site": "https://proceedings.mlr.press/v28/yu13.html",
        "author": "Yaoliang Yu; Hao Cheng; Dale Schuurmans; Csaba Szepesvari",
        "abstract": "The representer theorem assures that kernel methods retain optimality under penalized empirical risk minimization. While a sufficient condition on the form of the regularizer guaranteeing the representer theorem has been known since the initial development of kernel methods, necessary conditions have only been investigated recently. In this paper we completely characterize the necessary and sufficient conditions on the regularizer that ensure the representer theorem holds. The results are surprisingly simple yet broaden the conditions where the representer theorem is known to hold. Extension to the matrix domain is also addressed.",
        "bibtex": "@InProceedings{pmlr-v28-yu13,\n  title = \t {Characterizing the Representer Theorem},\n  author = \t {Yu, Yaoliang and Cheng, Hao and Schuurmans, Dale and Szepesvari, Csaba},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {570--578},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/yu13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/yu13.html},\n  abstract = \t {The representer theorem assures that kernel methods retain optimality under penalized empirical risk minimization. While a sufficient condition on the form of the regularizer guaranteeing the representer theorem has been known since the initial development of kernel methods, necessary conditions have only been investigated recently. In this paper we completely characterize the necessary and sufficient conditions on the regularizer that ensure the representer theorem holds. The results are surprisingly simple yet broaden the conditions where the representer theorem is known to hold. Extension to the matrix domain is also addressed.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/yu13.pdf",
        "supp": "",
        "pdf_size": 406626,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7567185811991069229&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff": "Dept. of Computing Science, University of Alberta, Edmonton, AB, T6G 2E8 CANADA; Dept. of Computing Science, University of Alberta, Edmonton, AB, T6G 2E8 CANADA; Dept. of Computing Science, University of Alberta, Edmonton, AB, T6G 2E8 CANADA; Dept. of Computing Science, University of Alberta, Edmonton, AB, T6G 2E8 CANADA",
        "aff_domain": "cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca",
        "email": "cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca;cs.ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "Dept. of Computing Science",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Edmonton",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "f3d2aab68a",
        "title": "Coco-Q: Learning in Stochastic Games with Side Payments",
        "site": "https://proceedings.mlr.press/v28/sodomka13.html",
        "author": "Eric Sodomka; Elizabeth Hilliard; Michael Littman; Amy Greenwald",
        "abstract": "Coco (\"\"cooperative/competitive\"\") values are a solution concept for two-player normal-form games with transferable utility, when binding agreements and side payments between players are possible. In this paper, we show that coco values can also be defined for stochastic games and can be learned using a simple variant of Q-learning that is provably convergent. We provide a set of examples showing how the strategies learned by the Coco-Q algorithm relate to those learned by existing multiagent Q-learning algorithms.",
        "bibtex": "@InProceedings{pmlr-v28-sodomka13,\n  title = \t {Coco-Q: Learning in Stochastic Games with Side Payments},\n  author = \t {Sodomka, Eric and Hilliard, Elizabeth and Littman, Michael and Greenwald, Amy},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1471--1479},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/sodomka13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/sodomka13.html},\n  abstract = \t {Coco (\"\"cooperative/competitive\"\") values are a solution concept for two-player normal-form games with transferable utility, when binding agreements and side payments between players are possible. In this paper, we show that coco values can also be defined for stochastic games and can be learned using a simple variant of Q-learning that is provably convergent. We provide a set of examples showing how the strategies learned by the Coco-Q algorithm relate to those learned by existing multiagent Q-learning algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/sodomka13.pdf",
        "supp": "",
        "pdf_size": 409591,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7505307272365904191&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Brown University; Brown University; Brown University; Brown University",
        "aff_domain": "cs.brown.edu;cs.brown.edu;cs.brown.edu;cs.brown.edu",
        "email": "cs.brown.edu;cs.brown.edu;cs.brown.edu;cs.brown.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Brown University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.brown.edu",
        "aff_unique_abbr": "Brown",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f4f816bed6",
        "title": "Collaborative hyperparameter tuning",
        "site": "https://proceedings.mlr.press/v28/bardenet13.html",
        "author": "R\u00e9mi Bardenet; M\u00e1ty\u00e1s Brendel; Bal\u00e1zs K\u00e9gl; Mich\u00e8le Sebag",
        "abstract": "Hyperparameter learning has traditionally been a manual task because of the limited number of trials. Today\u2019s computing infrastructures allow bigger evaluation budgets, thus opening the way for algorithmic approaches. Recently, surrogate-based optimization was successfully applied to hyperparameter learning for deep belief networks and to WEKA classifiers. The methods combined brute force computational power with model building about the behavior of the error function in the hyperparameter space, and they could significantly improve on manual hyperparameter tuning. What may make experienced practitioners even better at hyperparameter optimization is their ability to generalize across similar learning problems. In this paper, we propose a generic method to incorporate knowledge from previous experiments when simultaneously tuning a learning algorithm on new problems at hand. To this end, we combine surrogate-based ranking and optimization techniques for surrogate-based collaborative tuning (SCoT). We demonstrate SCoT in two experiments where it outperforms standard tuning techniques and single-problem surrogate-based optimization.",
        "bibtex": "@InProceedings{pmlr-v28-bardenet13,\n  title = \t {Collaborative hyperparameter tuning},\n  author = \t {Bardenet, R\u00e9mi and Brendel, M\u00e1ty\u00e1s and K\u00e9gl, Bal\u00e1zs and Sebag, Mich\u00e8le},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {199--207},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/bardenet13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/bardenet13.html},\n  abstract = \t {Hyperparameter learning has traditionally been a manual task because of the limited number of trials. Today\u2019s computing infrastructures allow bigger evaluation budgets, thus opening the way for algorithmic approaches. Recently, surrogate-based optimization was successfully applied to hyperparameter learning for deep belief networks and to WEKA classifiers. The methods combined brute force computational power with model building about the behavior of the error function in the hyperparameter space, and they could significantly improve on manual hyperparameter tuning. What may make experienced practitioners even better at hyperparameter optimization is their ability to generalize across similar learning problems. In this paper, we propose a generic method to incorporate knowledge from previous experiments when simultaneously tuning a learning algorithm on new problems at hand. To this end, we combine surrogate-based ranking and optimization techniques for surrogate-based collaborative tuning (SCoT). We demonstrate SCoT in two experiments where it outperforms standard tuning techniques and single-problem surrogate-based optimization.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/bardenet13.pdf",
        "supp": "",
        "pdf_size": 356417,
        "gs_citation": 537,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6964750566837921193&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "Dept. of Statistics, University of Oxford; Linear Accelerator Laboratory (LAL), CNRS/University of Paris Sud + Computer Science Laboratory (LRI), CNRS/University of Paris Sud; Linear Accelerator Laboratory (LAL), CNRS/University of Paris Sud + Computer Science Laboratory (LRI), CNRS/University of Paris Sud; Computer Science Laboratory (LRI), CNRS/University of Paris Sud + Ferchau Engineering",
        "aff_domain": "gmail.com;gmail.com;gmail.com;lri.fr",
        "email": "gmail.com;gmail.com;gmail.com;lri.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2;1+2;2+3",
        "aff_unique_norm": "University of Oxford;CNRS/University of Paris Sud;University of Paris Sud;Ferchau Engineering",
        "aff_unique_dep": "Department of Statistics;Linear Accelerator Laboratory (LAL);Computer Science Laboratory (LRI);",
        "aff_unique_url": "https://www.ox.ac.uk;https://www.universite-paris-sud.fr;https://www.universite-paris-sud.fr;https://www.ferchau.com",
        "aff_unique_abbr": "Oxford;;Paris Sud;",
        "aff_campus_unique_index": "0;;;",
        "aff_campus_unique": "Oxford;",
        "aff_country_unique_index": "0;1+1;1+1;1+2",
        "aff_country_unique": "United Kingdom;France;Germany"
    },
    {
        "id": "3bf1965096",
        "title": "Collective Stability in Structured Prediction: Generalization from One Example",
        "site": "https://proceedings.mlr.press/v28/london13.html",
        "author": "Ben London; Bert Huang; Ben Taskar; Lise Getoor",
        "abstract": "Structured predictors enable joint inference over multiple interdependent output variables. These models are often trained on a small number of examples with large internal structure. Existing distribution-free generalization bounds do not guarantee generalization in this setting, though this contradicts a large body of empirical evidence from computer vision, natural language processing, social networks and other fields. In this paper, we identify a set of natural conditions \u2013 weak dependence, hypothesis complexity and a new measure, collective stability \u2013 that are sufficient for generalization from even a single example, without imposing an explicit generative model of the data. We then demonstrate that the complexity and stability conditions are satisfied by a broad class of models, including marginal inference in templated graphical models. We thus obtain uniform convergence rates that can decrease significantly faster than previous bounds, particularly when each structured example is sufficiently large and the number of training examples is constant, even one.",
        "bibtex": "@InProceedings{pmlr-v28-london13,\n  title = \t {Collective Stability in Structured Prediction: Generalization from One Example},\n  author = \t {London, Ben and Huang, Bert and Taskar, Ben and Getoor, Lise},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {828--836},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/london13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/london13.html},\n  abstract = \t {Structured predictors enable joint inference over multiple interdependent output variables. These models are often trained on a small number of examples with large internal structure. Existing distribution-free generalization bounds do not guarantee generalization in this setting, though this contradicts a large body of empirical evidence from computer vision, natural language processing, social networks and other fields. In this paper, we identify a set of natural conditions \u2013 weak dependence, hypothesis complexity and a new measure, collective stability \u2013 that are sufficient for generalization from even a single example, without imposing an explicit generative model of the data. We then demonstrate that the complexity and stability conditions are satisfied by a broad class of models, including marginal inference in templated graphical models. We thus obtain uniform convergence rates that can decrease significantly faster than previous bounds, particularly when each structured example is sufficiently large and the number of training examples is constant, even one.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/london13.pdf",
        "supp": "",
        "pdf_size": 353559,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16941676658364420592&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "University of Maryland, College Park, MD 20742 USA; University of Maryland, College Park, MD 20742 USA; University of Washington, Seattle, WA 98195 USA; University of Maryland, College Park, MD 20742 USA",
        "aff_domain": "cs.umd.edu;cs.umd.edu;cs.washington.edu;cs.umd.edu",
        "email": "cs.umd.edu;cs.umd.edu;cs.washington.edu;cs.umd.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Maryland;University of Washington",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www/umd.edu;https://www.washington.edu",
        "aff_unique_abbr": "UMD;UW",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "College Park;Seattle",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "790dcff8b4",
        "title": "Combinatorial Multi-Armed Bandit: General Framework and Applications",
        "site": "https://proceedings.mlr.press/v28/chen13a.html",
        "author": "Wei Chen; Yajun Wang; Yang Yuan",
        "abstract": "We define a general framework for a large class of combinatorial multi-armed bandit (CMAB) problems, where simple arms with unknown istributions  form \\em super arms. In each round, a super arm is played and the outcomes of its related simple arms are observed, which helps the selection of super arms in future rounds. The reward of the super arm depends on the outcomes of played arms, and it only needs to satisfy two mild assumptions, which allow a large class of nonlinear reward instances. We assume the availability of an (\u03b1,\u03b2)-approximation oracle that takes the  means of the distributions of arms and outputs a super arm that with probability \u03b2generates  an \u03b1fraction of the optimal expected reward. The objective of a CMAB algorithm is to minimize \\em (\u03b1,\u03b2)-approximation regret, which is the difference in total expected reward between the \u03b1\u03b2fraction of expected reward when always playing the optimal super arm, and the expected reward of playing super arms according to the algorithm. We provide CUCB algorithm that achieves O(\\log n) regret, where n is the number of rounds played, and we further provide distribution-independent bounds for a large class of reward functions. Our regret analysis is tight in that it matches the bound for classical MAB problem up to a constant factor, and it significantly improves the regret bound in a recent paper on combinatorial bandits with linear rewards. We apply our CMAB framework to two new applications, probabilistic maximum coverage (PMC) for online advertising and social influence maximization for viral marketing, both having nonlinear reward structures.",
        "bibtex": "@InProceedings{pmlr-v28-chen13a,\n  title = \t {Combinatorial Multi-Armed Bandit: General Framework and Applications},\n  author = \t {Chen, Wei and Wang, Yajun and Yuan, Yang},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {151--159},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/chen13a.pdf},\n  url = \t {https://proceedings.mlr.press/v28/chen13a.html},\n  abstract = \t {We define a general framework for a large class of combinatorial multi-armed bandit (CMAB) problems, where simple arms with unknown istributions  form \\em super arms. In each round, a super arm is played and the outcomes of its related simple arms are observed, which helps the selection of super arms in future rounds. The reward of the super arm depends on the outcomes of played arms, and it only needs to satisfy two mild assumptions, which allow a large class of nonlinear reward instances. We assume the availability of an (\u03b1,\u03b2)-approximation oracle that takes the  means of the distributions of arms and outputs a super arm that with probability \u03b2generates  an \u03b1fraction of the optimal expected reward. The objective of a CMAB algorithm is to minimize \\em (\u03b1,\u03b2)-approximation regret, which is the difference in total expected reward between the \u03b1\u03b2fraction of expected reward when always playing the optimal super arm, and the expected reward of playing super arms according to the algorithm. We provide CUCB algorithm that achieves O(\\log n) regret, where n is the number of rounds played, and we further provide distribution-independent bounds for a large class of reward functions. Our regret analysis is tight in that it matches the bound for classical MAB problem up to a constant factor, and it significantly improves the regret bound in a recent paper on combinatorial bandits with linear rewards. We apply our CMAB framework to two new applications, probabilistic maximum coverage (PMC) for online advertising and social influence maximization for viral marketing, both having nonlinear reward structures.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/chen13a.pdf",
        "supp": "",
        "pdf_size": 353316,
        "gs_citation": 944,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2199816012007173043&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Microsoft Research Asia, Beijing, China; Microsoft Research Asia, Beijing, China; Computer Science Department, Cornell University, Ithaca, NY, USA",
        "aff_domain": "microsoft.com;microsoft.com;cs.cornell.edu",
        "email": "microsoft.com;microsoft.com;cs.cornell.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Microsoft;Cornell University",
        "aff_unique_dep": "Research;Computer Science Department",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/group/asia;https://www.cornell.edu",
        "aff_unique_abbr": "MSRA;Cornell",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Beijing;Ithaca",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "a14eb61656",
        "title": "Computation-Risk Tradeoffs for Covariance-Thresholded Regression",
        "site": "https://proceedings.mlr.press/v28/shender13.html",
        "author": "Dinah Shender; John Lafferty",
        "abstract": "We present a family of linear regression estimators that provides a fine-grained tradeoff between statistical accuracy and computational efficiency.  The estimators are based on hard thresholding of the sample covariance matrix entries together with l2-regularizion(ridge regression).  We analyze the predictive risk of this family of estimators as a function of the threshold and regularization parameter.  With appropriate parameter choices, the estimate is the solution to a sparse, diagonally dominant linear system, solvable in near-linear time.  Our analysis shows how the risk varies with the sparsity and regularization level, thus establishing a statistical estimation setting for which there is an explicit, smooth tradeoff between risk and computation.  Simulations are provided to support the theoretical analyses.",
        "bibtex": "@InProceedings{pmlr-v28-shender13,\n  title = \t {Computation-Risk Tradeoffs for Covariance-Thresholded Regression},\n  author = \t {Shender, Dinah and Lafferty, John},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {756--764},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/shender13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/shender13.html},\n  abstract = \t {We present a family of linear regression estimators that provides a fine-grained tradeoff between statistical accuracy and computational efficiency.  The estimators are based on hard thresholding of the sample covariance matrix entries together with l2-regularizion(ridge regression).  We analyze the predictive risk of this family of estimators as a function of the threshold and regularization parameter.  With appropriate parameter choices, the estimate is the solution to a sparse, diagonally dominant linear system, solvable in near-linear time.  Our analysis shows how the risk varies with the sparsity and regularization level, thus establishing a statistical estimation setting for which there is an explicit, smooth tradeoff between risk and computation.  Simulations are provided to support the theoretical analyses.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/shender13.pdf",
        "supp": "",
        "pdf_size": 353975,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15187318467318951391&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Statistics* + Department of Computer Science\u2021; Department of Statistics* + Department of Computer Science\u2021",
        "aff_domain": "uchicago.edu;uchicago.edu",
        "email": "uchicago.edu;uchicago.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "University Affiliation Not Specified;Department of Computer Science",
        "aff_unique_dep": "Department of Statistics;Computer Science",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": ";",
        "aff_country_unique": ""
    },
    {
        "id": "8ffd1a02c3",
        "title": "Concurrent Reinforcement Learning from Customer Interactions",
        "site": "https://proceedings.mlr.press/v28/silver13.html",
        "author": "David Silver; Leonard Newnham; David Barker; Suzanne Weller; Jason McFall",
        "abstract": "In this paper, we explore applications in which a company interacts concurrently with many customers. The company has an objective function, such as maximising revenue, customer satisfaction, or customer loyalty, which depends primarily on the sequence of interactions between company and customer. A key aspect of this setting is that interactions with different customers occur in parallel. As a result, it is imperative to learn online from partial interaction sequences, so that information acquired from one customer is efficiently assimilated and applied in subsequent interactions with other customers. We present the first framework for concurrent reinforcement learning, using a variant of temporal-difference learning to learn efficiently from partial interaction sequences.   We evaluate our algorithms in two large-scale test-beds for online and email interaction respectively, generated from a database of 300,000 customer records.",
        "bibtex": "@InProceedings{pmlr-v28-silver13,\n  title = \t {Concurrent Reinforcement Learning from Customer Interactions},\n  author = \t {Silver, David and Newnham, Leonard and Barker, David and Weller, Suzanne and McFall, Jason},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {924--932},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/silver13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/silver13.html},\n  abstract = \t {In this paper, we explore applications in which a company interacts concurrently with many customers. The company has an objective function, such as maximising revenue, customer satisfaction, or customer loyalty, which depends primarily on the sequence of interactions between company and customer. A key aspect of this setting is that interactions with different customers occur in parallel. As a result, it is imperative to learn online from partial interaction sequences, so that information acquired from one customer is efficiently assimilated and applied in subsequent interactions with other customers. We present the first framework for concurrent reinforcement learning, using a variant of temporal-difference learning to learn efficiently from partial interaction sequences.   We evaluate our algorithms in two large-scale test-beds for online and email interaction respectively, generated from a database of 300,000 customer records. }\n}",
        "pdf": "http://proceedings.mlr.press/v28/silver13.pdf",
        "supp": "",
        "pdf_size": 429905,
        "gs_citation": 85,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6949760442598631195&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, CSML, University College London, London WC1E 6BT; Causata Ltd., 33 Glasshouse Street, London W1B 5DG; Causata Ltd., 33 Glasshouse Street, London W1B 5DG; Causata Ltd., 33 Glasshouse Street, London W1B 5DG; Causata Ltd., 33 Glasshouse Street, London W1B 5DG",
        "aff_domain": "cs.ucl.ac.uk;causata.com; ; ; ",
        "email": "cs.ucl.ac.uk;causata.com; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1;1",
        "aff_unique_norm": "University College London;Causata Ltd.",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.ucl.ac.uk;",
        "aff_unique_abbr": "UCL;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "London;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "dbfd5d1a39",
        "title": "Connecting the Dots with Landmarks:  Discriminatively Learning Domain-Invariant Features for Unsupervised Domain Adaptation",
        "site": "https://proceedings.mlr.press/v28/gong13.html",
        "author": "Boqing Gong; Kristen Grauman; Fei Sha",
        "abstract": "Learning domain-invariant features is of vital importance to unsupervised domain adaptation, where classifiers trained on the source domain need to be adapted to a different target domain for which no labeled examples are available. In this paper, we propose a novel approach for learning such features. The central idea is to exploit the existence of landmarks, which are a subset of labeled data instances in the source domain that are distributed most similarly to the target domain. Our approach automatically discovers the landmarks and use them to bridge the source to the target by constructing provably easier auxiliary domain adaptation tasks. The solutions of those auxiliary tasks form the basis to compose invariant features for the original task. We show how this composition can be optimized discriminatively without requiring labels from the target domain. We validate the method on standard benchmark datasets for visual object recognition and sentiment analysis of text. Empirical results show the proposed method outperforms the state-of-the-art significantly.",
        "bibtex": "@InProceedings{pmlr-v28-gong13,\n  title = \t {Connecting the Dots with Landmarks:  Discriminatively Learning Domain-Invariant Features for Unsupervised Domain Adaptation},\n  author = \t {Gong, Boqing and Grauman, Kristen and Sha, Fei},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {222--230},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/gong13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/gong13.html},\n  abstract = \t {Learning domain-invariant features is of vital importance to unsupervised domain adaptation, where classifiers trained on the source domain need to be adapted to a different target domain for which no labeled examples are available. In this paper, we propose a novel approach for learning such features. The central idea is to exploit the existence of landmarks, which are a subset of labeled data instances in the source domain that are distributed most similarly to the target domain. Our approach automatically discovers the landmarks and use them to bridge the source to the target by constructing provably easier auxiliary domain adaptation tasks. The solutions of those auxiliary tasks form the basis to compose invariant features for the original task. We show how this composition can be optimized discriminatively without requiring labels from the target domain. We validate the method on standard benchmark datasets for visual object recognition and sentiment analysis of text. Empirical results show the proposed method outperforms the state-of-the-art significantly.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/gong13.pdf",
        "supp": "",
        "pdf_size": 2070038,
        "gs_citation": 517,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11402473606411858297&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, University of Southern California, Los Angeles, CA 90089; Department of Computer Science, University of Texas at Austin, Austin, TX 78701; Department of Computer Science, University of Southern California, Los Angeles, CA 90089",
        "aff_domain": "usc.edu;cs.utexas.edu;usc.edu",
        "email": "usc.edu;cs.utexas.edu;usc.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Southern California;University of Texas at Austin",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.usc.edu;https://www.utexas.edu",
        "aff_unique_abbr": "USC;UT Austin",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Los Angeles;Austin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e07850ca24",
        "title": "Consistency of Online Random Forests",
        "site": "https://proceedings.mlr.press/v28/denil13.html",
        "author": "Misha Denil; David Matheson; Nando Freitas",
        "abstract": "As a testament to their success, the theory of random forests has long been outpaced by their application in practice. In this paper, we take a step towards narrowing this gap by providing a consistency result for online random forests.",
        "bibtex": "@InProceedings{pmlr-v28-denil13,\n  title = \t {Consistency of Online Random Forests},\n  author = \t {Denil, Misha and Matheson, David and Freitas, Nando},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1256--1264},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/denil13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/denil13.html},\n  abstract = \t {As a testament to their success, the theory of random forests has long been outpaced by their application in practice. In this paper, we take a step towards narrowing this gap by providing a consistency result for online random forests.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/denil13.pdf",
        "supp": "",
        "pdf_size": 573538,
        "gs_citation": 120,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9399386201488551155&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "University of British Columbia; University of British Columbia; University of British Columbia",
        "aff_domain": "cs.ubc.ca;cs.ubc.ca;cs.ubc.ca",
        "email": "cs.ubc.ca;cs.ubc.ca;cs.ubc.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of British Columbia",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ubc.ca",
        "aff_unique_abbr": "UBC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "055a276b5d",
        "title": "Consistency versus Realizable H-Consistency for Multiclass Classification",
        "site": "https://proceedings.mlr.press/v28/long13.html",
        "author": "Phil Long; Rocco Servedio",
        "abstract": "A consistent loss function for multiclass classification is one such  that for any source of labeled examples, any tuple  of scoring functions that  minimizes the expected loss will have classification accuracy close to that  of the Bayes optimal classifier. While consistency has been proposed as a  desirable property for multiclass loss functions, we  give experimental and theoretical results exhibiting a  sequence of linearly separable data sources with the following property:  a multiclass classification algorithm which optimizes a loss function  due to Crammer and Singer (which is known not to be consistent) produces  classifiers whose expected error goes to 0, while the expected error  of an algorithm which optimizes a generalization of the loss  function used by LogitBoost (a loss function which is known to be consistent)  is bounded below by a positive constant.    We identify a property of a loss function, realizable  consistency with respect to a restricted class of scoring functions,  that accounts for this difference.   As our main technical results we show  that the Crammer\u2013Singer loss function is  realizable consistent for the class of linear scoring functions, while  the generalization of LogitBoost is not.  Our result for LogitBoost is  a special case of a more general theorem that applies to several other  loss functions that have been proposed for multiclass classification.",
        "bibtex": "@InProceedings{pmlr-v28-long13,\n  title = \t {Consistency versus Realizable H-Consistency for Multiclass Classification},\n  author = \t {Long, Phil and Servedio, Rocco},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {801--809},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/long13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/long13.html},\n  abstract = \t {A consistent loss function for multiclass classification is one such  that for any source of labeled examples, any tuple  of scoring functions that  minimizes the expected loss will have classification accuracy close to that  of the Bayes optimal classifier. While consistency has been proposed as a  desirable property for multiclass loss functions, we  give experimental and theoretical results exhibiting a  sequence of linearly separable data sources with the following property:  a multiclass classification algorithm which optimizes a loss function  due to Crammer and Singer (which is known not to be consistent) produces  classifiers whose expected error goes to 0, while the expected error  of an algorithm which optimizes a generalization of the loss  function used by LogitBoost (a loss function which is known to be consistent)  is bounded below by a positive constant.    We identify a property of a loss function, realizable  consistency with respect to a restricted class of scoring functions,  that accounts for this difference.   As our main technical results we show  that the Crammer\u2013Singer loss function is  realizable consistent for the class of linear scoring functions, while  the generalization of LogitBoost is not.  Our result for LogitBoost is  a special case of a more general theorem that applies to several other  loss functions that have been proposed for multiclass classification.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/long13.pdf",
        "supp": "",
        "pdf_size": 206280,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17629825390148256399&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Microsoft, 1020 Enterprise Way, Sunnyvale, CA 94089; Department of Computer Science, Columbia University, New York, NY 10027",
        "aff_domain": "microsoft.com;cs.columbia.edu",
        "email": "microsoft.com;cs.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Microsoft;Columbia University",
        "aff_unique_dep": "Microsoft Corporation;Department of Computer Science",
        "aff_unique_url": "https://www.microsoft.com;https://www.columbia.edu",
        "aff_unique_abbr": "Microsoft;Columbia",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Sunnyvale;New York",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "06cde5b7bc",
        "title": "Constrained fractional set programs and their  application in local clustering and community detection",
        "site": "https://proceedings.mlr.press/v28/buhler13.html",
        "author": "Thomas B\u00fchler; Shyam Sundar Rangapuram; Simon Setzer; Matthias Hein",
        "abstract": "The (constrained) minimization of a ratio of set functions is a problem frequently occurring in clustering and community detection. As these optimization problems are typically NP-hard, one uses convex or spectral relaxations in practice. While these relaxations can be solved globally optimally, they are often too loose and thus lead to results far away from the optimum. In this paper we show that every constrained minimization problem of a ratio of non-negative set functions allows a tight relaxation into an unconstrained continuous optimization problem. This result leads to a flexible framework for solving constrained problems in network analysis. While a globally optimal solution for the resulting non-convex problem cannot be guaranteed, we outperform the loose convex or spectral relaxations by a large margin on constrained local clustering problems.",
        "bibtex": "@InProceedings{pmlr-v28-buhler13,\n  title = \t {Constrained fractional set programs and their  application in local clustering and community detection},\n  author = \t {B\u00fchler, Thomas and Rangapuram, Shyam Sundar and Setzer, Simon and Hein, Matthias},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {624--632},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/buhler13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/buhler13.html},\n  abstract = \t {The (constrained) minimization of a ratio of set functions is a problem frequently occurring in clustering and community detection. As these optimization problems are typically NP-hard, one uses convex or spectral relaxations in practice. While these relaxations can be solved globally optimally, they are often too loose and thus lead to results far away from the optimum. In this paper we show that every constrained minimization problem of a ratio of non-negative set functions allows a tight relaxation into an unconstrained continuous optimization problem. This result leads to a flexible framework for solving constrained problems in network analysis. While a globally optimal solution for the resulting non-convex problem cannot be guaranteed, we outperform the loose convex or spectral relaxations by a large margin on constrained local clustering problems.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/buhler13.pdf",
        "supp": "",
        "pdf_size": 764819,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16478542472546479884&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Saarland University, Saarbr\u00a8 ucken, Germany; Max Planck Institute for Informatics & Saarland University, Saarbr\u00a8 ucken, Germany; Saarland University, Saarbr\u00a8 ucken, Germany; Saarland University, Saarbr\u00a8 ucken, Germany",
        "aff_domain": "cs.uni-saarland.de;mpi-inf.mpg.de;mia.uni-saarland.de;cs.uni-saarland.de",
        "email": "cs.uni-saarland.de;mpi-inf.mpg.de;mia.uni-saarland.de;cs.uni-saarland.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Saarland University;Max Planck Institute for Informatics",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uni-saarland.de;https://mpi-inf.mpg.de",
        "aff_unique_abbr": "UdS;MPII",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Saarbr\u00fccken",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "0d09d54a45",
        "title": "Convex Adversarial Collective Classification",
        "site": "https://proceedings.mlr.press/v28/torkamani13.html",
        "author": "MohamadAli Torkamani; Daniel Lowd",
        "abstract": "In this paper, we present a novel method for robustly  performing collective classification in the presence of a malicious  adversary that can modify up to a fixed number of binary-valued  attributes.  Our method is formulated as a convex quadratic program  that guarantees optimal weights against a worst-case adversary in  polynomial time.  In addition to increased robustness against active  adversaries, this kind of adversarial regularization can also lead to  improved generalization even when no adversary is present.  In  experiments on real and simulated data, our method consistently  outperforms both non-adversarial and non-relational baselines.",
        "bibtex": "@InProceedings{pmlr-v28-torkamani13,\n  title = \t {Convex Adversarial Collective Classification},\n  author = \t {Torkamani, MohamadAli and Lowd, Daniel},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {642--650},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/torkamani13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/torkamani13.html},\n  abstract = \t {In this paper, we present a novel method for robustly  performing collective classification in the presence of a malicious  adversary that can modify up to a fixed number of binary-valued  attributes.  Our method is formulated as a convex quadratic program  that guarantees optimal weights against a worst-case adversary in  polynomial time.  In addition to increased robustness against active  adversaries, this kind of adversarial regularization can also lead to  improved generalization even when no adversary is present.  In  experiments on real and simulated data, our method consistently  outperforms both non-adversarial and non-relational baselines.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/torkamani13.pdf",
        "supp": "",
        "pdf_size": 239294,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13178908397958861910&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer and Information Science, University of Oregon; Department of Computer and Information Science, University of Oregon",
        "aff_domain": "cs.uoregon.edu;cs.uoregon.edu",
        "email": "cs.uoregon.edu;cs.uoregon.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Oregon",
        "aff_unique_dep": "Department of Computer and Information Science",
        "aff_unique_url": "https://www.uoregon.edu",
        "aff_unique_abbr": "UO",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5214814812",
        "title": "Convex Relaxations for Learning Bounded-Treewidth Decomposable Graphs",
        "site": "https://proceedings.mlr.press/v28/kumar13c.html",
        "author": "K. S. Sesh Kumar; Francis Bach",
        "abstract": "We consider the problem of learning the structure of undirected graphical models with bounded treewidth, within the maximum likelihood framework. This is an NP-hard problem and most approaches consider local search techniques. In this paper, we pose it as a combinatorial optimization problem, which is then relaxed to a convex optimization problem that involves searching over the forest and hyperforest polytopes with special structures. A supergradient method is used to solve the dual problem, with a run-time complexity of O(k^3 n^k+2 \\log n) for each iteration, where n is the number of variables and k is a bound on the treewidth. We compare our approach to state-of-the-art methods on synthetic datasets and classical benchmarks, showing the gains of the novel convex approach.",
        "bibtex": "@InProceedings{pmlr-v28-kumar13c,\n  title = \t {Convex Relaxations for Learning Bounded-Treewidth Decomposable Graphs},\n  author = \t {Kumar, K. S. Sesh and Bach, Francis},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {525--533},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/kumar13c.pdf},\n  url = \t {https://proceedings.mlr.press/v28/kumar13c.html},\n  abstract = \t {We consider the problem of learning the structure of undirected graphical models with bounded treewidth, within the maximum likelihood framework. This is an NP-hard problem and most approaches consider local search techniques. In this paper, we pose it as a combinatorial optimization problem, which is then relaxed to a convex optimization problem that involves searching over the forest and hyperforest polytopes with special structures. A supergradient method is used to solve the dual problem, with a run-time complexity of O(k^3 n^k+2 \\log n) for each iteration, where n is the number of variables and k is a bound on the treewidth. We compare our approach to state-of-the-art methods on synthetic datasets and classical benchmarks, showing the gains of the novel convex approach.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/kumar13c.pdf",
        "supp": "",
        "pdf_size": 242037,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14464978374735110730&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "SIERRA project-team, INRIA - D\u00b4epartement d\u2019Informatique de l\u2019Ecole Normale Sup\u00b4erieure, Paris, France; SIERRA project-team, INRIA - D\u00b4epartement d\u2019Informatique de l\u2019Ecole Normale Sup\u00b4erieure, Paris, France",
        "aff_domain": "inria.fr;inria.fr",
        "email": "inria.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "INRIA",
        "aff_unique_dep": "D\u00b4epartement d\u2019Informatique de l\u2019Ecole Normale Sup\u00b4erieure",
        "aff_unique_url": "https://www.inria.fr",
        "aff_unique_abbr": "INRIA",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Paris",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "005def3e1f",
        "title": "Convex formulations of radius-margin based Support Vector Machines",
        "site": "https://proceedings.mlr.press/v28/do13.html",
        "author": "Huyen Do; Alexandros Kalousis",
        "abstract": "We consider Support Vector Machines (SVMs) learned together with linear transformations of the feature spaces on which they are applied. Under this scenario the radius of the smallest data enclosing sphere is no longer fixed. Therefore optimizing the SVM error bound by considering both the radius and the margin has the potential to deliver a tighter error bound.  In this paper we present two novel algorithms: R-SVM_\u03bc^+\u2014a SVM radius-margin based feature selection algorithm, and R-SVM^+ \u2014  a metric learning-based SVM. We derive our algorithms by exploiting a new tighter approximation of the radius and a metric learning interpretation of SVM. Both optimize directly the radius-margin error bound using linear transformations. Unlike almost all existing radius-margin based SVM algorithms which are either non-convex or combinatorial, our algorithms are standard quadratic convex optimization problems with linear or quadratic constraints. We perform a number of experiments on benchmark datasets.   R-SVM_\u03bc^+ exhibits excellent feature selection performance compared to the state-of-the-art feature selection methods, such as L_1-norm and elastic-net based methods.  R-SVM^+ achieves a significantly better classification performance compared to SVM and its other state-of-the-art variants. From the results it is clear that the incorporation of the radius, as a means to control the data spread, in the cost function has strong beneficial effects.",
        "bibtex": "@InProceedings{pmlr-v28-do13,\n  title = \t {Convex formulations of radius-margin based Support Vector Machines},\n  author = \t {Do, Huyen and Kalousis, Alexandros},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {169--177},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/do13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/do13.html},\n  abstract = \t {We consider Support Vector Machines (SVMs) learned together with linear transformations of the feature spaces on which they are applied. Under this scenario the radius of the smallest data enclosing sphere is no longer fixed. Therefore optimizing the SVM error bound by considering both the radius and the margin has the potential to deliver a tighter error bound.  In this paper we present two novel algorithms: R-SVM_\u03bc^+\u2014a SVM radius-margin based feature selection algorithm, and R-SVM^+ \u2014  a metric learning-based SVM. We derive our algorithms by exploiting a new tighter approximation of the radius and a metric learning interpretation of SVM. Both optimize directly the radius-margin error bound using linear transformations. Unlike almost all existing radius-margin based SVM algorithms which are either non-convex or combinatorial, our algorithms are standard quadratic convex optimization problems with linear or quadratic constraints. We perform a number of experiments on benchmark datasets.   R-SVM_\u03bc^+ exhibits excellent feature selection performance compared to the state-of-the-art feature selection methods, such as L_1-norm and elastic-net based methods.  R-SVM^+ achieves a significantly better classification performance compared to SVM and its other state-of-the-art variants. From the results it is clear that the incorporation of the radius, as a means to control the data spread, in the cost function has strong beneficial effects.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/do13.pdf",
        "supp": "",
        "pdf_size": 414938,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4280707948123299168&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Computer Science Department, University of Geneva, Switzerland; Business Informatics, University of Applied Sciences Western Switzerland",
        "aff_domain": "unige.ch;hesge.ch",
        "email": "unige.ch;hesge.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Geneva;University of Applied Sciences Western Switzerland",
        "aff_unique_dep": "Computer Science Department;Business Informatics",
        "aff_unique_url": "https://www.unige.ch;https://www.hes-so.ch/en",
        "aff_unique_abbr": "UNIGE;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "d49d66e599",
        "title": "Copy or Coincidence? A Model for Detecting Social Influence and Duplication Events",
        "site": "https://proceedings.mlr.press/v28/friedland13.html",
        "author": "Lisa Friedland; David Jensen; Michael Lavine",
        "abstract": "In this paper, we analyze the task of inferring rare links between pairs of entities that seem too similar to have occurred by chance. Variations of this task appear in such diverse areas as social network analysis, security, fraud detection, and entity resolution. To address the task in a general form, we propose a simple, flexible mixture model in which most entities are generated independently from a distribution but a small number of pairs are constrained to be similar. We predict the true pairs using a likelihood ratio that trades off the entities\u2019 similarity with their rarity. This method always outperforms using only similarity; however, with certain parameter settings, similarity turns out to be surprisingly competitive. Using real data, we apply the model to detect twins given their birth weights and to re-identify cell phone users based on distinctive usage patterns.",
        "bibtex": "@InProceedings{pmlr-v28-friedland13,\n  title = \t {Copy or Coincidence? A Model for Detecting Social Influence and Duplication Events},\n  author = \t {Friedland, Lisa and Jensen, David and Lavine, Michael},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1175--1183},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/friedland13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/friedland13.html},\n  abstract = \t {In this paper, we analyze the task of inferring rare links between pairs of entities that seem too similar to have occurred by chance. Variations of this task appear in such diverse areas as social network analysis, security, fraud detection, and entity resolution. To address the task in a general form, we propose a simple, flexible mixture model in which most entities are generated independently from a distribution but a small number of pairs are constrained to be similar. We predict the true pairs using a likelihood ratio that trades off the entities\u2019 similarity with their rarity. This method always outperforms using only similarity; however, with certain parameter settings, similarity turns out to be surprisingly competitive. Using real data, we apply the model to detect twins given their birth weights and to re-identify cell phone users based on distinctive usage patterns.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/friedland13.pdf",
        "supp": "",
        "pdf_size": 499398,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13184226028624800419&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "School of Computer Science, University of Massachusetts, Amherst, MA 01003 USA; School of Computer Science, University of Massachusetts, Amherst, MA 01003 USA; Department of Math and Statistics, University of Massachusetts, Amherst, MA 01003 USA",
        "aff_domain": "cs.umass.edu;cs.umass.edu;math.umass.edu",
        "email": "cs.umass.edu;cs.umass.edu;math.umass.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Massachusetts Amherst",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.umass.edu",
        "aff_unique_abbr": "UMass Amherst",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Amherst",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7440341d25",
        "title": "Cost-Sensitive Tree of Classifiers",
        "site": "https://proceedings.mlr.press/v28/xu13.html",
        "author": "Zhixiang Xu; Matt Kusner; Kilian Weinberger; Minmin Chen",
        "abstract": "Recently, machine learning algorithms have successfully entered large-scale real-world industrial applications (e.g. search engines and email spam filters). Here, the CPU cost during test-time must be budgeted and accounted for. In this paper, we address the challenge of balancing test-time cost and the classifier accuracy in a principled fashion. The test-time cost of a classifier is often dominated by the computation required for feature extraction-which can vary drastically across features. We incorporate this extraction time by constructing a tree of classifiers, through which test inputs traverse along individual paths. Each path extracts different features and is optimized for a specific sub-partition of the input space. By only computing features for inputs that benefit from them the most, our cost-sensitive tree of classifiers can match the high accuracies of the current state-of-the-art at a small fraction of the computational cost.",
        "bibtex": "@InProceedings{pmlr-v28-xu13,\n  title = \t {Cost-Sensitive Tree of Classifiers},\n  author = \t {Xu, Zhixiang and Kusner, Matt and Weinberger, Kilian and Chen, Minmin},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {133--141},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/xu13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/xu13.html},\n  abstract = \t {Recently, machine learning algorithms have successfully entered large-scale real-world industrial applications (e.g. search engines and email spam filters). Here, the CPU cost during test-time must be budgeted and accounted for. In this paper, we address the challenge of balancing test-time cost and the classifier accuracy in a principled fashion. The test-time cost of a classifier is often dominated by the computation required for feature extraction-which can vary drastically across features. We incorporate this extraction time by constructing a tree of classifiers, through which test inputs traverse along individual paths. Each path extracts different features and is optimized for a specific sub-partition of the input space. By only computing features for inputs that benefit from them the most, our cost-sensitive tree of classifiers can match the high accuracies of the current state-of-the-art at a small fraction of the computational cost.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/xu13.pdf",
        "supp": "",
        "pdf_size": 1116139,
        "gs_citation": 154,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16877901752129101475&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Washington University; Washington University; Washington University; Washington University",
        "aff_domain": "cse.wustl.edu;wustl.edu;wustl.edu;wustl.edu",
        "email": "cse.wustl.edu;wustl.edu;wustl.edu;wustl.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Washington University in St. Louis",
        "aff_unique_dep": "",
        "aff_unique_url": "https://wustl.edu",
        "aff_unique_abbr": "WUSTL",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "St. Louis",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a3317dea24",
        "title": "Cost-sensitive Multiclass Classification Risk Bounds",
        "site": "https://proceedings.mlr.press/v28/avilapires13.html",
        "author": "Bernardo \u00c1vila Pires; Csaba Szepesvari; Mohammad Ghavamzadeh",
        "abstract": "A commonly used approach to multiclass classification is to replace the 0-1 loss with a convex surrogate so as to make empirical risk minimization computationally tractable. Previous work has uncovered sufficient and necessary conditions for the consistency of the resulting procedures. In this paper, we strengthen these results by showing how the 0-1 excess loss of a predictor can be upper bounded as a function of the excess loss of the predictor measured using the convex surrogate. The bound is developed for the case of cost-sensitive multiclass classification and a convex surrogate loss that goes back to the work of  Lee, Lin and Wahba. The bounds are as easy to calculate as in binary classification. Furthermore, we also show that our analysis extends to the analysis of the recently introduced \u201cSimplex Coding\u201d scheme.",
        "bibtex": "@InProceedings{pmlr-v28-avilapires13,\n  title = \t {Cost-sensitive Multiclass Classification Risk Bounds},\n  author = \t {\u00c1vila Pires, Bernardo and Szepesvari, Csaba and Ghavamzadeh, Mohammad},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1391--1399},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/avilapires13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/avilapires13.html},\n  abstract = \t {A commonly used approach to multiclass classification is to replace the 0-1 loss with a convex surrogate so as to make empirical risk minimization computationally tractable. Previous work has uncovered sufficient and necessary conditions for the consistency of the resulting procedures. In this paper, we strengthen these results by showing how the 0-1 excess loss of a predictor can be upper bounded as a function of the excess loss of the predictor measured using the convex surrogate. The bound is developed for the case of cost-sensitive multiclass classification and a convex surrogate loss that goes back to the work of  Lee, Lin and Wahba. The bounds are as easy to calculate as in binary classification. Furthermore, we also show that our analysis extends to the analysis of the recently introduced \u201cSimplex Coding\u201d scheme.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/avilapires13.pdf",
        "supp": "",
        "pdf_size": 675920,
        "gs_citation": 70,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9344877748939399825&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Department of Computing Science, University of Alberta; Team SequeL, INRIA Lille - Nord Europe; Department of Computing Science, University of Alberta",
        "aff_domain": "ualberta.ca;inria.fr;ualberta.ca",
        "email": "ualberta.ca;inria.fr;ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Alberta;INRIA Lille - Nord Europe",
        "aff_unique_dep": "Department of Computing Science;Team SequeL",
        "aff_unique_url": "https://www.ualberta.ca;https://www.inria.fr/en/centre/lille-nord-europe",
        "aff_unique_abbr": "UAlberta;INRIA",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Lille",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Canada;France"
    },
    {
        "id": "c646397587",
        "title": "Covariate Shift in Hilbert Space: A Solution via Sorrogate Kernels",
        "site": "https://proceedings.mlr.press/v28/zhang13b.html",
        "author": "Kai Zhang; Vincent Zheng; Qiaojun Wang; James Kwok; Qiang Yang; Ivan Marsic",
        "abstract": "Covariate shift is a unconventional learning scenario in which training and testing data have different distributions. A general principle to solve the problem is to make the training data distribution similar to the test one, such that classifiers computed on the former generalizes well to the latter. Current approaches typically target on the sample distribution in the input space, however, for kernel-based learning methods, the algorithm performance depends directly on the geometry of the kernel-induced feature space. Motivated by this, we propose to match data distributions in the Hilbert space, which, given a pre-defined empirical kernel map, can be formulated as aligning kernel matrices across domains. In particular, to evaluate similarity of kernel matrices defined on arbitrarily different samples, the novel concept of surrogate kernel is introduced based on the Mercer's theorem. Our approach caters the model adaptation specifically to kernel-based learning mechanism, and demonstrates promising results on several real-world applications.",
        "bibtex": "@InProceedings{pmlr-v28-zhang13b,\n  title = \t {Covariate Shift in Hilbert Space: A Solution via Sorrogate Kernels},\n  author = \t {Zhang, Kai and Zheng, Vincent and Wang, Qiaojun and Kwok, James and Yang, Qiang and Marsic, Ivan},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {388--395},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/zhang13b.pdf},\n  url = \t {https://proceedings.mlr.press/v28/zhang13b.html},\n  abstract = \t {Covariate shift is a unconventional learning scenario in which training and testing data have different distributions. A general principle to solve the problem is to make the training data distribution similar to the test one, such that classifiers computed on the former generalizes well to the latter. Current approaches typically target on the sample distribution in the input space, however, for kernel-based learning methods, the algorithm performance depends directly on the geometry of the kernel-induced feature space. Motivated by this, we propose to match data distributions in the Hilbert space, which, given a pre-defined empirical kernel map, can be formulated as aligning kernel matrices across domains. In particular, to evaluate similarity of kernel matrices defined on arbitrarily different samples, the novel concept of surrogate kernel is introduced based on the Mercer's theorem. Our approach caters the model adaptation specifically to kernel-based learning mechanism, and demonstrates promising results on several real-world applications.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/zhang13b.pdf",
        "supp": "",
        "pdf_size": 290848,
        "gs_citation": 70,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=716255399911758880&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "6e1994fa07",
        "title": "Deep Canonical Correlation Analysis",
        "site": "https://proceedings.mlr.press/v28/andrew13.html",
        "author": "Galen Andrew; Raman Arora; Jeff Bilmes; Karen Livescu",
        "abstract": "We introduce Deep Canonical Correlation Analysis (DCCA), a method to learn complex nonlinear transformations of two views of data such that the resulting representations are highly linearly correlated. Parameters of both transformations are jointly learned to maximize the (regularized) total correlation.   It can be viewed as a nonlinear extension of the linear method \\emphcanonical correlation analysis (CCA).  It is an alternative to the nonparametric method \\emphkernel canonical correlation analysis (KCCA) for learning correlated nonlinear transformations. Unlike KCCA, DCCA does not require an inner product, and has the advantages of a parametric method: training time scales well with data size and the training data need not be referenced when computing the representations of unseen instances.  In experiments on two real-world datasets, we find that DCCA learns representations with significantly higher correlation than those learned by CCA and KCCA. We also introduce a novel non-saturating sigmoid function based on the cube root that may be useful more generally in feedforward neural networks.",
        "bibtex": "@InProceedings{pmlr-v28-andrew13,\n  title = \t {Deep Canonical Correlation Analysis},\n  author = \t {Andrew, Galen and Arora, Raman and Bilmes, Jeff and Livescu, Karen},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1247--1255},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/andrew13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/andrew13.html},\n  abstract = \t {We introduce Deep Canonical Correlation Analysis (DCCA), a method to learn complex nonlinear transformations of two views of data such that the resulting representations are highly linearly correlated. Parameters of both transformations are jointly learned to maximize the (regularized) total correlation.   It can be viewed as a nonlinear extension of the linear method \\emphcanonical correlation analysis (CCA).  It is an alternative to the nonparametric method \\emphkernel canonical correlation analysis (KCCA) for learning correlated nonlinear transformations. Unlike KCCA, DCCA does not require an inner product, and has the advantages of a parametric method: training time scales well with data size and the training data need not be referenced when computing the representations of unseen instances.  In experiments on two real-world datasets, we find that DCCA learns representations with significantly higher correlation than those learned by CCA and KCCA. We also introduce a novel non-saturating sigmoid function based on the cube root that may be useful more generally in feedforward neural networks.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/andrew13.pdf",
        "supp": "",
        "pdf_size": 430034,
        "gs_citation": 2497,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6902252272651036995&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "University of Washington; Toyota Technological Institute at Chicago; University of Washington; Toyota Technological Institute at Chicago",
        "aff_domain": "cs.washington.edu;ttic.edu;ee.washington.edu;ttic.edu",
        "email": "cs.washington.edu;ttic.edu;ee.washington.edu;ttic.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "University of Washington;Toyota Technological Institute at Chicago",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.washington.edu;https://www.tti-chicago.org",
        "aff_unique_abbr": "UW;TTI Chicago",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Chicago",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d0036b2e9e",
        "title": "Deep learning with COTS HPC systems",
        "site": "https://proceedings.mlr.press/v28/coates13.html",
        "author": "Adam Coates; Brody Huval; Tao Wang; David Wu; Bryan Catanzaro; Ng Andrew",
        "abstract": "Scaling up deep learning algorithms has been shown to lead to increased performance in benchmark tasks and to enable discovery of complex high-level features.  Recent efforts to train extremely large networks (with over 1 billion parameters) have relied on cloud-like computing infrastructure and thousands of CPU cores.  In this paper, we present technical details and results from our own system based on Commodity Off-The-Shelf High Performance Computing (COTS HPC) technology: a cluster of GPU servers with Infiniband interconnects and MPI.  Our system is able to train 1 billion parameter networks on just 3 machines in a couple of days, and we show that it can scale to networks with over 11 billion parameters using just 16 machines.  As this infrastructure is much more easily marshaled by others, the approach enables much wider-spread research with extremely large neural networks.",
        "bibtex": "@InProceedings{pmlr-v28-coates13,\n  title = \t {Deep learning with COTS HPC systems},\n  author = \t {Coates, Adam and Huval, Brody and Wang, Tao and Wu, David and Catanzaro, Bryan and Andrew, Ng},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1337--1345},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/coates13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/coates13.html},\n  abstract = \t {Scaling up deep learning algorithms has been shown to lead to increased performance in benchmark tasks and to enable discovery of complex high-level features.  Recent efforts to train extremely large networks (with over 1 billion parameters) have relied on cloud-like computing infrastructure and thousands of CPU cores.  In this paper, we present technical details and results from our own system based on Commodity Off-The-Shelf High Performance Computing (COTS HPC) technology: a cluster of GPU servers with Infiniband interconnects and MPI.  Our system is able to train 1 billion parameter networks on just 3 machines in a couple of days, and we show that it can scale to networks with over 11 billion parameters using just 16 machines.  As this infrastructure is much more easily marshaled by others, the approach enables much wider-spread research with extremely large neural networks.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/coates13.pdf",
        "supp": "",
        "pdf_size": 757477,
        "gs_citation": 999,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13800828167861315786&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 15,
        "aff": "Stanford University Computer Science Dept., 353 Serra Mall, Stanford, CA 94305 USA; Stanford University Computer Science Dept., 353 Serra Mall, Stanford, CA 94305 USA; Stanford University Computer Science Dept., 353 Serra Mall, Stanford, CA 94305 USA; Stanford University Computer Science Dept., 353 Serra Mall, Stanford, CA 94305 USA; Stanford University Computer Science Dept., 353 Serra Mall, Stanford, CA 94305 USA; NVIDIA Corporation, 2701 San Tomas Expressway, Santa Clara, CA 95050",
        "aff_domain": "cs.stanford.edu;stanford.edu;stanford.edu;cs.stanford.edu;cs.stanford.edu;nvidia.com",
        "email": "cs.stanford.edu;stanford.edu;stanford.edu;cs.stanford.edu;cs.stanford.edu;nvidia.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;1",
        "aff_unique_norm": "Stanford University;NVIDIA",
        "aff_unique_dep": "Computer Science Dept.;NVIDIA Corporation",
        "aff_unique_url": "https://www.stanford.edu;https://www.nvidia.com",
        "aff_unique_abbr": "Stanford;NVIDIA",
        "aff_campus_unique_index": "0;0;0;0;0;1",
        "aff_campus_unique": "Stanford;Santa Clara",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2dce27b9c4",
        "title": "Dependent Normalized Random Measures",
        "site": "https://proceedings.mlr.press/v28/chen13i.html",
        "author": "Changyou Chen; Vinayak Rao; Wray Buntine; Yee Whye Teh",
        "abstract": "In this paper we propose two constructions of dependent normalized random measures, a class of nonparametric priors over dependent probability measures. Our constructions, which we call mixed normalized random measures (MNRM) and thinned normalized random measures (TNRM), involve (respectively) weighting and thinning parts of a shared underlying Poisson process before combining them together. We show that both MNRM and TNRM are marginally normalized random measures, resulting in well understood theoretical properties. We develop marginal and slice samplers for both models, the latter necessary for inference in TNRM. In time-varying topic modelling experiments, both models exhibit superior performance over related dependent models such as the hierarchical Dirichlet process and the spatial normalized Gamma process.",
        "bibtex": "@InProceedings{pmlr-v28-chen13i,\n  title = \t {Dependent Normalized Random Measures},\n  author = \t {Chen, Changyou and Rao, Vinayak and Buntine, Wray and Whye Teh, Yee},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {969--977},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/chen13i.pdf},\n  url = \t {https://proceedings.mlr.press/v28/chen13i.html},\n  abstract = \t {In this paper we propose two constructions of dependent normalized random measures, a class of nonparametric priors over dependent probability measures. Our constructions, which we call mixed normalized random measures (MNRM) and thinned normalized random measures (TNRM), involve (respectively) weighting and thinning parts of a shared underlying Poisson process before combining them together. We show that both MNRM and TNRM are marginally normalized random measures, resulting in well understood theoretical properties. We develop marginal and slice samplers for both models, the latter necessary for inference in TNRM. In time-varying topic modelling experiments, both models exhibit superior performance over related dependent models such as the hierarchical Dirichlet process and the spatial normalized Gamma process.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/chen13i.pdf",
        "supp": "",
        "pdf_size": 480642,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7523076921831765737&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 23,
        "aff": "RSISE, Australian National University, Australia + National ICT, Canberra, Australia; Dept. Statistical Science, Duke University, USA; National ICT, Canberra, Australia + RSISE, Australian National University, Australia; Dept. Statistics, University of Oxford, UK",
        "aff_domain": "nicta.com.au;gatsby.ucl.ac.uk;nicta.com.au;stats.ox.ac.uk",
        "email": "nicta.com.au;gatsby.ucl.ac.uk;nicta.com.au;stats.ox.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;1+0;3",
        "aff_unique_norm": "Australian National University;National ICT Australia;Duke University;University of Oxford",
        "aff_unique_dep": "Research School of Information Sciences and Engineering;;Department of Statistical Science;Dept. of Statistics",
        "aff_unique_url": "https://www.anu.edu.au;;https://www.duke.edu;https://www.ox.ac.uk",
        "aff_unique_abbr": "ANU;;Duke;Oxford",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Canberra",
        "aff_country_unique_index": "0+0;1;0+0;2",
        "aff_country_unique": "Australia;United States;United Kingdom"
    },
    {
        "id": "55a2bbf153",
        "title": "Differentially Private Learning with Kernels",
        "site": "https://proceedings.mlr.press/v28/jain13.html",
        "author": "Prateek Jain; Abhradeep Thakurta",
        "abstract": "In this paper, we consider the problem of differentially private learning where access to the training features is through a kernel function only. Existing work on this problem is restricted to translation invariant kernels only, where (approximate) training features are available explicitly.  In fact, for general class of kernel functions and in general setting of releasing different private predictor (\\w^*), the problem is impossible to solve \\citeCMS11. In this work, we relax the problem setting into three different easier but practical settings. In our first problem setting, we consider an interactive model where the user sends its test set to a trusted learner who sends back differentially private predictions over the test points. This setting is prevalent in modern online systems like search engines, ad engines etc. In the second model, the learner sends back a differentially private version of the optimal parameter vector \\w^* but requires access to a small subset of unlabeled test set beforehand. This also is a practical setting that involves two parties interacting through trusted third party. Our third model is similar to the traditional model, where learner is oblivious to the test set and needs to send a differentially private version of \\w^*, but the kernels are restricted to efficiently computable functions over low-dimensional vector spaces. For each of the models, we derive differentially private learning algorithms with provable \u201cutlity\u201d or error bounds. Moreover, we  show that our methods can also be applied to the traditional setting of \\cite Rubinstein09, CMS11. Here, our sample complexity bounds have only O(d^1/3) dependence on the dimensionality d while existing methods require O(d^1/2) samples to achieve same generalization error.",
        "bibtex": "@InProceedings{pmlr-v28-jain13,\n  title = \t {Differentially Private Learning with Kernels},\n  author = \t {Jain, Prateek and Thakurta, Abhradeep},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {118--126},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/jain13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/jain13.html},\n  abstract = \t {In this paper, we consider the problem of differentially private learning where access to the training features is through a kernel function only. Existing work on this problem is restricted to translation invariant kernels only, where (approximate) training features are available explicitly.  In fact, for general class of kernel functions and in general setting of releasing different private predictor (\\w^*), the problem is impossible to solve \\citeCMS11. In this work, we relax the problem setting into three different easier but practical settings. In our first problem setting, we consider an interactive model where the user sends its test set to a trusted learner who sends back differentially private predictions over the test points. This setting is prevalent in modern online systems like search engines, ad engines etc. In the second model, the learner sends back a differentially private version of the optimal parameter vector \\w^* but requires access to a small subset of unlabeled test set beforehand. This also is a practical setting that involves two parties interacting through trusted third party. Our third model is similar to the traditional model, where learner is oblivious to the test set and needs to send a differentially private version of \\w^*, but the kernels are restricted to efficiently computable functions over low-dimensional vector spaces. For each of the models, we derive differentially private learning algorithms with provable \u201cutlity\u201d or error bounds. Moreover, we  show that our methods can also be applied to the traditional setting of \\cite Rubinstein09, CMS11. Here, our sample complexity bounds have only O(d^1/3) dependence on the dimensionality d while existing methods require O(d^1/2) samples to achieve same generalization error.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/jain13.pdf",
        "supp": "",
        "pdf_size": 675477,
        "gs_citation": 127,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11528552776257937959&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Microsoft Research Labs, Bangalore, INDIA; Stanford University and Microsoft Research Silicon Valley Campus",
        "aff_domain": "microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Microsoft;Stanford University",
        "aff_unique_dep": "Research Labs;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/labs/microsoft-research-india;https://www.stanford.edu",
        "aff_unique_abbr": "MSR;Stanford",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Bangalore;Stanford",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "India;United States"
    },
    {
        "id": "d4813e3962",
        "title": "Direct Modeling of Complex Invariances for Visual Object Features",
        "site": "https://proceedings.mlr.press/v28/yuhui13.html",
        "author": "Ka Yu Hui",
        "abstract": "View-invariant object representations created from feature pooling networks have been widely adopted in state-of-the-art visual recognition systems. Recently, the research community seeks to improve these view-invariant representations further by additional invariance and receptive field learning, or by taking on the challenge of processing massive amounts of learning data. In this paper we consider an alternate strategy of directly modeling complex invariances of object features. While this may sound like a naive and inferior approach, our experiments show that this approach can achieve competitive and state-of-the-art accuracy on visual recognition data sets such as CIFAR-10 and STL-10. We present an highly applicable dictionary learning algorithm on complex invariances that can be used in most feature pooling network settings. It also has the merits of simplicity and requires no additional tuning. We also discuss the implication of our experiment results concerning recent observations on the usefulness of pre-trained features, and the role of direct invariance modeling in invariance learning.",
        "bibtex": "@InProceedings{pmlr-v28-yuhui13,\n  title = \t {Direct Modeling of Complex Invariances for Visual Object Features},\n  author = \t {Yu Hui, Ka},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {352--360},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/yuhui13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/yuhui13.html},\n  abstract = \t {View-invariant object representations created from feature pooling networks have been widely adopted in state-of-the-art visual recognition systems. Recently, the research community seeks to improve these view-invariant representations further by additional invariance and receptive field learning, or by taking on the challenge of processing massive amounts of learning data. In this paper we consider an alternate strategy of directly modeling complex invariances of object features. While this may sound like a naive and inferior approach, our experiments show that this approach can achieve competitive and state-of-the-art accuracy on visual recognition data sets such as CIFAR-10 and STL-10. We present an highly applicable dictionary learning algorithm on complex invariances that can be used in most feature pooling network settings. It also has the merits of simplicity and requires no additional tuning. We also discuss the implication of our experiment results concerning recent observations on the usefulness of pre-trained features, and the role of direct invariance modeling in invariance learning.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/yuhui13.pdf",
        "supp": "",
        "pdf_size": 733285,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=859200172968415798&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Information Engineering, The Chinese University of Hong Kong, Shatin, Hong Kong",
        "aff_domain": "gmail.com",
        "email": "gmail.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Chinese University of Hong Kong",
        "aff_unique_dep": "Department of Information Engineering",
        "aff_unique_url": "https://www.cuhk.edu.hk",
        "aff_unique_abbr": "CUHK",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0",
        "aff_country_unique": "China"
    },
    {
        "id": "aa590cc9a6",
        "title": "Discriminatively Activated Sparselets",
        "site": "https://proceedings.mlr.press/v28/girshick13.html",
        "author": "Ross Girshick; Hyun Oh Song; Trevor Darrell",
        "abstract": "Shared representations are highly appealing due to their potential  for gains in computational and statistical efficiency.  Compressing  a shared representation leads to greater computational savings, but  at the same time can severely decrease performance on a target task.  Recently, sparselets (Song et al., 2012) were introduced as a new  shared intermediate representation for multiclass object detection  with deformable part models (Felzenszwalb et al., 2010a), showing  significant speedup factors, but with a large decrease in task  performance.  In this paper we describe a new training framework  that learns which sparselets to activate in order to optimize a  discriminative objective, leading to larger speedup factors with  no decrease in task performance.  We first reformulate sparselets  in a general structured output prediction framework, then analyze  when sparselets lead to computational efficiency gains, and lastly  show experimental results on object detection and image classification  tasks.  Our experimental results demonstrate that discriminative  activation substantially outperforms the previous reconstructive  approach which, together with our structured output prediction  formulation, make sparselets broadly applicable and significantly  more effective.",
        "bibtex": "@InProceedings{pmlr-v28-girshick13,\n  title = \t {Discriminatively Activated Sparselets},\n  author = \t {Girshick, Ross and Song, Hyun Oh and Darrell, Trevor},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {196--204},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/girshick13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/girshick13.html},\n  abstract = \t {Shared representations are highly appealing due to their potential  for gains in computational and statistical efficiency.  Compressing  a shared representation leads to greater computational savings, but  at the same time can severely decrease performance on a target task.  Recently, sparselets (Song et al., 2012) were introduced as a new  shared intermediate representation for multiclass object detection  with deformable part models (Felzenszwalb et al., 2010a), showing  significant speedup factors, but with a large decrease in task  performance.  In this paper we describe a new training framework  that learns which sparselets to activate in order to optimize a  discriminative objective, leading to larger speedup factors with  no decrease in task performance.  We first reformulate sparselets  in a general structured output prediction framework, then analyze  when sparselets lead to computational efficiency gains, and lastly  show experimental results on object detection and image classification  tasks.  Our experimental results demonstrate that discriminative  activation substantially outperforms the previous reconstructive  approach which, together with our structured output prediction  formulation, make sparselets broadly applicable and significantly  more effective.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/girshick13.pdf",
        "supp": "",
        "pdf_size": 538347,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8528477580687184942&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "University of California, Berkeley; University of California, Berkeley; University of California, Berkeley",
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "bfb5ecf072",
        "title": "Distributed training of Large-scale Logistic models",
        "site": "https://proceedings.mlr.press/v28/gopal13.html",
        "author": "Siddharth Gopal; Yiming Yang",
        "abstract": "Regularized Multinomial Logistic regression has emerged as one of the most common methods for performing data classification and analysis. With the advent of large-scale data it is common to find scenarios where the number of possible multinomial outcomes is large (in the order of thousands to tens of thousands). In such cases, the computational cost of training logistic models or even simply iterating through all the model parameters is prohibitively expensive. In this paper, we propose a training method for large-scale multinomial logistic models that breaks this bottleneck by enabling parallel optimization of the likelihood objective. Our experiments on large-scale datasets showed an order of magnitude reduction in training time.",
        "bibtex": "@InProceedings{pmlr-v28-gopal13,\n  title = \t {Distributed training of Large-scale Logistic models},\n  author = \t {Gopal, Siddharth and Yang, Yiming},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {289--297},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/gopal13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/gopal13.html},\n  abstract = \t {Regularized Multinomial Logistic regression has emerged as one of the most common methods for performing data classification and analysis. With the advent of large-scale data it is common to find scenarios where the number of possible multinomial outcomes is large (in the order of thousands to tens of thousands). In such cases, the computational cost of training logistic models or even simply iterating through all the model parameters is prohibitively expensive. In this paper, we propose a training method for large-scale multinomial logistic models that breaks this bottleneck by enabling parallel optimization of the likelihood objective. Our experiments on large-scale datasets showed an order of magnitude reduction in training time.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/gopal13.pdf",
        "supp": "",
        "pdf_size": 548184,
        "gs_citation": 81,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5903228120224770455&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "http://lshtc.iit.demokritos.gr/",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8246066411",
        "title": "Distribution to Distribution Regression",
        "site": "https://proceedings.mlr.press/v28/oliva13.html",
        "author": "Junier Oliva; Barnabas Poczos; Jeff Schneider",
        "abstract": "We analyze \u2019Distribution to Distribution regression\u2019 where one is regressing a mapping where both the covariate (inputs) and response (outputs) are distributions. No parameters on the input or output distributions are assumed, nor are any strong assumptions made on the measure from which input distributions are drawn from. We develop an estimator and derive an upper bound for the L2 risk; also, we show that when the effective dimension is small enough (as measured by the doubling dimension), then the risk converges to zero with a polynomial rate.",
        "bibtex": "@InProceedings{pmlr-v28-oliva13,\n  title = \t {Distribution to Distribution Regression},\n  author = \t {Oliva, Junier and Poczos, Barnabas and Schneider, Jeff},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1049--1057},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/oliva13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/oliva13.html},\n  abstract = \t {We analyze \u2019Distribution to Distribution regression\u2019 where one is regressing a mapping where both the covariate (inputs) and response (outputs) are distributions. No parameters on the input or output distributions are assumed, nor are any strong assumptions made on the measure from which input distributions are drawn from. We develop an estimator and derive an upper bound for the L2 risk; also, we show that when the effective dimension is small enough (as measured by the doubling dimension), then the risk converges to zero with a polynomial rate.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/oliva13.pdf",
        "supp": "",
        "pdf_size": 2559454,
        "gs_citation": 94,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=231488308130287546&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Machine Learning Department, School of Computer Science, Carnegie Mellon University; Machine Learning Department, School of Computer Science, Carnegie Mellon University; Machine Learning Department, School of Computer Science, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Machine Learning Department",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d654e9bc51",
        "title": "Domain Adaptation for Sequence Labeling Tasks with a Probabilistic Language Adaptation Model",
        "site": "https://proceedings.mlr.press/v28/xiao13.html",
        "author": "Min Xiao; Yuhong Guo",
        "abstract": "In this paper, we propose to address the problem of domain adaptation for sequence labeling tasks via distributed representation learning by using a log-bilinear language adaptation model. The proposed neural probabilistic language model simultaneously models two different but related data distributions in the source and target domains   based on induced distributed representations, which encode both generalizable and domain-specific latent features. We then use the learned dense real-valued representation as   augmenting features for natural language processing systems. We empirically evaluate the proposed learning technique on WSJ and MEDLINE domains with POS tagging systems, and on WSJ and Brown corpora with syntactic chunking and name entity recognition systems. Our primary results show that the proposed domain adaptation method outperforms a number comparison methods for cross domain sequence labeling tasks.",
        "bibtex": "@InProceedings{pmlr-v28-xiao13,\n  title = \t {Domain Adaptation for Sequence Labeling Tasks with a Probabilistic Language Adaptation Model},\n  author = \t {Xiao, Min and Guo, Yuhong},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {293--301},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/xiao13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/xiao13.html},\n  abstract = \t {In this paper, we propose to address the problem of domain adaptation for sequence labeling tasks via distributed representation learning by using a log-bilinear language adaptation model. The proposed neural probabilistic language model simultaneously models two different but related data distributions in the source and target domains   based on induced distributed representations, which encode both generalizable and domain-specific latent features. We then use the learned dense real-valued representation as   augmenting features for natural language processing systems. We empirically evaluate the proposed learning technique on WSJ and MEDLINE domains with POS tagging systems, and on WSJ and Brown corpora with syntactic chunking and name entity recognition systems. Our primary results show that the proposed domain adaptation method outperforms a number comparison methods for cross domain sequence labeling tasks. }\n}",
        "pdf": "http://proceedings.mlr.press/v28/xiao13.pdf",
        "supp": "",
        "pdf_size": 173169,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15063960330151409706&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer and Information Sciences, Temple University, Philadelphia, PA 19122, USA; Department of Computer and Information Sciences, Temple University, Philadelphia, PA 19122, USA",
        "aff_domain": "temple.edu;temple.edu",
        "email": "temple.edu;temple.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Temple University",
        "aff_unique_dep": "Department of Computer and Information Sciences",
        "aff_unique_url": "https://www.temple.edu",
        "aff_unique_abbr": "Temple",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Philadelphia",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b5f2f61425",
        "title": "Domain Adaptation under Target and Conditional Shift",
        "site": "https://proceedings.mlr.press/v28/zhang13d.html",
        "author": "Kun Zhang; Bernhard Sch\u00f6lkopf; Krikamol Muandet; Zhikun Wang",
        "abstract": "Let X denote the feature and Y the target. We consider domain adaptation under three possible scenarios: (1) the marginal P_Y changes, while the conditional P_X|Y stays the same (\\it target shift), (2) the marginal P_Y is fixed, while the conditional P_X|Y changes with certain constraints (\\it conditional shift), and (3) the marginal P_Y changes, and the conditional P_X|Y changes with constraints (\\it generalized target shift). Using background knowledge, causal interpretations allow us to determine the correct situation for a problem at hand. We exploit importance reweighting or sample transformation to find the learning machine that works well on test data, and propose to estimate the weights or transformations by \\it reweighting or transforming training data to reproduce the covariate distribution on the test domain. Thanks to kernel embedding of conditional as well as marginal distributions, the proposed approaches avoid distribution estimation, and are applicable for high-dimensional problems. Numerical evaluations on synthetic and real-world datasets demonstrate the effectiveness of the proposed framework.",
        "bibtex": "@InProceedings{pmlr-v28-zhang13d,\n  title = \t {Domain Adaptation under Target and Conditional Shift},\n  author = \t {Zhang, Kun and Sch\u00f6lkopf, Bernhard and Muandet, Krikamol and Wang, Zhikun},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {819--827},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/zhang13d.pdf},\n  url = \t {https://proceedings.mlr.press/v28/zhang13d.html},\n  abstract = \t {Let X denote the feature and Y the target. We consider domain adaptation under three possible scenarios: (1) the marginal P_Y changes, while the conditional P_X|Y stays the same (\\it target shift), (2) the marginal P_Y is fixed, while the conditional P_X|Y changes with certain constraints (\\it conditional shift), and (3) the marginal P_Y changes, and the conditional P_X|Y changes with constraints (\\it generalized target shift). Using background knowledge, causal interpretations allow us to determine the correct situation for a problem at hand. We exploit importance reweighting or sample transformation to find the learning machine that works well on test data, and propose to estimate the weights or transformations by \\it reweighting or transforming training data to reproduce the covariate distribution on the test domain. Thanks to kernel embedding of conditional as well as marginal distributions, the proposed approaches avoid distribution estimation, and are applicable for high-dimensional problems. Numerical evaluations on synthetic and real-world datasets demonstrate the effectiveness of the proposed framework.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/zhang13d.pdf",
        "supp": "",
        "pdf_size": 2913963,
        "gs_citation": 827,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5724484728674297162&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Max Plank Institute for Intelligent Systems, T\"ubingen, Germany; Max Plank Institute for Intelligent Systems, T\"ubingen, Germany; Max Plank Institute for Intelligent Systems, T\"ubingen, Germany; Max Plank Institute for Intelligent Systems, T\"ubingen, Germany",
        "aff_domain": "tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de",
        "email": "tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.mpi-iis.mpg.de",
        "aff_unique_abbr": "MPI-IS",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "T\u00fcbingen",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "6a587258e1",
        "title": "Domain Generalization via Invariant Feature Representation",
        "site": "https://proceedings.mlr.press/v28/muandet13.html",
        "author": "Krikamol Muandet; David Balduzzi; Bernhard Sch\u00f6lkopf",
        "abstract": "This paper investigates domain generalization: How to take knowledge acquired from an arbitrary number of related domains and apply it to previously unseen domains? We propose Domain-Invariant Component Analysis (DICA), a kernel-based optimization algorithm that learns an invariant transformation by minimizing the dissimilarity across domains, whilst preserving the functional relationship between input and output variables. A learning-theoretic analysis shows that reducing dissimilarity improves the expected generalization ability of classifiers on new domains, motivating the proposed algorithm. Experimental results on synthetic and real-world datasets demonstrate that DICA successfully learns invariant features and improves classifier performance in practice.",
        "bibtex": "@InProceedings{pmlr-v28-muandet13,\n  title = \t {Domain Generalization via Invariant Feature Representation},\n  author = \t {Muandet, Krikamol and Balduzzi, David and Sch\u00f6lkopf, Bernhard},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {10--18},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/muandet13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/muandet13.html},\n  abstract = \t {This paper investigates domain generalization: How to take knowledge acquired from an arbitrary number of related domains and apply it to previously unseen domains? We propose Domain-Invariant Component Analysis (DICA), a kernel-based optimization algorithm that learns an invariant transformation by minimizing the dissimilarity across domains, whilst preserving the functional relationship between input and output variables. A learning-theoretic analysis shows that reducing dissimilarity improves the expected generalization ability of classifiers on new domains, motivating the proposed algorithm. Experimental results on synthetic and real-world datasets demonstrate that DICA successfully learns invariant features and improves classifier performance in practice.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/muandet13.pdf",
        "supp": "",
        "pdf_size": 1551655,
        "gs_citation": 1407,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4651322990057915347&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Max Planck Institute for Intelligent Systems, Spemannstra\u00dfe 38, 72076 T\u00fcbingen, Germany; Department of Computer Science, ETH Zurich, Universit\u00e4tstrasse 6, 8092 Zurich, Switzerland; Max Planck Institute for Intelligent Systems, Spemannstra\u00dfe 38, 72076 T\u00fcbingen, Germany",
        "aff_domain": "tuebingen.mpg.de;inf.ethz.ch;tuebingen.mpg.de",
        "email": "tuebingen.mpg.de;inf.ethz.ch;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems;ETH Zurich",
        "aff_unique_dep": ";Department of Computer Science",
        "aff_unique_url": "https://www.mpi-is.mpg.de;https://www.ethz.ch",
        "aff_unique_abbr": "MPI-IS;ETHZ",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "T\u00fcbingen;Zurich",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Germany;Switzerland"
    },
    {
        "id": "fe41a39845",
        "title": "Dual Averaging and Proximal Gradient Descent for Online Alternating Direction Multiplier Method",
        "site": "https://proceedings.mlr.press/v28/suzuki13.html",
        "author": "Taiji Suzuki",
        "abstract": "We develop new stochastic optimization methods that are applicable to   a wide range of structured regularizations.  Basically our methods are combinations of   basic stochastic optimization techniques and Alternating Direction Multiplier Method (ADMM).  ADMM is a general framework for optimizing a composite function,  and has a wide range of applications.  We propose two types of online variants of ADMM,   which correspond to online proximal gradient descent and regularized dual averaging respectively.  The proposed algorithms are computationally efficient and easy to implement.  Our methods yield O(1/\\sqrtT) convergence of the expected risk.  Moreover, the online proximal gradient descent type method yields   O(\\log(T)/T) convergence for a strongly convex loss.  Numerical experiments show effectiveness of our methods in learning tasks with structured sparsity  such as overlapped group lasso.",
        "bibtex": "@InProceedings{pmlr-v28-suzuki13,\n  title = \t {Dual Averaging and Proximal Gradient Descent for Online Alternating Direction Multiplier Method},\n  author = \t {Suzuki, Taiji},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {392--400},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/suzuki13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/suzuki13.html},\n  abstract = \t {We develop new stochastic optimization methods that are applicable to   a wide range of structured regularizations.  Basically our methods are combinations of   basic stochastic optimization techniques and Alternating Direction Multiplier Method (ADMM).  ADMM is a general framework for optimizing a composite function,  and has a wide range of applications.  We propose two types of online variants of ADMM,   which correspond to online proximal gradient descent and regularized dual averaging respectively.  The proposed algorithms are computationally efficient and easy to implement.  Our methods yield O(1/\\sqrtT) convergence of the expected risk.  Moreover, the online proximal gradient descent type method yields   O(\\log(T)/T) convergence for a strongly convex loss.  Numerical experiments show effectiveness of our methods in learning tasks with structured sparsity  such as overlapped group lasso.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/suzuki13.pdf",
        "supp": "",
        "pdf_size": 234065,
        "gs_citation": 188,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14263013663631534163&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Mathematical Informatics, The University of Tokyo, Tokyo 113-8656, Japan",
        "aff_domain": "stat.t.u-tokyo.ac.jp",
        "email": "stat.t.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Tokyo",
        "aff_unique_dep": "Department of Mathematical Informatics",
        "aff_unique_url": "https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "UTokyo",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Tokyo",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "79e0c55957",
        "title": "Dynamic Covariance Models for Multivariate Financial Time Series",
        "site": "https://proceedings.mlr.press/v28/wu13.html",
        "author": "Yue Wu; Jose Miguel Hernandez-Lobato; Ghahramani Zoubin",
        "abstract": "The accurate prediction of time-changing covariances is an important problem in the modeling of multivariate financial data. However, some of the most popular models suffer from a) overfitting problems and multiple local optima, b) failure to capture shifts in market conditions and c) large computational costs. To address these problems we introduce a novel dynamic model for time-changing covariances. Over-fitting and local optima are avoided by following a Bayesian approach instead of computing point estimates. Changes in market conditions are captured by assuming a diffusion process in parameter values, and finally computationally efficient and scalable inference is performed using particle filters. Experiments with financial data show excellent performance of the proposed method with respect to current standard models.",
        "bibtex": "@InProceedings{pmlr-v28-wu13,\n  title = \t {Dynamic Covariance Models for Multivariate Financial Time Series},\n  author = \t {Wu, Yue and Miguel Hernandez-Lobato, Jose and Zoubin, Ghahramani},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {558--566},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/wu13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/wu13.html},\n  abstract = \t {The accurate prediction of time-changing covariances is an important problem in the modeling of multivariate financial data. However, some of the most popular models suffer from a) overfitting problems and multiple local optima, b) failure to capture shifts in market conditions and c) large computational costs. To address these problems we introduce a novel dynamic model for time-changing covariances. Over-fitting and local optima are avoided by following a Bayesian approach instead of computing point estimates. Changes in market conditions are captured by assuming a diffusion process in parameter values, and finally computationally efficient and scalable inference is performed using particle filters. Experiments with financial data show excellent performance of the proposed method with respect to current standard models.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/wu13.pdf",
        "supp": "",
        "pdf_size": 916377,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14655058537998941792&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "University of Cambridge, Department of Engineering, Cambridge CB2 1PZ, UK; University of Cambridge, Department of Engineering, Cambridge CB2 1PZ, UK; University of Cambridge, Department of Engineering, Cambridge CB2 1PZ, UK",
        "aff_domain": "cam.ac.uk;cam.ac.uk;eng.cam.ac.uk",
        "email": "cam.ac.uk;cam.ac.uk;eng.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "Department of Engineering",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "764047e857",
        "title": "Dynamic Probabilistic Models for Latent Feature Propagation in Social Networks",
        "site": "https://proceedings.mlr.press/v28/heaukulani13.html",
        "author": "Creighton Heaukulani; Zoubin Ghahramani",
        "abstract": "Current Bayesian models for dynamic social network data have focused on modelling the influence of evolving unobserved structure on observed social interactions. However, an understanding of how observed social relationships from the past affect future unobserved structure in the network has been neglected.  In this paper, we introduce a new probabilistic model for capturing this phenomenon, which we call latent feature propagation, in social networks.  We demonstrate our model\u2019s capability for inferring such latent structure in varying types of social network datasets, and experimental studies show this structure achieves higher predictive performance on link prediction and forecasting tasks.",
        "bibtex": "@InProceedings{pmlr-v28-heaukulani13,\n  title = \t {Dynamic Probabilistic Models for Latent Feature Propagation in Social Networks},\n  author = \t {Heaukulani, Creighton and Ghahramani, Zoubin},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {275--283},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/heaukulani13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/heaukulani13.html},\n  abstract = \t {Current Bayesian models for dynamic social network data have focused on modelling the influence of evolving unobserved structure on observed social interactions. However, an understanding of how observed social relationships from the past affect future unobserved structure in the network has been neglected.  In this paper, we introduce a new probabilistic model for capturing this phenomenon, which we call latent feature propagation, in social networks.  We demonstrate our model\u2019s capability for inferring such latent structure in varying types of social network datasets, and experimental studies show this structure achieves higher predictive performance on link prediction and forecasting tasks.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/heaukulani13.pdf",
        "supp": "",
        "pdf_size": 453084,
        "gs_citation": 97,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14419887730120203205&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "University of Cambridge, Dept. of Engineering, Trumpington St., Cambridge, CB2 1PZ, UK; University of Cambridge, Dept. of Engineering, Trumpington St., Cambridge, CB2 1PZ, UK",
        "aff_domain": "cam.ac.uk;eng.cam.ac.uk",
        "email": "cam.ac.uk;eng.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "Dept. of Engineering",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "001c23eb75",
        "title": "Dynamical Models and tracking regret in online convex programming",
        "site": "https://proceedings.mlr.press/v28/hall13.html",
        "author": "Eric Hall; Rebecca Willett",
        "abstract": "This paper describes a new online convex optimization method which incorporates a family of candidate dynamical models and establishes novel tracking regret bounds that scale with comparator\u2019s deviation from the best dynamical model in this family. Previous online optimization methods are designed to have a total accumulated loss comparable to that of the best comparator sequence, and existing tracking or shifting regret bounds scale with the overall variation of the comparator sequence. In many practical scenarios, however, the environment is nonstationary and comparator sequences with small variation are quite weak, resulting in large losses. The proposed dynamic mirror descent method, in contrast, can yield low regret relative to highly variable comparator sequences by both tracking the best dynamical model and forming predictions based on that model. This concept is demonstrated empirically in the context of sequential compressive observations of a dynamic scene and tracking a dynamic social network.",
        "bibtex": "@InProceedings{pmlr-v28-hall13,\n  title = \t {Dynamical Models and tracking regret in online convex programming},\n  author = \t {Hall, Eric and Willett, Rebecca},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {579--587},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/hall13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/hall13.html},\n  abstract = \t {This paper describes a new online convex optimization method which incorporates a family of candidate dynamical models and establishes novel tracking regret bounds that scale with comparator\u2019s deviation from the best dynamical model in this family. Previous online optimization methods are designed to have a total accumulated loss comparable to that of the best comparator sequence, and existing tracking or shifting regret bounds scale with the overall variation of the comparator sequence. In many practical scenarios, however, the environment is nonstationary and comparator sequences with small variation are quite weak, resulting in large losses. The proposed dynamic mirror descent method, in contrast, can yield low regret relative to highly variable comparator sequences by both tracking the best dynamical model and forming predictions based on that model. This concept is demonstrated empirically in the context of sequential compressive observations of a dynamic scene and tracking a dynamic social network.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/hall13.pdf",
        "supp": "",
        "pdf_size": 1320492,
        "gs_citation": 127,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17894295775268498861&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Duke University, Department of Electrical and Computer Engineering, Durham, NC 27708; Duke University, Department of Electrical and Computer Engineering, Durham, NC 27708",
        "aff_domain": "duke.edu;duke.edu",
        "email": "duke.edu;duke.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Duke University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.duke.edu",
        "aff_unique_abbr": "Duke",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Durham",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6f692cb0c1",
        "title": "ELLA: An Efficient Lifelong Learning Algorithm",
        "site": "https://proceedings.mlr.press/v28/ruvolo13.html",
        "author": "Paul Ruvolo; Eric Eaton",
        "abstract": "The problem of learning multiple consecutive tasks, known as lifelong learning, is of great importance to the creation of intelligent, general-purpose, and flexible machines.  In this paper, we develop a method for online multi-task learning in the lifelong learning setting.  The proposed Efficient Lifelong Learning Algorithm (ELLA) maintains a sparsely shared basis for all task models, transfers knowledge from the basis to learn each new task, and refines the basis over time to maximize performance across all tasks. We show that ELLA has strong connections to both online dictionary learning for sparse coding and state-of-the-art batch multi-task learning methods, and provide robust theoretical performance guarantees.  We show empirically that ELLA yields nearly identical performance to batch multi-task learning while learning tasks sequentially in three orders of magnitude (over 1,000x) less time.",
        "bibtex": "@InProceedings{pmlr-v28-ruvolo13,\n  title = \t {{ELLA}: An Efficient Lifelong Learning Algorithm},\n  author = \t {Ruvolo, Paul and Eaton, Eric},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {507--515},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/ruvolo13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/ruvolo13.html},\n  abstract = \t {The problem of learning multiple consecutive tasks, known as lifelong learning, is of great importance to the creation of intelligent, general-purpose, and flexible machines.  In this paper, we develop a method for online multi-task learning in the lifelong learning setting.  The proposed Efficient Lifelong Learning Algorithm (ELLA) maintains a sparsely shared basis for all task models, transfers knowledge from the basis to learn each new task, and refines the basis over time to maximize performance across all tasks. We show that ELLA has strong connections to both online dictionary learning for sparse coding and state-of-the-art batch multi-task learning methods, and provide robust theoretical performance guarantees.  We show empirically that ELLA yields nearly identical performance to batch multi-task learning while learning tasks sequentially in three orders of magnitude (over 1,000x) less time.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/ruvolo13.pdf",
        "supp": "",
        "pdf_size": 1991068,
        "gs_citation": 480,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11226635703271850866&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Bryn Mawr College, Computer Science Department, 101 North Merion Avenue, Bryn Mawr, PA 19010 USA; Bryn Mawr College, Computer Science Department, 101 North Merion Avenue, Bryn Mawr, PA 19010 USA",
        "aff_domain": "cs.brynmawr.edu;cs.brynmawr.edu",
        "email": "cs.brynmawr.edu;cs.brynmawr.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Bryn Mawr College",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.brynmawr.edu",
        "aff_unique_abbr": "Bryn Mawr College",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Bryn Mawr",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b6e1bf49fe",
        "title": "Efficient Active Learning of Halfspaces: an Aggressive Approach",
        "site": "https://proceedings.mlr.press/v28/gonen13.html",
        "author": "Alon Gonen; Sivan Sabato; Shai Shalev-Shwartz",
        "abstract": "We study pool-based active learning of half-spaces. We revisit the aggressive approach for active learning in the realizable case, and show that it can be made efficient and  practical, while also having theoretical guarantees under reasonable assumptions. We further show, both theoretically and experimentally, that it can be preferable to mellow approaches.  Our efficient aggressive active learner of half-spaces has formal approximation guarantees that hold when the pool is separable with a margin. While our analysis is focused on the realizable setting, we show that a simple heuristic allows using the same algorithm successfully for pools with low error as well. We further compare the aggressive approach to the mellow approach, and prove that there are cases in which the aggressive approach results in significantly better label complexity compared to the mellow approach. We demonstrate experimentally that substantial improvements in label complexity can be achieved using the aggressive approach, for both realizable and low-error settings.",
        "bibtex": "@InProceedings{pmlr-v28-gonen13,\n  title = \t {Efficient Active Learning of Halfspaces: an Aggressive Approach},\n  author = \t {Gonen, Alon and Sabato, Sivan and Shalev-Shwartz, Shai},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {480--488},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/gonen13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/gonen13.html},\n  abstract = \t {We study pool-based active learning of half-spaces. We revisit the aggressive approach for active learning in the realizable case, and show that it can be made efficient and  practical, while also having theoretical guarantees under reasonable assumptions. We further show, both theoretically and experimentally, that it can be preferable to mellow approaches.  Our efficient aggressive active learner of half-spaces has formal approximation guarantees that hold when the pool is separable with a margin. While our analysis is focused on the realizable setting, we show that a simple heuristic allows using the same algorithm successfully for pools with low error as well. We further compare the aggressive approach to the mellow approach, and prove that there are cases in which the aggressive approach results in significantly better label complexity compared to the mellow approach. We demonstrate experimentally that substantial improvements in label complexity can be achieved using the aggressive approach, for both realizable and low-error settings.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/gonen13.pdf",
        "supp": "",
        "pdf_size": 965476,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6133540608942675292&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "Benin School of CSE, The Hebrew University; Microsoft Research New England; Benin School of CSE, The Hebrew University",
        "aff_domain": "cs.huji.ac.il;microsoft.com;cs.huji.ac.il",
        "email": "cs.huji.ac.il;microsoft.com;cs.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Hebrew University;Microsoft",
        "aff_unique_dep": "School of CSE;Microsoft Research",
        "aff_unique_url": "http://www.huji.ac.il;https://www.microsoft.com/en-us/research/group/microsoft-research-new-england",
        "aff_unique_abbr": "HUJI;MSR NE",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Benin;New England",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Israel;United States"
    },
    {
        "id": "1f43592580",
        "title": "Efficient Dimensionality Reduction for  Canonical Correlation Analysis",
        "site": "https://proceedings.mlr.press/v28/avron13.html",
        "author": "Haim Avron; Christos Boutsidis; Sivan Toledo; Anastasios Zouzias",
        "abstract": "We present a fast algorithm for approximate Canonical Correlation Analysis (CCA). Given a pair of tall-and-thin matrices, the proposed algorithm first employs a randomized  dimensionality reduction transform to reduce the size of the input matrices, and then applies any standard CCA algorithm to the new pair of matrices. The algorithm computes an approximate CCA to the original pair of matrices with provable guarantees, while requiring asymptotically less operations than the state-of-the-art exact algorithms.",
        "bibtex": "@InProceedings{pmlr-v28-avron13,\n  title = \t {Efficient Dimensionality Reduction for  Canonical Correlation Analysis},\n  author = \t {Avron, Haim and Boutsidis, Christos and Toledo, Sivan and Zouzias, Anastasios},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {347--355},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/avron13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/avron13.html},\n  abstract = \t {We present a fast algorithm for approximate Canonical Correlation Analysis (CCA). Given a pair of tall-and-thin matrices, the proposed algorithm first employs a randomized  dimensionality reduction transform to reduce the size of the input matrices, and then applies any standard CCA algorithm to the new pair of matrices. The algorithm computes an approximate CCA to the original pair of matrices with provable guarantees, while requiring asymptotically less operations than the state-of-the-art exact algorithms.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/avron13.pdf",
        "supp": "",
        "pdf_size": 435653,
        "gs_citation": 70,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1579395186755389855&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 26,
        "aff": "IBM T.J. Watson Research Center; IBM T.J. Watson Research Center; Tel-Aviv University; University of Toronto",
        "aff_domain": "us.ibm.com;us.ibm.com;tau.ac.il;cs.toronto.edu",
        "email": "us.ibm.com;us.ibm.com;tau.ac.il;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "IBM;Tel Aviv University;University of Toronto",
        "aff_unique_dep": "Research Center;;",
        "aff_unique_url": "https://www.ibm.com/research/watson;https://www.tau.ac.il;https://www.utoronto.ca",
        "aff_unique_abbr": "IBM;TAU;U of T",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "T.J. Watson;",
        "aff_country_unique_index": "0;0;1;2",
        "aff_country_unique": "United States;Israel;Canada"
    },
    {
        "id": "bef84cc2f8",
        "title": "Efficient Multi-label Classification with Many Labels",
        "site": "https://proceedings.mlr.press/v28/bi13.html",
        "author": "Wei Bi; James Kwok",
        "abstract": "Multi-label classification deals with the problem where each instance can be associated with a set of class labels. However, in many real-world applications, the number of class labels can be in the hundreds or even thousands, and existing multi-label classification methods often become computationally inefficient. In recent years, a number of remedies have been proposed. However, they are either based on simple dimension reduction techniques or involve expensive optimization problems. In this paper, we address this problem by selecting a small subset of class labels that can approximately span the original label space. This is performed by randomized sampling where the sampling probability of each class label reflects its importance among all the labels. Theoretical analysis shows that this randomized sampling approach is highly efficient. Experiments on a number of real-world multi-label datasets with many labels demonstrate the appealing performance and efficiency of the proposed algorithm.",
        "bibtex": "@InProceedings{pmlr-v28-bi13,\n  title = \t {Efficient Multi-label Classification with Many Labels},\n  author = \t {Bi, Wei and Kwok, James},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {405--413},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/bi13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/bi13.html},\n  abstract = \t {Multi-label classification deals with the problem where each instance can be associated with a set of class labels. However, in many real-world applications, the number of class labels can be in the hundreds or even thousands, and existing multi-label classification methods often become computationally inefficient. In recent years, a number of remedies have been proposed. However, they are either based on simple dimension reduction techniques or involve expensive optimization problems. In this paper, we address this problem by selecting a small subset of class labels that can approximately span the original label space. This is performed by randomized sampling where the sampling probability of each class label reflects its importance among all the labels. Theoretical analysis shows that this randomized sampling approach is highly efficient. Experiments on a number of real-world multi-label datasets with many labels demonstrate the appealing performance and efficiency of the proposed algorithm.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/bi13.pdf",
        "supp": "",
        "pdf_size": 502532,
        "gs_citation": 253,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10804961988973970446&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "Department of Computer Science and Engineering, Hong Kong University of Science and Technology; Department of Computer Science and Engineering, Hong Kong University of Science and Technology",
        "aff_domain": "cse.ust.hk;cse.ust.hk",
        "email": "cse.ust.hk;cse.ust.hk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.ust.hk",
        "aff_unique_abbr": "HKUST",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "4d76df1e8d",
        "title": "Efficient Ranking from Pairwise Comparisons",
        "site": "https://proceedings.mlr.press/v28/wauthier13.html",
        "author": "Fabian Wauthier; Michael Jordan; Nebojsa Jojic",
        "abstract": "The ranking of n objects based on pairwise comparisons is a core machine learning problem, arising in recommender systems, ad placement, player ranking, biological applications and others. In many practical situations the true pairwise comparisons cannot be actively measured, but a subset of all n(n-1)/2 comparisons is passively and noisily observed. Optimization algorithms (e.g., the SVM) could be used to predict a ranking with fixed expected Kendall tau distance, while achieving an \u03a9(n) lower bound on the corresponding sample complexity. However, due to their centralized structure they are difficult to extend to online or distributed settings. In this paper we show that much simpler algorithms can match the same \u03a9(n) lower bound in expectation. Furthermore, if an average of O(n\\log(n)) binary comparisons are measured, then one algorithm recovers the true ranking in a uniform sense, while the other predicts the ranking more accurately near the top than the bottom. We discuss extensions to online and distributed ranking, with benefits over traditional alternatives.",
        "bibtex": "@InProceedings{pmlr-v28-wauthier13,\n  title = \t {Efficient Ranking from Pairwise Comparisons},\n  author = \t {Wauthier, Fabian and Jordan, Michael and Jojic, Nebojsa},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {109--117},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/wauthier13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/wauthier13.html},\n  abstract = \t {The ranking of n objects based on pairwise comparisons is a core machine learning problem, arising in recommender systems, ad placement, player ranking, biological applications and others. In many practical situations the true pairwise comparisons cannot be actively measured, but a subset of all n(n-1)/2 comparisons is passively and noisily observed. Optimization algorithms (e.g., the SVM) could be used to predict a ranking with fixed expected Kendall tau distance, while achieving an \u03a9(n) lower bound on the corresponding sample complexity. However, due to their centralized structure they are difficult to extend to online or distributed settings. In this paper we show that much simpler algorithms can match the same \u03a9(n) lower bound in expectation. Furthermore, if an average of O(n\\log(n)) binary comparisons are measured, then one algorithm recovers the true ranking in a uniform sense, while the other predicts the ranking more accurately near the top than the bottom. We discuss extensions to online and distributed ranking, with benefits over traditional alternatives.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/wauthier13.pdf",
        "supp": "",
        "pdf_size": 481104,
        "gs_citation": 259,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6460228315464550732&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Computer Science Division, University of California, Berkeley, CA 94720, USA; Computer Science Division, University of California, Berkeley, CA 94720, USA; Microsoft Research, Redmond, WA 98052, USA",
        "aff_domain": "cs.berkeley.edu;cs.berkeley.edu;microsoft.com",
        "email": "cs.berkeley.edu;cs.berkeley.edu;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of California, Berkeley;Microsoft",
        "aff_unique_dep": "Computer Science Division;Microsoft Research",
        "aff_unique_url": "https://www.berkeley.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "UC Berkeley;MSR",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Berkeley;Redmond",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3f8b272da9",
        "title": "Efficient Semi-supervised and Active Learning of Disjunctions",
        "site": "https://proceedings.mlr.press/v28/balcan13.html",
        "author": "Nina Balcan; Christopher Berlind; Steven Ehrlich; Yingyu Liang",
        "abstract": "We provide efficient algorithms for learning disjunctions in the semi-supervised setting under a natural regularity assumption introduced by (Balcan & Blum, 2005). We prove bounds on the sample complexity of our algorithms under a mild restriction on the data distribution. We also give an active learning algorithm with improved sample complexity and extend all our algorithms to the random classification noise setting.",
        "bibtex": "@InProceedings{pmlr-v28-balcan13,\n  title = \t {Efficient Semi-supervised and Active Learning of Disjunctions},\n  author = \t {Balcan, Nina and Berlind, Christopher and Ehrlich, Steven and Liang, Yingyu},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {633--641},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/balcan13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/balcan13.html},\n  abstract = \t {We provide efficient algorithms for learning disjunctions in the semi-supervised setting under a natural regularity assumption introduced by (Balcan & Blum, 2005). We prove bounds on the sample complexity of our algorithms under a mild restriction on the data distribution. We also give an active learning algorithm with improved sample complexity and extend all our algorithms to the random classification noise setting.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/balcan13.pdf",
        "supp": "",
        "pdf_size": 376078,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12235134519595873792&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "School of Computer Science, College of Computing, Georgia Institute of Technology, Atlanta, GA, USA; School of Computer Science, College of Computing, Georgia Institute of Technology, Atlanta, GA, USA; School of Computer Science, College of Computing, Georgia Institute of Technology, Atlanta, GA, USA; School of Computer Science, College of Computing, Georgia Institute of Technology, Atlanta, GA, USA",
        "aff_domain": "cc.gatech.edu;gatech.edu;cc.gatech.edu;gatech.edu",
        "email": "cc.gatech.edu;gatech.edu;cc.gatech.edu;gatech.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Georgia Institute of Technology",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.gatech.edu",
        "aff_unique_abbr": "Georgia Tech",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Atlanta",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f273175300",
        "title": "Efficient Sparse Group Feature Selection via Nonconvex Optimization",
        "site": "https://proceedings.mlr.press/v28/xiang13.html",
        "author": "Shuo Xiang; Xiaoshen Tong; Jieping Ye",
        "abstract": "Sparse feature selection has been demonstrated to be effective in handling high-dimensional data. While promising, most of the existing works use convex methods, which may be suboptimal in terms of the accuracy of feature selection and parameter estimation. In this paper, we expand a nonconvex paradigm to sparse group feature selection, which is motivated by applications that require identifying the underlying group structure and performing feature selection simultaneously. The main contributions of this article are twofold: (1) computationally, we introduce a nonconvex sparse group feature selection model and present an efficient optimization algorithm, of which the key step is a projection with two coupled constraints; (2) statistically, we show that the proposed model can reconstruct the oracle estimator. Therefore, consistent feature selection and parameter estimation can be achieved. Numerical results on synthetic and real-world data suggest that the proposed nonconvex method compares favorably against its competitors, thus achieving desired goal of delivering high performance.",
        "bibtex": "@InProceedings{pmlr-v28-xiang13,\n  title = \t {Efficient Sparse Group Feature Selection via Nonconvex Optimization},\n  author = \t {Xiang, Shuo and Tong, Xiaoshen and Ye, Jieping},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {284--292},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/xiang13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/xiang13.html},\n  abstract = \t {Sparse feature selection has been demonstrated to be effective in handling high-dimensional data. While promising, most of the existing works use convex methods, which may be suboptimal in terms of the accuracy of feature selection and parameter estimation. In this paper, we expand a nonconvex paradigm to sparse group feature selection, which is motivated by applications that require identifying the underlying group structure and performing feature selection simultaneously. The main contributions of this article are twofold: (1) computationally, we introduce a nonconvex sparse group feature selection model and present an efficient optimization algorithm, of which the key step is a projection with two coupled constraints; (2) statistically, we show that the proposed model can reconstruct the oracle estimator. Therefore, consistent feature selection and parameter estimation can be achieved. Numerical results on synthetic and real-world data suggest that the proposed nonconvex method compares favorably against its competitors, thus achieving desired goal of delivering high performance.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/xiang13.pdf",
        "supp": "",
        "pdf_size": 144557,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3940400420593208030&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Computer Science and Engineering, Arizona State University, Tempe, AZ 85287+Center for Evolutionary Medicine and Informatics, Arizona State University, Tempe, AZ 85287; School of Statistics, University of Minnesota, Minneapolis, MN 55347; Computer Science and Engineering, Arizona State University, Tempe, AZ 85287+Center for Evolutionary Medicine and Informatics, Arizona State University, Tempe, AZ 85287",
        "aff_domain": "asu.edu;umn.edu;asu.edu",
        "email": "asu.edu;umn.edu;asu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+0;1;0+0",
        "aff_unique_norm": "Arizona State University;University of Minnesota",
        "aff_unique_dep": "Computer Science and Engineering;School of Statistics",
        "aff_unique_url": "https://www.asu.edu;https://www.stat.umn.edu",
        "aff_unique_abbr": "ASU;UMN",
        "aff_campus_unique_index": "0+0;1;0+0",
        "aff_campus_unique": "Tempe;Minneapolis",
        "aff_country_unique_index": "0+0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3542d9872a",
        "title": "Ellipsoidal Multiple Instance Learning",
        "site": "https://proceedings.mlr.press/v28/krummenacher13.html",
        "author": "Gabriel Krummenacher; Cheng Soon Ong; Joachim Buhmann",
        "abstract": "We propose a large margin method for asymmetric learning with ellipsoids, called eMIL, suited to multiple instance learning (MIL). We derive the distance between ellipsoids and the hyperplane, generalising the standard support vector machine. Negative bags in MIL contain only negative instances, and we treat them akin to uncertain observations in the robust optimisation framework. However, our method allows positive bags to cross the margin, since it is not known which instances within are positive.  We show that representing bags as ellipsoids under the introduced distance is the most robust solution when treating a bag as a random variable with finite mean and covariance. Two algorithms are derived to solve the resulting non-convex optimization problem: a concave-convex procedure and a quasi-Newton method. Our method achieves competitive results on benchmark datasets. We introduce a MIL dataset from a real world application of detecting wheel defects from multiple partial observations, and show that eMIL outperforms competing approaches.",
        "bibtex": "@InProceedings{pmlr-v28-krummenacher13,\n  title = \t {Ellipsoidal Multiple Instance Learning},\n  author = \t {Krummenacher, Gabriel and Soon Ong, Cheng and Buhmann, Joachim},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {73--81},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/krummenacher13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/krummenacher13.html},\n  abstract = \t {We propose a large margin method for asymmetric learning with ellipsoids, called eMIL, suited to multiple instance learning (MIL). We derive the distance between ellipsoids and the hyperplane, generalising the standard support vector machine. Negative bags in MIL contain only negative instances, and we treat them akin to uncertain observations in the robust optimisation framework. However, our method allows positive bags to cross the margin, since it is not known which instances within are positive.  We show that representing bags as ellipsoids under the introduced distance is the most robust solution when treating a bag as a random variable with finite mean and covariance. Two algorithms are derived to solve the resulting non-convex optimization problem: a concave-convex procedure and a quasi-Newton method. Our method achieves competitive results on benchmark datasets. We introduce a MIL dataset from a real world application of detecting wheel defects from multiple partial observations, and show that eMIL outperforms competing approaches.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/krummenacher13.pdf",
        "supp": "",
        "pdf_size": 544848,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13315918977323679608&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "5c8b80f84a",
        "title": "Enhanced statistical rankings  via  targeted data collection",
        "site": "https://proceedings.mlr.press/v28/osting13.html",
        "author": "Braxton Osting; Christoph Brune; Stanley Osher",
        "abstract": "Given a graph where vertices represent alternatives and pairwise comparison data, y_ij, is given on the edges, the statistical ranking problem is to find a potential function, defined on the vertices, such that the gradient of the potential function agrees with pairwise comparisons. We study the dependence of the statistical ranking problem on the available pairwise data, i.e., pairs (i,j) for which the pairwise comparison data y_ij is known, and propose a framework to identify data which, when augmented with the current dataset, maximally increases the Fisher information of the ranking. Under certain assumptions, the data collection problem decouples, reducing to a problem of finding an edge set on the graph (with a fixed number of edges) such that the  second eigenvalue of the graph Laplacian is maximal. This reduction of the data collection problem to a spectral graph-theoretic question is one of the primary contributions of this work. As an application, we study the Yahoo! Movie user rating dataset and demonstrate that the addition of a small number of well-chosen pairwise comparisons can significantly increase the Fisher informativeness of the ranking.",
        "bibtex": "@InProceedings{pmlr-v28-osting13,\n  title = \t {Enhanced statistical rankings  via  targeted data collection},\n  author = \t {Osting, Braxton and Brune, Christoph and Osher, Stanley},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {489--497},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/osting13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/osting13.html},\n  abstract = \t {Given a graph where vertices represent alternatives and pairwise comparison data, y_ij, is given on the edges, the statistical ranking problem is to find a potential function, defined on the vertices, such that the gradient of the potential function agrees with pairwise comparisons. We study the dependence of the statistical ranking problem on the available pairwise data, i.e., pairs (i,j) for which the pairwise comparison data y_ij is known, and propose a framework to identify data which, when augmented with the current dataset, maximally increases the Fisher information of the ranking. Under certain assumptions, the data collection problem decouples, reducing to a problem of finding an edge set on the graph (with a fixed number of edges) such that the  second eigenvalue of the graph Laplacian is maximal. This reduction of the data collection problem to a spectral graph-theoretic question is one of the primary contributions of this work. As an application, we study the Yahoo! Movie user rating dataset and demonstrate that the addition of a small number of well-chosen pairwise comparisons can significantly increase the Fisher informativeness of the ranking. }\n}",
        "pdf": "http://proceedings.mlr.press/v28/osting13.pdf",
        "supp": "",
        "pdf_size": 2570845,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17967865883896348209&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Mathematics, University of California, Los Angeles; Department of Mathematics, University of California, Los Angeles; Department of Mathematics, University of California, Los Angeles",
        "aff_domain": "math.ucla.edu;math.ucla.edu;math.ucla.edu",
        "email": "math.ucla.edu;math.ucla.edu;math.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Los Angeles",
        "aff_unique_dep": "Department of Mathematics",
        "aff_unique_url": "https://www.ucla.edu",
        "aff_unique_abbr": "UCLA",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0d0cbf63ae",
        "title": "Entropic Affinities: Properties and Efficient Numerical Computation",
        "site": "https://proceedings.mlr.press/v28/vladymyrov13.html",
        "author": "Max Vladymyrov; Miguel Carreira-Perpinan",
        "abstract": "Gaussian affinities are commonly used in graph-based methods such as spectral clustering or nonlinear embedding. Hinton and Roweis (2003) introduced a way to set the scale individually for each point so that it has a distribution over neighbors with a desired perplexity, or effective number of neighbors. This gives very good affinities that adapt locally to the data but are harder to compute. We study the mathematical properties of these \u201centropic affinities\u201d and show that they implicitly define a continuously differentiable function in the input space and give bounds for it. We then devise a fast algorithm to compute the widths and affinities, based on robustified, quickly convergent root-finding methods combined with a tree- or density-based initialization scheme that exploits the slowly-varying behavior of this function. This algorithm is nearly optimal and much more accurate and fast than the existing bisection-based approach, particularly with large datasets, as we show with image and text data.",
        "bibtex": "@InProceedings{pmlr-v28-vladymyrov13,\n  title = \t {Entropic Affinities: Properties and Efficient Numerical Computation},\n  author = \t {Vladymyrov, Max and Carreira-Perpinan, Miguel},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {477--485},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/vladymyrov13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/vladymyrov13.html},\n  abstract = \t {Gaussian affinities are commonly used in graph-based methods such as spectral clustering or nonlinear embedding. Hinton and Roweis (2003) introduced a way to set the scale individually for each point so that it has a distribution over neighbors with a desired perplexity, or effective number of neighbors. This gives very good affinities that adapt locally to the data but are harder to compute. We study the mathematical properties of these \u201centropic affinities\u201d and show that they implicitly define a continuously differentiable function in the input space and give bounds for it. We then devise a fast algorithm to compute the widths and affinities, based on robustified, quickly convergent root-finding methods combined with a tree- or density-based initialization scheme that exploits the slowly-varying behavior of this function. This algorithm is nearly optimal and much more accurate and fast than the existing bisection-based approach, particularly with large datasets, as we show with image and text data.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/vladymyrov13.pdf",
        "supp": "",
        "pdf_size": 2803327,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4102249239230451575&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Electrical Engineering and Computer Science, School of Engineering, University of California, Merced; Electrical Engineering and Computer Science, School of Engineering, University of California, Merced",
        "aff_domain": "ucmerced.edu;ucmerced.edu",
        "email": "ucmerced.edu;ucmerced.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Merced",
        "aff_unique_dep": "Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.ucmerced.edu",
        "aff_unique_abbr": "UC Merced",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Merced",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "dbd9e08e2a",
        "title": "Estimating Unknown Sparsity in Compressed Sensing",
        "site": "https://proceedings.mlr.press/v28/lopes13.html",
        "author": "Miles Lopes",
        "abstract": "In the theory of compressed sensing (CS), the sparsity \\|x\\|_0 of the unknown signal x\u2208\\R^p is commonly assumed to be a known parameter. However, it is typically unknown in practice. Due to the fact that many aspects of  CS depend on knowing \\|x\\|_0, it is important to estimate this parameter in a data-driven way. A second practical concern is that \\|x\\|_0 is a highly unstable function of x. In particular, for real signals with entries not exactly equal to 0, the value \\|x\\|_0=p is not a useful description of the effective number of coordinates. In this paper, we propose to estimate a stable measure of sparsity s(x):=\\|x\\|_1^2/\\|x\\|_2^2, which is a sharp lower bound on \\|x\\|_0. Our estimation procedure uses only a small number of linear measurements, does not rely on any sparsity assumptions, and requires very little computation. A confidence interval for s(x) is  provided, and its width is shown to have no dependence on the signal dimension p. Moreover, this result extends naturally to the matrix recovery setting, where a soft version of matrix rank can be estimated with analogous guarantees. Finally, we show that the use of randomized measurements is essential to estimating s(x). This is accomplished by proving that the minimax risk for estimating s(x) with deterministic measurements is large when n\u226ap.",
        "bibtex": "@InProceedings{pmlr-v28-lopes13,\n  title = \t {Estimating Unknown Sparsity in Compressed Sensing},\n  author = \t {Lopes, Miles},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {217--225},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/lopes13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/lopes13.html},\n  abstract = \t {In the theory of compressed sensing (CS), the sparsity \\|x\\|_0 of the unknown signal x\u2208\\R^p is commonly assumed to be a known parameter. However, it is typically unknown in practice. Due to the fact that many aspects of  CS depend on knowing \\|x\\|_0, it is important to estimate this parameter in a data-driven way. A second practical concern is that \\|x\\|_0 is a highly unstable function of x. In particular, for real signals with entries not exactly equal to 0, the value \\|x\\|_0=p is not a useful description of the effective number of coordinates. In this paper, we propose to estimate a stable measure of sparsity s(x):=\\|x\\|_1^2/\\|x\\|_2^2, which is a sharp lower bound on \\|x\\|_0. Our estimation procedure uses only a small number of linear measurements, does not rely on any sparsity assumptions, and requires very little computation. A confidence interval for s(x) is  provided, and its width is shown to have no dependence on the signal dimension p. Moreover, this result extends naturally to the matrix recovery setting, where a soft version of matrix rank can be estimated with analogous guarantees. Finally, we show that the use of randomized measurements is essential to estimating s(x). This is accomplished by proving that the minimax risk for estimating s(x) with deterministic measurements is large when n\u226ap.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/lopes13.pdf",
        "supp": "",
        "pdf_size": 1689615,
        "gs_citation": 144,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10627853842011180171&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "UC Berkeley, Dept. Statistics",
        "aff_domain": "stat.berkeley.edu",
        "email": "stat.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "88480685b7",
        "title": "Estimation of Causal Peer Influence Effects",
        "site": "https://proceedings.mlr.press/v28/toulis13.html",
        "author": "Panos Toulis; Edward Kao",
        "abstract": "The broad adoption of social media has generated interest in leveraging peer influence for inducing desired user behavior. Quantifying the causal effect of peer influence presents technical challenges, however, including how to deal with social interference, complex response functions and network uncertainty. In this paper, we extend potential outcomes to allow for interference, we introduce well-defined causal estimands of peer-influence, and we develop two estimation procedures: a frequentist procedure relying on a sequential randomization design that requires knowledge of the network but operates under complicated response functions, and a Bayesian procedure which accounts for network uncertainty but relies on a linear response assumption to increase estimation precision. Our results show the advantages and disadvantages of the proposed methods in a number of situations.",
        "bibtex": "@InProceedings{pmlr-v28-toulis13,\n  title = \t {Estimation of Causal Peer Influence Effects},\n  author = \t {Toulis, Panos and Kao, Edward},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1489--1497},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/toulis13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/toulis13.html},\n  abstract = \t {The broad adoption of social media has generated interest in leveraging peer influence for inducing desired user behavior. Quantifying the causal effect of peer influence presents technical challenges, however, including how to deal with social interference, complex response functions and network uncertainty. In this paper, we extend potential outcomes to allow for interference, we introduce well-defined causal estimands of peer-influence, and we develop two estimation procedures: a frequentist procedure relying on a sequential randomization design that requires knowledge of the network but operates under complicated response functions, and a Bayesian procedure which accounts for network uncertainty but relies on a linear response assumption to increase estimation precision. Our results show the advantages and disadvantages of the proposed methods in a number of situations.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/toulis13.pdf",
        "supp": "",
        "pdf_size": 491074,
        "gs_citation": 220,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11399468027977099706&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Statistics, Harvard University; Department of Statistics, Harvard University",
        "aff_domain": "fas.harvard.edu;fas.harvard.edu",
        "email": "fas.harvard.edu;fas.harvard.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Harvard University",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.harvard.edu",
        "aff_unique_abbr": "Harvard",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "93552642f7",
        "title": "Exact Rule Learning via Boolean Compressed Sensing",
        "site": "https://proceedings.mlr.press/v28/malioutov13.html",
        "author": "Dmitry Malioutov; Kush Varshney",
        "abstract": "We propose an interpretable rule-based classification system based on ideas from Boolean compressed sensing. We represent the problem of learning individual conjunctive clauses or individual disjunctive clauses as a Boolean group testing problem, and apply a novel linear programming relaxation to find solutions. We derive results for exact rule recovery which parallel the conditions for exact recovery of sparse signals in the compressed sensing literature: although the general rule recovery problem is NP-hard, under some conditions on the Boolean \u2018sensing\u2019 matrix, the rule can be recovered exactly. This is an exciting development in rule learning where most prior work focused on heuristic solutions. Furthermore we construct rule sets from these learned clauses using set covering and boosting.  We show competitive classification accuracy using the proposed approach.",
        "bibtex": "@InProceedings{pmlr-v28-malioutov13,\n  title = \t {Exact Rule Learning via Boolean Compressed Sensing},\n  author = \t {Malioutov, Dmitry and Varshney, Kush},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {765--773},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/malioutov13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/malioutov13.html},\n  abstract = \t {We propose an interpretable rule-based classification system based on ideas from Boolean compressed sensing. We represent the problem of learning individual conjunctive clauses or individual disjunctive clauses as a Boolean group testing problem, and apply a novel linear programming relaxation to find solutions. We derive results for exact rule recovery which parallel the conditions for exact recovery of sparse signals in the compressed sensing literature: although the general rule recovery problem is NP-hard, under some conditions on the Boolean \u2018sensing\u2019 matrix, the rule can be recovered exactly. This is an exciting development in rule learning where most prior work focused on heuristic solutions. Furthermore we construct rule sets from these learned clauses using set covering and boosting.  We show competitive classification accuracy using the proposed approach.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/malioutov13.pdf",
        "supp": "",
        "pdf_size": 168582,
        "gs_citation": 106,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10465372824666734565&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Business Analytics and Mathematical Sciences, IBM Thomas J. Watson Research Center; Business Analytics and Mathematical Sciences, IBM Thomas J. Watson Research Center",
        "aff_domain": "us.ibm.com;us.ibm.com",
        "email": "us.ibm.com;us.ibm.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "IBM",
        "aff_unique_dep": "Business Analytics and Mathematical Sciences",
        "aff_unique_url": "https://www.ibm.com/research/watson",
        "aff_unique_abbr": "IBM Watson",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9bf03cd511",
        "title": "Expensive Function Optimization with Stochastic Binary Outcomes",
        "site": "https://proceedings.mlr.press/v28/tesch13.html",
        "author": "Matthew Tesch; Jeff Schneider; Howie Choset",
        "abstract": "Real world systems often have parameterized controllers which can be tuned to improve performance. Bayesian optimization methods provide for efficient optimization of these controllers, so as to reduce the number of required experiments on the expensive physical system. In this paper we address Bayesian optimization in the setting where performance is only observed through a stochastic binary outcome \u2013 success or failure of the experiment. Unlike bandit problems, the goal is to maximize the system performance after this offline training phase rather than minimize regret during training.  In this work we define the stochastic binary optimization problem and propose an approach using an adaptation of Gaussian Processes for classification that presents a Bayesian optimization framework for this problem.  We propose an experiment selection metric for this setting based on expected improvement.  We demonstrate the algorithm\u2019s performance on synthetic problems and on a real snake robot learning to move over an obstacle.",
        "bibtex": "@InProceedings{pmlr-v28-tesch13,\n  title = \t {Expensive Function Optimization with Stochastic Binary Outcomes},\n  author = \t {Tesch, Matthew and Schneider, Jeff and Choset, Howie},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1283--1291},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/tesch13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/tesch13.html},\n  abstract = \t {Real world systems often have parameterized controllers which can be tuned to improve performance. Bayesian optimization methods provide for efficient optimization of these controllers, so as to reduce the number of required experiments on the expensive physical system. In this paper we address Bayesian optimization in the setting where performance is only observed through a stochastic binary outcome \u2013 success or failure of the experiment. Unlike bandit problems, the goal is to maximize the system performance after this offline training phase rather than minimize regret during training.  In this work we define the stochastic binary optimization problem and propose an approach using an adaptation of Gaussian Processes for classification that presents a Bayesian optimization framework for this problem.  We propose an experiment selection metric for this setting based on expected improvement.  We demonstrate the algorithm\u2019s performance on synthetic problems and on a real snake robot learning to move over an obstacle.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/tesch13.pdf",
        "supp": "",
        "pdf_size": 841369,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10732219926155687895&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "Robotics Institute, Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, PA 15213 USA; Robotics Institute, Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, PA 15213 USA; Robotics Institute, Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, PA 15213 USA",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Robotics Institute",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "57af1aa304",
        "title": "Exploiting Ontology Structures and Unlabeled Data for Learning",
        "site": "https://proceedings.mlr.press/v28/balcan13a.html",
        "author": "Nina Balcan; Avrim Blum; Yishay Mansour",
        "abstract": "We present and analyze a theoretical model designed to understand and  explain the effectiveness of ontologies for learning multiple related  tasks from primarily unlabeled data.  We present both  information-theoretic results as well as efficient algorithms.    We show in this model that an ontology, which specifies the  relationships between multiple outputs, in some cases is sufficient  to completely learn a classification using a large unlabeled data  source.",
        "bibtex": "@InProceedings{pmlr-v28-balcan13a,\n  title = \t {Exploiting Ontology Structures and Unlabeled Data for Learning},\n  author = \t {Balcan, Nina and Blum, Avrim and Mansour, Yishay},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1112--1120},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/balcan13a.pdf},\n  url = \t {https://proceedings.mlr.press/v28/balcan13a.html},\n  abstract = \t {We present and analyze a theoretical model designed to understand and  explain the effectiveness of ontologies for learning multiple related  tasks from primarily unlabeled data.  We present both  information-theoretic results as well as efficient algorithms.    We show in this model that an ontology, which specifies the  relationships between multiple outputs, in some cases is sufficient  to completely learn a classification using a large unlabeled data  source.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/balcan13a.pdf",
        "supp": "",
        "pdf_size": 368103,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15422049406201688904&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff": "School of Computer Science, Georgia Institute of Technology, Atlanta, GA; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA; Blavatnik School of Computer Science, Tel Aviv University, Tel Aviv, Israel",
        "aff_domain": "cc.gatech.edu;cs.cmu.edu;tau.ac.il",
        "email": "cc.gatech.edu;cs.cmu.edu;tau.ac.il",
        "github": "",
        "project": "rtw.ml.cmu.edu",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Georgia Institute of Technology;Carnegie Mellon University;Tel Aviv University",
        "aff_unique_dep": "School of Computer Science;School of Computer Science;Blavatnik School of Computer Science",
        "aff_unique_url": "https://www.gatech.edu;https://www.cmu.edu;https://www.tau.ac.il",
        "aff_unique_abbr": "Georgia Tech;CMU;TAU",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "Atlanta;Pittsburgh;Tel Aviv",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United States;Israel"
    },
    {
        "id": "4999fcbc84",
        "title": "Exploring the Mind: Integrating Questionnaires and fMRI",
        "site": "https://proceedings.mlr.press/v28/salazar13.html",
        "author": "Esther Salazar; Ryan Bogdan; Adam Gorka; Ahmad Hariri; Lawrence Carin",
        "abstract": "A new model is developed for joint analysis of ordered, categorical, real and count data. The ordered and categorical data are answers to questionnaires, the (word) count data correspond to the text questions from the questionnaires, and the real data correspond to fMRI responses for each subject. The Bayesian model employs the von Mises distribution in a novel manner to infer sparse graphical models jointly across people, questions, fMRI stimuli and brain region, with this integrated within a new matrix factorization based on latent binary features. The model is compared with simpler alternatives on two real datasets. We also demonstrate the ability to predict the response of the brain to visual stimuli (as measured by fMRI), based on knowledge of how the associated person answered classical questionnaires.",
        "bibtex": "@InProceedings{pmlr-v28-salazar13,\n  title = \t {Exploring the Mind: Integrating Questionnaires and fMRI},\n  author = \t {Salazar, Esther and Bogdan, Ryan and Gorka, Adam and Hariri, Ahmad and Carin, Lawrence},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {262--270},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/salazar13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/salazar13.html},\n  abstract = \t {A new model is developed for joint analysis of ordered, categorical, real and count data. The ordered and categorical data are answers to questionnaires, the (word) count data correspond to the text questions from the questionnaires, and the real data correspond to fMRI responses for each subject. The Bayesian model employs the von Mises distribution in a novel manner to infer sparse graphical models jointly across people, questions, fMRI stimuli and brain region, with this integrated within a new matrix factorization based on latent binary features. The model is compared with simpler alternatives on two real datasets. We also demonstrate the ability to predict the response of the brain to visual stimuli (as measured by fMRI), based on knowledge of how the associated person answered classical questionnaires.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/salazar13.pdf",
        "supp": "",
        "pdf_size": 1564735,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=522395725001238177&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Electrical & Computer Engineering, Duke University, Durham, NC, USA; Department of Psychology, Washington University, St. Louis, MO, USA; Department of Psychology & Neuroscience, Duke University, Durham, NC, USA + Institute for Genome Sciences & Policy, Duke University, Durham, NC, USA; Department of Psychology & Neuroscience, Duke University, Durham, NC, USA + Institute for Genome Sciences & Policy, Duke University, Durham, NC, USA; Department of Electrical & Computer Engineering, Duke University, Durham, NC, USA",
        "aff_domain": "duke.edu;artsci.wustl.edu;duke.edu;duke.edu;duke.edu",
        "email": "duke.edu;artsci.wustl.edu;duke.edu;duke.edu;duke.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0+0;0+0;0",
        "aff_unique_norm": "Duke University;Washington University",
        "aff_unique_dep": "Department of Electrical & Computer Engineering;Department of Psychology",
        "aff_unique_url": "https://www.duke.edu;https://wustl.edu",
        "aff_unique_abbr": "Duke;WashU",
        "aff_campus_unique_index": "0;1;0+0;0+0;0",
        "aff_campus_unique": "Durham;St. Louis",
        "aff_country_unique_index": "0;0;0+0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8bdb5a636a",
        "title": "Factorial Multi-Task Learning : A Bayesian Nonparametric Approach",
        "site": "https://proceedings.mlr.press/v28/gupta13a.html",
        "author": "Sunil Gupta; Dinh Phung; Svetha Venkatesh",
        "abstract": "Multi-task learning is a paradigm shown to improve the performance of related tasks through their joint learning. However, for real-world data, it is usually difficult to assess the task relatedness and joint learning with unrelated tasks may lead to serious performance degradations. To this end, we propose a framework that groups the tasks based on their relatedness in a low dimensional subspace and allows a varying degree of relatedness among tasks by sharing the subspace bases across the groups. This provides the flexibility of no sharing when two sets of tasks are unrelated and partial/total sharing when the tasks are related. Importantly, the number of task-groups and the subspace dimensionality are automatically inferred from the data. This feature keeps the model beyond a specific set of parameters. To realize our framework, we present a novel Bayesian nonparametric prior that extends the traditional hierarchical beta process prior using a Dirichlet process to permit potentially infinite number of child beta processes. We apply our model for multi-task regression and classification applications. Experimental results using several synthetic and real-world datasets show the superiority of our model to other recent state-of-the-art multi-task learning methods.",
        "bibtex": "@InProceedings{pmlr-v28-gupta13a,\n  title = \t {Factorial Multi-Task Learning : A Bayesian Nonparametric Approach},\n  author = \t {Gupta, Sunil and Phung, Dinh and Venkatesh, Svetha},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {657--665},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/gupta13a.pdf},\n  url = \t {https://proceedings.mlr.press/v28/gupta13a.html},\n  abstract = \t {Multi-task learning is a paradigm shown to improve the performance of related tasks through their joint learning. However, for real-world data, it is usually difficult to assess the task relatedness and joint learning with unrelated tasks may lead to serious performance degradations. To this end, we propose a framework that groups the tasks based on their relatedness in a low dimensional subspace and allows a varying degree of relatedness among tasks by sharing the subspace bases across the groups. This provides the flexibility of no sharing when two sets of tasks are unrelated and partial/total sharing when the tasks are related. Importantly, the number of task-groups and the subspace dimensionality are automatically inferred from the data. This feature keeps the model beyond a specific set of parameters. To realize our framework, we present a novel Bayesian nonparametric prior that extends the traditional hierarchical beta process prior using a Dirichlet process to permit potentially infinite number of child beta processes. We apply our model for multi-task regression and classification applications. Experimental results using several synthetic and real-world datasets show the superiority of our model to other recent state-of-the-art multi-task learning methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/gupta13a.pdf",
        "supp": "",
        "pdf_size": 703624,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8688692369280403221&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Center for Pattern Recognition and Data Analytics (PRaDA), Deakin University, VIC 3216, Australia; Center for Pattern Recognition and Data Analytics (PRaDA), Deakin University, VIC 3216, Australia; Center for Pattern Recognition and Data Analytics (PRaDA), Deakin University, VIC 3216, Australia",
        "aff_domain": "deakin.edu.au;deakin.edu.au;deakin.edu.au",
        "email": "deakin.edu.au;deakin.edu.au;deakin.edu.au",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Deakin University",
        "aff_unique_dep": "Center for Pattern Recognition and Data Analytics (PRaDA)",
        "aff_unique_url": "https://www.deakin.edu.au",
        "aff_unique_abbr": "Deakin",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "VIC",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "710720bd49",
        "title": "Fast Conical Hull Algorithms for Near-separable Non-negative Matrix Factorization",
        "site": "https://proceedings.mlr.press/v28/kumar13b.html",
        "author": "Abhishek Kumar; Vikas Sindhwani; Prabhanjan Kambadur",
        "abstract": "The separability assumption (Arora et al., 2012; Donoho & Stodden, 2003) turns non-negative matrix factorization (NMF) into a tractable problem. Recently, a new class of provably-correct NMF algorithms have emerged under this assumption. In this paper, we reformulate the separable NMF problem as that of finding the extreme rays of the conical hull of a finite set of vectors. From this geometric perspective, we derive new separable NMF algorithms that are highly scalable and empirically noise robust, and have several favorable properties in relation to existing methods. A parallel implementation of our algorithm scales excellently on shared and distributed-memory machines.",
        "bibtex": "@InProceedings{pmlr-v28-kumar13b,\n  title = \t {Fast Conical Hull Algorithms for Near-separable Non-negative Matrix Factorization},\n  author = \t {Kumar, Abhishek and Sindhwani, Vikas and Kambadur, Prabhanjan},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {231--239},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/kumar13b.pdf},\n  url = \t {https://proceedings.mlr.press/v28/kumar13b.html},\n  abstract = \t {The separability assumption (Arora et al., 2012; Donoho & Stodden, 2003) turns non-negative matrix factorization (NMF) into a tractable problem. Recently, a new class of provably-correct NMF algorithms have emerged under this assumption. In this paper, we reformulate the separable NMF problem as that of finding the extreme rays of the conical hull of a finite set of vectors. From this geometric perspective, we derive new separable NMF algorithms that are highly scalable and empirically noise robust, and have several favorable properties in relation to existing methods. A parallel implementation of our algorithm scales excellently on shared and distributed-memory machines.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/kumar13b.pdf",
        "supp": "",
        "pdf_size": 294484,
        "gs_citation": 186,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=478859217831322144&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Dept. of Computer Science, University of Maryland, College Park, MD 20742, USA; IBM T.J. Watson Research Center, Yorktown Heights, NY 10598 USA; IBM T.J. Watson Research Center, Yorktown Heights, NY 10598 USA",
        "aff_domain": "cs.umd.edu;us.ibm.com;us.ibm.com",
        "email": "cs.umd.edu;us.ibm.com;us.ibm.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Maryland, College Park;IBM",
        "aff_unique_dep": "Department of Computer Science;IBM T.J. Watson Research Center",
        "aff_unique_url": "https://www/umd.edu;https://www.ibm.com/research/watson",
        "aff_unique_abbr": "UMD;IBM Watson",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "College Park;Yorktown Heights",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7c169248a4",
        "title": "Fast Dual Variational Inference for Non-Conjugate Latent Gaussian Models",
        "site": "https://proceedings.mlr.press/v28/emtiyazkhan13.html",
        "author": "Mohammad Emtiyaz Khan; Aleksandr Aravkin; Michael Friedlander; Matthias Seeger",
        "abstract": "Latent Gaussian models (LGMs) are widely used in statistics and machine learning.  Bayesian inference in non-conjugate LGM is difficult due to intractable integrals involving the Gaussian prior and non-conjugate likelihoods.  Algorithms based on Variational Gaussian (VG) approximations are widely employed since they strike a favorable balance between accuracy, generality, speed, and ease of use.  However, the structure of optimization problems associated with them    remains poorly understood, and standard solvers take too long to converge.  In this paper, we derive a novel dual variational inference approach, which exploits the convexity property of the VG approximations.   The implications of our approach is that we obtain an algorithm that solves a convex optimization problem, reduces the number of variational parameters, and converges much faster than previous methods.  Using real world data, we demonstrate these advantages on a variety of LGMs including Gaussian process classification and latent Gaussian Markov random fields.",
        "bibtex": "@InProceedings{pmlr-v28-emtiyazkhan13,\n  title = \t {Fast Dual Variational Inference for Non-Conjugate Latent Gaussian Models},\n  author = \t {Emtiyaz Khan, Mohammad and Aravkin, Aleksandr and Friedlander, Michael and Seeger, Matthias},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {951--959},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/emtiyazkhan13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/emtiyazkhan13.html},\n  abstract = \t {Latent Gaussian models (LGMs) are widely used in statistics and machine learning.  Bayesian inference in non-conjugate LGM is difficult due to intractable integrals involving the Gaussian prior and non-conjugate likelihoods.  Algorithms based on Variational Gaussian (VG) approximations are widely employed since they strike a favorable balance between accuracy, generality, speed, and ease of use.  However, the structure of optimization problems associated with them    remains poorly understood, and standard solvers take too long to converge.  In this paper, we derive a novel dual variational inference approach, which exploits the convexity property of the VG approximations.   The implications of our approach is that we obtain an algorithm that solves a convex optimization problem, reduces the number of variational parameters, and converges much faster than previous methods.  Using real world data, we demonstrate these advantages on a variety of LGMs including Gaussian process classification and latent Gaussian Markov random fields.    }\n}",
        "pdf": "http://proceedings.mlr.press/v28/emtiyazkhan13.pdf",
        "supp": "",
        "pdf_size": 456040,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6210289254801724838&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "School of Computer and Communication Sciences, Ecole Polytechnique F\u00b4ed\u00b4erale de Lausanne, Switzerland; Numerical Analysis and Optimization, IBM T.J. Watson Research Center, Yorktown Heights, NY, USA; Department of Computer Science, University of British Columbia, Vancouver, Canada; School of Computer and Communication Sciences, Ecole Polytechnique F\u00b4ed\u00b4erale de Lausanne, Switzerland",
        "aff_domain": "epfl.ch;us.ibm.com;cs.ubc.ca;epfl.ch",
        "email": "epfl.ch;us.ibm.com;cs.ubc.ca;epfl.ch",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "EPFL;IBM;University of British Columbia",
        "aff_unique_dep": "School of Computer and Communication Sciences;Numerical Analysis and Optimization;Department of Computer Science",
        "aff_unique_url": "https://www.epfl.ch;https://www.ibm.com/research/watson;https://www.ubc.ca",
        "aff_unique_abbr": "EPFL;IBM Watson;UBC",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Yorktown Heights;Vancouver",
        "aff_country_unique_index": "0;1;2;0",
        "aff_country_unique": "Switzerland;United States;Canada"
    },
    {
        "id": "f4bd6ae0ef",
        "title": "Fast Image Tagging",
        "site": "https://proceedings.mlr.press/v28/chen13j.html",
        "author": "Minmin Chen; Alice Zheng; Kilian Weinberger",
        "abstract": "Automatic image annotation is a difficult and highly relevant machine learning task. Recent advances have significantly improved the state-of-the-art in retrieval accuracy with algorithms based on nearest neighbor classification in carefully learned metric spaces. But this comes at a price of increased computational complexity during training and testing. We propose FastTag, a novel algorithm that achieves comparable results with two simple linear mappings that are co-regularized in a joint convex loss function. The loss function can be efficiently optimized in closed form updates, which allows us to incorporate a large number of image descriptors cheaply. On several standard real-world benchmark data sets, we demonstrate that FastTag matches the current state-of-the-art in tagging quality, yet reduces the training and testing times by several orders of magnitude and has lower asymptotic complexity.",
        "bibtex": "@InProceedings{pmlr-v28-chen13j,\n  title = \t {Fast Image Tagging},\n  author = \t {Chen, Minmin and Zheng, Alice and Weinberger, Kilian},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1274--1282},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/chen13j.pdf},\n  url = \t {https://proceedings.mlr.press/v28/chen13j.html},\n  abstract = \t {Automatic image annotation is a difficult and highly relevant machine learning task. Recent advances have significantly improved the state-of-the-art in retrieval accuracy with algorithms based on nearest neighbor classification in carefully learned metric spaces. But this comes at a price of increased computational complexity during training and testing. We propose FastTag, a novel algorithm that achieves comparable results with two simple linear mappings that are co-regularized in a joint convex loss function. The loss function can be efficiently optimized in closed form updates, which allows us to incorporate a large number of image descriptors cheaply. On several standard real-world benchmark data sets, we demonstrate that FastTag matches the current state-of-the-art in tagging quality, yet reduces the training and testing times by several orders of magnitude and has lower asymptotic complexity.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/chen13j.pdf",
        "supp": "",
        "pdf_size": 4425576,
        "gs_citation": 289,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13083149037450044125&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Amazon.com, Seattle, WA 98109; Microsoft Research, Redmond, WA 98052; Washington University in St. Louis, St. Louis, MO 63130",
        "aff_domain": "amazon.com;microsoft.com;wustl.edu",
        "email": "amazon.com;microsoft.com;wustl.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Amazon;Microsoft;Washington University in St. Louis",
        "aff_unique_dep": "Amazon.com;Microsoft Research;",
        "aff_unique_url": "https://www.amazon.com;https://www.microsoft.com/en-us/research;https://wustl.edu",
        "aff_unique_abbr": "Amazon;MSR;WUSTL",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "Seattle;Redmond;St. Louis",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c1706b2ef8",
        "title": "Fast Max-Margin Matrix Factorization with Data Augmentation",
        "site": "https://proceedings.mlr.press/v28/xu13a.html",
        "author": "Minjie Xu; Jun Zhu; Bo Zhang",
        "abstract": "Existing max-margin matrix factorization (M3F) methods either are computationally inefficient or need a model selection procedure to determine the number of latent factors. In this paper we present a probabilistic M3F model that admits a highly efficient Gibbs sampling algorithm through data augmentation. We further extend our approach to incorporate Bayesian nonparametrics and build accordingly a truncation-free nonparametric M3F model where the number of latent factors is literally unbounded and inferred from data. Empirical studies on two large real-world data sets verify the efficacy of our proposed methods.",
        "bibtex": "@InProceedings{pmlr-v28-xu13a,\n  title = \t {Fast Max-Margin Matrix Factorization with Data Augmentation},\n  author = \t {Xu, Minjie and Zhu, Jun and Zhang, Bo},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {978--986},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/xu13a.pdf},\n  url = \t {https://proceedings.mlr.press/v28/xu13a.html},\n  abstract = \t {Existing max-margin matrix factorization (M3F) methods either are computationally inefficient or need a model selection procedure to determine the number of latent factors. In this paper we present a probabilistic M3F model that admits a highly efficient Gibbs sampling algorithm through data augmentation. We further extend our approach to incorporate Bayesian nonparametrics and build accordingly a truncation-free nonparametric M3F model where the number of latent factors is literally unbounded and inferred from data. Empirical studies on two large real-world data sets verify the efficacy of our proposed methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/xu13a.pdf",
        "supp": "",
        "pdf_size": 561744,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17755838375323784414&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Dept. of Comp. Sci. & Tech., LITS Lab, TNList Lab, Tsinghua University, Beijing 100084, China; Dept. of Comp. Sci. & Tech., LITS Lab, TNList Lab, Tsinghua University, Beijing 100084, China; Dept. of Comp. Sci. & Tech., LITS Lab, TNList Lab, Tsinghua University, Beijing 100084, China",
        "aff_domain": "mails.tsinghua.edu.cn;mail.tsinghua.edu.cn;mail.tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn;mail.tsinghua.edu.cn;mail.tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Tsinghua University",
        "aff_unique_dep": "Dept. of Comp. Sci. & Tech.",
        "aff_unique_url": "https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "THU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "a8cdb6c4f2",
        "title": "Fast Probabilistic Optimization from Noisy Gradients",
        "site": "https://proceedings.mlr.press/v28/hennig13.html",
        "author": "Philipp Hennig",
        "abstract": "Stochastic gradient descent remains popular in large-scale machine learning, on account of its very low computational cost and robustness to noise. However, gradient descent is only linearly efficient and not transformation invariant. Scaling by a local measure can substantially improve its performance. One natural choice of such a scale is the Hessian of the objective function: Were it available, it would turn linearly efficient gradient descent into the quadratically efficient Newton-Raphson optimization. Existing covariant methods, though, are either super-linearly expensive or do not address noise. Generalising recent results, this paper constructs a nonparametric Bayesian quasi-Newton algorithm that learns gradient and Hessian from noisy evaluations of the gradient. Importantly, the resulting algorithm, like stochastic gradient descent, has cost linear in the number of input dimensions.",
        "bibtex": "@InProceedings{pmlr-v28-hennig13,\n  title = \t {Fast Probabilistic Optimization from Noisy Gradients},\n  author = \t {Hennig, Philipp},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {62--70},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/hennig13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/hennig13.html},\n  abstract = \t {Stochastic gradient descent remains popular in large-scale machine learning, on account of its very low computational cost and robustness to noise. However, gradient descent is only linearly efficient and not transformation invariant. Scaling by a local measure can substantially improve its performance. One natural choice of such a scale is the Hessian of the objective function: Were it available, it would turn linearly efficient gradient descent into the quadratically efficient Newton-Raphson optimization. Existing covariant methods, though, are either super-linearly expensive or do not address noise. Generalising recent results, this paper constructs a nonparametric Bayesian quasi-Newton algorithm that learns gradient and Hessian from noisy evaluations of the gradient. Importantly, the resulting algorithm, like stochastic gradient descent, has cost linear in the number of input dimensions.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/hennig13.pdf",
        "supp": "",
        "pdf_size": 798069,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9439361223839551637&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 8,
        "aff": "Max Planck Institute for Intelligent Systems, Dpt. of Empirical Inference, Spemannstr. T\u00fcbingen, Germany",
        "aff_domain": "tuebingen.mpg.de",
        "email": "tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": "Department of Empirical Inference",
        "aff_unique_url": "https://www.mpituebingen.mpg.de",
        "aff_unique_abbr": "MPI-IS",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "T\u00fcbingen",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "e93aab3d71",
        "title": "Fast Semidifferential-based Submodular Function Optimization",
        "site": "https://proceedings.mlr.press/v28/iyer13.html",
        "author": "Rishabh Iyer; Stefanie Jegelka; Jeff Bilmes",
        "abstract": "We present a practical and powerful new framework for both unconstrained and constrained submodular function optimization based on discrete semidifferentials (sub- and super-differentials). The resulting algorithms, which repeatedly compute and then efficiently optimize submodular semigradients, offer new and generalize many old methods for submodular optimization.  Our approach, moreover, takes steps towards providing a unifying paradigm applicable to both submodular minimization and maximization, problems that historically have been treated quite distinctly. The practicality of our algorithms is important since interest in submodularity, owing to its natural and wide applicability, has recently been in ascendance within machine learning.  We analyze theoretical properties of our algorithms for minimization and maximization, and show that many state-of-the-art maximization algorithms are special cases. Lastly, we complement our theoretical analyses with supporting empirical experiments.",
        "bibtex": "@InProceedings{pmlr-v28-iyer13,\n  title = \t {Fast Semidifferential-based Submodular Function Optimization},\n  author = \t {Iyer, Rishabh and Jegelka, Stefanie and Bilmes, Jeff},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {855--863},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/iyer13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/iyer13.html},\n  abstract = \t {We present a practical and powerful new framework for both unconstrained and constrained submodular function optimization based on discrete semidifferentials (sub- and super-differentials). The resulting algorithms, which repeatedly compute and then efficiently optimize submodular semigradients, offer new and generalize many old methods for submodular optimization.  Our approach, moreover, takes steps towards providing a unifying paradigm applicable to both submodular minimization and maximization, problems that historically have been treated quite distinctly. The practicality of our algorithms is important since interest in submodularity, owing to its natural and wide applicability, has recently been in ascendance within machine learning.  We analyze theoretical properties of our algorithms for minimization and maximization, and show that many state-of-the-art maximization algorithms are special cases. Lastly, we complement our theoretical analyses with supporting empirical experiments.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/iyer13.pdf",
        "supp": "",
        "pdf_size": 368588,
        "gs_citation": 151,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17075479015508554739&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "University of Washington, Seattle, WA 98195, USA; University of California, Berkeley, CA 94720, USA; University of Washington, Seattle, WA 98195, USA",
        "aff_domain": "u.washington.edu;eecs.berkeley.edu;u.washington.edu",
        "email": "u.washington.edu;eecs.berkeley.edu;u.washington.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Washington;University of California, Berkeley",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.washington.edu;https://www.berkeley.edu",
        "aff_unique_abbr": "UW;UC Berkeley",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Seattle;Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6ec945d1ba",
        "title": "Fast algorithms for sparse principal component analysis based on Rayleigh quotient iteration",
        "site": "https://proceedings.mlr.press/v28/kuleshov13.html",
        "author": "Volodymyr Kuleshov",
        "abstract": "We introduce new algorithms for sparse principal component analysis (sPCA), a variation of PCA which aims to represent data in a sparse low-dimensional basis. Our algorithms possess a cubic rate of convergence and can compute principal components with k non-zero elements at a cost of O(nk + k^3) flops per iteration. We observe in numerical experiments that these components are of equal or greater quality than ones obtained from current state-of-the-art techniques, but require between one and two orders of magnitude fewer flops to be computed. Conceptually, our approach generalizes the Rayleigh quotient iteration algorithm for computing eigenvectors, and can be interpreted as a type of second-order optimization method. We demonstrate the applicability of our algorithms on several datasets, including the STL-10 machine vision dataset and gene expression data.",
        "bibtex": "@InProceedings{pmlr-v28-kuleshov13,\n  title = \t {Fast algorithms for sparse principal component analysis based on Rayleigh quotient iteration},\n  author = \t {Kuleshov, Volodymyr},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1418--1425},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/kuleshov13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/kuleshov13.html},\n  abstract = \t {We introduce new algorithms for sparse principal component analysis (sPCA), a variation of PCA which aims to represent data in a sparse low-dimensional basis. Our algorithms possess a cubic rate of convergence and can compute principal components with k non-zero elements at a cost of O(nk + k^3) flops per iteration. We observe in numerical experiments that these components are of equal or greater quality than ones obtained from current state-of-the-art techniques, but require between one and two orders of magnitude fewer flops to be computed. Conceptually, our approach generalizes the Rayleigh quotient iteration algorithm for computing eigenvectors, and can be interpreted as a type of second-order optimization method. We demonstrate the applicability of our algorithms on several datasets, including the STL-10 machine vision dataset and gene expression data.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/kuleshov13.pdf",
        "supp": "",
        "pdf_size": 326696,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5413568832842170998&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Science, Stanford University, Stanford, CA",
        "aff_domain": "stanford.edu",
        "email": "stanford.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "705a280517",
        "title": "Fast dropout training",
        "site": "https://proceedings.mlr.press/v28/wang13a.html",
        "author": "Sida Wang; Christopher Manning",
        "abstract": "Preventing feature co-adaptation by encouraging independent contributions from different features often improves classification and regression performance.  Dropout training (Hinton et al., 2012) does this by randomly dropping out (zeroing) hidden units and input features during training of neural networks. However, repeatedly sampling a random subset of input features makes training much slower. Based on an examination of the implied objective function of dropout training, we show how to do fast dropout training by sampling from or integrating a Gaussian approximation, instead of doing Monte Carlo optimization of this objective.  This approximation, justified by the central limit theorem and empirical evidence, gives an order of magnitude speedup and more stability.  We show how to do fast dropout training for classification, regression, and multilayer neural networks. Beyond dropout, our technique is extended to integrate out other types of noise and small image transformations.",
        "bibtex": "@InProceedings{pmlr-v28-wang13a,\n  title = \t {Fast dropout training},\n  author = \t {Wang, Sida and Manning, Christopher},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {118--126},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/wang13a.pdf},\n  url = \t {https://proceedings.mlr.press/v28/wang13a.html},\n  abstract = \t {Preventing feature co-adaptation by encouraging independent contributions from different features often improves classification and regression performance.  Dropout training (Hinton et al., 2012) does this by randomly dropping out (zeroing) hidden units and input features during training of neural networks. However, repeatedly sampling a random subset of input features makes training much slower. Based on an examination of the implied objective function of dropout training, we show how to do fast dropout training by sampling from or integrating a Gaussian approximation, instead of doing Monte Carlo optimization of this objective.  This approximation, justified by the central limit theorem and empirical evidence, gives an order of magnitude speedup and more stability.  We show how to do fast dropout training for classification, regression, and multilayer neural networks. Beyond dropout, our technique is extended to integrate out other types of noise and small image transformations. }\n}",
        "pdf": "http://proceedings.mlr.press/v28/wang13a.pdf",
        "supp": "",
        "pdf_size": 453829,
        "gs_citation": 625,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3909348209549170292&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "Department of Computer Science, Stanford University, Stanford, CA 94305; Department of Computer Science, Stanford University, Stanford, CA 94305",
        "aff_domain": "cs.stanford.edu;stanford.edu",
        "email": "cs.stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "dfd0d37c21",
        "title": "Fastfood - Computing Hilbert Space Expansions in loglinear time",
        "site": "https://proceedings.mlr.press/v28/le13.html",
        "author": "Quoc Le; Tamas Sarlos; Alexander Smola",
        "abstract": "Fast nonlinear function classes are crucial for nonparametric estimation, such as in kernel methods. This paper proposes an improvement to random kitchen sinks that offers significantly faster computation in log-linear time without sacrificing accuracy. Furthermore, we show how one may adjust the regularization properties of the kernel simply by changing the spectral distribution of the projection matrix. We provide experimental results which show that even for for moderately small problems we already achieve two orders of magnitude faster computation and three orders of magnitude lower memory footprint.",
        "bibtex": "@InProceedings{pmlr-v28-le13,\n  title = \t {Fastfood - Computing Hilbert Space Expansions in loglinear time},\n  author = \t {Le, Quoc and Sarlos, Tamas and Smola, Alexander},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {244--252},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/le13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/le13.html},\n  abstract = \t {Fast nonlinear function classes are crucial for nonparametric estimation, such as in kernel methods. This paper proposes an improvement to random kitchen sinks that offers significantly faster computation in log-linear time without sacrificing accuracy. Furthermore, we show how one may adjust the regularization properties of the kernel simply by changing the spectral distribution of the projection matrix. We provide experimental results which show that even for for moderately small problems we already achieve two orders of magnitude faster computation and three orders of magnitude lower memory footprint.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/le13.pdf",
        "supp": "",
        "pdf_size": 733872,
        "gs_citation": 202,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15545843110922180296&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Google Knowledge, 1600 Amphitheatre Pkwy, Mountain View 94043, CA USA; Google Knowledge, 1600 Amphitheatre Pkwy, Mountain View 94043, CA USA; ",
        "aff_domain": "google.com;google.com;smola.org",
        "email": "google.com;google.com;smola.org",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Knowledge",
        "aff_unique_url": "https://www.google.com",
        "aff_unique_abbr": "Google",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8ff2c5dd01",
        "title": "Feature Multi-Selection among Subjective Features",
        "site": "https://proceedings.mlr.press/v28/sabato13.html",
        "author": "Sivan Sabato; Adam Kalai",
        "abstract": "When dealing with subjective, noisy, or otherwise nebulous features, the \u201cwisdom of crowds\u201d suggests that one may benefit from multiple judgments of the same feature on the same object. We give theoretically-motivated \"\"feature multi-selection\"\" algorithms that choose, among a large set of candidate features, not only which features to judge but how many times to judge each one. We demonstrate the effectiveness of this approach for linear regression on a crowdsourced learning task of predicting people\u2019s height and weight from photos, using features such as \"\"gender\"\" and  \"\"estimated weight\"\" as well as culturally fraught ones such as \"\"attractive\"\".",
        "bibtex": "@InProceedings{pmlr-v28-sabato13,\n  title = \t {Feature Multi-Selection among Subjective Features},\n  author = \t {Sabato, Sivan and Kalai, Adam},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {810--818},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/sabato13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/sabato13.html},\n  abstract = \t {When dealing with subjective, noisy, or otherwise nebulous features, the \u201cwisdom of crowds\u201d suggests that one may benefit from multiple judgments of the same feature on the same object. We give theoretically-motivated \"\"feature multi-selection\"\" algorithms that choose, among a large set of candidate features, not only which features to judge but how many times to judge each one. We demonstrate the effectiveness of this approach for linear regression on a crowdsourced learning task of predicting people\u2019s height and weight from photos, using features such as \"\"gender\"\" and  \"\"estimated weight\"\" as well as culturally fraught ones such as \"\"attractive\"\".}\n}",
        "pdf": "http://proceedings.mlr.press/v28/sabato13.pdf",
        "supp": "",
        "pdf_size": 559030,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17433275891732714725&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Microsoft Research New England, 1 Memorial Dr., Cambridge, MA; Microsoft Research New England, 1 Memorial Dr., Cambridge, MA",
        "aff_domain": "microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Microsoft Research New England",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/group/new-england",
        "aff_unique_abbr": "MSR NE",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b131f8ed33",
        "title": "Feature Selection in High-Dimensional Classification",
        "site": "https://proceedings.mlr.press/v28/kolar13.html",
        "author": "Mladen Kolar; Han Liu",
        "abstract": "High-dimensional discriminant analysis is of fundamental importance in multivariate statistics. Existing theoretical results sharply characterize different procedures, providing sharp convergence results for the classification risk, as well as the l2 convergence results to the discriminative rule. However, sharp theoretical results for the problem of variable selection have not been established, even though model interpretation is of importance in many scientific domains. In this paper, we  bridge this gap by providing sharp sufficient conditions for consistent variable selection using the ROAD estimator (Fan et al., 2010). Our results provide novel theoretical insights for the ROAD estimator. Sufficient conditions are complemented by the necessary information theoretic limits on variable selection in high-dimensional discriminant analysis. This complementary result also establishes optimality of the ROAD estimator for a certain family of problems.",
        "bibtex": "@InProceedings{pmlr-v28-kolar13,\n  title = \t {Feature Selection in High-Dimensional Classification},\n  author = \t {Kolar, Mladen and Liu, Han},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {329--337},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/kolar13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/kolar13.html},\n  abstract = \t {High-dimensional discriminant analysis is of fundamental importance in multivariate statistics. Existing theoretical results sharply characterize different procedures, providing sharp convergence results for the classification risk, as well as the l2 convergence results to the discriminative rule. However, sharp theoretical results for the problem of variable selection have not been established, even though model interpretation is of importance in many scientific domains. In this paper, we  bridge this gap by providing sharp sufficient conditions for consistent variable selection using the ROAD estimator (Fan et al., 2010). Our results provide novel theoretical insights for the ROAD estimator. Sufficient conditions are complemented by the necessary information theoretic limits on variable selection in high-dimensional discriminant analysis. This complementary result also establishes optimality of the ROAD estimator for a certain family of problems.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/kolar13.pdf",
        "supp": "",
        "pdf_size": 239159,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6863049206307187370&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15217, USA; Department of Operations Research and Financial Engineering, Princeton University, Princeton, NJ 08544, USA",
        "aff_domain": "cs.cmu.edu;princeton.edu",
        "email": "cs.cmu.edu;princeton.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Carnegie Mellon University;Princeton University",
        "aff_unique_dep": "Machine Learning Department;Department of Operations Research and Financial Engineering",
        "aff_unique_url": "https://www.cmu.edu;https://www.princeton.edu",
        "aff_unique_abbr": "CMU;Princeton",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Pittsburgh;Princeton",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ec92d97a0e",
        "title": "Fixed-Point Model For Structured Labeling",
        "site": "https://proceedings.mlr.press/v28/li13b.html",
        "author": "Quannan Li; Jingdong Wang; David Wipf; Zhuowen Tu",
        "abstract": "In this paper, we propose a simple but effective  solution to the structured labeling problem:  a fixed-point model. Recently, layered  models with sequential classifiers/regressors  have gained an increasing amount of interests  for structural prediction. Here, we design an  algorithm with a new perspective on layered  models; we aim to find a fixed-point function  with the structured labels being both the  output and the input. Our approach alleviates  the burden in learning multiple/different  classifiers in different layers. We devise a  training strategy for our method and provide  justifications for the fixed-point function  to be a contraction mapping. The learned  function captures rich contextual information  and is easy to train and test. On several  widely used benchmark datasets, the proposed  method observes significant improvement  in both performance and efficiency over  many state-of-the-art algorithms.",
        "bibtex": "@InProceedings{pmlr-v28-li13b,\n  title = \t {Fixed-Point Model For Structured Labeling},\n  author = \t {Li, Quannan and Wang, Jingdong and Wipf, David and Tu, Zhuowen},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {214--221},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/li13b.pdf},\n  url = \t {https://proceedings.mlr.press/v28/li13b.html},\n  abstract = \t {In this paper, we propose a simple but effective  solution to the structured labeling problem:  a fixed-point model. Recently, layered  models with sequential classifiers/regressors  have gained an increasing amount of interests  for structural prediction. Here, we design an  algorithm with a new perspective on layered  models; we aim to find a fixed-point function  with the structured labels being both the  output and the input. Our approach alleviates  the burden in learning multiple/different  classifiers in different layers. We devise a  training strategy for our method and provide  justifications for the fixed-point function  to be a contraction mapping. The learned  function captures rich contextual information  and is easy to train and test. On several  widely used benchmark datasets, the proposed  method observes significant improvement  in both performance and efficiency over  many state-of-the-art algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/li13b.pdf",
        "supp": "",
        "pdf_size": 167558,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17859853667005046646&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 12,
        "aff": "Lab of Neuro Imaging and Department of Computer Science, UCL A; Microsoft Research Asia; Microsoft Research Asia; Lab of Neuro Imaging and Department of Computer Science, UCL A + Microsoft Research Asia",
        "aff_domain": "gmail.com;microsoft.com;gmail.com;gmail.com",
        "email": "gmail.com;microsoft.com;gmail.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0+1",
        "aff_unique_norm": "University College London;Microsoft",
        "aff_unique_dep": "Department of Computer Science;Research",
        "aff_unique_url": "https://www.ucl.ac.uk;https://www.microsoft.com/en-us/research/group/asia",
        "aff_unique_abbr": "UCL;MSR Asia",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Asia",
        "aff_country_unique_index": "0;1;1;0+1",
        "aff_country_unique": "United Kingdom;China"
    },
    {
        "id": "e4622019ee",
        "title": "Forecastable Component Analysis",
        "site": "https://proceedings.mlr.press/v28/goerg13.html",
        "author": "Georg Goerg",
        "abstract": "I introduce Forecastable Component Analysis (ForeCA), a novel dimension reduction technique for temporally dependent signals. Based on a new forecastability measure, ForeCA finds an optimal transformation to separate a multivariate time series into a forecastable and an orthogonal white noise space. I present a converging algorithm with a fast eigenvector solution. Applications to financial and macro-economic time series show that ForeCA can successfully discover informative structure, which can be used for forecasting as well as classification. The R package ForeCA accompanies this work and is publicly available on CRAN.",
        "bibtex": "@InProceedings{pmlr-v28-goerg13,\n  title = \t {Forecastable Component Analysis},\n  author = \t {Goerg, Georg},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {64--72},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/goerg13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/goerg13.html},\n  abstract = \t {I introduce Forecastable Component Analysis (ForeCA), a novel dimension reduction technique for temporally dependent signals. Based on a new forecastability measure, ForeCA finds an optimal transformation to separate a multivariate time series into a forecastable and an orthogonal white noise space. I present a converging algorithm with a fast eigenvector solution. Applications to financial and macro-economic time series show that ForeCA can successfully discover informative structure, which can be used for forecasting as well as classification. The R package ForeCA accompanies this work and is publicly available on CRAN.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/goerg13.pdf",
        "supp": "",
        "pdf_size": 1260897,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Carnegie Mellon University, Department of Statistics, Pittsburgh, PA 15213",
        "aff_domain": "stat.cmu.edu",
        "email": "stat.cmu.edu",
        "github": "",
        "project": "https://cran.r-project.org/package=ForeCA",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "976b177913",
        "title": "Gated Autoencoders with Tied Input Weights",
        "site": "https://proceedings.mlr.press/v28/alain13.html",
        "author": "Droniou Alain; Sigaud Olivier",
        "abstract": "The semantic interpretation of images is one of the core applications of deep learning. Several techniques have been recently proposed to model the relation between two images, with application to pose estimation, action recognition or invariant object recognition. Among these techniques, higher-order Boltzmann machines or relational autoencoders consider projections of the images on different subspaces and intermediate layers act as transformation specific detectors. In this work, we extend the mathematical study of (Memisevic, 2012b) to show that it is possible to use a unique projection for both images in a way that turns intermediate layers as spectrum encoders of transformations. We show that this results in networks that are easier to tune and have greater generalization capabilities.",
        "bibtex": "@InProceedings{pmlr-v28-alain13,\n  title = \t {Gated Autoencoders with Tied Input Weights},\n  author = \t {Alain, Droniou and Olivier, Sigaud},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {154--162},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/alain13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/alain13.html},\n  abstract = \t {The semantic interpretation of images is one of the core applications of deep learning. Several techniques have been recently proposed to model the relation between two images, with application to pose estimation, action recognition or invariant object recognition. Among these techniques, higher-order Boltzmann machines or relational autoencoders consider projections of the images on different subspaces and intermediate layers act as transformation specific detectors. In this work, we extend the mathematical study of (Memisevic, 2012b) to show that it is possible to use a unique projection for both images in a way that turns intermediate layers as spectrum encoders of transformations. We show that this results in networks that are easier to tune and have greater generalization capabilities.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/alain13.pdf",
        "supp": "",
        "pdf_size": 2074156,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15895548593427791942&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Institut des Syst\u00e8mes Intelligents et de Robotique (ISIR) + Universit\u00e9 Pierre et Marie Curie - CNRS7222; Institut des Syst\u00e8mes Intelligents et de Robotique (ISIR) + Universit\u00e9 Pierre et Marie Curie - CNRS7222",
        "aff_domain": "isir.upmc.fr;upmc.fr",
        "email": "isir.upmc.fr;upmc.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Institut des Syst\u00e8mes Intelligents et de Robotique;Universit\u00e9 Pierre et Marie Curie",
        "aff_unique_dep": "Robotique;",
        "aff_unique_url": "http://www.isir.upmc.fr;https://www.upmc.fr",
        "aff_unique_abbr": "ISIR;UPMC",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "France"
    },
    {
        "id": "22329c2593",
        "title": "Gaussian Process Kernels for Pattern Discovery and Extrapolation",
        "site": "https://proceedings.mlr.press/v28/wilson13.html",
        "author": "Andrew Wilson; Ryan Adams",
        "abstract": "Gaussian processes are rich distributions over functions, which provide a Bayesian nonparametric approach to smoothing and interpolation.  We introduce simple closed form kernels that can be used with Gaussian processes to discover patterns and enable extrapolation.  These kernels are derived by modelling a spectral density \u2013 the Fourier transform of a kernel \u2013 with a Gaussian mixture.  The proposed kernels support a broad class of stationary covariances, but Gaussian process inference remains simple and analytic.  We demonstrate the proposed kernels by discovering patterns and performing long range extrapolation on synthetic examples, as well as atmospheric CO2 trends and airline passenger data.  We also show that it is possible to reconstruct several popular standard covariances within our framework.",
        "bibtex": "@InProceedings{pmlr-v28-wilson13,\n  title = \t {Gaussian Process Kernels for Pattern Discovery and Extrapolation},\n  author = \t {Wilson, Andrew and Adams, Ryan},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1067--1075},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/wilson13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/wilson13.html},\n  abstract = \t {Gaussian processes are rich distributions over functions, which provide a Bayesian nonparametric approach to smoothing and interpolation.  We introduce simple closed form kernels that can be used with Gaussian processes to discover patterns and enable extrapolation.  These kernels are derived by modelling a spectral density \u2013 the Fourier transform of a kernel \u2013 with a Gaussian mixture.  The proposed kernels support a broad class of stationary covariances, but Gaussian process inference remains simple and analytic.  We demonstrate the proposed kernels by discovering patterns and performing long range extrapolation on synthetic examples, as well as atmospheric CO2 trends and airline passenger data.  We also show that it is possible to reconstruct several popular standard covariances within our framework.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/wilson13.pdf",
        "supp": "",
        "pdf_size": 886816,
        "gs_citation": 879,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5409381936530055621&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Department of Engineering, University of Cambridge, Cambridge, UK; School of Engineering and Applied Sciences, Harvard University, Cambridge, USA",
        "aff_domain": "cam.ac.uk;seas.harvard.edu",
        "email": "cam.ac.uk;seas.harvard.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Cambridge;Harvard University",
        "aff_unique_dep": "Department of Engineering;School of Engineering and Applied Sciences",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.harvard.edu",
        "aff_unique_abbr": "Cambridge;Harvard",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "4e5d56c82e",
        "title": "Gaussian Process Vine Copulas for Multivariate Dependence",
        "site": "https://proceedings.mlr.press/v28/lopez-paz13.html",
        "author": "David Lopez-Paz; Jose Miguel Hern\u00e1ndez-Lobato; Ghahramani Zoubin",
        "abstract": "Copulas allow to learn marginal distributions separately from the multivariate dependence structure (copula) that links them together into a density function.  Vine factorizations ease the learning of high-dimensional copulas by constructing a hierarchy of conditional bivariate copulas. However, to simplify inference, it is common to assume that each of these conditional bivariate copulas is independent from its conditioning variables.  In this paper, we relax this assumption by discovering the latent functions that specify the shape of a conditional copula given its conditioning variables.  We learn these functions by following a Bayesian approach based on sparse Gaussian processes with expectation propagation for scalable, approximate inference. Experiments on real-world datasets show that, when modeling all conditional dependencies, we obtain better estimates of the underlying copula of the data.",
        "bibtex": "@InProceedings{pmlr-v28-lopez-paz13,\n  title = \t {Gaussian Process Vine Copulas for Multivariate Dependence},\n  author = \t {Lopez-Paz, David and Hern\u00e1ndez-Lobato, Jose Miguel and Zoubin, Ghahramani},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {10--18},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/lopez-paz13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/lopez-paz13.html},\n  abstract = \t {Copulas allow to learn marginal distributions separately from the multivariate dependence structure (copula) that links them together into a density function.  Vine factorizations ease the learning of high-dimensional copulas by constructing a hierarchy of conditional bivariate copulas. However, to simplify inference, it is common to assume that each of these conditional bivariate copulas is independent from its conditioning variables.  In this paper, we relax this assumption by discovering the latent functions that specify the shape of a conditional copula given its conditioning variables.  We learn these functions by following a Bayesian approach based on sparse Gaussian processes with expectation propagation for scalable, approximate inference. Experiments on real-world datasets show that, when modeling all conditional dependencies, we obtain better estimates of the underlying copula of the data.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/lopez-paz13.pdf",
        "supp": "",
        "pdf_size": 579174,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4710527061499095733&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Max Planck Institute for Intelligent Systems; University of Cambridge; University of Cambridge",
        "aff_domain": "tue.mpg.de;eng.cam.ac.uk;eng.cam.ac.uk",
        "email": "tue.mpg.de;eng.cam.ac.uk;eng.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems;University of Cambridge",
        "aff_unique_dep": "Intelligent Systems;",
        "aff_unique_url": "https://www.mpi-is.mpg.de;https://www.cam.ac.uk",
        "aff_unique_abbr": "MPI-IS;Cambridge",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Germany;United Kingdom"
    },
    {
        "id": "a4ff5d64dc",
        "title": "General Functional Matrix Factorization Using Gradient Boosting",
        "site": "https://proceedings.mlr.press/v28/chen13e.html",
        "author": "Tianqi Chen; Hang Li; Qiang Yang; Yong Yu",
        "abstract": "Matrix factorization is among the most successful techniques for collaborative filtering. One challenge of collaborative filtering is how to utilize available auxiliary information to improve prediction accuracy. In this paper, we study the  problem of utilizing auxiliary information as features of   factorization and propose formalizing the problem as general functional matrix factorization, whose model includes conventional matrix factorization models as its special cases. Moreover, we propose a gradient boosting based algorithm to efficiently solve the optimization problem. Finally, we give two specific algorithms for efficient feature function construction for two specific tasks. Our method can construct more suitable feature functions by searching in an infinite functional space based on training data and thus can yield better prediction accuracy. The experimental results demonstrate that the proposed method outperforms the baseline methods on three real-world datasets.",
        "bibtex": "@InProceedings{pmlr-v28-chen13e,\n  title = \t {General Functional Matrix Factorization Using Gradient Boosting},\n  author = \t {Chen, Tianqi and Li, Hang and Yang, Qiang and Yu, Yong},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {436--444},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/chen13e.pdf},\n  url = \t {https://proceedings.mlr.press/v28/chen13e.html},\n  abstract = \t {Matrix factorization is among the most successful techniques for collaborative filtering. One challenge of collaborative filtering is how to utilize available auxiliary information to improve prediction accuracy. In this paper, we study the  problem of utilizing auxiliary information as features of   factorization and propose formalizing the problem as general functional matrix factorization, whose model includes conventional matrix factorization models as its special cases. Moreover, we propose a gradient boosting based algorithm to efficiently solve the optimization problem. Finally, we give two specific algorithms for efficient feature function construction for two specific tasks. Our method can construct more suitable feature functions by searching in an infinite functional space based on training data and thus can yield better prediction accuracy. The experimental results demonstrate that the proposed method outperforms the baseline methods on three real-world datasets.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/chen13e.pdf",
        "supp": "",
        "pdf_size": 1318327,
        "gs_citation": 71,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11247755587304903392&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Shanghai Jiao Tong University, China; Huawei Noah\u2019s Ark Lab, Hong Kong; Huawei Noah\u2019s Ark Lab, Hong Kong; Shanghai Jiao Tong University, China",
        "aff_domain": "apex.sjtu.edu.cn;huawei.com;cse.ust.hk;apex.sjtu.edu.cn",
        "email": "apex.sjtu.edu.cn;huawei.com;cse.ust.hk;apex.sjtu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Shanghai Jiao Tong University;Huawei",
        "aff_unique_dep": ";Huawei Noah\u2019s Ark Lab",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://www.huawei.com/en/ai/noahs-ark-lab",
        "aff_unique_abbr": "SJTU;Huawei Noah\u2019s Ark Lab",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "139383a4c4",
        "title": "Generic Exploration and K-armed Voting Bandits",
        "site": "https://proceedings.mlr.press/v28/urvoy13.html",
        "author": "Tanguy Urvoy; Fabrice Clerot; Raphael F\u00e9raud; Sami Naamane",
        "abstract": "We study a stochastic online learning scheme with partial feedback where the utility of decisions is only observable through an estimation of the environment parameters. We propose a generic pure-exploration algorithm, able to cope with various utility functions from multi-armed bandits settings to dueling bandits. The primary application of this setting is to offer a natural generalization of dueling bandits for situations where the environment parameters reflect the idiosyncratic preferences of a mixed crowd.",
        "bibtex": "@InProceedings{pmlr-v28-urvoy13,\n  title = \t {Generic Exploration and {K}-armed Voting Bandits},\n  author = \t {Urvoy, Tanguy and Clerot, Fabrice and F\u00e9raud, Raphael and Naamane, Sami},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {91--99},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/urvoy13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/urvoy13.html},\n  abstract = \t {We study a stochastic online learning scheme with partial feedback where the utility of decisions is only observable through an estimation of the environment parameters. We propose a generic pure-exploration algorithm, able to cope with various utility functions from multi-armed bandits settings to dueling bandits. The primary application of this setting is to offer a natural generalization of dueling bandits for situations where the environment parameters reflect the idiosyncratic preferences of a mixed crowd.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/urvoy13.pdf",
        "supp": "",
        "pdf_size": 604647,
        "gs_citation": 128,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11671263662586804901&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Orange-labs, 2 avenue Pierre Marzin, 22307 Lannion, FRANCE; Orange-labs, 2 avenue Pierre Marzin, 22307 Lannion, FRANCE; Orange-labs, 2 avenue Pierre Marzin, 22307 Lannion, FRANCE; Orange-labs, 2 avenue Pierre Marzin, 22307 Lannion, FRANCE",
        "aff_domain": "orange.com;orange.com;orange.com;orange.com",
        "email": "orange.com;orange.com;orange.com;orange.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Orange Labs",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.orange.com",
        "aff_unique_abbr": "Orange",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "09779556b8",
        "title": "Gibbs Max-Margin Topic Models with Fast Sampling Algorithms",
        "site": "https://proceedings.mlr.press/v28/zhu13.html",
        "author": "Jun Zhu; Ning Chen; Hugh Perkins; Bo Zhang",
        "abstract": "Existing max-margin supervised topic models rely on an iterative procedure to solve multiple latent SVM subproblems with additional mean-field assumptions on the desired posterior distributions. This paper presents Gibbs max-margin supervised topic models by minimizing an expected margin loss, an upper bound of the existing margin loss derived from an expected prediction rule. By introducing augmented variables, we develop simple and fast Gibbs sampling algorithms with no restricting assumptions and no need to solve SVM subproblems for both classification and regression. Empirical results demonstrate significant improvements on time efficiency. The classification performance is also significantly improved over competitors.",
        "bibtex": "@InProceedings{pmlr-v28-zhu13,\n  title = \t {{G}ibbs Max-Margin Topic Models with Fast Sampling Algorithms},\n  author = \t {Zhu, Jun and Chen, Ning and Perkins, Hugh and Zhang, Bo},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {124--132},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/zhu13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/zhu13.html},\n  abstract = \t {Existing max-margin supervised topic models rely on an iterative procedure to solve multiple latent SVM subproblems with additional mean-field assumptions on the desired posterior distributions. This paper presents Gibbs max-margin supervised topic models by minimizing an expected margin loss, an upper bound of the existing margin loss derived from an expected prediction rule. By introducing augmented variables, we develop simple and fast Gibbs sampling algorithms with no restricting assumptions and no need to solve SVM subproblems for both classification and regression. Empirical results demonstrate significant improvements on time efficiency. The classification performance is also significantly improved over competitors.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/zhu13.pdf",
        "supp": "",
        "pdf_size": 224158,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14506132034300080109&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "a15b4a5294",
        "title": "Gossip-based distributed stochastic bandit algorithms",
        "site": "https://proceedings.mlr.press/v28/szorenyi13.html",
        "author": "Balazs Szorenyi; Robert Busa-Fekete; Istvan Hegedus; Robert Ormandi; Mark Jelasity; Balazs Kegl",
        "abstract": "The multi-armed bandit problem has attracted remarkable attention in the machine learning community and many efficient algorithms have been proposed to handle the so-called exploitation-exploration dilemma in various bandit setups. At the same time, significantly less effort has been devoted to adapting bandit algorithms to particular architectures, such as sensor networks, multi-core machines, or peer-to-peer (P2P) environments, which could potentially speed up their convergence. Our goal is to adapt stochastic bandit algorithms to P2P networks.  In our setup, the same set of arms is available in each peer. In every iteration each peer can pull one arm independently of the other peers, and then some limited communication is possible with a few random other peers.  As our main result, we show that our adaptation achieves a linear speedup in terms of the number of peers participating in the network.  More precisely, we show that the probability of playing a suboptimal arm at a peer in iteration t = \u03a9( \\log N ) is proportional to 1/(Nt) where N denotes the number of peers.  The theoretical results are supported by simulation experiments showing that our algorithm scales gracefully with the size of network.",
        "bibtex": "@InProceedings{pmlr-v28-szorenyi13,\n  title = \t {Gossip-based distributed stochastic bandit algorithms},\n  author = \t {Szorenyi, Balazs and Busa-Fekete, Robert and Hegedus, Istvan and Ormandi, Robert and Jelasity, Mark and Kegl, Balazs},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {19--27},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/szorenyi13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/szorenyi13.html},\n  abstract = \t {The multi-armed bandit problem has attracted remarkable attention in the machine learning community and many efficient algorithms have been proposed to handle the so-called exploitation-exploration dilemma in various bandit setups. At the same time, significantly less effort has been devoted to adapting bandit algorithms to particular architectures, such as sensor networks, multi-core machines, or peer-to-peer (P2P) environments, which could potentially speed up their convergence. Our goal is to adapt stochastic bandit algorithms to P2P networks.  In our setup, the same set of arms is available in each peer. In every iteration each peer can pull one arm independently of the other peers, and then some limited communication is possible with a few random other peers.  As our main result, we show that our adaptation achieves a linear speedup in terms of the number of peers participating in the network.  More precisely, we show that the probability of playing a suboptimal arm at a peer in iteration t = \u03a9( \\log N ) is proportional to 1/(Nt) where N denotes the number of peers.  The theoretical results are supported by simulation experiments showing that our algorithm scales gracefully with the size of network.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/szorenyi13.pdf",
        "supp": "",
        "pdf_size": 583241,
        "gs_citation": 134,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17909952961015138294&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 20,
        "aff": "INRIy Lille d Nord Europec SequeL projectc kg avenue Halleyc lpmlg Villeneuve d\u2019yscqc France + Research Group on yIc Hungarian ycade Scie and Unive of Szegedc yradi v\u00b4ertan\u00b4uk tere hec Hdmnig Szegedc Hungary; Research Group on yIc Hungarian ycade Scie and Unive of Szegedc yradi v\u00b4ertan\u00b4uk tere hec Hdmnig Szegedc Hungary + Mathematics and Computer Sciencec University of Marburgc HansdMeerweindStrec jlgji Marburgc Germany; Research Group on yIc Hungarian ycade Scie and Unive of Szegedc yradi v\u00b4ertan\u00b4uk tere hec Hdmnig Szegedc Hungary; Research Group on yIc Hungarian ycade Scie and Unive of Szegedc yradi v\u00b4ertan\u00b4uk tere hec Hdmnig Szegedc Hungary; Research Group on yIc Hungarian ycade Scie and Unive of Szegedc yradi v\u00b4ertan\u00b4uk tere hec Hdmnig Szegedc Hungary; Linear yccelerator Laboratory VLyLW T Computer Science Laboratory VLRIWc CNRSfUniversity of Paris Sudc phkglOrsayc France",
        "aff_domain": "inf.u-szeged.hu;inf.u-szeged.hu;inf.u-szeged.hu;inf.u-szeged.hu;inf.u-szeged.hu;gmail.com",
        "email": "inf.u-szeged.hu;inf.u-szeged.hu;inf.u-szeged.hu;inf.u-szeged.hu;inf.u-szeged.hu;gmail.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1+2;1;1;1;3",
        "aff_unique_norm": "INRIA Lille - Nord Europe;University of Szeged;University of Marburg;University of Paris Sud",
        "aff_unique_dep": "SequeL project;Research Group on yIc Hungarian ycade Scie;Mathematics and Computer Science;Computer Science Laboratory",
        "aff_unique_url": "https://www.inria.fr/lille-nord-europe;;https://www.uni-marburg.de;https://www.universite-paris-sud.fr",
        "aff_unique_abbr": "INRIA;;UM;UPS",
        "aff_campus_unique_index": "0+1;1+2;1;1;1;3",
        "aff_campus_unique": "Lille;Szeged;Marburg;Orsay",
        "aff_country_unique_index": "0+1;1+2;1;1;1;0",
        "aff_country_unique": "France;Hungary;Germany"
    },
    {
        "id": "038ed72cd7",
        "title": "Guaranteed Sparse Recovery under Linear Transformation",
        "site": "https://proceedings.mlr.press/v28/liu13.html",
        "author": "Ji Liu; Lei Yuan; Jieping Ye",
        "abstract": "We consider the following signal recovery problem: given a  measurement matrix \u03a6\u2208\\mathbbR^n\\times p and a noisy  observation vector c\u2208\\mathbbR^n constructed from c =  \u03a6\u03b8^* + \u03b5where \u03b5\u2208\\mathbbR^n is the  noise vector whose entries follow i.i.d. centered sub-Gaussian  distribution, how to recover the signal \u03b8^* if D\u03b8^* is  sparse \\rca under a linear transformation D\u2208\\mathbbR^m\\times  p? One natural method using convex optimization is to solve the  following problem: $\\min_\u03b8\u00a01\\over 2\\|\u03a6\u03b8- c\\|^2 +  \u03bb\\|D\u03b8\\|_1. This paper provides an upper bound of the  estimate error and shows the consistency property of this method by  assuming that the design matrix \u03a6is a Gaussian random matrix.  Specifically, we show 1) in the noiseless case, if the condition  number of D is bounded and the measurement number n\u2265\u03a9(s\\log(p)) where s is the sparsity number, then the true  solution can be recovered with high probability; and 2) in the noisy  case, if the condition number of D is bounded and the measurement  increases faster than s\\log(p), that is, s\\log(p)=o(n), the  estimate error converges to zero with probability 1 when p and s  go to infinity. Our results are consistent with those for the  special case D=\\boldI_p\\times p (equivalently LASSO) and  improve the existing analysis. The condition number of D plays a  critical role in our analysis. We consider the condition numbers in  two cases including the fused LASSO and the random graph: the  condition number in the fused LASSO case is bounded by a constant,  while the condition number in the random graph case is bounded with  high probability if m\\over p (i.e., #\\textedge\\over  #\\textvertex$) is larger than a certain constant. Numerical  simulations are consistent with our theoretical results.",
        "bibtex": "@InProceedings{pmlr-v28-liu13,\n  title = \t {Guaranteed Sparse Recovery under Linear Transformation},\n  author = \t {Liu, Ji and Yuan, Lei and Ye, Jieping},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {91--99},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/liu13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/liu13.html},\n  abstract = \t {We consider the following signal recovery problem: given a  measurement matrix \u03a6\u2208\\mathbbR^n\\times p and a noisy  observation vector c\u2208\\mathbbR^n constructed from c =  \u03a6\u03b8^* + \u03b5where \u03b5\u2208\\mathbbR^n is the  noise vector whose entries follow i.i.d. centered sub-Gaussian  distribution, how to recover the signal \u03b8^* if D\u03b8^* is  sparse \\rca under a linear transformation D\u2208\\mathbbR^m\\times  p? One natural method using convex optimization is to solve the  following problem: $\\min_\u03b8\u00a01\\over 2\\|\u03a6\u03b8- c\\|^2 +  \u03bb\\|D\u03b8\\|_1. This paper provides an upper bound of the  estimate error and shows the consistency property of this method by  assuming that the design matrix \u03a6is a Gaussian random matrix.  Specifically, we show 1) in the noiseless case, if the condition  number of D is bounded and the measurement number n\u2265\u03a9(s\\log(p)) where s is the sparsity number, then the true  solution can be recovered with high probability; and 2) in the noisy  case, if the condition number of D is bounded and the measurement  increases faster than s\\log(p), that is, s\\log(p)=o(n), the  estimate error converges to zero with probability 1 when p and s  go to infinity. Our results are consistent with those for the  special case D=\\boldI_p\\times p (equivalently LASSO) and  improve the existing analysis. The condition number of D plays a  critical role in our analysis. We consider the condition numbers in  two cases including the fused LASSO and the random graph: the  condition number in the fused LASSO case is bounded by a constant,  while the condition number in the random graph case is bounded with  high probability if m\\over p (i.e., #\\textedge\\over  #\\textvertex$) is larger than a certain constant. Numerical  simulations are consistent with our theoretical results.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/liu13.pdf",
        "supp": "",
        "pdf_size": 370487,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3930874254012408898&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Sciences, University of Wisconsin-Madison, Madison, WI 53706 USA; Department of Computer Science and Engineering, Arizona State University, Tempe, AZ 85287 USA; Department of Computer Science and Engineering, Arizona State University, Tempe, AZ 85287 USA",
        "aff_domain": "cs.wisc.edu;asu.edu;asu.edu",
        "email": "cs.wisc.edu;asu.edu;asu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Wisconsin-Madison;Arizona State University",
        "aff_unique_dep": "Department of Computer Sciences;Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.wisc.edu;https://www.asu.edu",
        "aff_unique_abbr": "UW-Madison;ASU",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Madison;Tempe",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1aa791fdcc",
        "title": "Guided Policy Search",
        "site": "https://proceedings.mlr.press/v28/levine13.html",
        "author": "Sergey Levine; Vladlen Koltun",
        "abstract": "Direct policy search can effectively scale to high-dimensional systems, but complex policies with hundreds of parameters often present a challenge for such methods, requiring numerous samples and often falling into poor local optima. We present a guided policy search algorithm that uses trajectory optimization to direct policy learning and avoid poor local optima. We show how differential dynamic programming can be used to generate suitable guiding samples, and describe a regularized importance sampled policy optimization that incorporates these samples into the policy search. We evaluate the method by learning neural network controllers for planar swimming, hopping, and walking, as well as simulated 3D humanoid running.",
        "bibtex": "@InProceedings{pmlr-v28-levine13,\n  title = \t {Guided Policy Search},\n  author = \t {Levine, Sergey and Koltun, Vladlen},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1--9},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/levine13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/levine13.html},\n  abstract = \t {Direct policy search can effectively scale to high-dimensional systems, but complex policies with hundreds of parameters often present a challenge for such methods, requiring numerous samples and often falling into poor local optima. We present a guided policy search algorithm that uses trajectory optimization to direct policy learning and avoid poor local optima. We show how differential dynamic programming can be used to generate suitable guiding samples, and describe a regularized importance sampled policy optimization that incorporates these samples into the policy search. We evaluate the method by learning neural network controllers for planar swimming, hopping, and walking, as well as simulated 3D humanoid running.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/levine13.pdf",
        "supp": "",
        "pdf_size": 1594905,
        "gs_citation": 1440,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15178865421588146962&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 23,
        "aff": "Computer Science Department, Stanford University, Stanford, CA 94305 USA; Computer Science Department, Stanford University, Stanford, CA 94305 USA",
        "aff_domain": "stanford.edu;stanford.edu",
        "email": "stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c480f427b4",
        "title": "Hierarchical Regularization Cascade for Joint Learning",
        "site": "https://proceedings.mlr.press/v28/zweig13.html",
        "author": "Alon Zweig; Daphna Weinshall",
        "abstract": "As the sheer volume of available benchmark datasets increases, the problem of joint learning of classifiers and knowledge-transfer between classifiers, becomes more and more relevant. We present a hierarchical approach which exploits information sharing among different classification tasks, in multi-task and multi-class settings. It engages a top-down iterative method, which begins by posing an optimization problem with an incentive for large scale sharing among all classes. This incentive to share is gradually decreased,until there is no sharing and all tasks are considered separately. The method therefore exploits different levels of sharing within a given group of related tasks, without having to make hard decisions about the grouping of tasks. In order to deal with large scale problems, with many tasks and many classes, we extend our batch approach to an online setting and provide regret analysis of the algorithm. We tested our approach extensively on synthetic and real datasets, showing significant improvement over baseline and state-of-the-art methods.",
        "bibtex": "@InProceedings{pmlr-v28-zweig13,\n  title = \t {Hierarchical Regularization Cascade for Joint Learning},\n  author = \t {Zweig, Alon and Weinshall, Daphna},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {37--45},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/zweig13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/zweig13.html},\n  abstract = \t {As the sheer volume of available benchmark datasets increases, the problem of joint learning of classifiers and knowledge-transfer between classifiers, becomes more and more relevant. We present a hierarchical approach which exploits information sharing among different classification tasks, in multi-task and multi-class settings. It engages a top-down iterative method, which begins by posing an optimization problem with an incentive for large scale sharing among all classes. This incentive to share is gradually decreased,until there is no sharing and all tasks are considered separately. The method therefore exploits different levels of sharing within a given group of related tasks, without having to make hard decisions about the grouping of tasks. In order to deal with large scale problems, with many tasks and many classes, we extend our batch approach to an online setting and provide regret analysis of the algorithm. We tested our approach extensively on synthetic and real datasets, showing significant improvement over baseline and state-of-the-art methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/zweig13.pdf",
        "supp": "",
        "pdf_size": 196944,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7726491068644246201&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "School of Computer Science and Engineering, The Hebrew University of Jerusalem, Jerusalem, Israel, 91904; School of Computer Science and Engineering, The Hebrew University of Jerusalem, Jerusalem, Israel, 91904",
        "aff_domain": "mail.huji.ac.il;cs.huji.ac.il",
        "email": "mail.huji.ac.il;cs.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hebrew University of Jerusalem",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "http://www.huji.ac.il",
        "aff_unique_abbr": "HUJI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Jerusalem",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "56b2cbdef5",
        "title": "Hierarchical Tensor Decomposition of Latent Tree Graphical Models",
        "site": "https://proceedings.mlr.press/v28/song13.html",
        "author": "Le Song; Mariya Ishteva; Ankur Parikh; Eric Xing; Haesun Park",
        "abstract": "We approach the problem of estimating the parameters of a latent tree graphical model from a hierarchical tensor decomposition point of view. In this new view, the marginal probability table of the observed variables in a latent tree is treated as a tensor, and we show that: (i) the latent variables induce low rank structures in various matricizations of the tensor; (ii) this collection of low rank matricizations induce a hierarchical low rank decomposition of the tensor. Exploiting these properties, we derive an optimization problem for estimating the parameters of a latent tree graphical model, i.e., hierarchical decomposion of a tensor which minimizes the Frobenius norm of the difference between the original tensor and its decomposition.    When the latent tree graphical models are correctly specified, we show that a global optimum of the optimization problem can be obtained via a recursive decomposition algorithm. This algorithm recovers previous spectral algorithms for hidden Markov models (Hsu et al., 2009; Foster et al., 2012) and latent tree graphical models (Parikh et al., 2011; Song et al., 2011) as special cases, elucidating the global objective these algorithms are optimizing for. When the latent tree graphical models are misspecified, we derive a better decomposition based on our framework, and provide approximation guarantee for this new estimator. In both synthetic and real world data, this new estimator significantly improves over the-state-of-the-art.",
        "bibtex": "@InProceedings{pmlr-v28-song13,\n  title = \t {Hierarchical Tensor Decomposition of Latent Tree Graphical Models},\n  author = \t {Song, Le and Ishteva, Mariya and Parikh, Ankur and Xing, Eric and Park, Haesun},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {334--342},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/song13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/song13.html},\n  abstract = \t {We approach the problem of estimating the parameters of a latent tree graphical model from a hierarchical tensor decomposition point of view. In this new view, the marginal probability table of the observed variables in a latent tree is treated as a tensor, and we show that: (i) the latent variables induce low rank structures in various matricizations of the tensor; (ii) this collection of low rank matricizations induce a hierarchical low rank decomposition of the tensor. Exploiting these properties, we derive an optimization problem for estimating the parameters of a latent tree graphical model, i.e., hierarchical decomposion of a tensor which minimizes the Frobenius norm of the difference between the original tensor and its decomposition.    When the latent tree graphical models are correctly specified, we show that a global optimum of the optimization problem can be obtained via a recursive decomposition algorithm. This algorithm recovers previous spectral algorithms for hidden Markov models (Hsu et al., 2009; Foster et al., 2012) and latent tree graphical models (Parikh et al., 2011; Song et al., 2011) as special cases, elucidating the global objective these algorithms are optimizing for. When the latent tree graphical models are misspecified, we derive a better decomposition based on our framework, and provide approximation guarantee for this new estimator. In both synthetic and real world data, this new estimator significantly improves over the-state-of-the-art.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/song13.pdf",
        "supp": "",
        "pdf_size": 822443,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11221596987988021167&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "College of Computing, Georgia Institute of Technology, Atlanta, GA 30332, USA; College of Computing, Georgia Institute of Technology, Atlanta, GA 30332, USA; ELEC, Vrije Universiteit Brussel, 1050 Brussels, Belgium; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA",
        "aff_domain": "cc.gatech.edu;cc.gatech.edu;vub.ac.be;cs.cmu.edu;cs.cmu.edu",
        "email": "cc.gatech.edu;cc.gatech.edu;vub.ac.be;cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2;2",
        "aff_unique_norm": "Georgia Institute of Technology;Vrije Universiteit Brussel;Carnegie Mellon University",
        "aff_unique_dep": "College of Computing;ELEC;School of Computer Science",
        "aff_unique_url": "https://www.gatech.edu;https://www.vub.be;https://www.cmu.edu",
        "aff_unique_abbr": "Georgia Tech;VUB;CMU",
        "aff_campus_unique_index": "0;0;1;2;2",
        "aff_campus_unique": "Atlanta;Brussels;Pittsburgh",
        "aff_country_unique_index": "0;0;1;0;0",
        "aff_country_unique": "United States;Belgium"
    },
    {
        "id": "5f1b40ef9f",
        "title": "Hierarchically-coupled hidden Markov models for learning kinetic rates from single-molecule data",
        "site": "https://proceedings.mlr.press/v28/willemvandemeent13.html",
        "author": "Jan-Willem Meent; Jonathan Bronson; Frank Wood; Ruben Gonzalez Jr.; Chris Wiggins",
        "abstract": "We address the problem of analyzing sets of noisy time-varying signals that all report on the same process but confound straightforward analyses due to complex inter-signal heterogeneities and measurement artifacts.  In particular we consider single-molecule experiments which indirectly measure the distinct steps in a biomolecular process via observations of noisy time-dependent signals such as a fluorescence intensity or bead position. Straightforward hidden Markov model (HMM) analyses attempt to characterize such processes in terms of a set of conformational states, the transitions that can occur between these states, and the associated rates at which those transitions occur; but require ad-hoc post-processing steps to combine multiple signals.  Here we develop a hierarchically coupled HMM that allows experimentalists to deal with inter-signal variability in a principled and automatic way. Our approach is a generalized expectation maximization hyperparameter point estimation procedure with variational Bayes at the level of individual time series that learns an single interpretable representation of the overall data generating process.",
        "bibtex": "@InProceedings{pmlr-v28-willemvandemeent13,\n  title = \t {Hierarchically-coupled hidden {M}arkov models for learning kinetic rates from single-molecule data},\n  author = \t {Meent, Jan-Willem and Bronson, Jonathan and Wood, Frank and Gonzalez Jr., Ruben and Wiggins, Chris},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {361--369},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/willemvandemeent13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/willemvandemeent13.html},\n  abstract = \t {We address the problem of analyzing sets of noisy time-varying signals that all report on the same process but confound straightforward analyses due to complex inter-signal heterogeneities and measurement artifacts.  In particular we consider single-molecule experiments which indirectly measure the distinct steps in a biomolecular process via observations of noisy time-dependent signals such as a fluorescence intensity or bead position. Straightforward hidden Markov model (HMM) analyses attempt to characterize such processes in terms of a set of conformational states, the transitions that can occur between these states, and the associated rates at which those transitions occur; but require ad-hoc post-processing steps to combine multiple signals.  Here we develop a hierarchically coupled HMM that allows experimentalists to deal with inter-signal variability in a principled and automatic way. Our approach is a generalized expectation maximization hyperparameter point estimation procedure with variational Bayes at the level of individual time series that learns an single interpretable representation of the overall data generating process.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/willemvandemeent13.pdf",
        "supp": "",
        "pdf_size": 2729599,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=72491080477722729&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Columbia University, New York, NY, USA; Columbia University, New York, NY, USA; University of Oxford, Oxford, UK; Columbia University, New York, NY, USA; Columbia University, New York, NY, USA",
        "aff_domain": "columbia.edu;gmail.com;robots.ox.ac.uk;columbia.edu;columbia.edu",
        "email": "columbia.edu;gmail.com;robots.ox.ac.uk;columbia.edu;columbia.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "Columbia University;University of Oxford",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.columbia.edu;https://www.ox.ac.uk",
        "aff_unique_abbr": "Columbia;Oxford",
        "aff_campus_unique_index": "0;0;1;0;0",
        "aff_campus_unique": "New York;Oxford",
        "aff_country_unique_index": "0;0;1;0;0",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "a5f1a6eabf",
        "title": "Human Boosting",
        "site": "https://proceedings.mlr.press/v28/pareek13.html",
        "author": "Harsh Pareek; Pradeep Ravikumar",
        "abstract": "Humans may be exceptional learners but they have biological limitations and moreover, inductive biases similar to machine learning algorithms. This puts limits on human learning ability and on the kinds of learning tasks humans can easily handle. In this paper, we consider the problem of \u201cboosting\u201d human learners to extend the learning ability of human learners and achieve improved performance on tasks which individual humans find difficult. We consider classification (category learning) tasks, propose a boosting algorithm for human learners and give theoretical justifications. We conduct experiments using Amazon\u2019s Mechanical Turk on two synthetic datasets \u2013 a crosshair task with a nonlinear decision boundary and a gabor patch task with a linear boundary but which is inaccessible to human learners \u2013 and one real world dataset \u2013 the Opinion Spam detection task introduced in (Ott et al). Our results show that boosting human learners produces gains in accuracy and can overcome some fundamental limitations of human learners.",
        "bibtex": "@InProceedings{pmlr-v28-pareek13,\n  title = \t {Human Boosting},\n  author = \t {Pareek, Harsh and Ravikumar, Pradeep},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {338--346},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/pareek13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/pareek13.html},\n  abstract = \t {Humans may be exceptional learners but they have biological limitations and moreover, inductive biases similar to machine learning algorithms. This puts limits on human learning ability and on the kinds of learning tasks humans can easily handle. In this paper, we consider the problem of \u201cboosting\u201d human learners to extend the learning ability of human learners and achieve improved performance on tasks which individual humans find difficult. We consider classification (category learning) tasks, propose a boosting algorithm for human learners and give theoretical justifications. We conduct experiments using Amazon\u2019s Mechanical Turk on two synthetic datasets \u2013 a crosshair task with a nonlinear decision boundary and a gabor patch task with a linear boundary but which is inaccessible to human learners \u2013 and one real world dataset \u2013 the Opinion Spam detection task introduced in (Ott et al). Our results show that boosting human learners produces gains in accuracy and can overcome some fundamental limitations of human learners.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/pareek13.pdf",
        "supp": "",
        "pdf_size": 384870,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13142580284173455933&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Computer Science, The University of Texas, Austin, TX 78712, USA; Department of Computer Science, The University of Texas, Austin, TX 78712, USA",
        "aff_domain": "cs.utexas.edu;cs.utexas.edu",
        "email": "cs.utexas.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a5d411f11b",
        "title": "Inference algorithms for pattern-based CRFs on sequence data",
        "site": "https://proceedings.mlr.press/v28/takhanov13.html",
        "author": "Rustem Takhanov; Vladimir Kolmogorov",
        "abstract": "We consider \\em Conditional Random Fields (CRFs) with pattern-based potentials  defined on a chain. In this model the energy of a string (labeling) x_1\\ldots x_n  is the sum of terms over intervals [i,j] where each term is non-zero only if the substring x_i\\ldots x_j  equals a prespecified pattern \u03b1. Such CRFs can be naturally applied to many sequence tagging problems. We present efficient algorithms for the three standard inference tasks in a CRF, namely  computing (i) the partition function, (ii) marginals, and (iii) computing the MAP.  Their complexities are respectively  O(n L), O(n L \\ell_\\max) and  O(n L \\min{|D|,\\log (\\ell_\\max + 1)})  where L is the combined length of input patterns, \\ell_\\max is the maximum length of a pattern,  and D is the input alphabet.  This improves on the previous algorithms of\u00a0\\citeYe:NIPS09 whose complexities are respectively O(n L |D|),  O\\left(n |\u0393| L^2 \\ell_\\max^2\\right) and O(n L |D|),  where |\u0393| is the number of input patterns. In addition, we give an efficient algorithm for sampling,  and revisit the case of MAP with non-positive weights. Finally, we apply pattern-based CRFs to the problem of the protein dihedral angles prediction.",
        "bibtex": "@InProceedings{pmlr-v28-takhanov13,\n  title = \t {Inference algorithms for pattern-based CRFs on sequence data},\n  author = \t {Takhanov, Rustem and Kolmogorov, Vladimir},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {145--153},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/takhanov13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/takhanov13.html},\n  abstract = \t {We consider \\em Conditional Random Fields (CRFs) with pattern-based potentials  defined on a chain. In this model the energy of a string (labeling) x_1\\ldots x_n  is the sum of terms over intervals [i,j] where each term is non-zero only if the substring x_i\\ldots x_j  equals a prespecified pattern \u03b1. Such CRFs can be naturally applied to many sequence tagging problems. We present efficient algorithms for the three standard inference tasks in a CRF, namely  computing (i) the partition function, (ii) marginals, and (iii) computing the MAP.  Their complexities are respectively  O(n L), O(n L \\ell_\\max) and  O(n L \\min{|D|,\\log (\\ell_\\max + 1)})  where L is the combined length of input patterns, \\ell_\\max is the maximum length of a pattern,  and D is the input alphabet.  This improves on the previous algorithms of\u00a0\\citeYe:NIPS09 whose complexities are respectively O(n L |D|),  O\\left(n |\u0393| L^2 \\ell_\\max^2\\right) and O(n L |D|),  where |\u0393| is the number of input patterns. In addition, we give an efficient algorithm for sampling,  and revisit the case of MAP with non-positive weights. Finally, we apply pattern-based CRFs to the problem of the protein dihedral angles prediction.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/takhanov13.pdf",
        "supp": "",
        "pdf_size": 518665,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5223879349902663196&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff": "Institute of Science and Technology (IST), Austria; Institute of Science and Technology (IST), Austria",
        "aff_domain": "mail.ru;ist.ac.at",
        "email": "mail.ru;ist.ac.at",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Institute of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ist.ac.at",
        "aff_unique_abbr": "IST Austria",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Austria"
    },
    {
        "id": "eb5e591bd3",
        "title": "Infinite Markov-Switching Maximum Entropy Discrimination Machines",
        "site": "https://proceedings.mlr.press/v28/chatzis13.html",
        "author": "Sotirios Chatzis",
        "abstract": "In this paper, we present a method that combines the merits of Bayesian  nonparametrics, specifically stick-breaking priors, and large-margin  kernel machines in the context of sequential data classification.  The proposed model postulates a set of (theoretically) infinite interdependent  large-margin classifiers as model components, that robustly capture  local nonlinearity of complex data. The postulated large-margin classifiers  are connected in the context of a Markov-switching construction that  allows for capturing complex temporal dynamics in the modeled datasets.  Appropriate stick-breaking priors are imposed over the component switching  mechanism of our model to allow for data-driven determination of the  optimal number of component large-margin classifiers, under a standard  nonparametric Bayesian inference scheme. Efficient model training  is performed under the maximum entropy discrimination (MED) framework,  which integrates the large-margin principle with Bayesian posterior  inference. We evaluate our method using several real-world datasets,  and compare it to state-of-the-art alternatives.",
        "bibtex": "@InProceedings{pmlr-v28-chatzis13,\n  title = \t {Infinite Markov-Switching Maximum Entropy Discrimination Machines},\n  author = \t {Chatzis, Sotirios},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {729--737},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/chatzis13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/chatzis13.html},\n  abstract = \t {In this paper, we present a method that combines the merits of Bayesian  nonparametrics, specifically stick-breaking priors, and large-margin  kernel machines in the context of sequential data classification.  The proposed model postulates a set of (theoretically) infinite interdependent  large-margin classifiers as model components, that robustly capture  local nonlinearity of complex data. The postulated large-margin classifiers  are connected in the context of a Markov-switching construction that  allows for capturing complex temporal dynamics in the modeled datasets.  Appropriate stick-breaking priors are imposed over the component switching  mechanism of our model to allow for data-driven determination of the  optimal number of component large-margin classifiers, under a standard  nonparametric Bayesian inference scheme. Efficient model training  is performed under the maximum entropy discrimination (MED) framework,  which integrates the large-margin principle with Bayesian posterior  inference. We evaluate our method using several real-world datasets,  and compare it to state-of-the-art alternatives.   }\n}",
        "pdf": "http://proceedings.mlr.press/v28/chatzis13.pdf",
        "supp": "",
        "pdf_size": 2945031,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14913735970896900932&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "6f46038421",
        "title": "Infinite Positive Semidefinite Tensor Factorization for Source Separation of Mixture Signals",
        "site": "https://proceedings.mlr.press/v28/yoshii13.html",
        "author": "Kazuyoshi Yoshii; Ryota Tomioka; Daichi Mochihashi; Masataka Goto",
        "abstract": "This paper presents a new class of tensor factorization called positive semidefinite tensor factorization (PSDTF) that decomposes a set of positive semidefinite (PSD) matrices into the convex combinations of fewer PSD basis matrices. PSDTF can be viewed as a natural extension of nonnegative matrix factorization. One of the main problems of PSDTF is that an appropriate number of bases should be given in advance. To solve this problem, we propose a nonparametric Bayesian model based on a gamma process that can instantiate only a limited number of necessary bases from the infinitely many bases assumed to exist. We derive a variational Bayesian algorithm for closed-form posterior inference and a multiplicative update rule for maximum-likelihood estimation. We evaluated PSDTF on both synthetic data and real music recordings to show its superiority.",
        "bibtex": "@InProceedings{pmlr-v28-yoshii13,\n  title = \t {Infinite Positive Semidefinite Tensor Factorization for Source Separation of Mixture Signals},\n  author = \t {Yoshii, Kazuyoshi and Tomioka, Ryota and Mochihashi, Daichi and Goto, Masataka},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {576--584},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/yoshii13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/yoshii13.html},\n  abstract = \t {This paper presents a new class of tensor factorization called positive semidefinite tensor factorization (PSDTF) that decomposes a set of positive semidefinite (PSD) matrices into the convex combinations of fewer PSD basis matrices. PSDTF can be viewed as a natural extension of nonnegative matrix factorization. One of the main problems of PSDTF is that an appropriate number of bases should be given in advance. To solve this problem, we propose a nonparametric Bayesian model based on a gamma process that can instantiate only a limited number of necessary bases from the infinitely many bases assumed to exist. We derive a variational Bayesian algorithm for closed-form posterior inference and a multiplicative update rule for maximum-likelihood estimation. We evaluated PSDTF on both synthetic data and real music recordings to show its superiority.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/yoshii13.pdf",
        "supp": "",
        "pdf_size": 705477,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1434288568671387936&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "National Institute of Advanced Industrial Science and Technology (AIST), Tsukuba, Ibaraki 305-8568, Japan; The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan; The Institute of Statistical Mathematics (ISM), 10-3 Midori-cho, Tachikawa, Tokyo 190-8562, Japan; National Institute of Advanced Industrial Science and Technology (AIST), Tsukuba, Ibaraki 305-8568, Japan",
        "aff_domain": "aist.go.jp;mist.i.u-tokyo.ac.jp;ism.ac.jp;aist.go.jp",
        "email": "aist.go.jp;mist.i.u-tokyo.ac.jp;ism.ac.jp;aist.go.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "National Institute of Advanced Industrial Science and Technology;University of Tokyo;Institute of Statistical Mathematics",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.aist.go.jp;https://www.u-tokyo.ac.jp;https://www.ism.ac.jp",
        "aff_unique_abbr": "AIST;UTokyo;ISM",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Tsukuba;Tokyo;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "4a6a534fa4",
        "title": "Infinitesimal Annealing for Training Semi-Supervised Support Vector Machines",
        "site": "https://proceedings.mlr.press/v28/ogawa13a.html",
        "author": "Kohei Ogawa; Motoki Imamura; Ichiro Takeuchi; Masashi Sugiyama",
        "abstract": "The semi-supervised support vector machine (S3VM) is a maximum-margin classification algorithm based on both labeled and unlabeled data. Training S3VM involves either a combinatorial or non-convex optimization problem and thus finding the global optimal solution is intractable in practice. It has been demonstrated that a key to successfully find a good (local) solution of S3VM is to gradually increase the effect of unlabeled data, a la annealing. However, existing algorithms suffer from the trade-off between the resolution of annealing steps and the computation cost. In this paper, we go beyond this trade-off by proposing a novel training algorithm that efficiently performs annealing with an infinitesimal resolution. Through experiments, we demonstrate that the proposed infinitesimal annealing algorithm tends to produce better solutions with less computation time than existing approaches.",
        "bibtex": "@InProceedings{pmlr-v28-ogawa13a,\n  title = \t {Infinitesimal Annealing for Training Semi-Supervised Support Vector Machines},\n  author = \t {Ogawa, Kohei and Imamura, Motoki and Takeuchi, Ichiro and Sugiyama, Masashi},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {897--905},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/ogawa13a.pdf},\n  url = \t {https://proceedings.mlr.press/v28/ogawa13a.html},\n  abstract = \t {The semi-supervised support vector machine (S3VM) is a maximum-margin classification algorithm based on both labeled and unlabeled data. Training S3VM involves either a combinatorial or non-convex optimization problem and thus finding the global optimal solution is intractable in practice. It has been demonstrated that a key to successfully find a good (local) solution of S3VM is to gradually increase the effect of unlabeled data, a la annealing. However, existing algorithms suffer from the trade-off between the resolution of annealing steps and the computation cost. In this paper, we go beyond this trade-off by proposing a novel training algorithm that efficiently performs annealing with an infinitesimal resolution. Through experiments, we demonstrate that the proposed infinitesimal annealing algorithm tends to produce better solutions with less computation time than existing approaches.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/ogawa13a.pdf",
        "supp": "",
        "pdf_size": 398617,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5309303552293558852&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Nagoya Institute of Technology, Nagoya, Japan; Nagoya Institute of Technology, Nagoya, Japan; Nagoya Institute of Technology, Nagoya, Japan; Tokyo Institute of Technology, Tokyo, Japan",
        "aff_domain": "gmail.com;gmail.com;nitech.ac.jp;cs.titech.ac.jp",
        "email": "gmail.com;gmail.com;nitech.ac.jp;cs.titech.ac.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Nagoya Institute of Technology;Tokyo Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.nitech.ac.jp;https://www.titech.ac.jp",
        "aff_unique_abbr": "NIT;Titech",
        "aff_campus_unique_index": "0;0;0;1",
        "aff_campus_unique": "Nagoya;Tokyo",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "ce2ec8840e",
        "title": "Intersecting singularities for multi-structured estimation",
        "site": "https://proceedings.mlr.press/v28/richard13.html",
        "author": "Emile Richard; Francis BACH; Jean-Philippe Vert",
        "abstract": "We address the problem of designing a convex nonsmooth regularizer encouraging multiple structural effects simultaneously. Focusing on the inference of sparse and low-rank matrices we suggest a new complexity index and a convex penalty approximating it. The new penalty term can be written as the trace norm of a linear function of the matrix. By analyzing theoretical properties of this family of regularizers  we come up with oracle inequalities and compressed sensing results ensuring the quality of our regularized estimator. We also provide algorithms and supporting numerical experiments.",
        "bibtex": "@InProceedings{pmlr-v28-richard13,\n  title = \t {Intersecting singularities for multi-structured estimation},\n  author = \t {Richard, Emile and BACH, Francis and Vert, Jean-Philippe},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1157--1165},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/richard13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/richard13.html},\n  abstract = \t {We address the problem of designing a convex nonsmooth regularizer encouraging multiple structural effects simultaneously. Focusing on the inference of sparse and low-rank matrices we suggest a new complexity index and a convex penalty approximating it. The new penalty term can be written as the trace norm of a linear function of the matrix. By analyzing theoretical properties of this family of regularizers  we come up with oracle inequalities and compressed sensing results ensuring the quality of our regularized estimator. We also provide algorithms and supporting numerical experiments.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/richard13.pdf",
        "supp": "",
        "pdf_size": 466274,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5455072765633915607&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 21,
        "aff": "CBIO Mines ParisTech, INSERM U900, Institut Curie; SIERRA project-team, INRIA - D\u00b4 epartement d\u2019Informatique de l\u2019Ecole Normale Sup\u00b4 erieure, Paris; CBIO Mines ParisTech, INSERM U900, Institut Curie",
        "aff_domain": "mines-paristech.fr;inria.fr;mines-paristech.fr",
        "email": "mines-paristech.fr;inria.fr;mines-paristech.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "MINES ParisTech;INRIA",
        "aff_unique_dep": "CBIO;D'epartement d'Informatique de l'Ecole Normale Superieure",
        "aff_unique_url": "https://www.minesparistech.fr;https://www.inria.fr",
        "aff_unique_abbr": "Mines ParisTech;INRIA",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Paris",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "1d77e2d7ea",
        "title": "Iterative Learning and Denoising in Convolutional Neural Associative Memories",
        "site": "https://proceedings.mlr.press/v28/karbasi13.html",
        "author": "Amin Karbasi; Amir Hesam Salavati; Amin Shokrollahi",
        "abstract": "The task of a neural associative memory is to retrieve a set of previously memorized patterns from their noisy versions by using a network of neurons. Hence, an ideal network should be able to 1) gradually learn a set of patterns, 2) retrieve the correct pattern from noisy queries and 3) maximize the number of memorized patterns while maintaining the reliability in responding to queries. We show that by considering the inherent redundancy in the memorized patterns, one can obtain all the mentioned properties at once. This is in sharp contrast with the previous work that could only improve one or two aspects at the expense of the third. More specifically, we devise an iterative algorithm that learns the redundancy among the patterns. The resulting network has a  retrieval capacity that is exponential in the size of the network. Lastly, by considering the local structures of the network, the asymptotic error correction performance  can be made linear in the size of the network.",
        "bibtex": "@InProceedings{pmlr-v28-karbasi13,\n  title = \t {Iterative Learning and Denoising in Convolutional Neural Associative Memories},\n  author = \t {Karbasi, Amin and Salavati, Amir Hesam and Shokrollahi, Amin},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {445--453},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/karbasi13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/karbasi13.html},\n  abstract = \t {The task of a neural associative memory is to retrieve a set of previously memorized patterns from their noisy versions by using a network of neurons. Hence, an ideal network should be able to 1) gradually learn a set of patterns, 2) retrieve the correct pattern from noisy queries and 3) maximize the number of memorized patterns while maintaining the reliability in responding to queries. We show that by considering the inherent redundancy in the memorized patterns, one can obtain all the mentioned properties at once. This is in sharp contrast with the previous work that could only improve one or two aspects at the expense of the third. More specifically, we devise an iterative algorithm that learns the redundancy among the patterns. The resulting network has a  retrieval capacity that is exponential in the size of the network. Lastly, by considering the local structures of the network, the asymptotic error correction performance  can be made linear in the size of the network.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/karbasi13.pdf",
        "supp": "",
        "pdf_size": 360127,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6289140955312844097&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Information Processing Group, Ecole Polytechnique Federale de Lausanne (EPFL), 1015 Lausanne, Switzerland; Algorithms Laboratory, Ecole Polytechnique Federale de Lausanne (EPFL), 1015 Lausanne, Switzerland; Algorithms Laboratory, Ecole Polytechnique Federale de Lausanne (EPFL), 1015 Lausanne, Switzerland",
        "aff_domain": "epfl.ch;epfl.ch;epfl.ch",
        "email": "epfl.ch;epfl.ch;epfl.ch",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Ecole Polytechnique Federale de Lausanne",
        "aff_unique_dep": "Information Processing Group",
        "aff_unique_url": "https://www.epfl.ch",
        "aff_unique_abbr": "EPFL",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Lausanne",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "c623a2d5ff",
        "title": "Joint Transfer and Batch-mode Active Learning",
        "site": "https://proceedings.mlr.press/v28/chattopadhyay13.html",
        "author": "Rita Chattopadhyay; Wei Fan; Ian Davidson; Sethuraman Panchanathan; Jieping Ye",
        "abstract": "Active learning and transfer learning are two different methodologies that address the common  problem of insufficient labels. Transfer learning addresses this problem by using the knowledge gained from a related and already labeled data source, whereas active learning focuses on selecting a small set of informative samples  for manual annotation. Recently, there has been much interest in developing frameworks that combine both transfer and active learning  methodologies. A few such frameworks reported in literature perform transfer and active learning in two separate stages. In this work, we present an integrated framework that performs transfer and active learning simultaneously by solving a  single convex optimization problem. The proposed framework computes the weights of source domain data and selects the samples from the target domain data simultaneously, by minimizing a common objective of reducing distribution difference between the data set consisting of reweighted source and the queried target domain data and the set of unlabeled target domain data. Comprehensive experiments on three real world data sets demonstrate that the proposed method improves the classification accuracy by 5% to  10% over the existing two-stage approach",
        "bibtex": "@InProceedings{pmlr-v28-chattopadhyay13,\n  title = \t {Joint Transfer and Batch-mode Active Learning},\n  author = \t {Chattopadhyay, Rita and Fan, Wei and Davidson, Ian and Panchanathan, Sethuraman and Ye, Jieping},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {253--261},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/chattopadhyay13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/chattopadhyay13.html},\n  abstract = \t {Active learning and transfer learning are two different methodologies that address the common  problem of insufficient labels. Transfer learning addresses this problem by using the knowledge gained from a related and already labeled data source, whereas active learning focuses on selecting a small set of informative samples  for manual annotation. Recently, there has been much interest in developing frameworks that combine both transfer and active learning  methodologies. A few such frameworks reported in literature perform transfer and active learning in two separate stages. In this work, we present an integrated framework that performs transfer and active learning simultaneously by solving a  single convex optimization problem. The proposed framework computes the weights of source domain data and selects the samples from the target domain data simultaneously, by minimizing a common objective of reducing distribution difference between the data set consisting of reweighted source and the queried target domain data and the set of unlabeled target domain data. Comprehensive experiments on three real world data sets demonstrate that the proposed method improves the classification accuracy by 5% to  10% over the existing two-stage approach}\n}",
        "pdf": "http://proceedings.mlr.press/v28/chattopadhyay13.pdf",
        "supp": "",
        "pdf_size": 401742,
        "gs_citation": 118,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17218391461931607737&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Arizona State University; Huawei Noah\u2019s Ark Lab; Department of Computer Science, University of California, Davis; Arizona State University; Arizona State University",
        "aff_domain": "asu.edu;huawei.com;huawei.com;asu.edu;asu.edu",
        "email": "asu.edu;huawei.com;huawei.com;asu.edu;asu.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0;0",
        "aff_unique_norm": "Arizona State University;Huawei;University of California, Davis",
        "aff_unique_dep": ";Noah\u2019s Ark Lab;Department of Computer Science",
        "aff_unique_url": "https://www.asu.edu;https://www.huawei.com;https://www.ucdavis.edu",
        "aff_unique_abbr": "ASU;Huawei;UC Davis",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Davis",
        "aff_country_unique_index": "0;1;0;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "3aad62e3d9",
        "title": "Kernelized Bayesian Matrix Factorization",
        "site": "https://proceedings.mlr.press/v28/gonen13a.html",
        "author": "Mehmet G\u00f6nen; Suleiman Khan; Samuel Kaski",
        "abstract": "We extend kernelized matrix factorization with a fully Bayesian treatment and with an ability to work with multiple side information sources expressed as different kernels. Kernel functions have been introduced to matrix factorization to integrate side information about the rows and columns (e.g., objects and users in recommender systems), which is necessary for making out-of-matrix (i.e., cold start) predictions. We discuss specifically bipartite graph inference, where the output matrix is binary, but extensions to more general matrices are straightforward. We extend the state of the art in two key aspects: (i) A fully conjugate probabilistic formulation of the kernelized matrix factorization problem enables an efficient variational approximation, whereas fully Bayesian treatments are not computationally feasible in the earlier approaches. (ii) Multiple side information sources are included, treated as different kernels in multiple kernel learning that additionally reveals which side information sources are informative. Our method outperforms alternatives in predicting drug-protein interactions on two data sets. We then show that our framework can also be used for solving multilabel learning problems by considering samples and labels as the two domains where matrix factorization operates on. Our algorithm obtains the lowest Hamming loss values on 10 out of 14 multilabel classification data sets compared to five state-of-the-art multilabel learning algorithms.",
        "bibtex": "@InProceedings{pmlr-v28-gonen13a,\n  title = \t {Kernelized Bayesian Matrix Factorization},\n  author = \t {G\u00f6nen, Mehmet and Khan, Suleiman and Kaski, Samuel},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {864--872},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/gonen13a.pdf},\n  url = \t {https://proceedings.mlr.press/v28/gonen13a.html},\n  abstract = \t {We extend kernelized matrix factorization with a fully Bayesian treatment and with an ability to work with multiple side information sources expressed as different kernels. Kernel functions have been introduced to matrix factorization to integrate side information about the rows and columns (e.g., objects and users in recommender systems), which is necessary for making out-of-matrix (i.e., cold start) predictions. We discuss specifically bipartite graph inference, where the output matrix is binary, but extensions to more general matrices are straightforward. We extend the state of the art in two key aspects: (i) A fully conjugate probabilistic formulation of the kernelized matrix factorization problem enables an efficient variational approximation, whereas fully Bayesian treatments are not computationally feasible in the earlier approaches. (ii) Multiple side information sources are included, treated as different kernels in multiple kernel learning that additionally reveals which side information sources are informative. Our method outperforms alternatives in predicting drug-protein interactions on two data sets. We then show that our framework can also be used for solving multilabel learning problems by considering samples and labels as the two domains where matrix factorization operates on. Our algorithm obtains the lowest Hamming loss values on 10 out of 14 multilabel classification data sets compared to five state-of-the-art multilabel learning algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/gonen13a.pdf",
        "supp": "",
        "pdf_size": 377220,
        "gs_citation": 128,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2115714174839206294&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 31,
        "aff": "Helsinki Institute for Information Technology HIIT + Department of Information and Computer Science, Aalto University; Helsinki Institute for Information Technology HIIT + Department of Information and Computer Science, Aalto University; Helsinki Institute for Information Technology HIIT + Department of Information and Computer Science, Aalto University + Department of Computer Science, University of Helsinki",
        "aff_domain": "aalto.fi;aalto.fi;aalto.fi",
        "email": "aalto.fi;aalto.fi;aalto.fi",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1+2",
        "aff_unique_norm": "Helsinki Institute for Information Technology;Aalto University;University of Helsinki",
        "aff_unique_dep": "HIIT;Department of Information and Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.hiit.fi;https://www.aalto.fi;https://www.helsinki.fi",
        "aff_unique_abbr": "HIIT;Aalto;UH",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0+0",
        "aff_country_unique": "Finland"
    },
    {
        "id": "e94ab49a3c",
        "title": "LDA Topic Model with Soft Assignment of Descriptors to Words",
        "site": "https://proceedings.mlr.press/v28/weinshall13.html",
        "author": "Daphna Weinshall; Gal Levi; Dmitri Hanukaev",
        "abstract": "The LDA topic model is being used to model corpora of documents that can be represented by bags of words. Here we extend the LDA model to deal with documents that are represented more naturally by bags of continuous descriptors. Given a finite dictionary of words which are generative models of descriptors, our extended LDA model allows for the soft assignment of descriptors to (many) dictionary words. We derive variational inference and parameter estimation procedures for the extended model, which closely resemble those obtained for the original model, with two important differences: First, the histogram of word counts is replaced by a histogram of pseudo word counts, or sums of responsibilities over all descriptors. Second, parameter estimation now depends on the average covariance matrix between these pseudo-counts, reflecting the fact that with soft assignment words are not independent.    We use this approach to address novelty detection, where we seek to identify video events with low posterior probability. Video events are described by a generative dynamic texture model, from which we naturally derive a dictionary of generative words.  Using a benchmark dataset for novelty detection, we show a very significant improvement in the detection of novel events when using our extended LDA model with soft assignment to words as against hard assignment (the original model), achieving state of the art novelty detection results.",
        "bibtex": "@InProceedings{pmlr-v28-weinshall13,\n  title = \t {LDA Topic Model with Soft Assignment of Descriptors to Words},\n  author = \t {Weinshall, Daphna and Levi, Gal and Hanukaev, Dmitri},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {711--719},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/weinshall13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/weinshall13.html},\n  abstract = \t {The LDA topic model is being used to model corpora of documents that can be represented by bags of words. Here we extend the LDA model to deal with documents that are represented more naturally by bags of continuous descriptors. Given a finite dictionary of words which are generative models of descriptors, our extended LDA model allows for the soft assignment of descriptors to (many) dictionary words. We derive variational inference and parameter estimation procedures for the extended model, which closely resemble those obtained for the original model, with two important differences: First, the histogram of word counts is replaced by a histogram of pseudo word counts, or sums of responsibilities over all descriptors. Second, parameter estimation now depends on the average covariance matrix between these pseudo-counts, reflecting the fact that with soft assignment words are not independent.    We use this approach to address novelty detection, where we seek to identify video events with low posterior probability. Video events are described by a generative dynamic texture model, from which we naturally derive a dictionary of generative words.  Using a benchmark dataset for novelty detection, we show a very significant improvement in the detection of novel events when using our extended LDA model with soft assignment to words as against hard assignment (the original model), achieving state of the art novelty detection results.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/weinshall13.pdf",
        "supp": "",
        "pdf_size": 908601,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5993807427870020632&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "School of Computer Science and Engineering and the Center for Neural Computation, The Hebrew University of Jerusalem, Jerusalem, Israel 91904; School of Computer Science and Engineering and the Center for Neural Computation, The Hebrew University of Jerusalem, Jerusalem, Israel 91904; School of Computer Science and Engineering and the Center for Neural Computation, The Hebrew University of Jerusalem, Jerusalem, Israel 91904",
        "aff_domain": "cs.huji.ac.il;mail.huji.ac.il;mail.huji.ac.il",
        "email": "cs.huji.ac.il;mail.huji.ac.il;mail.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Hebrew University of Jerusalem",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "http://www.huji.ac.il",
        "aff_unique_abbr": "HUJI",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Jerusalem",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "35383da9d2",
        "title": "Label Partitioning For Sublinear Ranking",
        "site": "https://proceedings.mlr.press/v28/weston13.html",
        "author": "Jason Weston; Ameesh Makadia; Hector Yee",
        "abstract": "We consider the case of ranking a very large set of labels, items, or documents, which is common to information retrieval, recommendation, and large-scale annotation tasks. We present a general approach for converting an algorithm which has linear time in the size of the set to a sublinear one via label partitioning. Our method consists of learning an input partition and a label assignment  to each partition of the space such that precision at k is optimized, which is the loss function of interest in this setting. Experiments on large-scale ranking and recommendation tasks show that our method not only makes the original linear time algorithm computationally tractable, but can also improve its performance.",
        "bibtex": "@InProceedings{pmlr-v28-weston13,\n  title = \t {Label Partitioning For Sublinear Ranking},\n  author = \t {Weston, Jason and Makadia, Ameesh and Yee, Hector},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {181--189},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/weston13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/weston13.html},\n  abstract = \t {We consider the case of ranking a very large set of labels, items, or documents, which is common to information retrieval, recommendation, and large-scale annotation tasks. We present a general approach for converting an algorithm which has linear time in the size of the set to a sublinear one via label partitioning. Our method consists of learning an input partition and a label assignment  to each partition of the space such that precision at k is optimized, which is the loss function of interest in this setting. Experiments on large-scale ranking and recommendation tasks show that our method not only makes the original linear time algorithm computationally tractable, but can also improve its performance.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/weston13.pdf",
        "supp": "",
        "pdf_size": 508705,
        "gs_citation": 147,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10978756488698930764&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Google Inc., 76 9th Avenue, New York, NY 10011 USA; Google Inc., 76 9th Avenue, New York, NY 10011 USA; Google Inc., 901 Cherry Avenue, San Bruno, CA 94066 USA",
        "aff_domain": "google.com;google.com;google.com",
        "email": "google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Inc.",
        "aff_unique_url": "https://www.google.com",
        "aff_unique_abbr": "Google",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "New York;San Bruno",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2c5603afbe",
        "title": "Large-Scale Bandit Problems and KWIK Learning",
        "site": "https://proceedings.mlr.press/v28/abernethy13.html",
        "author": "Jacob Abernethy; Kareem Amin; Michael Kearns; Moez Draief",
        "abstract": "We show that parametric multi-armed bandit (MAB) problems with large state and action spaces can be algorithmically reduced to the supervised learning model known as Knows What It Knows or KWIK learning. We give matching impossibility results showing that the KWIK learnability requirement cannot be replaced by weaker supervised learning assumptions. We provide such results in both the standard parametric MAB setting, as well as for a new model in which the action space is finite but growing with time.",
        "bibtex": "@InProceedings{pmlr-v28-abernethy13,\n  title = \t {Large-Scale Bandit Problems and {KWIK} Learning},\n  author = \t {Abernethy, Jacob and Amin, Kareem and Kearns, Michael and Draief, Moez},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {588--596},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/abernethy13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/abernethy13.html},\n  abstract = \t {We show that parametric multi-armed bandit (MAB) problems with large state and action spaces can be algorithmically reduced to the supervised learning model known as Knows What It Knows or KWIK learning. We give matching impossibility results showing that the KWIK learnability requirement cannot be replaced by weaker supervised learning assumptions. We provide such results in both the standard parametric MAB setting, as well as for a new model in which the action space is finite but growing with time.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/abernethy13.pdf",
        "supp": "",
        "pdf_size": 399782,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4569171002750280146&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Computer and Information Science, University of Pennsylvania; Computer and Information Science, University of Pennsylvania; Electrical and Electronic Engineering, Imperial College, London; Computer and Information Science, University of Pennsylvania",
        "aff_domain": "seas.upenn.edu;seas.upenn.edu;imperial.ac.uk;cis.upenn.edu",
        "email": "seas.upenn.edu;seas.upenn.edu;imperial.ac.uk;cis.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Pennsylvania;Imperial College London",
        "aff_unique_dep": "Computer and Information Science;Electrical and Electronic Engineering",
        "aff_unique_url": "https://www.upenn.edu;https://www.imperial.ac.uk",
        "aff_unique_abbr": "UPenn;Imperial",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";London",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "7c2c16b73d",
        "title": "Large-Scale Learning with Less RAM via Randomization",
        "site": "https://proceedings.mlr.press/v28/golovin13.html",
        "author": "Daniel Golovin; D. Sculley; Brendan McMahan; Michael Young",
        "abstract": "We reduce the memory footprint of popular large-scale online learning methods by projecting our weight vector onto a coarse discrete set using randomized rounding. Compared to standard 32-bit float encodings, this reduces RAM usage by more than 50% during training and by up 95% when making predictions from a fixed model, with almost no loss in accuracy. We also show that randomized counting can be used to implement per-coordinate learning rates, improving model quality with little additional RAM. We prove these memory-saving methods achieve regret guarantees similar to their exact variants. Empirical evaluation confirms excellent performance, dominating standard approaches across memory versus accuracy tradeoffs.",
        "bibtex": "@InProceedings{pmlr-v28-golovin13,\n  title = \t {Large-Scale Learning with Less RAM via Randomization},\n  author = \t {Golovin, Daniel and Sculley, D. and McMahan, Brendan and Young, Michael},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {325--333},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/golovin13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/golovin13.html},\n  abstract = \t {We reduce the memory footprint of popular large-scale online learning methods by projecting our weight vector onto a coarse discrete set using randomized rounding. Compared to standard 32-bit float encodings, this reduces RAM usage by more than 50% during training and by up 95% when making predictions from a fixed model, with almost no loss in accuracy. We also show that randomized counting can be used to implement per-coordinate learning rates, improving model quality with little additional RAM. We prove these memory-saving methods achieve regret guarantees similar to their exact variants. Empirical evaluation confirms excellent performance, dominating standard approaches across memory versus accuracy tradeoffs.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/golovin13.pdf",
        "supp": "",
        "pdf_size": 460691,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11064373388829400605&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "Google, Inc., Pittsburgh, PA, and Seattle, WA; Google, Inc., Pittsburgh, PA, and Seattle, WA; Google, Inc., Pittsburgh, PA, and Seattle, WA; Google, Inc., Pittsburgh, PA, and Seattle, WA",
        "aff_domain": "google.com;google.com;google.com;google.com",
        "email": "google.com;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google, Inc.",
        "aff_unique_url": "https://www.google.com",
        "aff_unique_abbr": "Google",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "916236ab6c",
        "title": "Learning Connections in Financial Time Series",
        "site": "https://proceedings.mlr.press/v28/ganeshapillai13.html",
        "author": "Gartheeban Ganeshapillai; John Guttag; Andrew Lo",
        "abstract": "To reduce risk, investors seek assets that have high expected return and are unlikely to move in tandem. Correlation measures are generally used to quantify the connections between equities. The 2008 financial crisis, and its aftermath, demonstrated the need for a better way to quantify these connections. We present a machine learning-based method to build a connectedness matrix to address the shortcomings of correlation in capturing events such as large losses. Our method uses an unconstrained optimization to learn this matrix, while ensuring that the resulting matrix is positive semi-definite. We show that this matrix can be used to build portfolios that not only \u201cbeat the market,\u201d  but also outperform optimal (i.e., minimum variance) portfolios.",
        "bibtex": "@InProceedings{pmlr-v28-ganeshapillai13,\n  title = \t {Learning Connections in Financial Time Series},\n  author = \t {Ganeshapillai, Gartheeban and Guttag, John and Lo, Andrew},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {109--117},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/ganeshapillai13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/ganeshapillai13.html},\n  abstract = \t {To reduce risk, investors seek assets that have high expected return and are unlikely to move in tandem. Correlation measures are generally used to quantify the connections between equities. The 2008 financial crisis, and its aftermath, demonstrated the need for a better way to quantify these connections. We present a machine learning-based method to build a connectedness matrix to address the shortcomings of correlation in capturing events such as large losses. Our method uses an unconstrained optimization to learn this matrix, while ensuring that the resulting matrix is positive semi-definite. We show that this matrix can be used to build portfolios that not only \u201cbeat the market,\u201d  but also outperform optimal (i.e., minimum variance) portfolios.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/ganeshapillai13.pdf",
        "supp": "",
        "pdf_size": 531480,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7797397309216266961&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Massachusetts Institute of Technology, 77 Massachusetts Ave, Cambridge MA 02139 USA; Massachusetts Institute of Technology, 77 Massachusetts Ave, Cambridge MA 02139 USA; Massachusetts Institute of Technology, 77 Massachusetts Ave, Cambridge MA 02139 USA",
        "aff_domain": "mit.edu;mit.edu;mit.edu",
        "email": "mit.edu;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "cfe30f2a9b",
        "title": "Learning Convex QP Relaxations for Structured Prediction",
        "site": "https://proceedings.mlr.press/v28/jancsary13.html",
        "author": "Jeremy Jancsary; Sebastian Nowozin; Carsten Rother",
        "abstract": "We introduce a new large margin approach to discriminative training of intractable discrete graphical models. Our approach builds on a convex quadratic programming relaxation of the MAP inference problem. The model parameters are trained directly within this restricted class of energy functions so as to optimize the predictions on the training data. We address the issue of how to parameterize the resulting model and point out its relation to existing approaches. The primary motivation behind our use of the QP relaxation is its computational efficiency; yet, empirically, its predictive accuracy compares favorably to more expensive approaches. This makes it an appealing choice for many practical tasks.",
        "bibtex": "@InProceedings{pmlr-v28-jancsary13,\n  title = \t {Learning Convex QP Relaxations for Structured Prediction},\n  author = \t {Jancsary, Jeremy and Nowozin, Sebastian and Rother, Carsten},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {915--923},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/jancsary13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/jancsary13.html},\n  abstract = \t {We introduce a new large margin approach to discriminative training of intractable discrete graphical models. Our approach builds on a convex quadratic programming relaxation of the MAP inference problem. The model parameters are trained directly within this restricted class of energy functions so as to optimize the predictions on the training data. We address the issue of how to parameterize the resulting model and point out its relation to existing approaches. The primary motivation behind our use of the QP relaxation is its computational efficiency; yet, empirically, its predictive accuracy compares favorably to more expensive approaches. This makes it an appealing choice for many practical tasks.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/jancsary13.pdf",
        "supp": "",
        "pdf_size": 607396,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12724334879365377958&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Microsoft Research Cambridge, 21 Station Road, Cambridge, CB1 2FB, United Kingdom; Microsoft Research Cambridge, 21 Station Road, Cambridge, CB1 2FB, United Kingdom; Microsoft Research Cambridge, 21 Station Road, Cambridge, CB1 2FB, United Kingdom",
        "aff_domain": "microsoft.com;microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Microsoft Research",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/group/microsoft-research-cambridge",
        "aff_unique_abbr": "MSR Cambridge",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "37e40e0645",
        "title": "Learning Fair Representations",
        "site": "https://proceedings.mlr.press/v28/zemel13.html",
        "author": "Rich Zemel; Yu Wu; Kevin Swersky; Toni Pitassi; Cynthia Dwork",
        "abstract": "We propose a learning algorithm for fair classification that achieves both group fairness (the proportion of members in a protected group receiving positive classification is identical to the proportion in the population as a  whole), and individual fairness (similar individuals should be treated similarly).  We formulate fairness as an optimization problem of finding a  good representation of the data with two competing goals: to encode the data as well as possible, while simultaneously obfuscating any information about membership in the protected group.  We show positive results of our algorithm relative to other known techniques, on three datasets.  Moreover, we demonstrate several advantages to our approach.  First, our intermediate representation can be used for other classification tasks (i.e., transfer  learning is possible); secondly, we take a step toward learning a distance metric which can find important dimensions of the data for classification.",
        "bibtex": "@InProceedings{pmlr-v28-zemel13,\n  title = \t {Learning Fair Representations},\n  author = \t {Zemel, Rich and Wu, Yu and Swersky, Kevin and Pitassi, Toni and Dwork, Cynthia},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {325--333},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/zemel13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/zemel13.html},\n  abstract = \t {We propose a learning algorithm for fair classification that achieves both group fairness (the proportion of members in a protected group receiving positive classification is identical to the proportion in the population as a  whole), and individual fairness (similar individuals should be treated similarly).  We formulate fairness as an optimization problem of finding a  good representation of the data with two competing goals: to encode the data as well as possible, while simultaneously obfuscating any information about membership in the protected group.  We show positive results of our algorithm relative to other known techniques, on three datasets.  Moreover, we demonstrate several advantages to our approach.  First, our intermediate representation can be used for other classification tasks (i.e., transfer  learning is possible); secondly, we take a step toward learning a distance metric which can find important dimensions of the data for classification.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/zemel13.pdf",
        "supp": "",
        "pdf_size": 501796,
        "gs_citation": 2431,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6130171396735864663&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "University of Toronto; University of Toronto; University of Toronto; University of Toronto; Microsoft Research",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu;cs.toronto.edu;microsoft.com",
        "email": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu;cs.toronto.edu;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "University of Toronto;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.utoronto.ca;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "U of T;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;1",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "513aad87d6",
        "title": "Learning Hash Functions Using Column Generation",
        "site": "https://proceedings.mlr.press/v28/li13a.html",
        "author": "Xi Li; Guosheng Lin; Chunhua Shen; Anton Hengel; Anthony Dick",
        "abstract": "Fast nearest neighbor searching is becoming  an increasingly important tool in solving many  large-scale problems. Recently a number of approaches  to learning data-dependent hash functions  have been developed. In this work,  we propose a column generation based method  for learning data-dependent hash functions on  the basis of proximity comparison information.  Given a set of triplets that encode the pairwise  proximity comparison information, our method  learns hash functions that preserve the relative  comparison relationships in the data as well as  possible within the large-margin learning framework.  The learning procedure is implemented  using column generation and hence is named  CGHash. At each iteration of the column generation  procedure, the best hash function is selected.  Unlike most other hashing methods, our  method generalizes to new data points naturally;  and has a training objective which is convex, thus  ensuring that the global optimum can be identified.  Experiments demonstrate that the proposed  method learns compact binary codes and that its  retrieval performance compares favorably with  state-of-the-art methods when tested on a few  benchmark datasets.",
        "bibtex": "@InProceedings{pmlr-v28-li13a,\n  title = \t {Learning Hash Functions Using Column Generation},\n  author = \t {Li, Xi and Lin, Guosheng and Shen, Chunhua and Hengel, Anton and Dick, Anthony},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {142--150},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/li13a.pdf},\n  url = \t {https://proceedings.mlr.press/v28/li13a.html},\n  abstract = \t {Fast nearest neighbor searching is becoming  an increasingly important tool in solving many  large-scale problems. Recently a number of approaches  to learning data-dependent hash functions  have been developed. In this work,  we propose a column generation based method  for learning data-dependent hash functions on  the basis of proximity comparison information.  Given a set of triplets that encode the pairwise  proximity comparison information, our method  learns hash functions that preserve the relative  comparison relationships in the data as well as  possible within the large-margin learning framework.  The learning procedure is implemented  using column generation and hence is named  CGHash. At each iteration of the column generation  procedure, the best hash function is selected.  Unlike most other hashing methods, our  method generalizes to new data points naturally;  and has a training objective which is convex, thus  ensuring that the global optimum can be identified.  Experiments demonstrate that the proposed  method learns compact binary codes and that its  retrieval performance compares favorably with  state-of-the-art methods when tested on a few  benchmark datasets.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/li13a.pdf",
        "supp": "",
        "pdf_size": 1841935,
        "gs_citation": 158,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13587316684756581759&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Australian Centre for Visual Technologies, School of Computer Science, The University of Adelaide, Australia; Australian Centre for Visual Technologies, School of Computer Science, The University of Adelaide, Australia; Australian Centre for Visual Technologies, School of Computer Science, The University of Adelaide, Australia; Australian Centre for Visual Technologies, School of Computer Science, The University of Adelaide, Australia; Australian Centre for Visual Technologies, School of Computer Science, The University of Adelaide, Australia",
        "aff_domain": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "email": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Adelaide",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.adelaide.edu.au",
        "aff_unique_abbr": "Adelaide",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "2537995179",
        "title": "Learning Heteroscedastic Models by Convex Programming under Group Sparsity",
        "site": "https://proceedings.mlr.press/v28/dalalyan13.html",
        "author": "Arnak Dalalyan; Mohamed Hebiri; Katia Meziani; Joseph Salmon",
        "abstract": "Sparse estimation methods based on l1 relaxation,  such as the Lasso and the Dantzig  selector, require the knowledge of the variance  of the noise in order to properly tune the  regularization parameter. This constitutes a   major obstacle in applying these methods in  several frameworks, such as time series, random fields, inverse problems, for which noise  is rarely homoscedastic and the noise level is  hard to know in advance.  In this paper, we  propose a new approach to the joint estimation  of the conditional mean and the conditional  variance in a high-dimensional (auto-)  regression setting.   An attractive feature of the proposed estimator is   that it is efficiently computable even for very large   scale problems by solving a second-order cone program  (SOCP). We present theoretical analysis and  numerical results assessing the performance  of the proposed procedure.",
        "bibtex": "@InProceedings{pmlr-v28-dalalyan13,\n  title = \t {Learning Heteroscedastic Models by Convex Programming under Group Sparsity},\n  author = \t {Dalalyan, Arnak and Hebiri, Mohamed and Meziani, Katia and Salmon, Joseph},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {379--387},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/dalalyan13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/dalalyan13.html},\n  abstract = \t {Sparse estimation methods based on l1 relaxation,  such as the Lasso and the Dantzig  selector, require the knowledge of the variance  of the noise in order to properly tune the  regularization parameter. This constitutes a   major obstacle in applying these methods in  several frameworks, such as time series, random fields, inverse problems, for which noise  is rarely homoscedastic and the noise level is  hard to know in advance.  In this paper, we  propose a new approach to the joint estimation  of the conditional mean and the conditional  variance in a high-dimensional (auto-)  regression setting.   An attractive feature of the proposed estimator is   that it is efficiently computable even for very large   scale problems by solving a second-order cone program  (SOCP). We present theoretical analysis and  numerical results assessing the performance  of the proposed procedure.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/dalalyan13.pdf",
        "supp": "",
        "pdf_size": 501666,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15510440177054168402&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff": "ENSAE-CREST-GENES; LAMA, Universit\u00e9 Paris Est; CEREMADE, Universit\u00e9 Paris Dauphine; Institut Mines-T\u00e9l\u00e9com + T\u00e9l\u00e9com ParisTech + CNRS LTCI",
        "aff_domain": "ensae.fr;univ-mlv.fr;ceremade.dauphine.fr;telecom-paristech.fr",
        "email": "ensae.fr;univ-mlv.fr;ceremade.dauphine.fr;telecom-paristech.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3+4+5",
        "aff_unique_norm": "ENSAE-CREST;Universit\u00e9 Paris Est;Universit\u00e9 Paris Dauphine;Institut Mines-T\u00e9l\u00e9com;T\u00e9l\u00e9com ParisTech;CNRS",
        "aff_unique_dep": "GENES;LAMA;CEREMADE;;;Laboratoire Traitement du signal et des images",
        "aff_unique_url": "https://www.ensae.fr;https://www.univ-paris-est.fr;https://www.univ-paris-dauphine.fr;https://www.imt.fr;https://www.telecom-paristech.fr;https://www.ltci.cnrs.fr",
        "aff_unique_abbr": "ENSAE;;;IMT;TP;LTCI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+0+0",
        "aff_country_unique": "France"
    },
    {
        "id": "0cd0de39b7",
        "title": "Learning Linear Bayesian Networks with Latent Variables",
        "site": "https://proceedings.mlr.press/v28/anandkumar13.html",
        "author": "Animashree Anandkumar; Daniel Hsu; Adel Javanmard; Sham Kakade",
        "abstract": "This work considers the problem of learning linear Bayesian networks when  some of the variables are unobserved.  Identifiability and efficient recovery from low-order observable moments  are established under a novel graphical constraint.  The constraint concerns the expansion properties of the underlying directed  acyclic graph (DAG) between observed and unobserved variables in the  network, and it is satisfied by many natural families of DAGs that include  multi-level DAGs, DAGs with effective depth one, as well as certain  families of polytrees.",
        "bibtex": "@InProceedings{pmlr-v28-anandkumar13,\n  title = \t {Learning Linear Bayesian Networks with Latent Variables},\n  author = \t {Anandkumar, Animashree and Hsu, Daniel and Javanmard, Adel and Kakade, Sham},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {249--257},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/anandkumar13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/anandkumar13.html},\n  abstract = \t {This work considers the problem of learning linear Bayesian networks when  some of the variables are unobserved.  Identifiability and efficient recovery from low-order observable moments  are established under a novel graphical constraint.  The constraint concerns the expansion properties of the underlying directed  acyclic graph (DAG) between observed and unobserved variables in the  network, and it is satisfied by many natural families of DAGs that include  multi-level DAGs, DAGs with effective depth one, as well as certain  families of polytrees.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/anandkumar13.pdf",
        "supp": "",
        "pdf_size": 286218,
        "gs_citation": 119,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1284502555709346671&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Department of EECS, University of California, Irvine; Microsoft Research New England; Department of Electrical Engineering, Stanford University; Microsoft Research New England",
        "aff_domain": "uci.edu;microsoft.com;stanford.edu;microsoft.com",
        "email": "uci.edu;microsoft.com;stanford.edu;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;1",
        "aff_unique_norm": "University of California, Irvine;Microsoft;Stanford University",
        "aff_unique_dep": "Department of EECS;Microsoft Research;Department of Electrical Engineering",
        "aff_unique_url": "https://www.uci.edu;https://www.microsoft.com/en-us/research/group/microsoft-research-new-england;https://www.stanford.edu",
        "aff_unique_abbr": "UCI;MSR NE;Stanford",
        "aff_campus_unique_index": "0;1;2;1",
        "aff_campus_unique": "Irvine;New England;Stanford",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e4e370b837",
        "title": "Learning Multiple Behaviors from Unlabeled Demonstrations in a Latent Controller Space",
        "site": "https://proceedings.mlr.press/v28/almingol13.html",
        "author": "Javier Almingol; Lui Montesano; Manuel Lopes",
        "abstract": "In this paper we introduce a method to learn multiple behaviors in the form of motor primitives from an unlabeled dataset. One of the difficulties of this problem is that in the measurement space, behaviors can be very mixed, despite existing a latent representation where they can be easily separated.  We propose a mixture model based on Dirichlet Process (DP) to simultaneously cluster the observed time-series and recover a sparse representation of the behaviors using a Laplacian prior as the base measure of the DP. We show that for linear models, e.g potential functions generated by linear combinations of a large number of features, it is possible to compute analytically the marginal of the observations and derive an efficient sampler. The method is evaluated using robot behaviors and real data from human motion and compared to other techniques.",
        "bibtex": "@InProceedings{pmlr-v28-almingol13,\n  title = \t {Learning Multiple Behaviors from Unlabeled Demonstrations in a Latent Controller Space},\n  author = \t {Almingol, Javier and Montesano, Lui and Lopes, Manuel},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {136--144},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/almingol13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/almingol13.html},\n  abstract = \t {In this paper we introduce a method to learn multiple behaviors in the form of motor primitives from an unlabeled dataset. One of the difficulties of this problem is that in the measurement space, behaviors can be very mixed, despite existing a latent representation where they can be easily separated.  We propose a mixture model based on Dirichlet Process (DP) to simultaneously cluster the observed time-series and recover a sparse representation of the behaviors using a Laplacian prior as the base measure of the DP. We show that for linear models, e.g potential functions generated by linear combinations of a large number of features, it is possible to compute analytically the marginal of the observations and derive an efficient sampler. The method is evaluated using robot behaviors and real data from human motion and compared to other techniques.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/almingol13.pdf",
        "supp": "",
        "pdf_size": 1416816,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16693202989570034546&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "I3A, Universidad de Zaragoza, Spain; I3A, Universidad de Zaragoza, Spain; Inria Bordeaux Sud-Ouest, France",
        "aff_domain": "unizar.es;unizar.es;inria.fr",
        "email": "unizar.es;unizar.es;inria.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Universidad de Zaragoza;Inria Bordeaux Sud-Ouest",
        "aff_unique_dep": "I3A;",
        "aff_unique_url": "https://www.unizar.es;https://www.inria.fr/centre/bordeaux-sud-ouest",
        "aff_unique_abbr": ";Inria Bordeaux",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Bordeaux",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Spain;France"
    },
    {
        "id": "7424bfcad2",
        "title": "Learning Optimally Sparse Support Vector Machines",
        "site": "https://proceedings.mlr.press/v28/cotter13.html",
        "author": "Andrew Cotter; Shai Shalev-Shwartz; Nati Srebro",
        "abstract": "We show how to train SVMs with an optimal guarantee on the number of support vectors (up to constants), and with sample complexity and training runtime bounds matching the best known for kernel SVM optimization (i.e. without any additional asymptotic cost beyond standard SVM training). Our method is simple to implement and works well in practice.",
        "bibtex": "@InProceedings{pmlr-v28-cotter13,\n  title = \t {Learning Optimally Sparse Support Vector Machines},\n  author = \t {Cotter, Andrew and Shalev-Shwartz, Shai and Srebro, Nati},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {266--274},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/cotter13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/cotter13.html},\n  abstract = \t {We show how to train SVMs with an optimal guarantee on the number of support vectors (up to constants), and with sample complexity and training runtime bounds matching the best known for kernel SVM optimization (i.e. without any additional asymptotic cost beyond standard SVM training). Our method is simple to implement and works well in practice.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/cotter13.pdf",
        "supp": "",
        "pdf_size": 428340,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6430962497445042687&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "6a091032cb",
        "title": "Learning Policies for Contextual Submodular Prediction",
        "site": "https://proceedings.mlr.press/v28/ross13b.html",
        "author": "Stephane Ross; Jiaji Zhou; Yisong Yue; Debadeepta Dey; Drew Bagnell",
        "abstract": "Many prediction domains, such as ad placement, recommendation, trajectory prediction, and document summarization, require predicting a set or list of options. Such lists are often evaluated using submodular reward functions that measure both quality and diversity. We propose a simple, efficient, and provably near-optimal approach to optimizing such prediction problems based on no-regret learning. Our method leverages a surprising result from online submodular optimization: a single no-regret online learner can compete with an optimal sequence of predictions. Compared to previous work, which either learn a sequence of classifiers or rely on stronger assumptions such as realizability, we ensure both data-efficiency as well as performance guarantees in the fully agnostic setting. Experiments validate the efficiency and applicability of the approach on a wide range of problems including manipulator trajectory optimization, news recommendation and document summarization.",
        "bibtex": "@InProceedings{pmlr-v28-ross13b,\n  title = \t {Learning Policies for Contextual Submodular Prediction},\n  author = \t {Ross, Stephane and Zhou, Jiaji and Yue, Yisong and Dey, Debadeepta and Bagnell, Drew},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1364--1372},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/ross13b.pdf},\n  url = \t {https://proceedings.mlr.press/v28/ross13b.html},\n  abstract = \t {Many prediction domains, such as ad placement, recommendation, trajectory prediction, and document summarization, require predicting a set or list of options. Such lists are often evaluated using submodular reward functions that measure both quality and diversity. We propose a simple, efficient, and provably near-optimal approach to optimizing such prediction problems based on no-regret learning. Our method leverages a surprising result from online submodular optimization: a single no-regret online learner can compete with an optimal sequence of predictions. Compared to previous work, which either learn a sequence of classifiers or rely on stronger assumptions such as realizability, we ensure both data-efficiency as well as performance guarantees in the fully agnostic setting. Experiments validate the efficiency and applicability of the approach on a wide range of problems including manipulator trajectory optimization, news recommendation and document summarization.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/ross13b.pdf",
        "supp": "",
        "pdf_size": 478090,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15545315741116867593&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA",
        "aff_domain": "cmu.edu;andrew.cmu.edu;cmu.edu;cs.cmu.edu;ri.cmu.edu",
        "email": "cmu.edu;andrew.cmu.edu;cmu.edu;cs.cmu.edu;ri.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7b1fd1afb1",
        "title": "Learning Sparse Penalties for Change-point Detection using Max Margin Interval Regression",
        "site": "https://proceedings.mlr.press/v28/hocking13.html",
        "author": "Toby Hocking; Guillem Rigaill; Jean-Philippe Vert; Francis Bach",
        "abstract": "In segmentation models, the number of change-points is typically    chosen using a penalized cost function.  In this work, we propose to    learn the penalty and its constants in databases of signals with    weak change-point annotations. We propose a convex relaxation for    the resulting interval regression problem, and solve it using    accelerated proximal gradient methods. We show that this method    achieves state-of-the-art change-point detection in a database of    annotated DNA copy number profiles from neuroblastoma tumors.",
        "bibtex": "@InProceedings{pmlr-v28-hocking13,\n  title = \t {Learning Sparse Penalties for Change-point Detection using Max Margin Interval Regression},\n  author = \t {Hocking, Toby and Rigaill, Guillem and Vert, Jean-Philippe and Bach, Francis},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {172--180},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/hocking13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/hocking13.html},\n  abstract = \t {In segmentation models, the number of change-points is typically    chosen using a penalized cost function.  In this work, we propose to    learn the penalty and its constants in databases of signals with    weak change-point annotations. We propose a convex relaxation for    the resulting interval regression problem, and solve it using    accelerated proximal gradient methods. We show that this method    achieves state-of-the-art change-point detection in a database of    annotated DNA copy number profiles from neuroblastoma tumors.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/hocking13.pdf",
        "supp": "",
        "pdf_size": 470640,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10123569871572076594&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "Unit\u00e9 de Recherche en G\u00e9nomique V\u00e9g\u00e9tale INRA-CNRS-Universit\u00e9 d\u2019Evry Val d\u2019Essonne, Evry, France; INRIA \u2013 Sierra project-team, D\u00e9partement d\u2019Informatique de l\u2019\u00c9cole Normale Sup\u00e9rieure, Paris, France; INRIA \u2013 Sierra project-team, D\u00e9partement d\u2019Informatique de l\u2019\u00c9cole Normale Sup\u00e9rieure, Paris, France; Mines ParisTech \u2013 CBIO, INSERM U900, Institut Curie, Paris, France",
        "aff_domain": "evry.inra.fr;inria.fr;inria.fr;mines.org",
        "email": "evry.inra.fr;inria.fr;inria.fr;mines.org",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;2",
        "aff_unique_norm": "INRA-CNRS-Universit\u00e9 d\u2019Evry Val d\u2019Essonne;INRIA;MINES ParisTech",
        "aff_unique_dep": "Unit\u00e9 de Recherche en G\u00e9nomique V\u00e9g\u00e9tale;D\u00e9partement d\u2019Informatique de l\u2019\u00c9cole Normale Sup\u00e9rieure;CBIO",
        "aff_unique_url": ";https://www.inria.fr;https://www.minesparistech.fr",
        "aff_unique_abbr": ";INRIA;Mines ParisTech",
        "aff_campus_unique_index": "0;1;1;1",
        "aff_campus_unique": "Evry;Paris",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "cf91cd14fc",
        "title": "Learning Spatio-Temporal Structure from RGB-D Videos for Human Activity Detection and Anticipation",
        "site": "https://proceedings.mlr.press/v28/koppula13.html",
        "author": "Hema Koppula; Ashutosh Saxena",
        "abstract": "We consider the problem of detecting past activities as well as anticipating which activity will happen in the future and how. We start by modeling the rich spatio-temporal relations between human poses and objects (called affordances) using a conditional random field (CRF). However, because of the ambiguity in the temporal segmentation of the sub-activities that constitute an activity, in the past as well as in the future, multiple graph structures are possible. In this paper, we reason about these alternate possibilities by reasoning over multiple possible graph structures. We obtain them by approximating the graph with only additive features, which lends to efficient dynamic programming. Starting with this proposal graph structure, we then design moves to obtain several other likely graph structures. We then show that our approach improves the state-of-the-art significantly for detecting past activities as well as for anticipating future activities, on a dataset of 120 activity videos collected from four subjects.",
        "bibtex": "@InProceedings{pmlr-v28-koppula13,\n  title = \t {Learning Spatio-Temporal Structure from RGB-D Videos for Human Activity Detection and Anticipation},\n  author = \t {Koppula, Hema and Saxena, Ashutosh},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {792--800},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/koppula13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/koppula13.html},\n  abstract = \t {We consider the problem of detecting past activities as well as anticipating which activity will happen in the future and how. We start by modeling the rich spatio-temporal relations between human poses and objects (called affordances) using a conditional random field (CRF). However, because of the ambiguity in the temporal segmentation of the sub-activities that constitute an activity, in the past as well as in the future, multiple graph structures are possible. In this paper, we reason about these alternate possibilities by reasoning over multiple possible graph structures. We obtain them by approximating the graph with only additive features, which lends to efficient dynamic programming. Starting with this proposal graph structure, we then design moves to obtain several other likely graph structures. We then show that our approach improves the state-of-the-art significantly for detecting past activities as well as for anticipating future activities, on a dataset of 120 activity videos collected from four subjects.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/koppula13.pdf",
        "supp": "",
        "pdf_size": 1097757,
        "gs_citation": 270,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7378795695637276427&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Computer Science Department, Cornell University, Ithaca, NY 14853 USA; Computer Science Department, Cornell University, Ithaca, NY 14853 USA",
        "aff_domain": "cs.cornell.edu;cs.cornell.edu",
        "email": "cs.cornell.edu;cs.cornell.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Cornell University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.cornell.edu",
        "aff_unique_abbr": "Cornell",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Ithaca",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "73ca93c628",
        "title": "Learning Triggering Kernels for Multi-dimensional Hawkes Processes",
        "site": "https://proceedings.mlr.press/v28/zhou13.html",
        "author": "Ke Zhou; Hongyuan Zha; Le Song",
        "abstract": "How does the activity of one person affect that of another person? Does the strength of influence remain periodic or decay exponentially over time? In this paper, we study these critical questions in social network analysis quantitatively under the framework of multi-dimensional Hawkes processes. In particular, we focus on the nonparametric learning of the  triggering kernels, and propose an algorithm \\sf MMEL that combines the idea of decoupling the parameters through constructing a tight upper-bound of the objective function and application of Euler-Lagrange equations for optimization in infinite dimensional functional space.    We show that the proposed method performs significantly better than alternatives in experiments on both synthetic and real world datasets.",
        "bibtex": "@InProceedings{pmlr-v28-zhou13,\n  title = \t {Learning Triggering Kernels for Multi-dimensional Hawkes Processes},\n  author = \t {Zhou, Ke and Zha, Hongyuan and Song, Le},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1301--1309},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/zhou13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/zhou13.html},\n  abstract = \t {How does the activity of one person affect that of another person? Does the strength of influence remain periodic or decay exponentially over time? In this paper, we study these critical questions in social network analysis quantitatively under the framework of multi-dimensional Hawkes processes. In particular, we focus on the nonparametric learning of the  triggering kernels, and propose an algorithm \\sf MMEL that combines the idea of decoupling the parameters through constructing a tight upper-bound of the objective function and application of Euler-Lagrange equations for optimization in infinite dimensional functional space.    We show that the proposed method performs significantly better than alternatives in experiments on both synthetic and real world datasets.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/zhou13.pdf",
        "supp": "",
        "pdf_size": 242128,
        "gs_citation": 301,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14677960205377188473&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology",
        "aff_domain": "gatech.edu;cc.gatech.edu;cc.gatech.edu",
        "email": "gatech.edu;cc.gatech.edu;cc.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Georgia Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.gatech.edu",
        "aff_unique_abbr": "Georgia Tech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9a40a80869",
        "title": "Learning an Internal Dynamics Model from Control Demonstration",
        "site": "https://proceedings.mlr.press/v28/golub13.html",
        "author": "Matthew Golub; Steven Chase; Byron Yu",
        "abstract": "Much work in optimal control and inverse control has assumed that the controller has perfect knowledge of plant dynamics.  However, if the controller is a human or animal subject, the subject\u2019s internal dynamics model may differ from the true plant dynamics.  Here, we consider the problem of learning the subject\u2019s internal model from demonstrations of control and knowledge of task goals.  Due to sensory feedback delay, the subject uses an internal model to generate an internal prediction of the current plant state, which may differ from the actual plant state.  We develop a probabilistic framework and exact EM algorithm to jointly estimate the internal model, internal state trajectories, and feedback delay. We applied this framework to demonstrations by a nonhuman primate of brain-machine interface (BMI) control. We discovered that the subject\u2019s internal model deviated from the true BMI plant dynamics and provided significantly better explanation of the recorded neural control signals than did the true plant dynamics.",
        "bibtex": "@InProceedings{pmlr-v28-golub13,\n  title = \t {Learning an Internal Dynamics Model from Control Demonstration},\n  author = \t {Golub, Matthew and Chase, Steven and Yu, Byron},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {606--614},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/golub13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/golub13.html},\n  abstract = \t {Much work in optimal control and inverse control has assumed that the controller has perfect knowledge of plant dynamics.  However, if the controller is a human or animal subject, the subject\u2019s internal dynamics model may differ from the true plant dynamics.  Here, we consider the problem of learning the subject\u2019s internal model from demonstrations of control and knowledge of task goals.  Due to sensory feedback delay, the subject uses an internal model to generate an internal prediction of the current plant state, which may differ from the actual plant state.  We develop a probabilistic framework and exact EM algorithm to jointly estimate the internal model, internal state trajectories, and feedback delay. We applied this framework to demonstrations by a nonhuman primate of brain-machine interface (BMI) control. We discovered that the subject\u2019s internal model deviated from the true BMI plant dynamics and provided significantly better explanation of the recorded neural control signals than did the true plant dynamics. }\n}",
        "pdf": "http://proceedings.mlr.press/v28/golub13.pdf",
        "supp": "",
        "pdf_size": 330557,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1845581395526252297&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": "cmu.edu;cmu.edu;cmu.edu",
        "email": "cmu.edu;cmu.edu;cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "381a05f186",
        "title": "Learning and Selecting Features Jointly with Point-wise Gated Boltzmann Machines",
        "site": "https://proceedings.mlr.press/v28/sohn13.html",
        "author": "Kihyuk Sohn; Guanyu Zhou; Chansoo Lee; Honglak Lee",
        "abstract": "Unsupervised feature learning has emerged as a promising tool in learning representations from unlabeled data. However, it is still challenging to learn useful high-level features when the data contains a significant amount of irrelevant patterns. Although feature selection can be used for such complex data, it may fail when we have to build a learning system from scratch (i.e., starting from the lack of useful raw features). To address this problem, we propose a point-wise gated Boltzmann machine, a unified generative model that combines feature learning and feature selection. Our model performs not only feature selection on learned high-level features (i.e., hidden units), but also dynamic feature selection on raw features (i.e., visible units) through a gating mechanism. For each example, the model can adaptively focus on a variable subset of visible nodes corresponding to the task-relevant patterns, while ignoring the visible units corresponding to the task-irrelevant patterns. In experiments, our method achieves improved performance over state-of-the-art in several visual recognition benchmarks.",
        "bibtex": "@InProceedings{pmlr-v28-sohn13,\n  title = \t {Learning and Selecting Features Jointly with Point-wise Gated {B}oltzmann Machines},\n  author = \t {Sohn, Kihyuk and Zhou, Guanyu and Lee, Chansoo and Lee, Honglak},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {217--225},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/sohn13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/sohn13.html},\n  abstract = \t {Unsupervised feature learning has emerged as a promising tool in learning representations from unlabeled data. However, it is still challenging to learn useful high-level features when the data contains a significant amount of irrelevant patterns. Although feature selection can be used for such complex data, it may fail when we have to build a learning system from scratch (i.e., starting from the lack of useful raw features). To address this problem, we propose a point-wise gated Boltzmann machine, a unified generative model that combines feature learning and feature selection. Our model performs not only feature selection on learned high-level features (i.e., hidden units), but also dynamic feature selection on raw features (i.e., visible units) through a gating mechanism. For each example, the model can adaptively focus on a variable subset of visible nodes corresponding to the task-relevant patterns, while ignoring the visible units corresponding to the task-irrelevant patterns. In experiments, our method achieves improved performance over state-of-the-art in several visual recognition benchmarks. }\n}",
        "pdf": "http://proceedings.mlr.press/v28/sohn13.pdf",
        "supp": "",
        "pdf_size": 3897751,
        "gs_citation": 131,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15502760445574052049&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Dept. of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI 48109, USA; Dept. of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI 48109, USA; Dept. of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI 48109, USA; Dept. of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI 48109, USA",
        "aff_domain": "umich.edu;umich.edu;umich.edu;umich.edu",
        "email": "umich.edu;umich.edu;umich.edu;umich.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Dept. of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3c67bee52a",
        "title": "Learning from Human-Generated Lists",
        "site": "https://proceedings.mlr.press/v28/jun13.html",
        "author": "Kwang-Sung Jun; Jerry Zhu; Burr Settles; Timothy Rogers",
        "abstract": "Human-generated lists are a form of non-iid data with important applications in machine learning and cognitive psychology. We propose a generative model - sampling with reduced replacement (SWIRL) - for such lists. We discuss SWIRL\u2019s relation to standard sampling paradigms, provide the maximum likelihood estimate for learning, and demonstrate its value with two real-world applications: (i) In a \"\"feature volunteering\"\" task where non-experts spontaneously generate feature=>label pairs for text classification, SWIRL improves the accuracy of state-of-the-art feature-learning frameworks. (ii) In a \"\"verbal fluency\"\" task where brain-damaged patients generate word lists when prompted with a category, SWIRL parameters align well with existing psychological theories, and our model can classify healthy people vs. patients from the lists they generate.",
        "bibtex": "@InProceedings{pmlr-v28-jun13,\n  title = \t {Learning from Human-Generated Lists},\n  author = \t {Jun, Kwang-Sung and Zhu, Jerry and Settles, Burr and Rogers, Timothy},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {181--189},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/jun13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/jun13.html},\n  abstract = \t {Human-generated lists are a form of non-iid data with important applications in machine learning and cognitive psychology. We propose a generative model - sampling with reduced replacement (SWIRL) - for such lists. We discuss SWIRL\u2019s relation to standard sampling paradigms, provide the maximum likelihood estimate for learning, and demonstrate its value with two real-world applications: (i) In a \"\"feature volunteering\"\" task where non-experts spontaneously generate feature=>label pairs for text classification, SWIRL improves the accuracy of state-of-the-art feature-learning frameworks. (ii) In a \"\"verbal fluency\"\" task where brain-damaged patients generate word lists when prompted with a category, SWIRL parameters align well with existing psychological theories, and our model can classify healthy people vs. patients from the lists they generate.    }\n}",
        "pdf": "http://proceedings.mlr.press/v28/jun13.pdf",
        "supp": "",
        "pdf_size": 447196,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13528377688852858985&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Sciences, University of Wisconsin-Madison; Department of Computer Sciences, University of Wisconsin-Madison; Machine Learning Department, Carnegie Mellon University; Department of Psychology, University of Wisconsin-Madison",
        "aff_domain": "cs.wisc.edu;cs.wisc.edu;cs.cmu.edu;wisc.edu",
        "email": "cs.wisc.edu;cs.wisc.edu;cs.cmu.edu;wisc.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Wisconsin-Madison;Carnegie Mellon University",
        "aff_unique_dep": "Department of Computer Sciences;Machine Learning Department",
        "aff_unique_url": "https://www.wisc.edu;https://www.cmu.edu",
        "aff_unique_abbr": "UW-Madison;CMU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Madison;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4b2eef6933",
        "title": "Learning invariant features by harnessing the aperture problem",
        "site": "https://proceedings.mlr.press/v28/memisevic13.html",
        "author": "Roland Memisevic; Georgios Exarchakis",
        "abstract": "The energy model is a simple, biologically inspired approach to extracting relationships between images in tasks like stereopsis and motion analysis. We discuss how adding an extra pooling layer to the energy model makes it possible to learn encodings of transformations that are mostly invariant with respect to image content, and to learn encodings of images that are mostly invariant with respect to the observed transformations. We show how this makes it possible to learn 3D pose-invariant features of objects by watching videos of the objects. We test our approach on a dataset of videos derived from the NORB dataset.",
        "bibtex": "@InProceedings{pmlr-v28-memisevic13,\n  title = \t {Learning invariant features by harnessing the aperture problem},\n  author = \t {Memisevic, Roland and Exarchakis, Georgios},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {100--108},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/memisevic13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/memisevic13.html},\n  abstract = \t {The energy model is a simple, biologically inspired approach to extracting relationships between images in tasks like stereopsis and motion analysis. We discuss how adding an extra pooling layer to the energy model makes it possible to learn encodings of transformations that are mostly invariant with respect to image content, and to learn encodings of images that are mostly invariant with respect to the observed transformations. We show how this makes it possible to learn 3D pose-invariant features of objects by watching videos of the objects. We test our approach on a dataset of videos derived from the NORB dataset.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/memisevic13.pdf",
        "supp": "",
        "pdf_size": 543532,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11822736330832656570&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "University of Montreal; University of California, Berkeley",
        "aff_domain": "iro.umontreal.ca;berkeley.edu",
        "email": "iro.umontreal.ca;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Montreal;University of California, Berkeley",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://wwwumontreal.ca;https://www.berkeley.edu",
        "aff_unique_abbr": "UM;UC Berkeley",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "2d2ab84283",
        "title": "Learning the Structure of Sum-Product Networks",
        "site": "https://proceedings.mlr.press/v28/gens13.html",
        "author": "Robert Gens; Domingos Pedro",
        "abstract": "Sum-product networks (SPNs) are a new class of deep probabilistic models. SPNs can have unbounded treewidth but inference in them is always tractable. An SPN is either a univariate distribution, a product of SPNs over disjoint variables, or a weighted sum of SPNs over the same variables. We propose the first algorithm for learning the structure of SPNs that takes full advantage of their expressiveness. At each step, the algorithm attempts to divide the current variables into approximately independent subsets. If successful, it returns the product of recursive calls on the subsets; otherwise it returns the sum of recursive calls on subsets of similar instances from the current training set. A comprehensive empirical study shows that the learned SPNs are typically comparable to graphical models in likelihood but superior in inference speed and accuracy.",
        "bibtex": "@InProceedings{pmlr-v28-gens13,\n  title = \t {Learning the Structure of Sum-Product Networks},\n  author = \t {Gens, Robert and Pedro, Domingos},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {873--880},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/gens13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/gens13.html},\n  abstract = \t {Sum-product networks (SPNs) are a new class of deep probabilistic models. SPNs can have unbounded treewidth but inference in them is always tractable. An SPN is either a univariate distribution, a product of SPNs over disjoint variables, or a weighted sum of SPNs over the same variables. We propose the first algorithm for learning the structure of SPNs that takes full advantage of their expressiveness. At each step, the algorithm attempts to divide the current variables into approximately independent subsets. If successful, it returns the product of recursive calls on the subsets; otherwise it returns the sum of recursive calls on subsets of similar instances from the current training set. A comprehensive empirical study shows that the learned SPNs are typically comparable to graphical models in likelihood but superior in inference speed and accuracy.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/gens13.pdf",
        "supp": "",
        "pdf_size": 425762,
        "gs_citation": 357,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6933311674980693609&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "Department of Computer Science & Engineering, University of Washington, Seattle, WA 98195, USA; Department of Computer Science & Engineering, University of Washington, Seattle, WA 98195, USA",
        "aff_domain": "cs.washington.edu;cs.washington.edu",
        "email": "cs.washington.edu;cs.washington.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "Department of Computer Science & Engineering",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ee34ee3aca",
        "title": "Learning the beta-Divergence in Tweedie Compound Poisson Matrix Factorization Models",
        "site": "https://proceedings.mlr.press/v28/simsekli13.html",
        "author": "Umut Simsekli; Ali Taylan Cemgil; Yusuf Kenan Yilmaz",
        "abstract": "In this study, we derive algorithms for estimating mixed \u03b2-divergences. Such cost functions are useful for Nonnegative Matrix and Tensor Factorization models with a compound Poisson observation model. Compound Poisson is a particular Tweedie model, an important special case of exponential dispersion models characterized by the fact that the variance is proportional to a power function of the mean. There are several well known matrix and tensor factorization algorithms that minimize the \u03b2-divergence; these estimate the mean parameter. The probabilistic interpretation gives us more flexibility and robustness by providing us additional tunable parameters such as power and dispersion. Estimation of the power parameter is useful for choosing a suitable divergence and estimation of dispersion is useful for data driven regularization and weighting in collective/coupled factorization of heterogeneous datasets. We present three inference algorithms for both estimating the factors and the additional parameters of the compound Poisson distribution. The methods are evaluated on two applications: modeling symbolic representations for polyphonic music and lyric prediction from audio features. Our conclusion is that the compound poisson based factorization models can be useful for sparse positive data.",
        "bibtex": "@InProceedings{pmlr-v28-simsekli13,\n  title = \t {Learning the beta-Divergence in Tweedie Compound Poisson Matrix Factorization Models},\n  author = \t {Simsekli, Umut and Taylan Cemgil, Ali and Kenan Yilmaz, Yusuf},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1409--1417},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/simsekli13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/simsekli13.html},\n  abstract = \t {In this study, we derive algorithms for estimating mixed \u03b2-divergences. Such cost functions are useful for Nonnegative Matrix and Tensor Factorization models with a compound Poisson observation model. Compound Poisson is a particular Tweedie model, an important special case of exponential dispersion models characterized by the fact that the variance is proportional to a power function of the mean. There are several well known matrix and tensor factorization algorithms that minimize the \u03b2-divergence; these estimate the mean parameter. The probabilistic interpretation gives us more flexibility and robustness by providing us additional tunable parameters such as power and dispersion. Estimation of the power parameter is useful for choosing a suitable divergence and estimation of dispersion is useful for data driven regularization and weighting in collective/coupled factorization of heterogeneous datasets. We present three inference algorithms for both estimating the factors and the additional parameters of the compound Poisson distribution. The methods are evaluated on two applications: modeling symbolic representations for polyphonic music and lyric prediction from audio features. Our conclusion is that the compound poisson based factorization models can be useful for sparse positive data.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/simsekli13.pdf",
        "supp": "",
        "pdf_size": 611661,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14829895314995401352&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Dept. of Computer Engineering, Bo\u02d8gazi\u00b8ci University, 34342 Bebek, Istanbul, Turkey; Dept. of Computer Engineering, Bo\u02d8gazi\u00b8ci University, 34342 Bebek, Istanbul, Turkey; Sibnet Computers Ltd, 34742 Kad\u0131k\u00a8oy, Istanbul, Turkey",
        "aff_domain": "boun.edu.tr;boun.edu.tr;sibnet.com.tr",
        "email": "boun.edu.tr;boun.edu.tr;sibnet.com.tr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Bo\u02d8gazi\u00b8ci University;Sibnet Computers Ltd",
        "aff_unique_dep": "Dept. of Computer Engineering;",
        "aff_unique_url": "https://www.boun.edu.tr;",
        "aff_unique_abbr": "Bogazici;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Istanbul;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "T\u00fcrkiye"
    },
    {
        "id": "2f5be803b9",
        "title": "Learning with Marginalized Corrupted Features",
        "site": "https://proceedings.mlr.press/v28/vandermaaten13.html",
        "author": "Laurens Maaten; Minmin Chen; Stephen Tyree; Kilian Weinberger",
        "abstract": "The goal of machine learning is to develop predictors that generalize well to test data. Ideally, this is achieved by training on very large (infinite) training data sets that capture all variations in the data distribution. In the case of finite training data, an effective solution is to extend the training set with artificially created examples \u2013 which, however, is also computationally costly. We propose to corrupt training examples with noise from known distributions within the exponential family and present a novel learning algorithm, called marginalized corrupted features (MCF), that trains robust predictors by minimizing the expected value of the loss function under the corrupting distribution \u2013 essentially learning with infinitely many (corrupted) training examples. We show empirically on a variety of data sets that MCF classifiers can be trained efficiently, may generalize substantially better to test data, and are more robust to feature deletion at test time.",
        "bibtex": "@InProceedings{pmlr-v28-vandermaaten13,\n  title = \t {Learning with Marginalized Corrupted Features},\n  author = \t {Maaten, Laurens and Chen, Minmin and Tyree, Stephen and Weinberger, Kilian},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {410--418},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/vandermaaten13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/vandermaaten13.html},\n  abstract = \t {The goal of machine learning is to develop predictors that generalize well to test data. Ideally, this is achieved by training on very large (infinite) training data sets that capture all variations in the data distribution. In the case of finite training data, an effective solution is to extend the training set with artificially created examples \u2013 which, however, is also computationally costly. We propose to corrupt training examples with noise from known distributions within the exponential family and present a novel learning algorithm, called marginalized corrupted features (MCF), that trains robust predictors by minimizing the expected value of the loss function under the corrupting distribution \u2013 essentially learning with infinitely many (corrupted) training examples. We show empirically on a variety of data sets that MCF classifiers can be trained efficiently, may generalize substantially better to test data, and are more robust to feature deletion at test time.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/vandermaaten13.pdf",
        "supp": "",
        "pdf_size": 569645,
        "gs_citation": 211,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6142503578748153832&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "Delft University of Technology; Washington University; Washington University; Washington University",
        "aff_domain": "gmail.com;cec.wustl.edu;wustl.edu;wustl.edu",
        "email": "gmail.com;cec.wustl.edu;wustl.edu;wustl.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Delft University of Technology;Washington University in St. Louis",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tudelft.nl;https://wustl.edu",
        "aff_unique_abbr": "TU Delft;WUSTL",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";St. Louis",
        "aff_country_unique_index": "0;1;1;1",
        "aff_country_unique": "Netherlands;United States"
    },
    {
        "id": "f7d896bc89",
        "title": "Local Deep Kernel Learning for Efficient Non-linear SVM Prediction",
        "site": "https://proceedings.mlr.press/v28/jose13.html",
        "author": "Cijo Jose; Prasoon Goyal; Parv Aggrwal; Manik Varma",
        "abstract": "Our objective is to speed up non-linear SVM prediction while maintaining classification accuracy above an acceptable limit. We generalize Localized Multiple Kernel Learning so as to learn a primal feature space embedding which is high dimensional, sparse and computationally deep. Primal based classification decouples prediction costs from the number of support vectors and our tree-structured features efficiently encode non-linearities while speeding up prediction exponentially over the state-of-the-art. We develop routines for optimizing over the space of tree-structured features and efficiently scale to problems with over half a million training points. Experiments on benchmark data sets reveal that our formulation can reduce prediction costs by more than three orders of magnitude in some cases with a moderate sacrifice in classification accuracy as compared to RBF-SVMs. Furthermore, our formulation leads to much better classification accuracies over leading methods.",
        "bibtex": "@InProceedings{pmlr-v28-jose13,\n  title = \t {Local Deep Kernel Learning for Efficient Non-linear SVM Prediction},\n  author = \t {Jose, Cijo and Goyal, Prasoon and Aggrwal, Parv and Varma, Manik},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {486--494},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/jose13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/jose13.html},\n  abstract = \t {Our objective is to speed up non-linear SVM prediction while maintaining classification accuracy above an acceptable limit. We generalize Localized Multiple Kernel Learning so as to learn a primal feature space embedding which is high dimensional, sparse and computationally deep. Primal based classification decouples prediction costs from the number of support vectors and our tree-structured features efficiently encode non-linearities while speeding up prediction exponentially over the state-of-the-art. We develop routines for optimizing over the space of tree-structured features and efficiently scale to problems with over half a million training points. Experiments on benchmark data sets reveal that our formulation can reduce prediction costs by more than three orders of magnitude in some cases with a moderate sacrifice in classification accuracy as compared to RBF-SVMs. Furthermore, our formulation leads to much better classification accuracies over leading methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/jose13.pdf",
        "supp": "",
        "pdf_size": 1230649,
        "gs_citation": 148,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9173382604628208393&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Indian Institute of Technology Delhi, Hauz Khas, New Delhi, India 110 016; Indian Institute of Technology Delhi, Hauz Khas, New Delhi, India 110 016; Indian Institute of Technology Delhi, Hauz Khas, New Delhi, India 110 016; Microsoft Research India, Bangalore, India 560 001",
        "aff_domain": "gmail.com;gmail.com;gmail.com;microsoft.com",
        "email": "gmail.com;gmail.com;gmail.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Indian Institute of Technology Delhi;Microsoft",
        "aff_unique_dep": ";Microsoft Research India",
        "aff_unique_url": "https://www.iitdelhi.ac.in;https://www.microsoft.com/en-us/research/group/microsoft-research-india",
        "aff_unique_abbr": "IIT Delhi;MSRI",
        "aff_campus_unique_index": "0;0;0;1",
        "aff_campus_unique": "New Delhi;Bangalore",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2b66217de8",
        "title": "Local Low-Rank Matrix Approximation",
        "site": "https://proceedings.mlr.press/v28/lee13.html",
        "author": "Joonseok Lee; Seungyeon Kim; Guy Lebanon; Yoram Singer",
        "abstract": "Matrix approximation is a common tool in recommendation systems, text mining, and computer vision. A prevalent assumption in constructing matrix approximations is that the partially observed matrix is of low-rank. We propose a new matrix approximation model where we assume instead that the matrix is locally of low-rank, leading to a representation of the observed matrix as a weighted sum of low-rank matrices. We analyze the accuracy of the proposed local low-rank modeling. Our experiments show improvements in prediction accuracy over classical approaches for recommendation tasks.",
        "bibtex": "@InProceedings{pmlr-v28-lee13,\n  title = \t {Local Low-Rank Matrix Approximation},\n  author = \t {Lee, Joonseok and Kim, Seungyeon and Lebanon, Guy and Singer, Yoram},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {82--90},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/lee13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/lee13.html},\n  abstract = \t {Matrix approximation is a common tool in recommendation systems, text mining, and computer vision. A prevalent assumption in constructing matrix approximations is that the partially observed matrix is of low-rank. We propose a new matrix approximation model where we assume instead that the matrix is locally of low-rank, leading to a representation of the observed matrix as a weighted sum of low-rank matrices. We analyze the accuracy of the proposed local low-rank modeling. Our experiments show improvements in prediction accuracy over classical approaches for recommendation tasks.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/lee13.pdf",
        "supp": "",
        "pdf_size": 721475,
        "gs_citation": 260,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14286429527364855433&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology; Google Research",
        "aff_domain": "gatech.edu;gatech.edu;cc.gatech.edu;google.com",
        "email": "gatech.edu;gatech.edu;cc.gatech.edu;google.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Georgia Institute of Technology;Google",
        "aff_unique_dep": ";Google Research",
        "aff_unique_url": "https://www.gatech.edu;https://research.google",
        "aff_unique_abbr": "Georgia Tech;Google Research",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "dfdae015bf",
        "title": "Loss-Proportional Subsampling for Subsequent ERM",
        "site": "https://proceedings.mlr.press/v28/mineiro13.html",
        "author": "Paul Mineiro; Nikos Karampatziakis",
        "abstract": "We propose a sampling scheme suitable for reducing a data set prior to  selecting a hypothesis with minimum empirical risk.  The sampling only  considers a subset of the ultimate (unknown) hypothesis set, but can  nonetheless guarantee that the final excess risk will compare favorably  with utilizing the entire original data set. We demonstrate the practical  benefits of our approach on a large dataset which we subsample and  subsequently fit with boosted trees.",
        "bibtex": "@InProceedings{pmlr-v28-mineiro13,\n  title = \t {Loss-Proportional Subsampling for Subsequent ERM},\n  author = \t {Mineiro, Paul and Karampatziakis, Nikos},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {522--530},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/mineiro13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/mineiro13.html},\n  abstract = \t {We propose a sampling scheme suitable for reducing a data set prior to  selecting a hypothesis with minimum empirical risk.  The sampling only  considers a subset of the ultimate (unknown) hypothesis set, but can  nonetheless guarantee that the final excess risk will compare favorably  with utilizing the entire original data set. We demonstrate the practical  benefits of our approach on a large dataset which we subsample and  subsequently fit with boosted trees.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/mineiro13.pdf",
        "supp": "",
        "pdf_size": 347333,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13777708704717634494&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Microsoft Cloud and Information Services Laboratory, One Microsoft Way, Redmond, WA 98052, USA; Microsoft Cloud and Information Services Laboratory, One Microsoft Way, Redmond, WA 98052, USA",
        "aff_domain": "microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Cloud and Information Services Laboratory",
        "aff_unique_url": "https://www.microsoft.com",
        "aff_unique_abbr": "Microsoft",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Redmond",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "891c41850b",
        "title": "MAD-Bayes: MAP-based Asymptotic Derivations from Bayes",
        "site": "https://proceedings.mlr.press/v28/broderick13.html",
        "author": "Tamara Broderick; Brian Kulis; Michael Jordan",
        "abstract": "The classical mixture of Gaussians model is related to K-means via small-variance asymptotics: as the covariances of the Gaussians tend to zero, the negative log-likelihood of the mixture of Gaussians model approaches the K-means objective, and the EM algorithm approaches the K-means algorithm. Kulis & Jordan (2012) used this observation to obtain a novel K-means-like algorithm from a Gibbs sampler for the Dirichlet process (DP) mixture. We instead consider applying small-variance asymptotics directly to the posterior in Bayesian nonparametric models. This framework is independent of any specific Bayesian inference algorithm, and it has the major advantage that it generalizes immediately to a range of models beyond the DP mixture. To illustrate, we apply our framework to the feature learning setting, where the beta process and Indian buffet process provide an appropriate Bayesian nonparametric prior. We obtain a novel objective function that goes beyond clustering to learn (and penalize new) groupings for which we relax the mutual exclusivity and exhaustivity assumptions of clustering. We demonstrate several other algorithms, all of which are scalable and simple to implement. Empirical results demonstrate the benefits of the new framework.",
        "bibtex": "@InProceedings{pmlr-v28-broderick13,\n  title = \t {MAD-Bayes: MAP-based Asymptotic Derivations from Bayes},\n  author = \t {Broderick, Tamara and Kulis, Brian and Jordan, Michael},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {226--234},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/broderick13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/broderick13.html},\n  abstract = \t {The classical mixture of Gaussians model is related to K-means via small-variance asymptotics: as the covariances of the Gaussians tend to zero, the negative log-likelihood of the mixture of Gaussians model approaches the K-means objective, and the EM algorithm approaches the K-means algorithm. Kulis & Jordan (2012) used this observation to obtain a novel K-means-like algorithm from a Gibbs sampler for the Dirichlet process (DP) mixture. We instead consider applying small-variance asymptotics directly to the posterior in Bayesian nonparametric models. This framework is independent of any specific Bayesian inference algorithm, and it has the major advantage that it generalizes immediately to a range of models beyond the DP mixture. To illustrate, we apply our framework to the feature learning setting, where the beta process and Indian buffet process provide an appropriate Bayesian nonparametric prior. We obtain a novel objective function that goes beyond clustering to learn (and penalize new) groupings for which we relax the mutual exclusivity and exhaustivity assumptions of clustering. We demonstrate several other algorithms, all of which are scalable and simple to implement. Empirical results demonstrate the benefits of the new framework.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/broderick13.pdf",
        "supp": "",
        "pdf_size": 852408,
        "gs_citation": 104,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17247985905912219852&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "we derkeleyK utatistics fepartment; qhio utate wniversityK eug fepartment; we derkeleyK utatistics fepartment + ggeu fepartment",
        "aff_domain": "stat.berkeley.edu;cse.ohio-state.edu;eecs.berkeley.edu",
        "email": "stat.berkeley.edu;cse.ohio-state.edu;eecs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0+2",
        "aff_unique_norm": "University of California, Berkeley;University K;GGEU Department",
        "aff_unique_dep": "Department of Statistics;Department;Department",
        "aff_unique_url": "https://www.stat.berkeley.edu;;",
        "aff_unique_abbr": "UC Berkeley;;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "4c6d21eb2b",
        "title": "MILEAGE: Multiple Instance LEArning with Global Embedding",
        "site": "https://proceedings.mlr.press/v28/zhang13a.html",
        "author": "Dan Zhang; Jingrui He; Luo Si; Richard Lawrence",
        "abstract": "Multiple Instance Learning (MIL) methods generally represent each example as a collection of  instances such that the features for local objects can be better captured, whereas traditional learning methods typically extract a global feature vector for each example as an integral part. However, there is limited research work on which of the two learning scenarios performs better. This paper proposes a novel framework \u2013 \\emphMultiple Instance LEArning with  Global Embedding (MILEAGE), in which  the global feature vectors for traditional learning methods are integrated into the MIL setting.  MILEAGE can leverage the benefits derived from both learning settings. Within the proposed framework, a large margin method is formulated. In particular,  the proposed method adaptively  tunes the weights on the two different kinds of feature  representations (i.e., global and multiple instance) for each example and trains the classifier simultaneously. An alternative algorithm is proposed to solve the resulting optimization problem, which extends the bundle method to the non-convex case. Some important properties of the proposed method, such as the convergence rate and the generalization error rate, are analyzed. A series of  experiments  have been conducted to demonstrate the advantages of the proposed method over several state-of-the-art multiple instance and traditional learning methods.",
        "bibtex": "@InProceedings{pmlr-v28-zhang13a,\n  title = \t {MILEAGE: Multiple Instance LEArning with Global Embedding},\n  author = \t {Zhang, Dan and He, Jingrui and Si, Luo and Lawrence, Richard},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {82--90},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/zhang13a.pdf},\n  url = \t {https://proceedings.mlr.press/v28/zhang13a.html},\n  abstract = \t {Multiple Instance Learning (MIL) methods generally represent each example as a collection of  instances such that the features for local objects can be better captured, whereas traditional learning methods typically extract a global feature vector for each example as an integral part. However, there is limited research work on which of the two learning scenarios performs better. This paper proposes a novel framework \u2013 \\emphMultiple Instance LEArning with  Global Embedding (MILEAGE), in which  the global feature vectors for traditional learning methods are integrated into the MIL setting.  MILEAGE can leverage the benefits derived from both learning settings. Within the proposed framework, a large margin method is formulated. In particular,  the proposed method adaptively  tunes the weights on the two different kinds of feature  representations (i.e., global and multiple instance) for each example and trains the classifier simultaneously. An alternative algorithm is proposed to solve the resulting optimization problem, which extends the bundle method to the non-convex case. Some important properties of the proposed method, such as the convergence rate and the generalization error rate, are analyzed. A series of  experiments  have been conducted to demonstrate the advantages of the proposed method over several state-of-the-art multiple instance and traditional learning methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/zhang13a.pdf",
        "supp": "",
        "pdf_size": 2117138,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15423343300510522059&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Facebook Incorporation, Menlo Park, CA 94025; Computer Science Department, Stevens Institute of Technology, Hoboken, NJ 07030; Computer Science Department, Purdue University, West Lafayette, IN 47907; IBM T.J. Watson Research Center, Yorktown Heights, NY 10562",
        "aff_domain": "gmail.com;gmail.com;cs.purdue.edu;us.ibm.com",
        "email": "gmail.com;gmail.com;cs.purdue.edu;us.ibm.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "Meta;Stevens Institute of Technology;Purdue University;IBM",
        "aff_unique_dep": "Facebook;Computer Science Department;Computer Science Department;IBM T.J. Watson Research Center",
        "aff_unique_url": "https://www.facebook.com;https://www.stevens.edu;https://www.purdue.edu;https://www.ibm.com/research/watson",
        "aff_unique_abbr": "FB;SIT;Purdue;IBM Watson",
        "aff_campus_unique_index": "0;1;2;3",
        "aff_campus_unique": "Menlo Park;Hoboken;West Lafayette;Yorktown Heights",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "80abd3a209",
        "title": "Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures",
        "site": "https://proceedings.mlr.press/v28/bergstra13.html",
        "author": "James Bergstra; Daniel Yamins; David Cox",
        "abstract": "Many computer vision algorithms depend on configuration settings that are typically hand-tuned in the course of evaluating the algorithm for a particular data set. While such parameter tuning is often presented as being incidental to the algorithm, correctly setting these parameter choices is frequently critical to realizing a method\u2019s full potential. Compounding matters, these parameters often must be re-tuned when the algorithm is applied to a new problem domain, and the tuning process itself often depends on personal experience and intuition in ways that are hard to quantify or describe. Since the performance of a given technique depends on both the fundamental quality of the algorithm and the details of its tuning, it is sometimes difficult to know whether a given technique is genuinely better, or simply better tuned.     In this work, we propose a meta-modeling approach to support automated hyperparameter optimization, with the goal of providing practical tools that replace hand-tuning with a reproducible and unbiased optimization process. Our approach is to expose the underlying expression graph of how a performance metric (e.g. classification accuracy on validation examples) is computed from hyperparameters that govern not only how individual processing steps are applied, but even which processing steps are included.  A hyperparameter optimization algorithm transforms this graph into a program for optimizing that performance metric.  Our approach yields state of the art results on three disparate computer vision problems: a face-matching verification task (LFW), a face identification task (PubFig83) and an object recognition task (CIFAR-10), using a single broad class of feed-forward vision architectures.",
        "bibtex": "@InProceedings{pmlr-v28-bergstra13,\n  title = \t {Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures},\n  author = \t {Bergstra, James and Yamins, Daniel and Cox, David},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {115--123},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/bergstra13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/bergstra13.html},\n  abstract = \t {Many computer vision algorithms depend on configuration settings that are typically hand-tuned in the course of evaluating the algorithm for a particular data set. While such parameter tuning is often presented as being incidental to the algorithm, correctly setting these parameter choices is frequently critical to realizing a method\u2019s full potential. Compounding matters, these parameters often must be re-tuned when the algorithm is applied to a new problem domain, and the tuning process itself often depends on personal experience and intuition in ways that are hard to quantify or describe. Since the performance of a given technique depends on both the fundamental quality of the algorithm and the details of its tuning, it is sometimes difficult to know whether a given technique is genuinely better, or simply better tuned.     In this work, we propose a meta-modeling approach to support automated hyperparameter optimization, with the goal of providing practical tools that replace hand-tuning with a reproducible and unbiased optimization process. Our approach is to expose the underlying expression graph of how a performance metric (e.g. classification accuracy on validation examples) is computed from hyperparameters that govern not only how individual processing steps are applied, but even which processing steps are included.  A hyperparameter optimization algorithm transforms this graph into a program for optimizing that performance metric.  Our approach yields state of the art results on three disparate computer vision problems: a face-matching verification task (LFW), a face identification task (PubFig83) and an object recognition task (CIFAR-10), using a single broad class of feed-forward vision architectures.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/bergstra13.pdf",
        "supp": "",
        "pdf_size": 723804,
        "gs_citation": 3423,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10987276144712583456&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Rowland Institute at Harvard; Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology; Rowland Institute at Harvard",
        "aff_domain": "rowland.harvard.edu;mit.edu;fas.harvard.edu",
        "email": "rowland.harvard.edu;mit.edu;fas.harvard.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Harvard University;Massachusetts Institute of Technology",
        "aff_unique_dep": "Rowland Institute;Department of Brain and Cognitive Sciences",
        "aff_unique_url": "https://rowland.harvard.edu;https://web.mit.edu",
        "aff_unique_abbr": "Harvard;MIT",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d8f6ce5b30",
        "title": "Manifold Preserving Hierarchical Topic Models for Quantization and Approximation",
        "site": "https://proceedings.mlr.press/v28/kim13a.html",
        "author": "Minje Kim; Paris Smaragdis",
        "abstract": "We present two complementary topic models to address the analysis of mixture data lying on manifolds. First, we propose a quantization method with an additional mid-layer latent variable, which selects only data points that best preserve the manifold structure of the input data. In order to address the case of modeling all the in-between parts of that manifold using this reduced representation of the input, we introduce a new model that provides a manifold-aware interpolation method. We demonstrate the advantages of these models with experiments on the hand-written digit recognition and the speech source separation tasks.",
        "bibtex": "@InProceedings{pmlr-v28-kim13a,\n  title = \t {Manifold Preserving Hierarchical Topic Models for Quantization and Approximation},\n  author = \t {Kim, Minje and Smaragdis, Paris},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1373--1381},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/kim13a.pdf},\n  url = \t {https://proceedings.mlr.press/v28/kim13a.html},\n  abstract = \t {We present two complementary topic models to address the analysis of mixture data lying on manifolds. First, we propose a quantization method with an additional mid-layer latent variable, which selects only data points that best preserve the manifold structure of the input data. In order to address the case of modeling all the in-between parts of that manifold using this reduced representation of the input, we introduce a new model that provides a manifold-aware interpolation method. We demonstrate the advantages of these models with experiments on the hand-written digit recognition and the speech source separation tasks.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/kim13a.pdf",
        "supp": "",
        "pdf_size": 663802,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10858785235984582365&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL 61801 USA; University of Illinois at Urbana-Champaign, Urbana, IL 61801 USA + Adobe Research, Adobe Systems Inc., San Francisco, CA 94103, USA",
        "aff_domain": "illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;Adobe",
        "aff_unique_dep": "Department of Computer Science;Adobe Research",
        "aff_unique_url": "https://illinois.edu;https://www.adobe.com/research.html",
        "aff_unique_abbr": "UIUC;Adobe",
        "aff_campus_unique_index": "0;0+1",
        "aff_campus_unique": "Urbana;San Francisco",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1630a662f2",
        "title": "Margins, Shrinkage, and Boosting",
        "site": "https://proceedings.mlr.press/v28/telgarsky13.html",
        "author": "Matus Telgarsky",
        "abstract": "This manuscript shows that AdaBoost and its immediate variants can produce approximately maximum margin classifiers simply by scaling their step size choices by a fixed small constant. In this way, when the unscaled step size is an optimal choice, these results provide guarantees for Friedman\u2019s empirically successful \u201cshrinkage\u201d procedure for gradient boosting (Friedman, 2000).  Guarantees are also provided for a variety of other step sizes, affirming the intuition that increasingly regularized line searches provide improved margin guarantees. The results hold for the exponential loss and similar losses, most notably the logistic loss.",
        "bibtex": "@InProceedings{pmlr-v28-telgarsky13,\n  title = \t {Margins, Shrinkage, and Boosting},\n  author = \t {Telgarsky, Matus},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {307--315},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/telgarsky13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/telgarsky13.html},\n  abstract = \t {This manuscript shows that AdaBoost and its immediate variants can produce approximately maximum margin classifiers simply by scaling their step size choices by a fixed small constant. In this way, when the unscaled step size is an optimal choice, these results provide guarantees for Friedman\u2019s empirically successful \u201cshrinkage\u201d procedure for gradient boosting (Friedman, 2000).  Guarantees are also provided for a variety of other step sizes, affirming the intuition that increasingly regularized line searches provide improved margin guarantees. The results hold for the exponential loss and similar losses, most notably the logistic loss.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/telgarsky13.pdf",
        "supp": "",
        "pdf_size": 565374,
        "gs_citation": 94,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4667447052366537503&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science and Engineering, UCSD",
        "aff_domain": "cs.ucsd.edu",
        "email": "cs.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "La Jolla",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9e906c8004",
        "title": "Markov Network Estimation From Multi-attribute Data",
        "site": "https://proceedings.mlr.press/v28/kolar13a.html",
        "author": "Mladen Kolar; Han Liu; Eric Xing",
        "abstract": "Many real world network problems often concern multivariate nodal attributes such as image, textual, and multi-view feature vectors on nodes, rather than simple univariate nodal attributes. The existing graph estimation methods built on Gaussian graphical models and covariance selection algorithms can not handle such data, neither can the theories developed around such methods be directly applied. In this paper, we propose a new principled framework for estimating multi-attribute graphs. Instead of estimating the partial correlation as in current literature, our method estimates the partial canonical correlations that naturally accommodate complex nodal features.  Computationally, we provide an efficient algorithm which utilizes the multi-attribute structure. Theoretically, we provide sufficient conditions which guarantee consistent graph recovery. Extensive simulation studies demonstrate performance of our method under various conditions.",
        "bibtex": "@InProceedings{pmlr-v28-kolar13a,\n  title = \t {Markov Network Estimation From Multi-attribute Data},\n  author = \t {Kolar, Mladen and Liu, Han and Xing, Eric},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {73--81},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/kolar13a.pdf},\n  url = \t {https://proceedings.mlr.press/v28/kolar13a.html},\n  abstract = \t {Many real world network problems often concern multivariate nodal attributes such as image, textual, and multi-view feature vectors on nodes, rather than simple univariate nodal attributes. The existing graph estimation methods built on Gaussian graphical models and covariance selection algorithms can not handle such data, neither can the theories developed around such methods be directly applied. In this paper, we propose a new principled framework for estimating multi-attribute graphs. Instead of estimating the partial correlation as in current literature, our method estimates the partial canonical correlations that naturally accommodate complex nodal features.  Computationally, we provide an efficient algorithm which utilizes the multi-attribute structure. Theoretically, we provide sufficient conditions which guarantee consistent graph recovery. Extensive simulation studies demonstrate performance of our method under various conditions.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/kolar13a.pdf",
        "supp": "",
        "pdf_size": 278362,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9494118196381031930&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15217 USA; Department of Operations Research and Financial Engineering, Princeton University, Princeton, NJ 08544 USA; Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15217 USA",
        "aff_domain": "cs.cmu.edu;princeton.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;princeton.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Carnegie Mellon University;Princeton University",
        "aff_unique_dep": "Machine Learning Department;Department of Operations Research and Financial Engineering",
        "aff_unique_url": "https://www.cmu.edu;https://www.princeton.edu",
        "aff_unique_abbr": "CMU;Princeton",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Pittsburgh;Princeton",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8f1f90402b",
        "title": "Max-Margin Multiple-Instance Dictionary Learning",
        "site": "https://proceedings.mlr.press/v28/wang13d.html",
        "author": "Xinggang Wang; Baoyuan Wang; Xiang Bai; Wenyu Liu; Zhuowen Tu",
        "abstract": "Dictionary learning has became an increasingly important task in machine learning, as it is fundamental to the representation problem. A number of emerging techniques specifically include a codebook learning step, in which a critical knowledge abstraction process is carried out. Existing approaches in dictionary (codebook) learning are either generative (unsupervised e.g. k-means) or discriminative (supervised e.g. extremely randomized forests). In this paper, we propose a multiple instance learning (MIL) strategy (along the line of weakly supervised learning) for dictionary learning. Each code is represented by a classifier, such as a linear SVM, which naturally performs metric fusion for multi-channel features. We design a formulation to simultaneously learn mixtures of codes by maximizing classification margins in MIL.  State-of-the-art results are observed in image classification benchmarks based on the learned codebooks, which observe both compactness and effectiveness.",
        "bibtex": "@InProceedings{pmlr-v28-wang13d,\n  title = \t {Max-Margin Multiple-Instance Dictionary Learning},\n  author = \t {Wang, Xinggang and Wang, Baoyuan and Bai, Xiang and Liu, Wenyu and Tu, Zhuowen},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {846--854},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/wang13d.pdf},\n  url = \t {https://proceedings.mlr.press/v28/wang13d.html},\n  abstract = \t {Dictionary learning has became an increasingly important task in machine learning, as it is fundamental to the representation problem. A number of emerging techniques specifically include a codebook learning step, in which a critical knowledge abstraction process is carried out. Existing approaches in dictionary (codebook) learning are either generative (unsupervised e.g. k-means) or discriminative (supervised e.g. extremely randomized forests). In this paper, we propose a multiple instance learning (MIL) strategy (along the line of weakly supervised learning) for dictionary learning. Each code is represented by a classifier, such as a linear SVM, which naturally performs metric fusion for multi-channel features. We design a formulation to simultaneously learn mixtures of codes by maximizing classification margins in MIL.  State-of-the-art results are observed in image classification benchmarks based on the learned codebooks, which observe both compactness and effectiveness.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/wang13d.pdf",
        "supp": "",
        "pdf_size": 1277864,
        "gs_citation": 146,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2038616232708685736&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Electronics and Information Engineering, Huazhong University of Science and Technology; Microsoft Research Asia; Department of Electronics and Information Engineering, Huazhong University of Science and Technology; Department of Electronics and Information Engineering, Huazhong University of Science and Technology; Microsoft Research Asia+Lab of Neuro Imaging and Department of Computer Science, University of California, Los Angeles",
        "aff_domain": "gmail.com;microsoft.com;hust.edu.cn;hust.edu.cn;gmail.com",
        "email": "gmail.com;microsoft.com;hust.edu.cn;hust.edu.cn;gmail.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0;1+2",
        "aff_unique_norm": "Huazhong University of Science and Technology;Microsoft;University of California, Los Angeles",
        "aff_unique_dep": "Department of Electronics and Information Engineering;Research;Department of Computer Science",
        "aff_unique_url": "http://www.hust.edu.cn;https://www.microsoft.com/en-us/research/group/asia;https://www.ucla.edu",
        "aff_unique_abbr": "HUST;MSR Asia;UCLA",
        "aff_campus_unique_index": "1;1+2",
        "aff_campus_unique": ";Asia;Los Angeles",
        "aff_country_unique_index": "0;0;0;0;0+1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "29a1ba300c",
        "title": "Maximum Variance Correction with Application to A* Search",
        "site": "https://proceedings.mlr.press/v28/chen13c.html",
        "author": "Wenlin Chen; Kilian Weinberger; Yixin Chen",
        "abstract": "In this paper we introduce Maximum Variance Correction (MVC), which finds large-scale feasible solutions to Maximum Variance Unfolding (MVU) by post-processing embeddings from any manifold learning algorithm. It increases the scale of MVU embeddings by several orders of magnitude and is naturally parallel. This unprecedented scalability opens up new avenues of applications for manifold learning, in particular the use of MVU embeddings as effective heuristics to speed-up A* search (Rayner et al. 2011).   We demonstrate that MVC embeddings lead to un-matched reductions in search time across several non-trivial A* benchmark search problems and bridge the gap between the manifold learning literature and one of its most promising high impact applications.",
        "bibtex": "@InProceedings{pmlr-v28-chen13c,\n  title = \t {Maximum Variance Correction with Application to A* Search},\n  author = \t {Chen, Wenlin and Weinberger, Kilian and Chen, Yixin},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {302--310},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/chen13c.pdf},\n  url = \t {https://proceedings.mlr.press/v28/chen13c.html},\n  abstract = \t {In this paper we introduce Maximum Variance Correction (MVC), which finds large-scale feasible solutions to Maximum Variance Unfolding (MVU) by post-processing embeddings from any manifold learning algorithm. It increases the scale of MVU embeddings by several orders of magnitude and is naturally parallel. This unprecedented scalability opens up new avenues of applications for manifold learning, in particular the use of MVU embeddings as effective heuristics to speed-up A* search (Rayner et al. 2011).   We demonstrate that MVC embeddings lead to un-matched reductions in search time across several non-trivial A* benchmark search problems and bridge the gap between the manifold learning literature and one of its most promising high impact applications.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/chen13c.pdf",
        "supp": "",
        "pdf_size": 1084869,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3487293278609615656&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Washington University; Washington University; Washington University",
        "aff_domain": "wustl.edu;wustl.edu;cse.wustl.edu",
        "email": "wustl.edu;wustl.edu;cse.wustl.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Washington University in St. Louis",
        "aff_unique_dep": "",
        "aff_unique_url": "https://wustl.edu",
        "aff_unique_abbr": "WUSTL",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "St. Louis",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2326d0f841",
        "title": "Maxout Networks",
        "site": "https://proceedings.mlr.press/v28/goodfellow13.html",
        "author": "Ian Goodfellow; David Warde-Farley; Mehdi Mirza; Aaron Courville; Yoshua Bengio",
        "abstract": "We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.",
        "bibtex": "@InProceedings{pmlr-v28-goodfellow13,\n  title = \t {Maxout Networks},\n  author = \t {Goodfellow, Ian and Warde-Farley, David and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1319--1327},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/goodfellow13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/goodfellow13.html},\n  abstract = \t {We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/goodfellow13.pdf",
        "supp": "",
        "pdf_size": 1046193,
        "gs_citation": 3230,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9154719469920231460&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 23,
        "aff": "D\u00b4epartement d\u2019Informatique et de Recherche Op\u00b4erationelle, Universit\u00b4e de Montr\u00b4eal; D\u00b4epartement d\u2019Informatique et de Recherche Op\u00b4erationelle, Universit\u00b4e de Montr\u00b4eal; D\u00b4epartement d\u2019Informatique et de Recherche Op\u00b4erationelle, Universit\u00b4e de Montr\u00b4eal; D\u00b4epartement d\u2019Informatique et de Recherche Op\u00b4erationelle, Universit\u00b4e de Montr\u00b4eal; D\u00b4epartement d\u2019Informatique et de Recherche Op\u00b4erationelle, Universit\u00b4e de Montr\u00b4eal",
        "aff_domain": "iro.umontreal.ca;iro.umontreal.ca;iro.umontreal.ca;umontreal.ca;umontreal.ca",
        "email": "iro.umontreal.ca;iro.umontreal.ca;iro.umontreal.ca;umontreal.ca;umontreal.ca",
        "github": "",
        "project": "http://www-etud.iro.umontreal.ca/~goodfeli/maxout.html",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Universit u00e9 de Montr u00e9al",
        "aff_unique_dep": "D u00e9partement d\u2019Informatique et de Recherche Op u00e9rationelle",
        "aff_unique_url": "https://www.umontreal.ca",
        "aff_unique_abbr": "UdeM",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Montr u00e9al",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "52ff130e8b",
        "title": "Mean Reversion with a Variance Threshold",
        "site": "https://proceedings.mlr.press/v28/cuturi13.html",
        "author": "Marco Cuturi; Alexandre D\u2019Aspremont",
        "abstract": "Starting from a multivariate data set, we study several techniques to isolate affine combinations of the variables with a maximum amount of mean reversion, while constraining the variance to be larger than a given threshold. We show that many of the optimization problems arising in this context can be solved exactly using semidefinite programming and some variant of the \\mathcalS-lemma. In finance, these methods are used to isolate statistical arbitrage opportunities, i.e. mean reverting portfolios with enough variance to overcome market friction. In a more general setting, mean reversion and its generalizations are also used as a proxy for stationarity, while variance simply measures signal strength.",
        "bibtex": "@InProceedings{pmlr-v28-cuturi13,\n  title = \t {Mean Reversion with a Variance Threshold},\n  author = \t {Cuturi, Marco and D\u2019Aspremont, Alexandre},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {271--279},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/cuturi13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/cuturi13.html},\n  abstract = \t {Starting from a multivariate data set, we study several techniques to isolate affine combinations of the variables with a maximum amount of mean reversion, while constraining the variance to be larger than a given threshold. We show that many of the optimization problems arising in this context can be solved exactly using semidefinite programming and some variant of the \\mathcalS-lemma. In finance, these methods are used to isolate statistical arbitrage opportunities, i.e. mean reverting portfolios with enough variance to overcome market friction. In a more general setting, mean reversion and its generalizations are also used as a proxy for stationarity, while variance simply measures signal strength.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/cuturi13.pdf",
        "supp": "",
        "pdf_size": 292075,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1590458520880139414&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Graduate School of Informatics, Kyoto University; CNRS - Ecole Polytechnique",
        "aff_domain": "i.kyoto-u.ac.jp;m4x.org",
        "email": "i.kyoto-u.ac.jp;m4x.org",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Kyoto University;Ecole Polytechnique",
        "aff_unique_dep": "Graduate School of Informatics;",
        "aff_unique_url": "https://www.kyoto-u.ac.jp;https://www.ec-polytechnique.fr",
        "aff_unique_abbr": "Kyoto U;X",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Kyoto;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Japan;France"
    },
    {
        "id": "6c476f5207",
        "title": "Message passing with l1 penalized KL minimization",
        "site": "https://proceedings.mlr.press/v28/qi13.html",
        "author": "Yuan Qi; Yandong Guo",
        "abstract": "Bayesian inference is often hampered by large computational expense.  As a generalization of belief propagation (BP), expectation propagation (EP) approximates exact Bayesian computation with efficient message passing updates. However, when an approximation family used by EP is far from exact posterior distributions, message passing may lead to poor approximation quality and suffer from divergence. To address this issue, we propose an  approximate inference method, relaxed expectation propagation(REP), based on a new divergence with a l1 penalty. Minimizing this penalized divergence adaptively relaxes EP\u2019s moment matching requirement for message passing. We apply REP to Gaussian process classification and experimental results demonstrate significant improvement of REP over EP and alpha-divergence based power EP \u2013 in terms of algorithmic stability, estimation accuracy, and predictive performance. Furthermore, we develop relaxed belief propagation(RBP), a special case of REP, to conduct inference on discrete Markov random fields (MRFs). Our results show improved estimation accuracy of RBP over BP and fractional BP when interactions between MRF nodes are strong.",
        "bibtex": "@InProceedings{pmlr-v28-qi13,\n  title = \t {Message passing with l1 penalized KL minimization},\n  author = \t {Qi, Yuan and Guo, Yandong},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {262--270},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/qi13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/qi13.html},\n  abstract = \t {Bayesian inference is often hampered by large computational expense.  As a generalization of belief propagation (BP), expectation propagation (EP) approximates exact Bayesian computation with efficient message passing updates. However, when an approximation family used by EP is far from exact posterior distributions, message passing may lead to poor approximation quality and suffer from divergence. To address this issue, we propose an  approximate inference method, relaxed expectation propagation(REP), based on a new divergence with a l1 penalty. Minimizing this penalized divergence adaptively relaxes EP\u2019s moment matching requirement for message passing. We apply REP to Gaussian process classification and experimental results demonstrate significant improvement of REP over EP and alpha-divergence based power EP \u2013 in terms of algorithmic stability, estimation accuracy, and predictive performance. Furthermore, we develop relaxed belief propagation(RBP), a special case of REP, to conduct inference on discrete Markov random fields (MRFs). Our results show improved estimation accuracy of RBP over BP and fractional BP when interactions between MRF nodes are strong.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/qi13.pdf",
        "supp": "",
        "pdf_size": 550440,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1192514376234518903&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Departments of Computer Science and Statistics, Purdue University, West Lafayette, IN 47907 USA; School of Electrical and Computer Engineering, Purdue University West Lafayette, IN 47907 USA",
        "aff_domain": "PURDUE.EDU;PURDUE.EDU",
        "email": "PURDUE.EDU;PURDUE.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Purdue University",
        "aff_unique_dep": "Departments of Computer Science and Statistics",
        "aff_unique_url": "https://www.purdue.edu",
        "aff_unique_abbr": "Purdue",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "West Lafayette",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e243fd6f9c",
        "title": "Mini-Batch Primal and Dual Methods for SVMs",
        "site": "https://proceedings.mlr.press/v28/takac13.html",
        "author": "Martin Takac; Avleen Bijral; Peter Richtarik; Nati Srebro",
        "abstract": "We address the issue of using mini-batches in stochastic  optimization of SVMs. We show that the same quantity, the  spectral norm of the data, controls the parallelization  speedup obtained for both primal stochastic subgradient descent(SGD) and stochastic dual coordinate ascent (SCDA) methods and use it to derive novel variants of mini-batched SDCA. Our guarantees for both methods are expressed in terms of the original nonsmooth primal problem based on the hinge-loss.",
        "bibtex": "@InProceedings{pmlr-v28-takac13,\n  title = \t {Mini-Batch Primal and Dual Methods for SVMs},\n  author = \t {Takac, Martin and Bijral, Avleen and Richtarik, Peter and Srebro, Nati},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1022--1030},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/takac13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/takac13.html},\n  abstract = \t {We address the issue of using mini-batches in stochastic  optimization of SVMs. We show that the same quantity, the  spectral norm of the data, controls the parallelization  speedup obtained for both primal stochastic subgradient descent(SGD) and stochastic dual coordinate ascent (SCDA) methods and use it to derive novel variants of mini-batched SDCA. Our guarantees for both methods are expressed in terms of the original nonsmooth primal problem based on the hinge-loss.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/takac13.pdf",
        "supp": "",
        "pdf_size": 257323,
        "gs_citation": 218,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5146216950599415235&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "University of Edinburgh; Toyota Technological Institute at Chicago; University of Edinburgh; Toyota Technological Institute at Chicago",
        "aff_domain": "gmail.com;ttic.edu;ed.ac.uk;uchicago.edu",
        "email": "gmail.com;ttic.edu;ed.ac.uk;uchicago.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "University of Edinburgh;Toyota Technological Institute at Chicago",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ed.ac.uk;https://www.tti-chicago.org",
        "aff_unique_abbr": "Edinburgh;TTI Chicago",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Chicago",
        "aff_country_unique_index": "0;1;0;1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "9512233228",
        "title": "Mixture of Mutually Exciting Processes for Viral Diffusion",
        "site": "https://proceedings.mlr.press/v28/yang13a.html",
        "author": "Shuang-Hong Yang; Hongyuan Zha",
        "abstract": "\\emphDiffusion network inference and \\emphmeme tracking have been two key challenges in viral diffusion. This paper shows that these two tasks can be addressed simultaneously with a probabilistic model involving a mixture of mutually exciting point processes. A fast learning algorithms is developed based on mean-field variational inference with budgeted diffusion bandwidth. The model is demonstrated with applications to the diffusion of viral texts in (1) online social networks (e.g., Twitter) and (2) the blogosphere on the Web.",
        "bibtex": "@InProceedings{pmlr-v28-yang13a,\n  title = \t {Mixture of Mutually Exciting Processes for Viral Diffusion},\n  author = \t {Yang, Shuang-Hong and Zha, Hongyuan},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1--9},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/yang13a.pdf},\n  url = \t {https://proceedings.mlr.press/v28/yang13a.html},\n  abstract = \t {\\emphDiffusion network inference and \\emphmeme tracking have been two key challenges in viral diffusion. This paper shows that these two tasks can be addressed simultaneously with a probabilistic model involving a mixture of mutually exciting point processes. A fast learning algorithms is developed based on mean-field variational inference with budgeted diffusion bandwidth. The model is demonstrated with applications to the diffusion of viral texts in (1) online social networks (e.g., Twitter) and (2) the blogosphere on the Web.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/yang13a.pdf",
        "supp": "",
        "pdf_size": 297124,
        "gs_citation": 236,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11548680350569943089&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Twitter Inc., 1355 Market St., San Francisco, CA 94103; College of Computing, Georgia Tech, Atlanta, GA 30332",
        "aff_domain": "twitter.com;cc.gatech.edu",
        "email": "twitter.com;cc.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Twitter Inc.;Georgia Institute of Technology",
        "aff_unique_dep": ";College of Computing",
        "aff_unique_url": "https://twitter.com;https://www.gatech.edu",
        "aff_unique_abbr": "Twitter;Georgia Tech",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Atlanta",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a9a6c16b7e",
        "title": "Modeling Information Propagation with Survival Theory",
        "site": "https://proceedings.mlr.press/v28/gomez-rodriguez13.html",
        "author": "Manuel Gomez-Rodriguez; Jure Leskovec; Bernhard Sch\u00f6lkopf",
        "abstract": "Networks provide a \u2018skeleton\u2019 for the spread of contagions, like, information, ideas, behaviors and diseases. Many times networks over which contagions diffuse are unobserved and need to be inferred. Here we apply survival theory to develop general additive and multiplicative risk models under which the network inference problems can be solved efficiently by exploiting their convexity. Our additive risk model generalizes several existing network inference models. We show all these models are particular cases of our more general model. Our multiplicative model allows for modeling scenarios in which a node can either increase or decrease the risk of activation of another node, in contrast with previous approaches, which consider only positive risk increments. We evaluate the performance of our network inference algorithms on large synthetic and real cascade datasets, and show that our models are able to predict the length and duration of cascades in real data.",
        "bibtex": "@InProceedings{pmlr-v28-gomez-rodriguez13,\n  title = \t {Modeling Information Propagation with Survival Theory},\n  author = \t {Gomez-Rodriguez, Manuel and Leskovec, Jure and Sch\u00f6lkopf, Bernhard},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {666--674},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/gomez-rodriguez13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/gomez-rodriguez13.html},\n  abstract = \t {Networks provide a \u2018skeleton\u2019 for the spread of contagions, like, information, ideas, behaviors and diseases. Many times networks over which contagions diffuse are unobserved and need to be inferred. Here we apply survival theory to develop general additive and multiplicative risk models under which the network inference problems can be solved efficiently by exploiting their convexity. Our additive risk model generalizes several existing network inference models. We show all these models are particular cases of our more general model. Our multiplicative model allows for modeling scenarios in which a node can either increase or decrease the risk of activation of another node, in contrast with previous approaches, which consider only positive risk increments. We evaluate the performance of our network inference algorithms on large synthetic and real cascade datasets, and show that our models are able to predict the length and duration of cascades in real data.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/gomez-rodriguez13.pdf",
        "supp": "",
        "pdf_size": 669558,
        "gs_citation": 218,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5595909019212232861&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "MPI for Intelligent Systems+Stanford University; Stanford University; MPI for Intelligent Systems",
        "aff_domain": "tuebingen.mpg.de;cs.stanford.edu;tuebingen.mpg.de",
        "email": "tuebingen.mpg.de;cs.stanford.edu;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;0",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems;Stanford University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.mpi-is.mpg.de;https://www.stanford.edu",
        "aff_unique_abbr": "MPI-IS;Stanford",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0+1;1;0",
        "aff_country_unique": "Germany;United States"
    },
    {
        "id": "49e718ef00",
        "title": "Modeling Musical Influence with Topic Models",
        "site": "https://proceedings.mlr.press/v28/shalit13.html",
        "author": "Uri Shalit; Daphna Weinshall; Gal Chechik",
        "abstract": "The role of musical influence has long been debated by scholars  and critics in the humanities, but never in a data-driven way.  In this work we approach the question of influence by applying topic-modeling tools (Blei & Lafferty, 2006; Gerrish & Blei, 2010) to a dataset of 24941 songs by 9222 artists, from the years 1922 to 2010. We find the models to be significantly correlated with a human-curated influence measure, and to clearly outperform a baseline method. Further using the learned model to study properties of influence, we find that musical influence and musical innovation are not monotonically correlated. However, we do find that the most influential songs were more innovative during two time periods: the early 1970\u2019s and the mid 1990\u2019s.",
        "bibtex": "@InProceedings{pmlr-v28-shalit13,\n  title = \t {Modeling Musical Influence with Topic Models},\n  author = \t {Shalit, Uri and Weinshall, Daphna and Chechik, Gal},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {244--252},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/shalit13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/shalit13.html},\n  abstract = \t {The role of musical influence has long been debated by scholars  and critics in the humanities, but never in a data-driven way.  In this work we approach the question of influence by applying topic-modeling tools (Blei & Lafferty, 2006; Gerrish & Blei, 2010) to a dataset of 24941 songs by 9222 artists, from the years 1922 to 2010. We find the models to be significantly correlated with a human-curated influence measure, and to clearly outperform a baseline method. Further using the learned model to study properties of influence, we find that musical influence and musical innovation are not monotonically correlated. However, we do find that the most influential songs were more innovative during two time periods: the early 1970\u2019s and the mid 1990\u2019s.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/shalit13.pdf",
        "supp": "",
        "pdf_size": 193733,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9761526224833973179&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "ICNC-ELSC & Computer Science Department, The Hebrew University of Jerusalem, 91904 Jerusalem Israel; Computer Science Department, The Hebrew University of Jerusalem, 91904 Jerusalem Israel; The Gonda Brain Research Center, Bar Ilan University, 52900 Ramat-Gan, Israel",
        "aff_domain": "mail.huji.ac.il;cs.huji.ac.il;biu.ac.il",
        "email": "mail.huji.ac.il;cs.huji.ac.il;biu.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Hebrew University of Jerusalem;Bar-Ilan University",
        "aff_unique_dep": "Computer Science Department;Gonda Brain Research Center",
        "aff_unique_url": "http://www.huji.ac.il;https://www.biu.ac.il",
        "aff_unique_abbr": "HUJI;BIU",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Jerusalem;Ramat-Gan",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "b94d63a54d",
        "title": "Modeling Temporal Evolution and Multiscale Structure in Networks",
        "site": "https://proceedings.mlr.press/v28/herlau13.html",
        "author": "Tue Herlau; Morten M\u00f8rup; Mikkel Schmidt",
        "abstract": "Many real-world networks exhibit both temporal evolution and multiscale structure.  We propose a model for temporally correlated multifurcating hierarchies in complex networks which jointly capture both effects. We use the Gibbs fragmentation tree as prior over multifurcating trees and a change-point model to account for the temporal evolution of each vertex.  We demonstrate that our model is able to infer time-varying multiscale structure in synthetic as well as three real world time-evolving complex networks.  Our modeling of the temporal evolution of hierarchies brings new insights into the changing roles and position of entities and possibilities for better understanding these dynamic complex systems.",
        "bibtex": "@InProceedings{pmlr-v28-herlau13,\n  title = \t {Modeling Temporal Evolution and Multiscale Structure in Networks},\n  author = \t {Herlau, Tue and M\u00f8rup, Morten and Schmidt, Mikkel},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {960--968},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/herlau13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/herlau13.html},\n  abstract = \t {Many real-world networks exhibit both temporal evolution and multiscale structure.  We propose a model for temporally correlated multifurcating hierarchies in complex networks which jointly capture both effects. We use the Gibbs fragmentation tree as prior over multifurcating trees and a change-point model to account for the temporal evolution of each vertex.  We demonstrate that our model is able to infer time-varying multiscale structure in synthetic as well as three real world time-evolving complex networks.  Our modeling of the temporal evolution of hierarchies brings new insights into the changing roles and position of entities and possibilities for better understanding these dynamic complex systems.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/herlau13.pdf",
        "supp": "",
        "pdf_size": 3502194,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9466397754725331583&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Technical University of Denmark, DTU Compute, 2800, Richard Pedersens Plads, Lyngby, Denmark; Technical University of Denmark, DTU Compute, 2800, Richard Pedersens Plads, Lyngby, Denmark; Technical University of Denmark, DTU Compute, 2800, Richard Pedersens Plads, Lyngby, Denmark",
        "aff_domain": "dtu.dk;dtu.dk;dtu.dk",
        "email": "dtu.dk;dtu.dk;dtu.dk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Technical University of Denmark",
        "aff_unique_dep": "DTU Compute",
        "aff_unique_url": "https://www.dtu.dk",
        "aff_unique_abbr": "DTU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Lyngby",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Denmark"
    },
    {
        "id": "d4ce6e3d71",
        "title": "Modelling Sparse Dynamical Systems with Compressed Predictive State Representations",
        "site": "https://proceedings.mlr.press/v28/hamilton13.html",
        "author": "William L. Hamilton; Mahdi Milani Fard; Joelle Pineau",
        "abstract": "Efficiently learning accurate models of dynamical systems is of central importance for developing rational agents that can succeed in a wide range of challenging domains. The difficulty of this learning problem is particularly acute in settings with large observation spaces and partial observability. We present a new algorithm, called Compressed Predictive State Representation (CPSR), for learning models of high-dimensional partially observable uncontrolled dynamical systems from small sample sets. The algorithm, which extends previous work on Predictive State Representations, exploits a particular sparse structure present in many domains. This sparse structure is used to compress information during learning, allowing for an increase in both the efficiency and predictive power. The compression technique also relieves the burden of domain specific feature selection and allows for domains with extremely large discrete observation spaces to be efficiently modelled. We present empirical results showing that the algorithm is able to build accurate models more efficiently than its uncompressed counterparts, and provide theoretical results on the accuracy of the learned compressed model.",
        "bibtex": "@InProceedings{pmlr-v28-hamilton13,\n  title = \t {Modelling Sparse Dynamical Systems with Compressed Predictive State Representations},\n  author = \t {Hamilton, William L. and Fard, Mahdi Milani and Pineau, Joelle},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {178--186},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/hamilton13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/hamilton13.html},\n  abstract = \t {Efficiently learning accurate models of dynamical systems is of central importance for developing rational agents that can succeed in a wide range of challenging domains. The difficulty of this learning problem is particularly acute in settings with large observation spaces and partial observability. We present a new algorithm, called Compressed Predictive State Representation (CPSR), for learning models of high-dimensional partially observable uncontrolled dynamical systems from small sample sets. The algorithm, which extends previous work on Predictive State Representations, exploits a particular sparse structure present in many domains. This sparse structure is used to compress information during learning, allowing for an increase in both the efficiency and predictive power. The compression technique also relieves the burden of domain specific feature selection and allows for domains with extremely large discrete observation spaces to be efficiently modelled. We present empirical results showing that the algorithm is able to build accurate models more efficiently than its uncompressed counterparts, and provide theoretical results on the accuracy of the learned compressed model.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/hamilton13.pdf",
        "supp": "",
        "pdf_size": 351065,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18332686762182083889&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Reasoning and Learning Laboratory, School of Computer Science, McGill University, Montreal, QC, Canada; Reasoning and Learning Laboratory, School of Computer Science, McGill University, Montreal, QC, Canada; Reasoning and Learning Laboratory, School of Computer Science, McGill University, Montreal, QC, Canada",
        "aff_domain": "cs.mcgill.ca;cs.mcgill.ca;cs.mcgill.ca",
        "email": "cs.mcgill.ca;cs.mcgill.ca;cs.mcgill.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "McGill University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.mcgill.ca",
        "aff_unique_abbr": "McGill",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Montreal",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "57370202b9",
        "title": "Monochromatic Bi-Clustering",
        "site": "https://proceedings.mlr.press/v28/wulff13.html",
        "author": "Sharon Wulff; Ruth Urner; Shai Ben-David",
        "abstract": "We propose a natural cost function for the bi-clustering task, the monochromatic cost.  This cost function is suitable for detecting meaningful homogeneous bi-clusters based on categorical valued input matrices. Such tasks arise in many applications, such as the analysis of social networks and in systems-biology where researchers try to infer functional grouping of biological agents based on their pairwise interactions. We analyze the computational complexity of the resulting optimization problem. We present a polynomial time approximation algorithm for this bi-clustering task and complement this result by showing that finding (exact) optimal solutions is NP-hard. As far as we know, these are the first positive approximation guarantees  and formal NP-hardness results  for any bi-clustering optimization problem.  In addition, we show that our optimization problem can be efficiently  solved by deterministic annealing,  yielding a promising heuristic for large problem instances.",
        "bibtex": "@InProceedings{pmlr-v28-wulff13,\n  title = \t {Monochromatic Bi-Clustering},\n  author = \t {Wulff, Sharon and Urner, Ruth and Ben-David, Shai},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {145--153},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/wulff13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/wulff13.html},\n  abstract = \t {We propose a natural cost function for the bi-clustering task, the monochromatic cost.  This cost function is suitable for detecting meaningful homogeneous bi-clusters based on categorical valued input matrices. Such tasks arise in many applications, such as the analysis of social networks and in systems-biology where researchers try to infer functional grouping of biological agents based on their pairwise interactions. We analyze the computational complexity of the resulting optimization problem. We present a polynomial time approximation algorithm for this bi-clustering task and complement this result by showing that finding (exact) optimal solutions is NP-hard. As far as we know, these are the first positive approximation guarantees  and formal NP-hardness results  for any bi-clustering optimization problem.  In addition, we show that our optimization problem can be efficiently  solved by deterministic annealing,  yielding a promising heuristic for large problem instances.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/wulff13.pdf",
        "supp": "",
        "pdf_size": 836113,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17138941314550762671&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, ETH Zurich, Switzerland; D.R.C. School of Computer Science, University of Waterloo, Canada, ON, N2L 3G1; D.R.C. School of Computer Science, University of Waterloo, Canada, ON, N2L 3G1",
        "aff_domain": "inf.ethz.ch;cs.uwaterloo.ca;cs.uwaterloo.ca",
        "email": "inf.ethz.ch;cs.uwaterloo.ca;cs.uwaterloo.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "ETH Zurich;University of Waterloo",
        "aff_unique_dep": "Department of Computer Science;School of Computer Science",
        "aff_unique_url": "https://www.ethz.ch;https://uwaterloo.ca",
        "aff_unique_abbr": "ETHZ;UW",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Waterloo",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Switzerland;Canada"
    },
    {
        "id": "1f0fa96c9e",
        "title": "Multi-Class Classification with Maximum Margin Multiple Kernel",
        "site": "https://proceedings.mlr.press/v28/cortes13.html",
        "author": "Corinna Cortes; Mehryar Mohri; Afshin Rostamizadeh",
        "abstract": "We present a new algorithm for multi-class classification with multiple kernels. Our algorithm is based on a natural notion of the multi-class margin of a kernel. We show that larger values of this quantity guarantee the existence of an accurate multi-class predictor and also define a family of multiple kernel algorithms based on the maximization of the multi-class margin of a kernel (M^3K).  We present an extensive theoretical analysis in support of our algorithm, including novel multi-class Rademacher complexity margin bounds.  Finally, we also report the results of a series of experiments with several data sets, including comparisons where we improve upon the performance of state-of-the-art algorithms both in binary and multi-class classification with multiple kernels.",
        "bibtex": "@InProceedings{pmlr-v28-cortes13,\n  title = \t {Multi-Class Classification with Maximum Margin Multiple Kernel},\n  author = \t {Cortes, Corinna and Mohri, Mehryar and Rostamizadeh, Afshin},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {46--54},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/cortes13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/cortes13.html},\n  abstract = \t {We present a new algorithm for multi-class classification with multiple kernels. Our algorithm is based on a natural notion of the multi-class margin of a kernel. We show that larger values of this quantity guarantee the existence of an accurate multi-class predictor and also define a family of multiple kernel algorithms based on the maximization of the multi-class margin of a kernel (M^3K).  We present an extensive theoretical analysis in support of our algorithm, including novel multi-class Rademacher complexity margin bounds.  Finally, we also report the results of a series of experiments with several data sets, including comparisons where we improve upon the performance of state-of-the-art algorithms both in binary and multi-class classification with multiple kernels.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/cortes13.pdf",
        "supp": "",
        "pdf_size": 482113,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11412939297715349678&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Google Research, 76 Ninth Avenue, New York, NY 10011; Courant Institute and Google Research, 251 Mercer Street, New York, NY 10012; Google Research, 76 Ninth Avenue, New York, NY 10011",
        "aff_domain": "google.com;cims.nyu.edu;google.com",
        "email": "google.com;cims.nyu.edu;google.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Google;Courant Institute of Mathematical Sciences",
        "aff_unique_dep": "Google Research;Mathematical Sciences",
        "aff_unique_url": "https://research.google;https://courant.nyu.edu",
        "aff_unique_abbr": "Google;Courant",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "New York",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "fc72289fb7",
        "title": "Multi-Task Learning with Gaussian Matrix Generalized Inverse Gaussian Model",
        "site": "https://proceedings.mlr.press/v28/yang13d.html",
        "author": "Ming Yang; Yingming Li; Zhongfei Zhang",
        "abstract": "In this paper, we study the multi-task learning problem with a new perspective of considering the structure of the residue error matrix and the low-rank approximation to the task covariance matrix simultaneously. In particular, we first introduce the Matrix Generalized Inverse Gaussian (MGIG) prior and define a Gaussian Matrix Generalized Inverse Gaussian (GMGIG) model for low-rank approximation to the task covariance matrix. Through combining the GMGIG model with the residual error structure assumption, we propose the GMGIG regression model for multi-task learning. To make the computation tractable, we simultaneously use variational inference and sampling techniques. In particular, we propose two sampling strategies for computing the statistics of the MGIG distribution. Experiments show that this model is superior to the peer methods in regression and prediction.",
        "bibtex": "@InProceedings{pmlr-v28-yang13d,\n  title = \t {Multi-Task Learning with Gaussian Matrix Generalized Inverse Gaussian Model},\n  author = \t {Yang, Ming and Li, Yingming and Zhang, Zhongfei},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {423--431},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/yang13d.pdf},\n  url = \t {https://proceedings.mlr.press/v28/yang13d.html},\n  abstract = \t {In this paper, we study the multi-task learning problem with a new perspective of considering the structure of the residue error matrix and the low-rank approximation to the task covariance matrix simultaneously. In particular, we first introduce the Matrix Generalized Inverse Gaussian (MGIG) prior and define a Gaussian Matrix Generalized Inverse Gaussian (GMGIG) model for low-rank approximation to the task covariance matrix. Through combining the GMGIG model with the residual error structure assumption, we propose the GMGIG regression model for multi-task learning. To make the computation tractable, we simultaneously use variational inference and sampling techniques. In particular, we propose two sampling strategies for computing the statistics of the MGIG distribution. Experiments show that this model is superior to the peer methods in regression and prediction.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/yang13d.pdf",
        "supp": "",
        "pdf_size": 464719,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13115397116880591045&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Information Science and Electronic Engineering, Zhejiang University, China; Department of Information Science and Electronic Engineering, Zhejiang University, China; Department of Information Science and Electronic Engineering, Zhejiang University, China",
        "aff_domain": "zju.edu.cn;zju.edu.cn;zju.edu.cn",
        "email": "zju.edu.cn;zju.edu.cn;zju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Zhejiang University",
        "aff_unique_dep": "Department of Information Science and Electronic Engineering",
        "aff_unique_url": "http://www.zju.edu.cn",
        "aff_unique_abbr": "ZJU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "f839df4114",
        "title": "Multi-View Clustering and Feature Learning via Structured Sparsity",
        "site": "https://proceedings.mlr.press/v28/wang13c.html",
        "author": "Hua Wang; Feiping Nie; Heng Huang",
        "abstract": "Combining information from various data sources has become an important research topic in machine learning with many scientific applications. Most previous studies employ kernels or graphs to integrate different types of features, which routinely assume one weight for one type of features. However, for many problems, the importance of features in one source to an individual cluster of data can be varied, which makes the previous approaches ineffective. In this paper, we propose a novel multi-view learning model to integrate all features and learn the weight for every feature with respect to each cluster individually via new joint structured sparsity-inducing norms. The proposed multi-view learning framework allows us not only to perform clustering tasks, but also to deal with classification tasks by an extension when the labeling knowledge is available. A new efficient algorithm is derived to solve the formulated objective with rigorous theoretical proof on its convergence. We applied our new data fusion method to five broadly used multi-view data sets for both clustering and classification. In all experimental results, our method clearly outperforms other related state-of-the-art methods.",
        "bibtex": "@InProceedings{pmlr-v28-wang13c,\n  title = \t {Multi-View Clustering and Feature Learning via Structured Sparsity},\n  author = \t {Wang, Hua and Nie, Feiping and Huang, Heng},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {352--360},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/wang13c.pdf},\n  url = \t {https://proceedings.mlr.press/v28/wang13c.html},\n  abstract = \t {Combining information from various data sources has become an important research topic in machine learning with many scientific applications. Most previous studies employ kernels or graphs to integrate different types of features, which routinely assume one weight for one type of features. However, for many problems, the importance of features in one source to an individual cluster of data can be varied, which makes the previous approaches ineffective. In this paper, we propose a novel multi-view learning model to integrate all features and learn the weight for every feature with respect to each cluster individually via new joint structured sparsity-inducing norms. The proposed multi-view learning framework allows us not only to perform clustering tasks, but also to deal with classification tasks by an extension when the labeling knowledge is available. A new efficient algorithm is derived to solve the formulated objective with rigorous theoretical proof on its convergence. We applied our new data fusion method to five broadly used multi-view data sets for both clustering and classification. In all experimental results, our method clearly outperforms other related state-of-the-art methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/wang13c.pdf",
        "supp": "",
        "pdf_size": 220775,
        "gs_citation": 340,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10472465306853636478&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Colorado School of Mines; The University of Texas at Arlington; The University of Texas at Arlington",
        "aff_domain": "gmail.com;gmail.com;uta.edu",
        "email": "gmail.com;gmail.com;uta.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Colorado School of Mines;University of Texas at Arlington",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.mines.edu;https://www.uta.edu",
        "aff_unique_abbr": "CSM;UTA",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Arlington",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8db52a84d4",
        "title": "Multilinear Multitask Learning",
        "site": "https://proceedings.mlr.press/v28/romera-paredes13.html",
        "author": "Bernardino Romera-Paredes; Hane Aung; Nadia Bianchi-Berthouze; Massimiliano Pontil",
        "abstract": "Many real world datasets occur or can be arranged into multi-modal structures.  With such datasets, the tasks to be learnt can be referenced by multiple indices.  Current multitask learning frameworks are not designed to account for the preservation of this information.  We propose the use of multilinear algebra as a natural way to model such a set of related tasks. We present two learning methods;   one is an adapted convex relaxation method used in the context of tensor completion.  The second method is based on the Tucker decomposition and on alternating minimization. Experiments on synthetic and real data indicate that the multilinear approaches provide a significant  improvement over other multitask learning methods. Overall our second  approach yields the best performance in all datasets.",
        "bibtex": "@InProceedings{pmlr-v28-romera-paredes13,\n  title = \t {Multilinear Multitask Learning},\n  author = \t {Romera-Paredes, Bernardino and Aung, Hane and Bianchi-Berthouze, Nadia and Pontil, Massimiliano},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1444--1452},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/romera-paredes13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/romera-paredes13.html},\n  abstract = \t {Many real world datasets occur or can be arranged into multi-modal structures.  With such datasets, the tasks to be learnt can be referenced by multiple indices.  Current multitask learning frameworks are not designed to account for the preservation of this information.  We propose the use of multilinear algebra as a natural way to model such a set of related tasks. We present two learning methods;   one is an adapted convex relaxation method used in the context of tensor completion.  The second method is based on the Tucker decomposition and on alternating minimization. Experiments on synthetic and real data indicate that the multilinear approaches provide a significant  improvement over other multitask learning methods. Overall our second  approach yields the best performance in all datasets.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/romera-paredes13.pdf",
        "supp": "",
        "pdf_size": 737163,
        "gs_citation": 200,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5605259947669941360&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science and UCL Interactive Centre, University College London, UK; Department of Computer Science and UCL Interactive Centre, University College London, UK; UCL Interactive Centre Division of Psychology & Language Sciences, University College London, UK; Department of Computer Science and Centre for Computational Statistics and Machine Learning, University College London, UK",
        "aff_domain": "ucl.ac.uk;ucl.ac.uk;ucl.ac.uk;cs.ucl.ac.uk",
        "email": "ucl.ac.uk;ucl.ac.uk;ucl.ac.uk;cs.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University College London",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ucl.ac.uk",
        "aff_unique_abbr": "UCL",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "London;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "63f3160a9b",
        "title": "Multiple Identifications in Multi-Armed Bandits",
        "site": "https://proceedings.mlr.press/v28/bubeck13.html",
        "author": "S\u00e9ebastian Bubeck; Tengyao Wang; Nitin Viswanathan",
        "abstract": "We study the problem of identifying the top m arms in a multi-armed bandit game. Our proposed solution relies on a new algorithm based on successive rejects of the seemingly bad arms, and successive accepts of the good ones. This algorithmic contribution allows to tackle other multiple identifications settings that were previously out of reach. In particular we show that this idea of successive accepts and rejects applies to the multi-bandit best arm identification problem.",
        "bibtex": "@InProceedings{pmlr-v28-bubeck13,\n  title = \t {Multiple Identifications in Multi-Armed Bandits},\n  author = \t {Bubeck, S\u00e9ebastian and Wang, Tengyao and Viswanathan, Nitin},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {258--265},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/bubeck13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/bubeck13.html},\n  abstract = \t {We study the problem of identifying the top m arms in a multi-armed bandit game. Our proposed solution relies on a new algorithm based on successive rejects of the seemingly bad arms, and successive accepts of the good ones. This algorithmic contribution allows to tackle other multiple identifications settings that were previously out of reach. In particular we show that this idea of successive accepts and rejects applies to the multi-bandit best arm identification problem.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/bubeck13.pdf",
        "supp": "",
        "pdf_size": 513087,
        "gs_citation": 251,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11597867103723640910&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Department of Operations Research and Financial Engineering, Princeton University; Department of Mathematics, Princeton University; Department of Computer Science, Princeton University",
        "aff_domain": "princeton.edu;princeton.edu;princeton.edu",
        "email": "princeton.edu;princeton.edu;princeton.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Princeton University",
        "aff_unique_dep": "Department of Operations Research and Financial Engineering",
        "aff_unique_url": "https://www.princeton.edu",
        "aff_unique_abbr": "Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e7d83f41f6",
        "title": "Multiple-source cross-validation",
        "site": "https://proceedings.mlr.press/v28/geras13.html",
        "author": "Krzysztof Geras; Charles Sutton",
        "abstract": "Cross-validation is an essential tool in machine learning and statistics. The typical procedure, in which data points are randomly assigned to one of the test sets, makes an implicit assumption that the data are exchangeable. A common case in which this does not hold is when the data come from multiple sources, in the sense used in transfer learning. In this case it is common to arrange the cross-validation procedure in a way that takes the source structure into account. Although common in practice, this procedure does not appear to have been theoretically analysed. We present new estimators of the variance of the cross-validation, both in the multiple-source setting and in the standard iid setting. These new estimators allow for much more accurate confidence intervals and hypothesis tests to compare algorithms.",
        "bibtex": "@InProceedings{pmlr-v28-geras13,\n  title = \t {Multiple-source cross-validation},\n  author = \t {Geras, Krzysztof and Sutton, Charles},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1292--1300},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/geras13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/geras13.html},\n  abstract = \t {Cross-validation is an essential tool in machine learning and statistics. The typical procedure, in which data points are randomly assigned to one of the test sets, makes an implicit assumption that the data are exchangeable. A common case in which this does not hold is when the data come from multiple sources, in the sense used in transfer learning. In this case it is common to arrange the cross-validation procedure in a way that takes the source structure into account. Although common in practice, this procedure does not appear to have been theoretically analysed. We present new estimators of the variance of the cross-validation, both in the multiple-source setting and in the standard iid setting. These new estimators allow for much more accurate confidence intervals and hypothesis tests to compare algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/geras13.pdf",
        "supp": "",
        "pdf_size": 937941,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12260697891789309119&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "School of Informatics, University of Edinburgh; School of Informatics, University of Edinburgh",
        "aff_domain": "sms.ed.ac.uk;inf.ed.ac.uk",
        "email": "sms.ed.ac.uk;inf.ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Edinburgh",
        "aff_unique_dep": "School of Informatics",
        "aff_unique_url": "https://www.ed.ac.uk",
        "aff_unique_abbr": "Edinburgh",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Edinburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "a859bc3b16",
        "title": "Natural Image Bases to Represent Neuroimaging Data",
        "site": "https://proceedings.mlr.press/v28/gupta13b.html",
        "author": "Ashish Gupta; Murat Ayhan; Anthony Maida",
        "abstract": "Visual inspection of neuroimagery is susceptible to human eye limitations.  Computerized methods have been shown to be equally or more effective   than human clinicians in diagnosing dementia from neuroimages. Nevertheless,   much of the work involves the use of domain expertise to extract hand-crafted features. The key technique in this paper is the use of cross-domain features to represent MRI data.  We used a sparse autoencoder to learn a set of bases from natural images and   then applied convolution to extract features from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) dataset.Using this new representation, we classify MRI instances into three categories: Alzheimer\u2019s Disease (AD), Mild Cognitive Impairment (MCI) and Healthy Control (HC).Our approach, in spite of being very simple, achieved high classification performance,   which is competitive with or better than other approaches.",
        "bibtex": "@InProceedings{pmlr-v28-gupta13b,\n  title = \t {Natural Image Bases to Represent Neuroimaging Data},\n  author = \t {Gupta, Ashish and Ayhan, Murat and Maida, Anthony},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {987--994},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/gupta13b.pdf},\n  url = \t {https://proceedings.mlr.press/v28/gupta13b.html},\n  abstract = \t {Visual inspection of neuroimagery is susceptible to human eye limitations.  Computerized methods have been shown to be equally or more effective   than human clinicians in diagnosing dementia from neuroimages. Nevertheless,   much of the work involves the use of domain expertise to extract hand-crafted features. The key technique in this paper is the use of cross-domain features to represent MRI data.  We used a sparse autoencoder to learn a set of bases from natural images and   then applied convolution to extract features from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) dataset.Using this new representation, we classify MRI instances into three categories: Alzheimer\u2019s Disease (AD), Mild Cognitive Impairment (MCI) and Healthy Control (HC).Our approach, in spite of being very simple, achieved high classification performance,   which is competitive with or better than other approaches.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/gupta13b.pdf",
        "supp": "",
        "pdf_size": 3530909,
        "gs_citation": 324,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3843456118525353104&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Lousiana at Lafayette; University of Lousiana at Lafayette; University of Lousiana at Lafayette",
        "aff_domain": "louisiana.edu;louisiana.edu;cacs.louisiana.edu",
        "email": "louisiana.edu;louisiana.edu;cacs.louisiana.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Louisiana at Lafayette",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ul Lafayette.edu",
        "aff_unique_abbr": "ULL",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Lafayette",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "240ec49745",
        "title": "Near-Optimal Bounds for Cross-Validation via Loss Stability",
        "site": "https://proceedings.mlr.press/v28/kumar13a.html",
        "author": "Ravi Kumar; Daniel Lokshtanov; Sergei Vassilvitskii; Andrea Vattani",
        "abstract": "Multi-fold cross-validation is an established practice to estimate the error rate   of a learning algorithm.  Quantifying the variance reduction gains due to cross-validation   has been challenging due to the inherent correlations introduced by the folds.    In this work we introduce a new and weak measure of stability called \\emphloss stability  and relate the cross-validation performance to loss stability; we also establish that this   relationship is near-optimal.  Our work thus quantitatively improves the current  best bounds on cross-validation.",
        "bibtex": "@InProceedings{pmlr-v28-kumar13a,\n  title = \t {Near-Optimal Bounds for Cross-Validation via Loss Stability},\n  author = \t {Kumar, Ravi and Lokshtanov, Daniel and Vassilvitskii, Sergei and Vattani, Andrea},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {27--35},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/kumar13a.pdf},\n  url = \t {https://proceedings.mlr.press/v28/kumar13a.html},\n  abstract = \t {Multi-fold cross-validation is an established practice to estimate the error rate   of a learning algorithm.  Quantifying the variance reduction gains due to cross-validation   has been challenging due to the inherent correlations introduced by the folds.    In this work we introduce a new and weak measure of stability called \\emphloss stability  and relate the cross-validation performance to loss stability; we also establish that this   relationship is near-optimal.  Our work thus quantitatively improves the current  best bounds on cross-validation.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/kumar13a.pdf",
        "supp": "",
        "pdf_size": 331230,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12293854756232008675&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Google, 1600 Amphitheater Parkway, Mountain View, CA 94043 USA; Department of Computer Science and Engineering, UC San Diego, La Jolla, CA 92093 USA; Google, 1600 Amphitheater Parkway, Mountain View, CA 94043 USA; Department of Computer Science and Engineering, UC San Diego, La Jolla, CA 92093 USA",
        "aff_domain": "gmail.com;ii.uib.no;google.com;cs.ucsd.edu",
        "email": "gmail.com;ii.uib.no;google.com;cs.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "Google;University of California, San Diego",
        "aff_unique_dep": "Google;Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.google.com;https://www.ucsd.edu",
        "aff_unique_abbr": "Google;UCSD",
        "aff_campus_unique_index": "0;1;0;1",
        "aff_campus_unique": "Mountain View;La Jolla",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "68cf3e3f9a",
        "title": "Near-optimal Batch Mode Active Learning and Adaptive Submodular Optimization",
        "site": "https://proceedings.mlr.press/v28/chen13b.html",
        "author": "Yuxin Chen; Andreas Krause",
        "abstract": "Active learning can lead to a dramatic reduction in labeling effort. However, in many practical implementations (such as crowdsourcing,  surveys, high-throughput experimental design), it is preferable to query labels for batches of examples to be labelled in parallel. While several heuristics have been proposed for batch-mode active learning, little is known about their theoretical performance.    We consider batch mode active learning and more general information-parallel stochastic optimization problems that exhibit adaptive submodularity, a natural diminishing returns condition. We prove that for such problems, a simple greedy strategy is competitive with the optimal batch-mode policy. In some cases, surprisingly, the use of batches incurs competitively low cost, even when compared to a fully sequential strategy. We demonstrate the effectiveness of our approach on batch-mode active learning tasks, where it outperforms the state of the art, as well as the novel problem of multi-stage influence maximization in social networks.",
        "bibtex": "@InProceedings{pmlr-v28-chen13b,\n  title = \t {Near-optimal Batch Mode Active Learning and Adaptive Submodular Optimization},\n  author = \t {Chen, Yuxin and Krause, Andreas},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {160--168},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/chen13b.pdf},\n  url = \t {https://proceedings.mlr.press/v28/chen13b.html},\n  abstract = \t {Active learning can lead to a dramatic reduction in labeling effort. However, in many practical implementations (such as crowdsourcing,  surveys, high-throughput experimental design), it is preferable to query labels for batches of examples to be labelled in parallel. While several heuristics have been proposed for batch-mode active learning, little is known about their theoretical performance.    We consider batch mode active learning and more general information-parallel stochastic optimization problems that exhibit adaptive submodularity, a natural diminishing returns condition. We prove that for such problems, a simple greedy strategy is competitive with the optimal batch-mode policy. In some cases, surprisingly, the use of batches incurs competitively low cost, even when compared to a fully sequential strategy. We demonstrate the effectiveness of our approach on batch-mode active learning tasks, where it outperforms the state of the art, as well as the novel problem of multi-stage influence maximization in social networks.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/chen13b.pdf",
        "supp": "",
        "pdf_size": 1023251,
        "gs_citation": 183,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15960278973997897778&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "ETH Zurich, Universit\u00a8 atstrasse 6, 8092 Z\u00a8 urich, Switzerland; ETH Zurich, Universit\u00a8 atstrasse 6, 8092 Z\u00a8 urich, Switzerland",
        "aff_domain": "inf.ethz.ch;ethz.ch",
        "email": "inf.ethz.ch;ethz.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Zurich",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "5db7c48464",
        "title": "Nested Chinese Restaurant Franchise Process:  Applications to User Tracking and Document Modeling",
        "site": "https://proceedings.mlr.press/v28/ahmed13.html",
        "author": "Amr Ahmed; Liangjie Hong; Alexander Smola",
        "abstract": "Much natural data is hierarchical in nature. Moreover, this hierarchy  is often shared between different instances. We introduce the  nested Chinese Restaurant Franchise Process as a means to obtain both  hierarchical tree-structured representations for objects, akin to (but more general than) the nested Chinese Restaurant Process while sharing their structure akin  to the Hierarchical Dirichlet Process.     Moreover, by decoupling the \\emphstructure generating part of the  process from the components responsible for the observations, we are  able to apply the same statistical approach to a variety of user  generated data. In particular, we model the joint distribution of  microblogs and locations for Twitter for users. This leads to a 40%  reduction in location uncertainty relative to the best previously  published results. Moreover, we model documents from the NIPS papers  dataset, obtaining excellent perplexity relative to (hierarchical)  Pachinko allocation and LDA.",
        "bibtex": "@InProceedings{pmlr-v28-ahmed13,\n  title = \t {Nested Chinese Restaurant Franchise Process:  Applications to User Tracking and Document Modeling},\n  author = \t {Ahmed, Amr and Hong, Liangjie and Smola, Alexander},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1426--1434},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/ahmed13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/ahmed13.html},\n  abstract = \t {Much natural data is hierarchical in nature. Moreover, this hierarchy  is often shared between different instances. We introduce the  nested Chinese Restaurant Franchise Process as a means to obtain both  hierarchical tree-structured representations for objects, akin to (but more general than) the nested Chinese Restaurant Process while sharing their structure akin  to the Hierarchical Dirichlet Process.     Moreover, by decoupling the \\emphstructure generating part of the  process from the components responsible for the observations, we are  able to apply the same statistical approach to a variety of user  generated data. In particular, we model the joint distribution of  microblogs and locations for Twitter for users. This leads to a 40%  reduction in location uncertainty relative to the best previously  published results. Moreover, we model documents from the NIPS papers  dataset, obtaining excellent perplexity relative to (hierarchical)  Pachinko allocation and LDA.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/ahmed13.pdf",
        "supp": "",
        "pdf_size": 710303,
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15586486223472758441&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Research @ Google; Yahoo! Labs; Carnegie Mellon University",
        "aff_domain": "google.com;yahoo-inc.com;smola.org",
        "email": "google.com;yahoo-inc.com;smola.org",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Google;Yahoo!;Carnegie Mellon University",
        "aff_unique_dep": "Google Research;Yahoo! Labs;",
        "aff_unique_url": "https://research.google;https://yahoo.com;https://www.cmu.edu",
        "aff_unique_abbr": "Google;Yahoo!;CMU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d3444139ff",
        "title": "No more pesky learning rates",
        "site": "https://proceedings.mlr.press/v28/schaul13.html",
        "author": "Tom Schaul; Sixin Zhang; Yann LeCun",
        "abstract": "The performance of stochastic gradient descent (SGD) depends critically on how learning rates are tuned and decreased over time. We propose a method to automatically adjust multiple learning rates so as to minimize the expected error at any one time. The method relies on local gradient variations across samples. In our approach, learning rates can increase as well as decrease, making it suitable for non-stationary problems. Using a number of convex and non-convex learning tasks, we show that the resulting algorithm matches the performance of the best settings obtained through systematic search, and effectively removes the need for learning rate tuning.",
        "bibtex": "@InProceedings{pmlr-v28-schaul13,\n  title = \t {No more pesky learning rates},\n  author = \t {Schaul, Tom and Zhang, Sixin and LeCun, Yann},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {343--351},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/schaul13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/schaul13.html},\n  abstract = \t {The performance of stochastic gradient descent (SGD) depends critically on how learning rates are tuned and decreased over time. We propose a method to automatically adjust multiple learning rates so as to minimize the expected error at any one time. The method relies on local gradient variations across samples. In our approach, learning rates can increase as well as decrease, making it suitable for non-stationary problems. Using a number of convex and non-convex learning tasks, we show that the resulting algorithm matches the performance of the best settings obtained through systematic search, and effectively removes the need for learning rate tuning.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/schaul13.pdf",
        "supp": "",
        "pdf_size": 580195,
        "gs_citation": 611,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2000364931588142833&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 23,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "16ddde7d0a",
        "title": "Noisy Sparse Subspace Clustering",
        "site": "https://proceedings.mlr.press/v28/wang13.html",
        "author": "Yu-Xiang Wang; Huan Xu",
        "abstract": "This paper considers the problem of subspace clustering under noise. Specifically, we study the behavior of Sparse Subspace Clustering (SSC) when either adversarial or random noise is added to the unlabelled input data points, which are assumed to lie in a union of low-dimensional subspaces.  We show that a modified version of SSC is \\emphprovably effective in correctly identifying the underlying subspaces, even with noisy data. This extends theoretical guarantee of this algorithm to the practical setting and provides justification to the success of SSC in a class of real applications.",
        "bibtex": "@InProceedings{pmlr-v28-wang13,\n  title = \t {Noisy Sparse Subspace Clustering},\n  author = \t {Wang, Yu-Xiang and Xu, Huan},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {89--97},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/wang13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/wang13.html},\n  abstract = \t {This paper considers the problem of subspace clustering under noise. Specifically, we study the behavior of Sparse Subspace Clustering (SSC) when either adversarial or random noise is added to the unlabelled input data points, which are assumed to lie in a union of low-dimensional subspaces.  We show that a modified version of SSC is \\emphprovably effective in correctly identifying the underlying subspaces, even with noisy data. This extends theoretical guarantee of this algorithm to the practical setting and provides justification to the success of SSC in a class of real applications.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/wang13.pdf",
        "supp": "",
        "pdf_size": 576409,
        "gs_citation": 243,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16305375188032010121&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Mechanical Engineering, National University of Singapore, Singapore 117576; Department of Mechanical Engineering, National University of Singapore, Singapore 117576",
        "aff_domain": "nus.edu.sg;nus.edu.sg",
        "email": "nus.edu.sg;nus.edu.sg",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "National University of Singapore",
        "aff_unique_dep": "Department of Mechanical Engineering",
        "aff_unique_url": "https://www.nus.edu.sg",
        "aff_unique_abbr": "NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "dcb3b2c9ec",
        "title": "Noisy and Missing Data Regression: Distribution-Oblivious Support Recovery",
        "site": "https://proceedings.mlr.press/v28/chen13d.html",
        "author": "Yudong Chen; Constantine Caramanis",
        "abstract": "Many models for sparse regression typically assume that the covariates are known completely, and without noise. Particularly in high-dimensional applications, this is often not the case. Worse yet, even estimating statistics of the noise (the noise covariance) can be a central challenge. In this paper we develop a simple variant of orthogonal matching pursuit (OMP) for precisely this setting. We show that without knowledge of the noise covariance, our algorithm recovers the support, and we provide matching lower bounds that show that our algorithm performs at the minimax optimal rate. While simple, this is the first algorithm that (provably) recovers support in a noise-distribution-oblivious manner. When knowledge of the noise-covariance is available, our algorithm matches the best-known \\ell^2-recovery bounds available. We show that these too are min-max optimal. Along the way, we also obtain improved performance guarantees for OMP for the standard sparse regression problem with Gaussian noise.",
        "bibtex": "@InProceedings{pmlr-v28-chen13d,\n  title = \t {Noisy and Missing Data Regression: Distribution-Oblivious Support Recovery},\n  author = \t {Chen, Yudong and Caramanis, Constantine},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {383--391},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/chen13d.pdf},\n  url = \t {https://proceedings.mlr.press/v28/chen13d.html},\n  abstract = \t {Many models for sparse regression typically assume that the covariates are known completely, and without noise. Particularly in high-dimensional applications, this is often not the case. Worse yet, even estimating statistics of the noise (the noise covariance) can be a central challenge. In this paper we develop a simple variant of orthogonal matching pursuit (OMP) for precisely this setting. We show that without knowledge of the noise covariance, our algorithm recovers the support, and we provide matching lower bounds that show that our algorithm performs at the minimax optimal rate. While simple, this is the first algorithm that (provably) recovers support in a noise-distribution-oblivious manner. When knowledge of the noise-covariance is available, our algorithm matches the best-known \\ell^2-recovery bounds available. We show that these too are min-max optimal. Along the way, we also obtain improved performance guarantees for OMP for the standard sparse regression problem with Gaussian noise.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/chen13d.pdf",
        "supp": "",
        "pdf_size": 461781,
        "gs_citation": 80,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14148126063872413393&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Electrical and Computer Engineering, The University of Texas at Austin; Department of Electrical and Computer Engineering, The University of Texas at Austin",
        "aff_domain": "utexas.edu;mail.utexas.edu",
        "email": "utexas.edu;mail.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7a8d61ac3e",
        "title": "Non-Linear Stationary Subspace Analysis with Application to Video Classification",
        "site": "https://proceedings.mlr.press/v28/baktashmotlagh13.html",
        "author": "Mahsa Baktashmotlagh; Mehrtash Harandi; Abbas Bigdeli; Brian Lovell; Mathieu Salzmann",
        "abstract": "Low-dimensional representations are key to the success of many video classification algorithms. However, the commonly-used dimensionality reduction techniques fail to account for the fact that only part of the signal is shared across all the videos in one class. As a consequence, the resulting representations contain instance-specific information, which introduces noise in the classification process. In this paper, we introduce Non-Linear Stationary Subspace Analysis: A method that overcomes this issue by explicitly separating the stationary parts of the video signal (i.e., the parts shared across all videos in one class), from its non-stationary parts (i.e., specific to individual videos). We demonstrate the effectiveness of our approach on action recognition, dynamic texture classification and scene recognition.",
        "bibtex": "@InProceedings{pmlr-v28-baktashmotlagh13,\n  title = \t {Non-Linear Stationary Subspace Analysis with Application to Video Classification},\n  author = \t {Baktashmotlagh, Mahsa and Harandi, Mehrtash and Bigdeli, Abbas and Lovell, Brian and Salzmann, Mathieu},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {450--458},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/baktashmotlagh13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/baktashmotlagh13.html},\n  abstract = \t {Low-dimensional representations are key to the success of many video classification algorithms. However, the commonly-used dimensionality reduction techniques fail to account for the fact that only part of the signal is shared across all the videos in one class. As a consequence, the resulting representations contain instance-specific information, which introduces noise in the classification process. In this paper, we introduce Non-Linear Stationary Subspace Analysis: A method that overcomes this issue by explicitly separating the stationary parts of the video signal (i.e., the parts shared across all videos in one class), from its non-stationary parts (i.e., specific to individual videos). We demonstrate the effectiveness of our approach on action recognition, dynamic texture classification and scene recognition.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/baktashmotlagh13.pdf",
        "supp": "",
        "pdf_size": 1088845,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18282017610682724862&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "University of Queensland, School of ITEE, QLD 4072, Australia+NICTA, Locked Bag 8001, Canberra, ACT 2601, Australia; NICTA, Locked Bag 8001, Canberra, ACT 2601, Australia; NICTA, Locked Bag 8001, Canberra, ACT 2601, Australia; University of Queensland, School of ITEE, QLD 4072, Australia; NICTA, Locked Bag 8001, Canberra, ACT 2601, Australia",
        "aff_domain": "nicta.com.au;nicta.com.au;nicta.com.au;itee.uq.edu.au;nicta.com.au",
        "email": "nicta.com.au;nicta.com.au;nicta.com.au;itee.uq.edu.au;nicta.com.au",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;1;0;1",
        "aff_unique_norm": "University of Queensland;NICTA",
        "aff_unique_dep": "School of Information Technology and Electrical Engineering;",
        "aff_unique_url": "https://www.uq.edu.au;",
        "aff_unique_abbr": "UQ;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "St. Lucia;",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "c4069e4f13",
        "title": "Nonparametric Mixture of Gaussian Processes with Constraints",
        "site": "https://proceedings.mlr.press/v28/ross13a.html",
        "author": "James Ross; Jennifer Dy",
        "abstract": "Motivated by the need to identify new and clinically relevant categories of lung disease, we propose a novel clustering with constraints method using a Dirichlet process mixture of Gaussian processes in a variational Bayesian nonparametric framework. We claim that individuals should be grouped according to biological and/or genetic similarity regardless of their level of disease severity; therefore, we introduce a new way of looking at subtyping/clustering by recasting it in terms of discovering associations of individuals to disease trajectories (i.e., grouping individuals based on their similarity in response to environmental and/or disease causing variables). The nonparametric nature of our algorithm allows for learning the unknown number of meaningful trajectories. Additionally, we acknowledge the usefulness of expert guidance by providing for their input using must-link and cannot- link constraints. These constraints are encoded with Markov random fields. We also provide an efficient variational approach for performing inference on our model.",
        "bibtex": "@InProceedings{pmlr-v28-ross13a,\n  title = \t {Nonparametric Mixture of Gaussian Processes with Constraints},\n  author = \t {Ross, James and Dy, Jennifer},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1346--1354},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/ross13a.pdf},\n  url = \t {https://proceedings.mlr.press/v28/ross13a.html},\n  abstract = \t {Motivated by the need to identify new and clinically relevant categories of lung disease, we propose a novel clustering with constraints method using a Dirichlet process mixture of Gaussian processes in a variational Bayesian nonparametric framework. We claim that individuals should be grouped according to biological and/or genetic similarity regardless of their level of disease severity; therefore, we introduce a new way of looking at subtyping/clustering by recasting it in terms of discovering associations of individuals to disease trajectories (i.e., grouping individuals based on their similarity in response to environmental and/or disease causing variables). The nonparametric nature of our algorithm allows for learning the unknown number of meaningful trajectories. Additionally, we acknowledge the usefulness of expert guidance by providing for their input using must-link and cannot- link constraints. These constraints are encoded with Markov random fields. We also provide an efficient variational approach for performing inference on our model.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/ross13a.pdf",
        "supp": "",
        "pdf_size": 1420092,
        "gs_citation": 84,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13078946941318461934&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Brigham and Women\u2019s Hospital, Boston, MA 02115 USA+Department of Electrical and Computer Engineering, Northeastern University, Boston, MA 02115 USA; Department of Electrical and Computer Engineering, Northeastern University, Boston, MA 02115 USA",
        "aff_domain": "bwh.harvard.edu;ece.neu.edu",
        "email": "bwh.harvard.edu;ece.neu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1",
        "aff_unique_norm": "Brigham and Women's Hospital;Northeastern University",
        "aff_unique_dep": ";Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.brighamandwomens.org;https://www.northeastern.edu",
        "aff_unique_abbr": ";NU",
        "aff_campus_unique_index": "0+0;0",
        "aff_campus_unique": "Boston",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "24fbfc2e2e",
        "title": "O(logT) Projections for Stochastic Optimization of Smooth and Strongly Convex Functions",
        "site": "https://proceedings.mlr.press/v28/zhang13e.html",
        "author": "Lijun Zhang; Tianbao Yang; Rong Jin; Xiaofei He",
        "abstract": "Traditional algorithms for stochastic optimization require projecting the solution at each iteration into a given domain to ensure its feasibility. When facing complex domains, such as the positive semidefinite cone, the projection operation can be expensive, leading to a high computational cost per iteration. In this paper, we present a novel algorithm that aims to reduce the number of projections for stochastic optimization. The proposed algorithm combines the strength of several recent developments in stochastic optimization, including mini-batches, extra-gradient, and epoch gradient descent, in order to effectively explore the smoothness and strong convexity. We show, both in expectation and with a high probability, that when the objective function is both smooth and strongly convex, the proposed algorithm achieves the optimal O(1/T) rate of convergence with only O(logT) projections. Our empirical study verifies the theoretical result.",
        "bibtex": "@InProceedings{pmlr-v28-zhang13e,\n  title = \t {O(logT) Projections for Stochastic Optimization of Smooth and Strongly Convex Functions},\n  author = \t {Zhang, Lijun and Yang, Tianbao and Jin, Rong and He, Xiaofei},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1121--1129},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/zhang13e.pdf},\n  url = \t {https://proceedings.mlr.press/v28/zhang13e.html},\n  abstract = \t {Traditional algorithms for stochastic optimization require projecting the solution at each iteration into a given domain to ensure its feasibility. When facing complex domains, such as the positive semidefinite cone, the projection operation can be expensive, leading to a high computational cost per iteration. In this paper, we present a novel algorithm that aims to reduce the number of projections for stochastic optimization. The proposed algorithm combines the strength of several recent developments in stochastic optimization, including mini-batches, extra-gradient, and epoch gradient descent, in order to effectively explore the smoothness and strong convexity. We show, both in expectation and with a high probability, that when the objective function is both smooth and strongly convex, the proposed algorithm achieves the optimal O(1/T) rate of convergence with only O(logT) projections. Our empirical study verifies the theoretical result.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/zhang13e.pdf",
        "supp": "",
        "pdf_size": 226723,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7959361462395948815&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computer Science and Engineering, Michigan State University, East Lansing, MI 48824, USA; GE Global Research, San Ramon, CA 94583, USA; Department of Computer Science and Engineering, Michigan State University, East Lansing, MI 48824, USA; State Key Laboratory of CAD&CG, College of Computer Science, Zhejiang University, Hangzhou 310027, China",
        "aff_domain": "msu.edu;ge.com;cse.msu.edu;cad.zju.edu.cn",
        "email": "msu.edu;ge.com;cse.msu.edu;cad.zju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "Michigan State University;GE Global Research;Zhejiang University",
        "aff_unique_dep": "Department of Computer Science and Engineering;;College of Computer Science",
        "aff_unique_url": "https://www.msu.edu;https://www.ge.com/research;http://www.zju.edu.cn",
        "aff_unique_abbr": "MSU;GE Research;ZJU",
        "aff_campus_unique_index": "0;1;0;2",
        "aff_campus_unique": "East Lansing;San Ramon;Hangzhou",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "f67a7e6637",
        "title": "On A Nonlinear Generalization of Sparse Coding and Dictionary Learning",
        "site": "https://proceedings.mlr.press/v28/ho13a.html",
        "author": "Jeffrey Ho; Yuchen Xie; Baba Vemuri",
        "abstract": "Existing dictionary learning algorithms are based on the  assumption that the data are vectors in an Euclidean vector  space, and the dictionary is learned from the training data using the vector space structure and its Euclidean metric. However, in many applications, features and data often originated from a Riemannian manifold that does not support a global linear (vector space) structure.  Furthermore, the extrinsic viewpoint of existing dictionary learning algorithms becomes inappropriate for modeling and incorporating the intrinsic geometry of the manifold that is potentially important and critical to the application. This paper proposes a novel framework for sparse coding and dictionary learning for data on a Riemannian manifold, and it shows that  the existing sparse coding and dictionary learning methods can be considered as special (Euclidean) cases of the more general framework proposed here. We show that both the dictionary and sparse coding can  be effectively computed for several important classes of Riemannian manifolds, and we validate the proposed method using two well-known classification problems in computer vision and medical imaging analysis.",
        "bibtex": "@InProceedings{pmlr-v28-ho13a,\n  title = \t {On A Nonlinear Generalization of Sparse Coding and Dictionary Learning},\n  author = \t {Ho, Jeffrey and Xie, Yuchen and Vemuri, Baba},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1480--1488},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/ho13a.pdf},\n  url = \t {https://proceedings.mlr.press/v28/ho13a.html},\n  abstract = \t {Existing dictionary learning algorithms are based on the  assumption that the data are vectors in an Euclidean vector  space, and the dictionary is learned from the training data using the vector space structure and its Euclidean metric. However, in many applications, features and data often originated from a Riemannian manifold that does not support a global linear (vector space) structure.  Furthermore, the extrinsic viewpoint of existing dictionary learning algorithms becomes inappropriate for modeling and incorporating the intrinsic geometry of the manifold that is potentially important and critical to the application. This paper proposes a novel framework for sparse coding and dictionary learning for data on a Riemannian manifold, and it shows that  the existing sparse coding and dictionary learning methods can be considered as special (Euclidean) cases of the more general framework proposed here. We show that both the dictionary and sparse coding can  be effectively computed for several important classes of Riemannian manifolds, and we validate the proposed method using two well-known classification problems in computer vision and medical imaging analysis.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/ho13a.pdf",
        "supp": "",
        "pdf_size": 1183687,
        "gs_citation": 118,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5659729959750811142&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Qualcomm Technologies, Inc., San Diego, CA 92121 USA; University of Florida, Gainesville, FL 32611 USA; University of Florida, Gainesville, FL 32611 USA",
        "aff_domain": "cise.ufl.edu;cise.ufl.edu;cise.ufl.edu",
        "email": "cise.ufl.edu;cise.ufl.edu;cise.ufl.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Qualcomm Technologies, Inc.;University of Florida",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.qualcomm.com;https://www.ufl.edu",
        "aff_unique_abbr": "QTI;UF",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "San Diego;Gainesville",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c1ba215a13",
        "title": "On Compact Codes for Spatially Pooled Features",
        "site": "https://proceedings.mlr.press/v28/jia13.html",
        "author": "Yangqing Jia; Oriol Vinyals; Trevor Darrell",
        "abstract": "Feature encoding with an overcomplete dictionary has demonstrated good performance in many applications, especially computer vision. In this paper we analyze the classification accuracy with respect to dictionary size by linking the encoding stage to kernel methods and \\nystrom sampling, and obtain useful bounds on accuracy as a function of size. The \\nystrom method also inspires us to revisit dictionary learning from local patches, and we propose to learn the dictionary in an end-to-end fashion taking into account pooling, a common computational layer in vision. We validate our contribution by showing how the derived bounds are able to explain the observed behavior of multiple datasets, and show that the pooling aware method efficiently reduces the dictionary size by a factor of two for a given accuracy.",
        "bibtex": "@InProceedings{pmlr-v28-jia13,\n  title = \t {On Compact Codes for Spatially Pooled Features},\n  author = \t {Jia, Yangqing and Vinyals, Oriol and Darrell, Trevor},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {549--557},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/jia13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/jia13.html},\n  abstract = \t {Feature encoding with an overcomplete dictionary has demonstrated good performance in many applications, especially computer vision. In this paper we analyze the classification accuracy with respect to dictionary size by linking the encoding stage to kernel methods and \\nystrom sampling, and obtain useful bounds on accuracy as a function of size. The \\nystrom method also inspires us to revisit dictionary learning from local patches, and we propose to learn the dictionary in an end-to-end fashion taking into account pooling, a common computational layer in vision. We validate our contribution by showing how the derived bounds are able to explain the observed behavior of multiple datasets, and show that the pooling aware method efficiently reduces the dictionary size by a factor of two for a given accuracy.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/jia13.pdf",
        "supp": "",
        "pdf_size": 1575311,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15915103927698334021&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "UC Berkeley EECS, Berkeley, CA 94704 USA; UC Berkeley EECS, Berkeley, CA 94704 USA; UC Berkeley EECS, Berkeley, CA 94704 USA",
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Electrical Engineering and Computer Sciences",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "57b51a0240",
        "title": "On autoencoder scoring",
        "site": "https://proceedings.mlr.press/v28/kamyshanska13.html",
        "author": "Hanna Kamyshanska; Roland Memisevic",
        "abstract": "Autoencoders are popular feature learning models because they are conceptually simple, easy to train and allow for efficient inference and training. Recent work has shown how certain autoencoders can assign an unnormalized \u201cscore\u201d to data which measures how well the autoencoder can represent the data. Scores are commonly computed by using training criteria that relate the autoencoder to a probabilistic model, such as the Restricted Boltzmann Machine. In this paper we show how an autoencoder can assign meaningful scores to data independently of training procedure and without reference to any probabilistic model, by interpreting it as a dynamical system. We discuss how, and under which conditions, running the dynamical system can be viewed as performing gradient descent in an energy function, which in turn allows us to derive a score via integration. We also show how one can combine multiple, unnormalized scores into a generative classifier.",
        "bibtex": "@InProceedings{pmlr-v28-kamyshanska13,\n  title = \t {On autoencoder scoring},\n  author = \t {Kamyshanska, Hanna and Memisevic, Roland},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {720--728},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/kamyshanska13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/kamyshanska13.html},\n  abstract = \t {Autoencoders are popular feature learning models because they are conceptually simple, easy to train and allow for efficient inference and training. Recent work has shown how certain autoencoders can assign an unnormalized \u201cscore\u201d to data which measures how well the autoencoder can represent the data. Scores are commonly computed by using training criteria that relate the autoencoder to a probabilistic model, such as the Restricted Boltzmann Machine. In this paper we show how an autoencoder can assign meaningful scores to data independently of training procedure and without reference to any probabilistic model, by interpreting it as a dynamical system. We discuss how, and under which conditions, running the dynamical system can be viewed as performing gradient descent in an energy function, which in turn allows us to derive a score via integration. We also show how one can combine multiple, unnormalized scores into a generative classifier.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/kamyshanska13.pdf",
        "supp": "",
        "pdf_size": 1133846,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10730550008172319091&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Goethe Universit\u00a8at Frankfurt; University of Montreal",
        "aff_domain": "fias.uni-frankfurt.de;iro.umontreal.ca",
        "email": "fias.uni-frankfurt.de;iro.umontreal.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Goethe University Frankfurt;University of Montreal",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uni-frankfurt.de;https://wwwumontreal.ca",
        "aff_unique_abbr": "GU Frankfurt;UM",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Frankfurt;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Germany;Canada"
    },
    {
        "id": "973d8befeb",
        "title": "On learning parametric-output HMMs",
        "site": "https://proceedings.mlr.press/v28/kontorovich13.html",
        "author": "Aryeh Kontorovich; Boaz Nadler; Roi Weiss",
        "abstract": "We present a novel approach to learning an HMM whose outputs are distributed according to a parametric family. This is done by \\em decoupling the learning task into two steps: first estimating the output parameters, and then estimating the hidden states transition probabilities. The first step is accomplished by fitting a mixture model to the output stationary distribution. Given the parameters of this mixture model, the second step is formulated as the solution of an easily solvable convex quadratic program. We provide an error analysis for the estimated transition probabilities and show they are robust to small perturbations in the estimates of the mixture parameters. Finally, we support our analysis with some encouraging empirical results.",
        "bibtex": "@InProceedings{pmlr-v28-kontorovich13,\n  title = \t {On learning parametric-output HMMs},\n  author = \t {Kontorovich, Aryeh and Nadler, Boaz and Weiss, Roi},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {702--710},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/kontorovich13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/kontorovich13.html},\n  abstract = \t {We present a novel approach to learning an HMM whose outputs are distributed according to a parametric family. This is done by \\em decoupling the learning task into two steps: first estimating the output parameters, and then estimating the hidden states transition probabilities. The first step is accomplished by fitting a mixture model to the output stationary distribution. Given the parameters of this mixture model, the second step is formulated as the solution of an easily solvable convex quadratic program. We provide an error analysis for the estimated transition probabilities and show they are robust to small perturbations in the estimates of the mixture parameters. Finally, we support our analysis with some encouraging empirical results.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/kontorovich13.pdf",
        "supp": "",
        "pdf_size": 511490,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=386436265666370914&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "Department of Computer Science, Ben-Gurion University, Beer Sheva 84105, Israel; Department of Computer Science and Applied Math, Weizmann Institute of Science, Rehovot, 76100, Israel; Department of Computer Science, Ben-Gurion University, Beer Sheva 84105, Israel",
        "aff_domain": "cs.bgu.ac.il;weizmann.ac.il;cs.bgu.ac.il",
        "email": "cs.bgu.ac.il;weizmann.ac.il;cs.bgu.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Ben-Gurion University;Weizmann Institute of Science",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science and Applied Math",
        "aff_unique_url": "https://www.bgu.ac.il;https://www.weizmann.ac.il",
        "aff_unique_abbr": "BGU;Weizmann",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Beer Sheva;Rehovot",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "c4a7587357",
        "title": "On the Generalization Ability of Online Learning Algorithms for Pairwise Loss Functions",
        "site": "https://proceedings.mlr.press/v28/kar13.html",
        "author": "Purushottam Kar; Bharath Sriperumbudur; Prateek Jain; Harish Karnick",
        "abstract": "In this paper, we study the generalization properties of online learning based stochastic methods for supervised learning problems where the loss function is dependent on more than one training sample (e.g., metric learning, ranking). We present a generic decoupling technique that enables us to provide Rademacher complexity-based generalization error bounds. Our bounds are in general tighter than those obtained by Wang et al. (COLT 2012) for the same problem. Using our decoupling technique, we are further able to obtain fast convergence rates for strongly con-vex pairwise loss functions. We are also able to analyze a class of memory efficient on-line learning algorithms for pairwise learning problems that use only a bounded subset of past training samples to update the hypothesis at each step. Finally, in order to complement our generalization bounds, we propose a novel memory efficient online learning algorithm for higher order learning problems with bounded regret guarantees.",
        "bibtex": "@InProceedings{pmlr-v28-kar13,\n  title = \t {On the Generalization Ability of Online Learning Algorithms for Pairwise Loss Functions},\n  author = \t {Kar, Purushottam and Sriperumbudur, Bharath and Jain, Prateek and Karnick, Harish},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {441--449},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/kar13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/kar13.html},\n  abstract = \t {In this paper, we study the generalization properties of online learning based stochastic methods for supervised learning problems where the loss function is dependent on more than one training sample (e.g., metric learning, ranking). We present a generic decoupling technique that enables us to provide Rademacher complexity-based generalization error bounds. Our bounds are in general tighter than those obtained by Wang et al. (COLT 2012) for the same problem. Using our decoupling technique, we are further able to obtain fast convergence rates for strongly con-vex pairwise loss functions. We are also able to analyze a class of memory efficient on-line learning algorithms for pairwise learning problems that use only a bounded subset of past training samples to update the hypothesis at each step. Finally, in order to complement our generalization bounds, we propose a novel memory efficient online learning algorithm for higher order learning problems with bounded regret guarantees.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/kar13.pdf",
        "supp": "",
        "pdf_size": 520466,
        "gs_citation": 91,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18372132787326079859&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 23,
        "aff": "Department of Computer Science and Engineering, Indian Institute of Technology, Kanpur, UP 208 016, INDIA; Statistical Laboratory, Centre for Mathematical Sciences, Wilberforce Road, Cambridge, CB3 0WB, ENGLAND; Microsoft Research India, \u201cVigyan\u201d, #9, Lavelle Road, Bangalore, KA 560 001, INDIA; Department of Computer Science and Engineering, Indian Institute of Technology, Kanpur, UP 208 016, INDIA",
        "aff_domain": "cse.iitk.ac.in;statslab.cam.ac.uk;microsoft.com;cse.iitk.ac.in",
        "email": "cse.iitk.ac.in;statslab.cam.ac.uk;microsoft.com;cse.iitk.ac.in",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Indian Institute of Technology Kanpur;University of Cambridge;Microsoft",
        "aff_unique_dep": "Department of Computer Science and Engineering;Centre for Mathematical Sciences;Microsoft Research India",
        "aff_unique_url": "https://www.iitk.ac.in;https://www.cam.ac.uk;https://www.microsoft.com/en-us/research/group/microsoft-research-india",
        "aff_unique_abbr": "IIT Kanpur;Cambridge;MSRI",
        "aff_campus_unique_index": "0;1;2;0",
        "aff_campus_unique": "Kanpur;Cambridge;Bangalore",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "India;United Kingdom"
    },
    {
        "id": "65ac7fb1de",
        "title": "On the Statistical Consistency of Algorithms for Binary Classification under Class Imbalance",
        "site": "https://proceedings.mlr.press/v28/menon13a.html",
        "author": "Aditya Menon; Harikrishna Narasimhan; Shivani Agarwal; Sanjay Chawla",
        "abstract": "Class imbalance situations, where one class is rare compared to the other, arise frequently in machine learning applications. It is well known that the usual misclassification error is ill-suited for measuring performance in such settings. A wide range of performance measures have been proposed for this problem, in machine learning as well as in data mining, artificial intelligence, and various applied fields. However, despite the large number of studies on this problem, little is understood about the statistical consistency of the algorithms proposed with respect to the performance measures of interest. In this paper, we study consistency with respect to one such performance measure, namely the arithmetic mean of the true positive and true negative rates (AM), and establish that some simple methods that have been used in practice, such as applying an empirically determined threshold to a suitable class probability estimate or performing an empirically balanced form of risk minimization, are in fact consistent with respect to the AM (under mild conditions on the underlying distribution). Our results employ balanced losses that have been used recently in analyses of ranking problems (Kotlowski et al., 2011) and build on recent results on consistent surrogates for cost-sensitive losses (Scott, 2012). Experimental results confirm our consistency theorems.",
        "bibtex": "@InProceedings{pmlr-v28-menon13a,\n  title = \t {On the Statistical Consistency of Algorithms for Binary Classification under Class Imbalance},\n  author = \t {Menon, Aditya and Narasimhan, Harikrishna and Agarwal, Shivani and Chawla, Sanjay},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {603--611},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/menon13a.pdf},\n  url = \t {https://proceedings.mlr.press/v28/menon13a.html},\n  abstract = \t {Class imbalance situations, where one class is rare compared to the other, arise frequently in machine learning applications. It is well known that the usual misclassification error is ill-suited for measuring performance in such settings. A wide range of performance measures have been proposed for this problem, in machine learning as well as in data mining, artificial intelligence, and various applied fields. However, despite the large number of studies on this problem, little is understood about the statistical consistency of the algorithms proposed with respect to the performance measures of interest. In this paper, we study consistency with respect to one such performance measure, namely the arithmetic mean of the true positive and true negative rates (AM), and establish that some simple methods that have been used in practice, such as applying an empirically determined threshold to a suitable class probability estimate or performing an empirically balanced form of risk minimization, are in fact consistent with respect to the AM (under mild conditions on the underlying distribution). Our results employ balanced losses that have been used recently in analyses of ranking problems (Kotlowski et al., 2011) and build on recent results on consistent surrogates for cost-sensitive losses (Scott, 2012). Experimental results confirm our consistency theorems.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/menon13a.pdf",
        "supp": "",
        "pdf_size": 489553,
        "gs_citation": 159,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7253930691512571937&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "University of California, San Diego, La Jolla CA 92093, USA; Indian Institute of Science, Bangalore 560012, India; Indian Institute of Science, Bangalore 560012, India; University of Sydney and NICTA, Sydney, Australia",
        "aff_domain": "ucsd.edu;csa.iisc.ernet.in;csa.iisc.ernet.in;sydney.edu.au",
        "email": "ucsd.edu;csa.iisc.ernet.in;csa.iisc.ernet.in;sydney.edu.au",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;2",
        "aff_unique_norm": "University of California, San Diego;Indian Institute of Science;University of Sydney",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ucsd.edu;https://www.iisc.ac.in;https://www.sydney.edu.au",
        "aff_unique_abbr": "UCSD;IISc;USYD",
        "aff_campus_unique_index": "0;1;1;2",
        "aff_campus_unique": "La Jolla;Bangalore;Sydney",
        "aff_country_unique_index": "0;1;1;2",
        "aff_country_unique": "United States;India;Australia"
    },
    {
        "id": "a7b7b52328",
        "title": "On the difficulty of training recurrent neural networks",
        "site": "https://proceedings.mlr.press/v28/pascanu13.html",
        "author": "Razvan Pascanu; Tomas Mikolov; Yoshua Bengio",
        "abstract": "There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.",
        "bibtex": "@InProceedings{pmlr-v28-pascanu13,\n  title = \t {On the difficulty of training recurrent neural networks},\n  author = \t {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1310--1318},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/pascanu13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/pascanu13.html},\n  abstract = \t {There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/pascanu13.pdf",
        "supp": "",
        "pdf_size": 642584,
        "gs_citation": 8376,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3353056030101542547&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Universit\u00e9 de Montr\u00e9al; Speech@FIT, Brno University of Technology; Universit\u00e9 de Montr\u00e9al",
        "aff_domain": "iro.umontreal.ca;gmail.com;umontreal.ca",
        "email": "iro.umontreal.ca;gmail.com;umontreal.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Universit\u00e9 de Montr\u00e9al;Brno University of Technology",
        "aff_unique_dep": ";Speech@FIT",
        "aff_unique_url": "https://www.umontreal.ca;https://www.vutbr.cz",
        "aff_unique_abbr": "UdeM;Brno UoT",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Brno",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Canada;Czech Republic"
    },
    {
        "id": "9212e920c3",
        "title": "On the importance of initialization and momentum in deep learning",
        "site": "https://proceedings.mlr.press/v28/sutskever13.html",
        "author": "Ilya Sutskever; James Martens; George Dahl; Geoffrey Hinton",
        "abstract": "Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned.     Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.",
        "bibtex": "@InProceedings{pmlr-v28-sutskever13,\n  title = \t {On the importance of initialization and momentum in deep learning},\n  author = \t {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1139--1147},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/sutskever13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/sutskever13.html},\n  abstract = \t {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned.     Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.   }\n}",
        "pdf": "http://proceedings.mlr.press/v28/sutskever13.pdf",
        "supp": "",
        "pdf_size": 538423,
        "gs_citation": 6940,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7449004388220998591&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 21,
        "aff": "Google; University of Toronto; University of Toronto; University of Toronto",
        "aff_domain": "google.com;cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "email": "google.com;cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Google;University of Toronto",
        "aff_unique_dep": "Google;",
        "aff_unique_url": "https://www.google.com;https://www.utoronto.ca",
        "aff_unique_abbr": "Google;U of T",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0;1;1;1",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "3185306da9",
        "title": "One-Bit Compressed Sensing: Provable Support and Vector Recovery",
        "site": "https://proceedings.mlr.press/v28/gopi13.html",
        "author": "Sivakant Gopi; Praneeth Netrapalli; Prateek Jain; Aditya Nori",
        "abstract": "In this paper, we study the problem of one-bit compressed sensing (1-bit CS), where the goal is to design a measurement matrix A and a recovery algorithm s.t. a k-sparse vector \\x^* can be efficiently recovered back from signed linear measurements, i.e., b=\\sign(A\\x^*). This is an important problem in the signal acquisition area and has several learning applications as well, e.g., multi-label classification \\citeHsuKLZ10. We study this problem in two settings: a) support recovery: recover \\supp(\\x^*), b) approximate vector recovery: recover a unit vector \\hx s.t. || \\hatx-\\x^*/||\\x^*|| ||_2\u2264\u03b5. For support recovery, we propose two novel and efficient solutions based on two combinatorial structures: union free family of sets and expanders. In contrast to  existing methods for  support recovery, our methods are universal i.e. a single measurement matrix A can recover almost all the signals. For approximate recovery, we propose the first  method to recover sparse vector using a near optimal number of measurements.  We also empirically demonstrate  effectiveness of our algorithms; we show that our algorithms are able to recover signals with smaller number of measurements than several existing methods.",
        "bibtex": "@InProceedings{pmlr-v28-gopi13,\n  title = \t {One-Bit Compressed Sensing: Provable Support and Vector Recovery},\n  author = \t {Gopi, Sivakant and Netrapalli, Praneeth and Jain, Prateek and Nori, Aditya},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {154--162},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/gopi13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/gopi13.html},\n  abstract = \t {In this paper, we study the problem of one-bit compressed sensing (1-bit CS), where the goal is to design a measurement matrix A and a recovery algorithm s.t. a k-sparse vector \\x^* can be efficiently recovered back from signed linear measurements, i.e., b=\\sign(A\\x^*). This is an important problem in the signal acquisition area and has several learning applications as well, e.g., multi-label classification \\citeHsuKLZ10. We study this problem in two settings: a) support recovery: recover \\supp(\\x^*), b) approximate vector recovery: recover a unit vector \\hx s.t. || \\hatx-\\x^*/||\\x^*|| ||_2\u2264\u03b5. For support recovery, we propose two novel and efficient solutions based on two combinatorial structures: union free family of sets and expanders. In contrast to  existing methods for  support recovery, our methods are universal i.e. a single measurement matrix A can recover almost all the signals. For approximate recovery, we propose the first  method to recover sparse vector using a near optimal number of measurements.  We also empirically demonstrate  effectiveness of our algorithms; we show that our algorithms are able to recover signals with smaller number of measurements than several existing methods. }\n}",
        "pdf": "http://proceedings.mlr.press/v28/gopi13.pdf",
        "supp": "",
        "pdf_size": 378765,
        "gs_citation": 102,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14703163870538680929&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "IIT Bombay, Mumbai, India; The University of Texas at Austin, Austin, TX, 78705 USA; Microsoft Research India, Bangalore, India; Microsoft Research India, Bangalore, India",
        "aff_domain": "gmail.com;utexas.edu;microsoft.com;microsoft.com",
        "email": "gmail.com;utexas.edu;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;2",
        "aff_unique_norm": "Indian Institute of Technology Bombay;University of Texas at Austin;Microsoft",
        "aff_unique_dep": ";;Microsoft Research India",
        "aff_unique_url": "https://www.iitb.ac.in;https://www.utexas.edu;https://www.microsoft.com/en-us/research/group/microsoft-research-india",
        "aff_unique_abbr": "IITB;UT Austin;MSRI",
        "aff_campus_unique_index": "0;1;2;2",
        "aff_campus_unique": "Mumbai;Austin;Bangalore",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "India;United States"
    },
    {
        "id": "86c8f8fe3b",
        "title": "One-Pass AUC Optimization",
        "site": "https://proceedings.mlr.press/v28/gao13.html",
        "author": "Wei Gao; Rong Jin; Shenghuo Zhu; Zhi-Hua Zhou",
        "abstract": "AUC is an important performance measure and many algorithms have been devoted to AUC optimization, mostly by minimizing a surrogate convex loss on a training data set. In this work, we focus on one-pass AUC optimization that requires only going through the training data once without storing the entire training dataset, where conventional online learning algorithms cannot be applied directly because AUC is measured by a sum of losses defined over pairs of instances from different classes. We develop a regression-based algorithm which only needs to maintain the first and second order statistics of training data in memory, resulting a storage requirement independent from the size of training data. To efficiently handle high dimensional data, we develop a randomized algorithm that approximates the covariance matrices by low rank matrices. We verify, both theoretically and empirically, the effectiveness of the proposed algorithm.",
        "bibtex": "@InProceedings{pmlr-v28-gao13,\n  title = \t {One-Pass AUC Optimization},\n  author = \t {Gao, Wei and Jin, Rong and Zhu, Shenghuo and Zhou, Zhi-Hua},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {906--914},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/gao13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/gao13.html},\n  abstract = \t {AUC is an important performance measure and many algorithms have been devoted to AUC optimization, mostly by minimizing a surrogate convex loss on a training data set. In this work, we focus on one-pass AUC optimization that requires only going through the training data once without storing the entire training dataset, where conventional online learning algorithms cannot be applied directly because AUC is measured by a sum of losses defined over pairs of instances from different classes. We develop a regression-based algorithm which only needs to maintain the first and second order statistics of training data in memory, resulting a storage requirement independent from the size of training data. To efficiently handle high dimensional data, we develop a randomized algorithm that approximates the covariance matrices by low rank matrices. We verify, both theoretically and empirically, the effectiveness of the proposed algorithm.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/gao13.pdf",
        "supp": "",
        "pdf_size": 327823,
        "gs_citation": 229,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16210065053805563556&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China; Department of Computer Science and Engineering, Michigan State University, East Lansing, MI, 48824, USA; NEC Laboratories America, CA, 95014, USA; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China",
        "aff_domain": "lamda.nju.edu.cn;cse.msu.edu;nec-labs.com;lamda.nju.edu.cn",
        "email": "lamda.nju.edu.cn;cse.msu.edu;nec-labs.com;lamda.nju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Nanjing University;Michigan State University;NEC Laboratories America",
        "aff_unique_dep": "National Key Laboratory for Novel Software Technology;Department of Computer Science and Engineering;",
        "aff_unique_url": "http://www.nju.edu.cn;https://www.msu.edu;https://www.nec-labs.com",
        "aff_unique_abbr": "Nanjing U;MSU;NEC Labs",
        "aff_campus_unique_index": "0;1;2;0",
        "aff_campus_unique": "Nanjing;East Lansing;CA",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "055a48b715",
        "title": "Online Feature Selection for Model-based Reinforcement Learning",
        "site": "https://proceedings.mlr.press/v28/nguyen13.html",
        "author": "Trung Nguyen; Zhuoru Li; Tomi Silander; Tze Yun Leong",
        "abstract": "We propose a new framework for learning the world dynamics of feature-rich environments in model-based reinforcement learning. The main idea is formalized as a new, factored state-transition representation that supports efficient online-learning of the relevant features. We construct the transition models through predicting how the actions change the world. We introduce an online sparse coding learning technique for feature selection in high-dimensional spaces. We derive theoretical guarantees for our framework and empirically demonstrate its practicality in both simulated and real robotics domains.",
        "bibtex": "@InProceedings{pmlr-v28-nguyen13,\n  title = \t {Online Feature Selection for Model-based Reinforcement Learning},\n  author = \t {Nguyen, Trung and Li, Zhuoru and Silander, Tomi and Yun Leong, Tze},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {498--506},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/nguyen13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/nguyen13.html},\n  abstract = \t {We propose a new framework for learning the world dynamics of feature-rich environments in model-based reinforcement learning. The main idea is formalized as a new, factored state-transition representation that supports efficient online-learning of the relevant features. We construct the transition models through predicting how the actions change the world. We introduce an online sparse coding learning technique for feature selection in high-dimensional spaces. We derive theoretical guarantees for our framework and empirically demonstrate its practicality in both simulated and real robotics domains.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/nguyen13.pdf",
        "supp": "",
        "pdf_size": 1123937,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1966532487560954238&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "School of Computing, National University of Singapore, Singapore, 117417; School of Computing, National University of Singapore, Singapore, 117417; School of Computing, National University of Singapore, Singapore, 117417; School of Computing, National University of Singapore, Singapore, 117417",
        "aff_domain": "comp.nus.edu.sg;comp.nus.edu.sg;comp.nus.edu.sg;comp.nus.edu.sg",
        "email": "comp.nus.edu.sg;comp.nus.edu.sg;comp.nus.edu.sg;comp.nus.edu.sg",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "National University of Singapore",
        "aff_unique_dep": "School of Computing",
        "aff_unique_url": "https://www.nus.edu.sg",
        "aff_unique_abbr": "NUS",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Singapore",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "89fce98877",
        "title": "Online Kernel Learning with a Near Optimal Sparsity Bound",
        "site": "https://proceedings.mlr.press/v28/zhang13c.html",
        "author": "Lijun Zhang; Jinfeng Yi; Rong Jin; Ming Lin; Xiaofei He",
        "abstract": "In this work, we focus on Online Sparse Kernel Learning that aims to online learn a kernel classifier with a bounded number of support vectors. Although many online learning algorithms have been proposed to learn a sparse kernel classifier, most of them fail to bound the number of support vectors used by the final solution which is the average of the intermediate kernel classifiers generated by online algorithms. The key idea of the proposed algorithm is to measure the difficulty in correctly classifying a training example by the derivative of a smooth loss function, and give a more chance to a difficult example to be a support vector than an easy one via a sampling scheme. Our analysis shows that when the loss function is smooth, the proposed algorithm yields similar performance guarantee as the standard online learning algorithm but with a near optimal number of support vectors (up to a poly(lnT) factor). Our empirical study shows promising performance of the proposed algorithm compared to the state-of-the-art algorithms for online sparse kernel learning.",
        "bibtex": "@InProceedings{pmlr-v28-zhang13c,\n  title = \t {Online Kernel Learning with a Near Optimal Sparsity Bound},\n  author = \t {Zhang, Lijun and Yi, Jinfeng and Jin, Rong and Lin, Ming and He, Xiaofei},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {621--629},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/zhang13c.pdf},\n  url = \t {https://proceedings.mlr.press/v28/zhang13c.html},\n  abstract = \t {In this work, we focus on Online Sparse Kernel Learning that aims to online learn a kernel classifier with a bounded number of support vectors. Although many online learning algorithms have been proposed to learn a sparse kernel classifier, most of them fail to bound the number of support vectors used by the final solution which is the average of the intermediate kernel classifiers generated by online algorithms. The key idea of the proposed algorithm is to measure the difficulty in correctly classifying a training example by the derivative of a smooth loss function, and give a more chance to a difficult example to be a support vector than an easy one via a sampling scheme. Our analysis shows that when the loss function is smooth, the proposed algorithm yields similar performance guarantee as the standard online learning algorithm but with a near optimal number of support vectors (up to a poly(lnT) factor). Our empirical study shows promising performance of the proposed algorithm compared to the state-of-the-art algorithms for online sparse kernel learning.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/zhang13c.pdf",
        "supp": "",
        "pdf_size": 197684,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1986496464092145343&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science and Engineering, Michigan State University, East Lansing, MI 48824, USA; Department of Computer Science and Engineering, Michigan State University, East Lansing, MI 48824, USA; Department of Computer Science and Engineering, Michigan State University, East Lansing, MI 48824, USA; Department of Automation, Tsinghua University, Beijing 100084, China; State Key Laboratory of CAD&CG, College of Computer Science, Zhejiang University, Hangzhou 310027, China",
        "aff_domain": "msu.edu;msu.edu;cse.msu.edu;mails.tsinghua.edu.cn;cad.zju.edu.cn",
        "email": "msu.edu;msu.edu;cse.msu.edu;mails.tsinghua.edu.cn;cad.zju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;2",
        "aff_unique_norm": "Michigan State University;Tsinghua University;Zhejiang University",
        "aff_unique_dep": "Department of Computer Science and Engineering;Department of Automation;College of Computer Science",
        "aff_unique_url": "https://www.msu.edu;https://www.tsinghua.edu.cn;http://www.zju.edu.cn",
        "aff_unique_abbr": "MSU;THU;ZJU",
        "aff_campus_unique_index": "0;0;0;1;2",
        "aff_campus_unique": "East Lansing;Beijing;Hangzhou",
        "aff_country_unique_index": "0;0;0;1;1",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "295be2bf27",
        "title": "Online Latent Dirichlet Allocation with Infinite Vocabulary",
        "site": "https://proceedings.mlr.press/v28/zhai13.html",
        "author": "Ke Zhai; Jordan Boyd-Graber",
        "abstract": "Topic models based on latent Dirichlet allocation (LDA) assume a predefined vocabulary a priori. This is reasonable in batch settings, but it is not reasonable when data are revealed over time, as is the case with streaming / online algorithms. To address this lacuna, we extend LDA by drawing topics from a Dirichlet process whose base distribution is a distribution over all strings rather than from a finite Dirichlet. We develop inference using online variational inference and because we only can consider a finite number of words for each truncated topic propose heuristics to dynamically organize, expand, and contract the set of words we consider in our vocabulary truncation. We show our model can successfully incorporate new words as it encounters new terms and that it performs better than online LDA in evaluations of topic quality and classification performance.",
        "bibtex": "@InProceedings{pmlr-v28-zhai13,\n  title = \t {Online Latent {D}irichlet Allocation with Infinite Vocabulary},\n  author = \t {Zhai, Ke and Boyd-Graber, Jordan},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {561--569},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/zhai13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/zhai13.html},\n  abstract = \t {Topic models based on latent Dirichlet allocation (LDA) assume a predefined vocabulary a priori. This is reasonable in batch settings, but it is not reasonable when data are revealed over time, as is the case with streaming / online algorithms. To address this lacuna, we extend LDA by drawing topics from a Dirichlet process whose base distribution is a distribution over all strings rather than from a finite Dirichlet. We develop inference using online variational inference and because we only can consider a finite number of words for each truncated topic propose heuristics to dynamically organize, expand, and contract the set of words we consider in our vocabulary truncation. We show our model can successfully incorporate new words as it encounters new terms and that it performs better than online LDA in evaluations of topic quality and classification performance.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/zhai13.pdf",
        "supp": "",
        "pdf_size": 1072821,
        "gs_citation": 93,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13032810155678237789&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Science, University of Maryland, College Park, MD USA; iSchool and UMIACS, University of Maryland, College Park, MD USA",
        "aff_domain": "cs.umd.edu;umiacs.umd.edu",
        "email": "cs.umd.edu;umiacs.umd.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Maryland",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www/umd.edu",
        "aff_unique_abbr": "UMD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "College Park",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6022624600",
        "title": "Online Learning under Delayed Feedback",
        "site": "https://proceedings.mlr.press/v28/joulani13.html",
        "author": "Pooria Joulani; Andras Gyorgy; Csaba Szepesvari",
        "abstract": "Online learning with delayed feedback has received increasing attention recently due to its several applications in distributed, web-based learning problems. In this paper we provide a systematic study of the topic, and analyze the effect of delay on the regret of online learning algorithms. Somewhat surprisingly, it turns out that delay increases the regret in a multiplicative way in adversarial problems, and in an additive way in stochastic problems. We give meta-algorithms that transform, in a black-box fashion, algorithms developed for the non-delayed case into ones that can handle the presence of delays in the feedback loop. Modifications of the well-known UCB algorithm are also developed for the bandit problem with delayed feedback, with the advantage over the meta-algorithms that they can be implemented with lower complexity.",
        "bibtex": "@InProceedings{pmlr-v28-joulani13,\n  title = \t {Online Learning under Delayed Feedback},\n  author = \t {Joulani, Pooria and Gyorgy, Andras and Szepesvari, Csaba},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1453--1461},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/joulani13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/joulani13.html},\n  abstract = \t {Online learning with delayed feedback has received increasing attention recently due to its several applications in distributed, web-based learning problems. In this paper we provide a systematic study of the topic, and analyze the effect of delay on the regret of online learning algorithms. Somewhat surprisingly, it turns out that delay increases the regret in a multiplicative way in adversarial problems, and in an additive way in stochastic problems. We give meta-algorithms that transform, in a black-box fashion, algorithms developed for the non-delayed case into ones that can handle the presence of delays in the feedback loop. Modifications of the well-known UCB algorithm are also developed for the bandit problem with delayed feedback, with the advantage over the meta-algorithms that they can be implemented with lower complexity.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/joulani13.pdf",
        "supp": "",
        "pdf_size": 378885,
        "gs_citation": 356,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5650823655112766279&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "Dept. of Computing Science, University of Alberta, Edmonton, AB, T6G 2E8 CANADA; Dept. of Computing Science, University of Alberta, Edmonton, AB, T6G 2E8 CANADA; Dept. of Computing Science, University of Alberta, Edmonton, AB, T6G 2E8 CANADA",
        "aff_domain": "ualberta.ca;ualberta.ca;ualberta.ca",
        "email": "ualberta.ca;ualberta.ca;ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "Dept. of Computing Science",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Edmonton",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "d8f340f384",
        "title": "Optimal Regret Bounds for  Selecting the State Representation in Reinforcement Learning",
        "site": "https://proceedings.mlr.press/v28/maillard13.html",
        "author": "Odalric-Ambrym Maillard; Phuong Nguyen; Ronald Ortner; Daniil Ryabko",
        "abstract": "We consider an agent interacting with an environment in a single stream of actions, observations, and rewards, with no reset. This process is not assumed to be a Markov Decision Process (MDP). Rather, the agent has several representations (mapping histories of past interactions to a discrete state space) of the environment with unknown dynamics, only some of which result in an MDP. The goal is to minimize the average regret criterion against an agent who knows   an MDP representation giving the highest optimal reward, and acts optimally in it. Recent regret bounds for this setting are of order O(T^2/3) with an additive term constant yet exponential in some characteristics of the optimal MDP.  We propose an algorithm whose regret after T time steps is O(\\sqrtT),  with all constants reasonably small.  This is optimal in T since O(\\sqrtT) is the optimal regret in the setting of learning in a (single discrete) MDP.",
        "bibtex": "@InProceedings{pmlr-v28-maillard13,\n  title = \t {Optimal Regret Bounds for  Selecting the State Representation in Reinforcement Learning},\n  author = \t {Maillard, Odalric-Ambrym and Nguyen, Phuong and Ortner, Ronald and Ryabko, Daniil},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {543--551},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/maillard13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/maillard13.html},\n  abstract = \t {We consider an agent interacting with an environment in a single stream of actions, observations, and rewards, with no reset. This process is not assumed to be a Markov Decision Process (MDP). Rather, the agent has several representations (mapping histories of past interactions to a discrete state space) of the environment with unknown dynamics, only some of which result in an MDP. The goal is to minimize the average regret criterion against an agent who knows   an MDP representation giving the highest optimal reward, and acts optimally in it. Recent regret bounds for this setting are of order O(T^2/3) with an additive term constant yet exponential in some characteristics of the optimal MDP.  We propose an algorithm whose regret after T time steps is O(\\sqrtT),  with all constants reasonably small.  This is optimal in T since O(\\sqrtT) is the optimal regret in the setting of learning in a (single discrete) MDP.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/maillard13.pdf",
        "supp": "",
        "pdf_size": 342346,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4080671939039993408&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 26,
        "aff": "The Technion, Faculty of Electrical Engineering, 32000 Haifa, ISRAEL; Australian National University and NICTA, Canberra ACT 0200, AUSTRALIA; Montanuniversit\u00a8 at Leoben, Franz-Josef-Strasse 18, A-8700 Leoben, AUSTRIA; INRIA Lille - Nord Europe, 40 Avenue Halley, 59650 Villeneuve d\u2019Ascq, FRANCE",
        "aff_domain": "gmail.com;cecs.anu.edu.au;unileoben.ac.at;ryabko.net",
        "email": "gmail.com;cecs.anu.edu.au;unileoben.ac.at;ryabko.net",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "Technion;Australian National University;Montanuniversit\u00e4t Leoben;INRIA Lille - Nord Europe",
        "aff_unique_dep": "Faculty of Electrical Engineering;;;",
        "aff_unique_url": "http://www.technion.ac.il;https://www.anu.edu.au;https://www.montanuni-leoben.at;https://www.inria.fr/lille-nord-europe",
        "aff_unique_abbr": "Technion;ANU;;INRIA",
        "aff_campus_unique_index": "0;1;2;3",
        "aff_campus_unique": "Haifa;Canberra;Leoben;Lille",
        "aff_country_unique_index": "0;1;2;3",
        "aff_country_unique": "Israel;Australia;Austria;France"
    },
    {
        "id": "a34eb1e07e",
        "title": "Optimal rates for stochastic convex optimization under Tsybakov noise condition",
        "site": "https://proceedings.mlr.press/v28/ramdas13.html",
        "author": "Aaditya Ramdas; Aarti Singh",
        "abstract": "We focus on the problem of minimizing a convex function f over a convex set S given T queries to a stochastic first order oracle. We argue that the complexity of convex minimization is only determined by the rate of growth of the function around its minimum x^*_f,S, as quantified by a Tsybakov-like noise condition. Specifically, we prove that if f grows at least as fast as \\|x-x^*_f,S\\|^\u03baaround its minimum, for some \u03ba> 1, then the optimal rate of learning f(x^*_f,S) is  \u0398(T^-\\frac\u03ba2\u03ba-2). The classic rate \u0398(1/\\sqrt T) for convex functions and \u0398(1/T) for strongly convex functions are special cases of our result for \u03ba\u2192\u221eand \u03ba=2, and even faster rates are attained for 1 < \u03ba< 2. We also derive tight bounds for the complexity of learning x_f,S^*, where the optimal rate is \u0398(T^-\\frac12\u03ba-2). Interestingly, these precise rates also characterize the complexity of active learning and our results further strengthen the connections between the fields of active learning and convex optimization, both of which rely on feedback-driven queries.",
        "bibtex": "@InProceedings{pmlr-v28-ramdas13,\n  title = \t {Optimal rates for stochastic convex optimization under Tsybakov noise condition},\n  author = \t {Ramdas, Aaditya and Singh, Aarti},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {365--373},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/ramdas13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/ramdas13.html},\n  abstract = \t {We focus on the problem of minimizing a convex function f over a convex set S given T queries to a stochastic first order oracle. We argue that the complexity of convex minimization is only determined by the rate of growth of the function around its minimum x^*_f,S, as quantified by a Tsybakov-like noise condition. Specifically, we prove that if f grows at least as fast as \\|x-x^*_f,S\\|^\u03baaround its minimum, for some \u03ba> 1, then the optimal rate of learning f(x^*_f,S) is  \u0398(T^-\\frac\u03ba2\u03ba-2). The classic rate \u0398(1/\\sqrt T) for convex functions and \u0398(1/T) for strongly convex functions are special cases of our result for \u03ba\u2192\u221eand \u03ba=2, and even faster rates are attained for 1 < \u03ba< 2. We also derive tight bounds for the complexity of learning x_f,S^*, where the optimal rate is \u0398(T^-\\frac12\u03ba-2). Interestingly, these precise rates also characterize the complexity of active learning and our results further strengthen the connections between the fields of active learning and convex optimization, both of which rely on feedback-driven queries.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/ramdas13.pdf",
        "supp": "",
        "pdf_size": 353961,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4019250773913148569&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, PA 15213, USA; Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, PA 15213, USA",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a2c3547e36",
        "title": "Optimistic Knowledge Gradient Policy for Optimal Budget Allocation in Crowdsourcing",
        "site": "https://proceedings.mlr.press/v28/chen13f.html",
        "author": "Xi Chen; Qihang Lin; Dengyong Zhou",
        "abstract": "In real crowdsourcing applications, each label from a crowd usually comes  with a certain cost. Given a pre- fixed amount of budget, since different tasks have different ambiguities and different workers have different expertises, we want to  find an optimal way to allocate the budget among instance-worker pairs such that the overall label quality can be maximized. To address this issue, we start from the simplest setting in which all workers are assumed to be perfect. We formulate the problem as a Bayesian Markov Decision Process (MDP). Using the dynamic programming (DP) algorithm, one can obtain the optimal allocation policy for a given budget. However,  DP is computationally intractable. To solve the computational challenge, we propose a novel approximate policy which is called optimistic knowledge gradient. It is practically efficient while theoretically its consistency can be guaranteed.  We then extend  the MDP framework to deal with inhomogeneous workers and tasks with contextual information available.  The experiments on both simulated and real data  demonstrate the superiority of our method.",
        "bibtex": "@InProceedings{pmlr-v28-chen13f,\n  title = \t {Optimistic Knowledge Gradient Policy for Optimal Budget Allocation in Crowdsourcing},\n  author = \t {Chen, Xi and Lin, Qihang and Zhou, Dengyong},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {64--72},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/chen13f.pdf},\n  url = \t {https://proceedings.mlr.press/v28/chen13f.html},\n  abstract = \t {In real crowdsourcing applications, each label from a crowd usually comes  with a certain cost. Given a pre- fixed amount of budget, since different tasks have different ambiguities and different workers have different expertises, we want to  find an optimal way to allocate the budget among instance-worker pairs such that the overall label quality can be maximized. To address this issue, we start from the simplest setting in which all workers are assumed to be perfect. We formulate the problem as a Bayesian Markov Decision Process (MDP). Using the dynamic programming (DP) algorithm, one can obtain the optimal allocation policy for a given budget. However,  DP is computationally intractable. To solve the computational challenge, we propose a novel approximate policy which is called optimistic knowledge gradient. It is practically efficient while theoretically its consistency can be guaranteed.  We then extend  the MDP framework to deal with inhomogeneous workers and tasks with contextual information available.  The experiments on both simulated and real data  demonstrate the superiority of our method.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/chen13f.pdf",
        "supp": "",
        "pdf_size": 461109,
        "gs_citation": 149,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18124968319981866707&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15213, USA; Tepper School of Business, Carnegie Mellon University, Pittsburgh, PA 15213, USA; Machine Learning Group, Microsoft Research, Redmond, WA 98052, USA",
        "aff_domain": "cs.cmu.edu;andrew.cmu.edu;microsoft.com",
        "email": "cs.cmu.edu;andrew.cmu.edu;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Carnegie Mellon University;Microsoft",
        "aff_unique_dep": "Machine Learning Department;Machine Learning Group",
        "aff_unique_url": "https://www.cmu.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "CMU;MSR",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Pittsburgh;Redmond",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7d32308c6a",
        "title": "Optimization with First-Order Surrogate Functions",
        "site": "https://proceedings.mlr.press/v28/mairal13.html",
        "author": "Julien Mairal",
        "abstract": "In this paper, we study optimization methods consisting of iteratively minimizing surrogates of an objective function. By proposing several algorithmic variants and simple convergence analyses, we make two main contributions.  First, we provide a unified viewpoint for several first-order optimization techniques such as accelerated proximal gradient, block coordinate descent, or Frank-Wolfe algorithms.  Second, we introduce a new incremental scheme that experimentally matches or outperforms state-of-the-art solvers for large-scale optimization problems typically arising in machine learning.",
        "bibtex": "@InProceedings{pmlr-v28-mairal13,\n  title = \t {Optimization with First-Order Surrogate Functions},\n  author = \t {Mairal, Julien},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {783--791},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/mairal13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/mairal13.html},\n  abstract = \t {In this paper, we study optimization methods consisting of iteratively minimizing surrogates of an objective function. By proposing several algorithmic variants and simple convergence analyses, we make two main contributions.  First, we provide a unified viewpoint for several first-order optimization techniques such as accelerated proximal gradient, block coordinate descent, or Frank-Wolfe algorithms.  Second, we introduce a new incremental scheme that experimentally matches or outperforms state-of-the-art solvers for large-scale optimization problems typically arising in machine learning.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/mairal13.pdf",
        "supp": "",
        "pdf_size": 320467,
        "gs_citation": 264,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6625460577430914859&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "INRIA LEAR Project-Team, Grenoble, France",
        "aff_domain": "inria.fr",
        "email": "inria.fr",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "INRIA",
        "aff_unique_dep": "LEAR Project-Team",
        "aff_unique_url": "https://www.inria.fr",
        "aff_unique_abbr": "INRIA",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Grenoble",
        "aff_country_unique_index": "0",
        "aff_country_unique": "France"
    },
    {
        "id": "b6f0825be0",
        "title": "Optimizing the F-Measure in Multi-Label Classification: Plug-in Rule Approach versus Structured Loss Minimization",
        "site": "https://proceedings.mlr.press/v28/dembczynski13.html",
        "author": "Krzysztof Dembczynski; Arkadiusz Jachnik; Wojciech Kotlowski; Willem Waegeman; Eyke Huellermeier",
        "abstract": "We compare the plug-in rule approach for optimizing the F-measure in multi-label classification with an approach based on structured loss minimization, such as the structured support vector machine (SSVM). Whereas the former derives an optimal prediction from a probabilistic model in a separate inference step, the latter seeks to optimize the F-measure directly during the training phase. We introduce a novel plug-in rule algorithm that estimates all parameters required for a Bayes-optimal prediction via a set of multinomial regression models, and we compare this algorithm with SSVMs in terms of computational complexity and statistical consistency. As a main theoretical result, we show that our plug-in rule algorithm is consistent, whereas the SSVM approaches are not. Finally, we present results of a large experimental study showing the benefits of the introduced algorithm.",
        "bibtex": "@InProceedings{pmlr-v28-dembczynski13,\n  title = \t {Optimizing the F-Measure in Multi-Label Classification: Plug-in Rule Approach versus Structured Loss Minimization},\n  author = \t {Dembczynski, Krzysztof and Jachnik, Arkadiusz and Kotlowski, Wojciech and Waegeman, Willem and Huellermeier, Eyke},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1130--1138},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/dembczynski13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/dembczynski13.html},\n  abstract = \t {We compare the plug-in rule approach for optimizing the F-measure in multi-label classification with an approach based on structured loss minimization, such as the structured support vector machine (SSVM). Whereas the former derives an optimal prediction from a probabilistic model in a separate inference step, the latter seeks to optimize the F-measure directly during the training phase. We introduce a novel plug-in rule algorithm that estimates all parameters required for a Bayes-optimal prediction via a set of multinomial regression models, and we compare this algorithm with SSVMs in terms of computational complexity and statistical consistency. As a main theoretical result, we show that our plug-in rule algorithm is consistent, whereas the SSVM approaches are not. Finally, we present results of a large experimental study showing the benefits of the introduced algorithm.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/dembczynski13.pdf",
        "supp": "",
        "pdf_size": 378963,
        "gs_citation": 137,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7770144824938949601&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "Institute of Computing Science, Pozna\u0144 University of Technology; Institute of Computing Science, Pozna\u0144 University of Technology; Institute of Computing Science, Pozna\u0144 University of Technology; NGDATA-Europe; Mathematics and Computer Science, Marburg University",
        "aff_domain": "cs.put.poznan.pl;cs.put.poznan.pl;cs.put.poznan.pl;gmail.com;mathematik.uni-marburg.de",
        "email": "cs.put.poznan.pl;cs.put.poznan.pl;cs.put.poznan.pl;gmail.com;mathematik.uni-marburg.de",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;2",
        "aff_unique_norm": "Pozna\u0144 University of Technology;NGDATA;Marburg University",
        "aff_unique_dep": "Institute of Computing Science;;Department of Mathematics and Computer Science",
        "aff_unique_url": "https://www.put.poznan.pl/;;https://www.uni-marburg.de",
        "aff_unique_abbr": "PUT;;Uni Marburg",
        "aff_campus_unique_index": "0;0;0;2",
        "aff_campus_unique": "Pozna\u0144;;Marburg",
        "aff_country_unique_index": "0;0;0;1;2",
        "aff_country_unique": "Poland;Unknown;Germany"
    },
    {
        "id": "8549d9c213",
        "title": "Parallel Markov Chain Monte Carlo for Nonparametric Mixture Models",
        "site": "https://proceedings.mlr.press/v28/williamson13.html",
        "author": "Sinead Williamson; Avinava Dubey; Eric Xing",
        "abstract": "Nonparametric mixture models based on the Dirichlet process are an elegant alternative to finite models when the number of underlying components is unknown, but inference in such models can be slow. Existing attempts to parallelize inference in such models have relied on introducing approximations, which can lead to inaccuracies in the posterior estimate. In this paper, we describe auxiliary variable representations for the Dirichlet process and the hierarchical Dirichlet process that allow us to perform MCMC using the correct equilibrium distribution, in a distributed manner. We show that our approach allows scalable inference without the deterioration in estimate quality that accompanies existing methods.",
        "bibtex": "@InProceedings{pmlr-v28-williamson13,\n  title = \t {Parallel {M}arkov Chain {M}onte {C}arlo for Nonparametric Mixture Models},\n  author = \t {Williamson, Sinead and Dubey, Avinava and Xing, Eric},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {98--106},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/williamson13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/williamson13.html},\n  abstract = \t {Nonparametric mixture models based on the Dirichlet process are an elegant alternative to finite models when the number of underlying components is unknown, but inference in such models can be slow. Existing attempts to parallelize inference in such models have relied on introducing approximations, which can lead to inaccuracies in the posterior estimate. In this paper, we describe auxiliary variable representations for the Dirichlet process and the hierarchical Dirichlet process that allow us to perform MCMC using the correct equilibrium distribution, in a distributed manner. We show that our approach allows scalable inference without the deterioration in estimate quality that accompanies existing methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/williamson13.pdf",
        "supp": "",
        "pdf_size": 661574,
        "gs_citation": 105,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8794825756748725922&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15201, USA; Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15201, USA; Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15201, USA",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Machine Learning Department",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "460dbf0363",
        "title": "Parameter Learning and Convergent Inference for Dense Random Fields",
        "site": "https://proceedings.mlr.press/v28/kraehenbuehl13.html",
        "author": "Philipp Kraehenbuehl; Vladlen Koltun",
        "abstract": "Dense random fields are models in which all pairs of variables are directly connected by pairwise potentials. It has recently been shown that mean field inference in dense random fields can be performed efficiently and that these models enable significant accuracy gains in computer vision applications. However, parameter estimation for dense random fields is still poorly understood. In this paper, we present an efficient algorithm for learning parameters in dense random fields. All parameters are estimated jointly, thus capturing dependencies between them. We show that gradients of a variety of loss functions over the mean field marginals can be computed efficiently. The resulting algorithm learns parameters that directly optimize the performance of mean field inference in the model.  As a supporting result, we present an efficient inference algorithm for dense random fields that is guaranteed to converge.",
        "bibtex": "@InProceedings{pmlr-v28-kraehenbuehl13,\n  title = \t {Parameter Learning and Convergent Inference for Dense Random Fields},\n  author = \t {Kraehenbuehl, Philipp and Koltun, Vladlen},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {513--521},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/kraehenbuehl13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/kraehenbuehl13.html},\n  abstract = \t {Dense random fields are models in which all pairs of variables are directly connected by pairwise potentials. It has recently been shown that mean field inference in dense random fields can be performed efficiently and that these models enable significant accuracy gains in computer vision applications. However, parameter estimation for dense random fields is still poorly understood. In this paper, we present an efficient algorithm for learning parameters in dense random fields. All parameters are estimated jointly, thus capturing dependencies between them. We show that gradients of a variety of loss functions over the mean field marginals can be computed efficiently. The resulting algorithm learns parameters that directly optimize the performance of mean field inference in the model.  As a supporting result, we present an efficient inference algorithm for dense random fields that is guaranteed to converge.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/kraehenbuehl13.pdf",
        "supp": "",
        "pdf_size": 339403,
        "gs_citation": 283,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1671514003099473915&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Computer Science Department, Stanford University, Stanford, CA 94305 USA; Computer Science Department, Stanford University, Stanford, CA 94305 USA",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e5696e3bab",
        "title": "Parsing epileptic events using a Markov switching process model for correlated time series",
        "site": "https://proceedings.mlr.press/v28/wulsin13.html",
        "author": "Drausin Wulsin; Emily Fox; Brian Litt",
        "abstract": "Patients with epilepsy can manifest short, sub-clinical epileptic \u201cbursts\u201d in addition to full-blown clinical seizures. We believe the relationship between these two classes of events\u2014something not previously studied quantitatively\u2014could yield important insights into the nature and intrinsic dynamics of seizures. A goal of our work is to parse these complex epileptic events into distinct dynamic regimes.  A challenge posed by the intracranial EEG (iEEG) data we study is the fact that the number and placement of electrodes can vary between patients.  We develop a Bayesian nonparametric Markov switching process that allows for (i) shared dynamic regimes between a variable numbers of channels, (ii) asynchronous regime-switching, and (iii) an unknown dictionary of dynamic regimes.  We encode a sparse and changing set of dependencies between the channels using a Markov-switching Gaussian graphical model for the innovations process driving the channel dynamics. We demonstrate the importance of this model in parsing and out-of-sample predictions of iEEG data.  We show that our model produces intuitive state assignments that can help automate clinical analysis of seizures and enable the comparison of sub-clinical bursts and full clinical seizures.",
        "bibtex": "@InProceedings{pmlr-v28-wulsin13,\n  title = \t {Parsing epileptic events using a Markov switching process model for correlated time series},\n  author = \t {Wulsin, Drausin and Fox, Emily and Litt, Brian},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {356--364},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/wulsin13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/wulsin13.html},\n  abstract = \t {Patients with epilepsy can manifest short, sub-clinical epileptic \u201cbursts\u201d in addition to full-blown clinical seizures. We believe the relationship between these two classes of events\u2014something not previously studied quantitatively\u2014could yield important insights into the nature and intrinsic dynamics of seizures. A goal of our work is to parse these complex epileptic events into distinct dynamic regimes.  A challenge posed by the intracranial EEG (iEEG) data we study is the fact that the number and placement of electrodes can vary between patients.  We develop a Bayesian nonparametric Markov switching process that allows for (i) shared dynamic regimes between a variable numbers of channels, (ii) asynchronous regime-switching, and (iii) an unknown dictionary of dynamic regimes.  We encode a sparse and changing set of dependencies between the channels using a Markov-switching Gaussian graphical model for the innovations process driving the channel dynamics. We demonstrate the importance of this model in parsing and out-of-sample predictions of iEEG data.  We show that our model produces intuitive state assignments that can help automate clinical analysis of seizures and enable the comparison of sub-clinical bursts and full clinical seizures.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/wulsin13.pdf",
        "supp": "",
        "pdf_size": 2821541,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11789326746454362580&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Dept. of Bioengineering, University of Pennsylvania, Philadelphia, PA USA; Dept. of Statistics, University of Washington, Seattle, WA USA; Depts. of Neurology and Bioengineering, University of Pennsylvania, Philadelphia, PA USA",
        "aff_domain": "seas.upenn.edu;uw.edu;mail.med.upenn.edu",
        "email": "seas.upenn.edu;uw.edu;mail.med.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Pennsylvania;University of Washington",
        "aff_unique_dep": "Dept. of Bioengineering;Dept. of Statistics",
        "aff_unique_url": "https://www.upenn.edu;https://www.washington.edu",
        "aff_unique_abbr": "UPenn;UW",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Philadelphia;Seattle",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e82196a39d",
        "title": "Planning by Prioritized Sweeping with Small Backups",
        "site": "https://proceedings.mlr.press/v28/vanseijen13.html",
        "author": "Harm Van Seijen; Rich Sutton",
        "abstract": "Efficient planning plays a crucial role in model-based reinforcement learning. Traditionally, the main planning operation is a full backup based on the current estimates of the successor states. Consequently, its computation time is proportional to the number of successor states. In this paper, we introduce a new planning backup that  uses only the current value of a single successor state and has a computation time independent of the number of successor states. This new backup, which we call a small backup, opens the door to a new class of model-based reinforcement learning methods that exhibit much finer control over their planning process than traditional methods. We empirically demonstrate that this increased flexibility allows for more efficient planning by showing that an implementation of prioritized sweeping based on small backups achieves a substantial performance improvement over classical implementations.",
        "bibtex": "@InProceedings{pmlr-v28-vanseijen13,\n  title = \t {Planning by Prioritized Sweeping with Small Backups},\n  author = \t {Van Seijen, Harm and Sutton, Rich},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {361--369},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/vanseijen13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/vanseijen13.html},\n  abstract = \t {Efficient planning plays a crucial role in model-based reinforcement learning. Traditionally, the main planning operation is a full backup based on the current estimates of the successor states. Consequently, its computation time is proportional to the number of successor states. In this paper, we introduce a new planning backup that  uses only the current value of a single successor state and has a computation time independent of the number of successor states. This new backup, which we call a small backup, opens the door to a new class of model-based reinforcement learning methods that exhibit much finer control over their planning process than traditional methods. We empirically demonstrate that this increased flexibility allows for more efficient planning by showing that an implementation of prioritized sweeping based on small backups achieves a substantial performance improvement over classical implementations. }\n}",
        "pdf": "http://proceedings.mlr.press/v28/vanseijen13.pdf",
        "supp": "",
        "pdf_size": 593108,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5938809284433742588&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computing Science, University of Alberta, Edmonton, Alberta, T6G 2E8, Canada; Department of Computing Science, University of Alberta, Edmonton, Alberta, T6G 2E8, Canada",
        "aff_domain": "ualberta.ca;cs.ualberta.ca",
        "email": "ualberta.ca;cs.ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "Department of Computing Science",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Edmonton",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "4a233c7bee",
        "title": "Precision-recall space to correct external indices for biclustering",
        "site": "https://proceedings.mlr.press/v28/hanczar13.html",
        "author": "Blaise Hanczar; Mohamed Nadif",
        "abstract": "Biclustering is a major tool of data mining in many domains and many algorithms have emerged in recent years. All these algorithms aim to obtain coherent biclusters and it is crucial to have a reliable procedure for their validation. We point out the problem of size bias in biclustering evaluation and show how it can lead to wrong conclusions in a comparative study. We present the theoretical corrections for all of the most popular measures in order to remove this bias. We introduce the corrected precision-recall space that combines the advantages of corrected measures, the ease of interpretation and visualization of uncorrected measures. Numerical experiments demonstrate the interest of our approach.",
        "bibtex": "@InProceedings{pmlr-v28-hanczar13,\n  title = \t {Precision-recall space to correct external indices for biclustering},\n  author = \t {Hanczar, Blaise and Nadif, Mohamed},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {136--144},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/hanczar13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/hanczar13.html},\n  abstract = \t {Biclustering is a major tool of data mining in many domains and many algorithms have emerged in recent years. All these algorithms aim to obtain coherent biclusters and it is crucial to have a reliable procedure for their validation. We point out the problem of size bias in biclustering evaluation and show how it can lead to wrong conclusions in a comparative study. We present the theoretical corrections for all of the most popular measures in order to remove this bias. We introduce the corrected precision-recall space that combines the advantages of corrected measures, the ease of interpretation and visualization of uncorrected measures. Numerical experiments demonstrate the interest of our approach. }\n}",
        "pdf": "http://proceedings.mlr.press/v28/hanczar13.pdf",
        "supp": "",
        "pdf_size": 335587,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10350643932783693627&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 7,
        "aff": "LIPADE, University Paris Descartes, 45 rue des saint-peres, 7 5006, Paris, France; LIPADE, University Paris Descartes, 45 rue des saint-peres, 7 5006, Paris, France",
        "aff_domain": "parisdescartes.fr;parisdescartes.fr",
        "email": "parisdescartes.fr;parisdescartes.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University Paris Descartes",
        "aff_unique_dep": "LIPADE",
        "aff_unique_url": "https://www.univ-paris-diderot.fr",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Paris",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "07763f441e",
        "title": "Predictable Dual-View Hashing",
        "site": "https://proceedings.mlr.press/v28/rastegari13.html",
        "author": "Mohammad Rastegari; Jonghyun Choi; Shobeir Fakhraei; Daume Hal; Larry Davis",
        "abstract": "We propose a Predictable Dual-View Hashing (PDH) algorithm which embeds proximity of data samples in the original spaces. We create a cross-view hamming space with the ability to compare information from previously incomparable domains with a notion of \u2018predictability\u2019.   By performing comparative experimental analysis on two large datasets, PASCAL-Sentence and SUN-Attribute, we demonstrate the superiority of our method to the state-of-the-art dual-view binary code learning algorithms.",
        "bibtex": "@InProceedings{pmlr-v28-rastegari13,\n  title = \t {Predictable Dual-View Hashing},\n  author = \t {Rastegari, Mohammad and Choi, Jonghyun and Fakhraei, Shobeir and Hal, Daume and Davis, Larry},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1328--1336},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/rastegari13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/rastegari13.html},\n  abstract = \t {We propose a Predictable Dual-View Hashing (PDH) algorithm which embeds proximity of data samples in the original spaces. We create a cross-view hamming space with the ability to compare information from previously incomparable domains with a notion of \u2018predictability\u2019.   By performing comparative experimental analysis on two large datasets, PASCAL-Sentence and SUN-Attribute, we demonstrate the superiority of our method to the state-of-the-art dual-view binary code learning algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/rastegari13.pdf",
        "supp": "",
        "pdf_size": 2466244,
        "gs_citation": 142,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12888299377839985036&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 21,
        "aff": "University of Maryland, College Park, MD 20742 USA; University of Maryland, College Park, MD 20742 USA; University of Maryland, College Park, MD 20742 USA; University of Maryland, College Park, MD 20742 USA; University of Maryland, College Park, MD 20742 USA",
        "aff_domain": "cs.umd.edu;umd.edu;cs.umd.edu;hal3.name;umiacs.umd.edu",
        "email": "cs.umd.edu;umd.edu;cs.umd.edu;hal3.name;umiacs.umd.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Maryland",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www/umd.edu",
        "aff_unique_abbr": "UMD",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "College Park",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "511b93e3a0",
        "title": "Principal Component Analysis on non-Gaussian Dependent Data",
        "site": "https://proceedings.mlr.press/v28/han13.html",
        "author": "Fang Han; Han Liu",
        "abstract": "In this paper, we analyze the performance of a semiparametric principal component analysis named Copula Component Analysis (COCA) (Han & Liu, 2012) when the data are dependent. The semiparametric model assumes that, after unspecified marginally monotone transformations, the distributions are multivariate Gaussian. We study the scenario where the observations are drawn from non-i.i.d. processes ($m$-dependency or a more general $\\phi$-mixing case). We show that COCA can allow weak dependence. In particular, we provide the generalization bounds of convergence for both support recovery and parameter estimation of COCA for the dependent data. We provide explicit sufficient conditions on the degree of dependence, under which the parametric rate can be maintained. To our knowledge, this is the first work analyzing the theoretical performance of PCA for the dependent data in high dimensional settings. Our results strictly generalize the analysis in Han & Liu (2012) and the techniques we used have the separate interest for analyzing a variety of other multivariate statistical methods.",
        "bibtex": "@InProceedings{pmlr-v28-han13,\n  title = \t {Principal Component Analysis on non-{G}aussian Dependent Data},\n  author = \t {Han, Fang and Liu, Han},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {240--248},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/han13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/han13.html},\n  abstract = \t {In this paper, we analyze the performance of a semiparametric principal component analysis named Copula Component Analysis (COCA) (Han & Liu, 2012) when the data are dependent. The semiparametric model assumes that, after unspecified marginally monotone transformations, the distributions are multivariate Gaussian. We study the scenario where the observations are drawn from non-i.i.d. processes ($m$-dependency or a more general $\\phi$-mixing case). We show that COCA can allow weak dependence. In particular, we provide the generalization bounds of convergence for both support recovery and parameter estimation of COCA for the dependent data. We provide explicit sufficient conditions on the degree of dependence, under which the parametric rate can be maintained. To our knowledge, this is the first work analyzing the theoretical performance of PCA for the dependent data in high dimensional settings. Our results strictly generalize the analysis in Han & Liu (2012) and the techniques we used have the separate interest for analyzing a variety of other multivariate statistical methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/han13.pdf",
        "supp": "",
        "pdf_size": 258049,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1172838577723612152&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Johns Hopkins University, 615 N.Wolfe Street, Baltimore, MD 21205 USA; Princeton University, 98 Charlton Street, Princeton, NJ 08544 USA",
        "aff_domain": "jhsph.edu;princeton.edu",
        "email": "jhsph.edu;princeton.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Johns Hopkins University;Princeton University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.jhu.edu;https://www.princeton.edu",
        "aff_unique_abbr": "JHU;Princeton",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Baltimore;Princeton",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "315ac5bfe7",
        "title": "Quantile Regression for Large-scale Applications",
        "site": "https://proceedings.mlr.press/v28/yang13f.html",
        "author": "Jiyan Yang; Xiangrui Meng; Michael Mahoney",
        "abstract": "Quantile regression is a method to estimate the quantiles of the   conditional distribution of a response variable, and as such it permits a   much more accurate portrayal of the relationship between the response variable   and observed covariates than methods such as Least-squares or   Least Absolute Deviations regression.  It can be expressed as a linear program,   and   interior-point methods can be used to find a solution for  moderately large problems.  Dealing with very large problems, \\emphe.g., involving data up to and   beyond the terabyte regime, remains a challenge.  Here, we present a randomized algorithm that runs in time that is nearly   linear in the size of the input and that, with constant probability,   computes a (1+\u03b5) approximate solution to an arbitrary quantile   regression problem.  Our algorithm computes a low-distortion subspace-preserving  embedding with respect to the loss function of quantile regression.  Our empirical evaluation illustrates that our algorithm is competitive with   the best previous work on small to medium-sized problems, and that   it can be implemented in MapReduce-like environments and    applied to terabyte-sized problems.",
        "bibtex": "@InProceedings{pmlr-v28-yang13f,\n  title = \t {Quantile Regression for Large-scale Applications},\n  author = \t {Yang, Jiyan and Meng, Xiangrui and Mahoney, Michael},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {881--887},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/yang13f.pdf},\n  url = \t {https://proceedings.mlr.press/v28/yang13f.html},\n  abstract = \t {Quantile regression is a method to estimate the quantiles of the   conditional distribution of a response variable, and as such it permits a   much more accurate portrayal of the relationship between the response variable   and observed covariates than methods such as Least-squares or   Least Absolute Deviations regression.  It can be expressed as a linear program,   and   interior-point methods can be used to find a solution for  moderately large problems.  Dealing with very large problems, \\emphe.g., involving data up to and   beyond the terabyte regime, remains a challenge.  Here, we present a randomized algorithm that runs in time that is nearly   linear in the size of the input and that, with constant probability,   computes a (1+\u03b5) approximate solution to an arbitrary quantile   regression problem.  Our algorithm computes a low-distortion subspace-preserving  embedding with respect to the loss function of quantile regression.  Our empirical evaluation illustrates that our algorithm is competitive with   the best previous work on small to medium-sized problems, and that   it can be implemented in MapReduce-like environments and    applied to terabyte-sized problems.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/yang13f.pdf",
        "supp": "",
        "pdf_size": 408288,
        "gs_citation": 66,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5725005792203668476&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 25,
        "aff": "ICME, Stanford University, Stanford, CA 94305; LinkedIn Corporation, 2029 Stierlin Ct, Mountain View, CA 94043; Dept. of Mathematics, Stanford University, Stanford, CA 94305",
        "aff_domain": "stanford.edu;linkedin.com;cs.stanford.edu",
        "email": "stanford.edu;linkedin.com;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Stanford University;LinkedIn Corporation",
        "aff_unique_dep": "Institute for Computational and Mathematical Engineering;",
        "aff_unique_url": "https://www.stanford.edu;https://www.linkedin.com",
        "aff_unique_abbr": "Stanford;LinkedIn",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Stanford;Mountain View",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7168a29095",
        "title": "Quickly Boosting Decision Trees \u2013 Pruning Underachieving Features Early",
        "site": "https://proceedings.mlr.press/v28/appel13.html",
        "author": "Ron Appel; Thomas Fuchs; Piotr Dollar; Pietro Perona",
        "abstract": "Boosted decision trees are one of the most popular and successful learning techniques used today.  While exhibiting fast speeds at test time, relatively slow training makes them impractical for applications with real-time learning requirements. We propose a principled approach to overcome this drawback. We prove a bound on the error of a decision stump given its preliminary error on a subset of the training data; the bound may be used to prune unpromising features early on in the training process. We propose a fast training algorithm that exploits this bound, yielding speedups of an order of magnitude at no cost in the final performance of the classifier. Our method is not a new variant of Boosting; rather, it may be used in conjunction with existing Boosting algorithms and other sampling heuristics to achieve even greater speedups.",
        "bibtex": "@InProceedings{pmlr-v28-appel13,\n  title = \t {Quickly Boosting Decision Trees -- Pruning Underachieving Features Early},\n  author = \t {Appel, Ron and Fuchs, Thomas and Dollar, Piotr and Perona, Pietro},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {594--602},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/appel13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/appel13.html},\n  abstract = \t {Boosted decision trees are one of the most popular and successful learning techniques used today.  While exhibiting fast speeds at test time, relatively slow training makes them impractical for applications with real-time learning requirements. We propose a principled approach to overcome this drawback. We prove a bound on the error of a decision stump given its preliminary error on a subset of the training data; the bound may be used to prune unpromising features early on in the training process. We propose a fast training algorithm that exploits this bound, yielding speedups of an order of magnitude at no cost in the final performance of the classifier. Our method is not a new variant of Boosting; rather, it may be used in conjunction with existing Boosting algorithms and other sampling heuristics to achieve even greater speedups.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/appel13.pdf",
        "supp": "",
        "pdf_size": 648355,
        "gs_citation": 180,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17655333704831352113&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Caltech, Pasadena, CA 91125 USA; Caltech, Pasadena, CA 91125 USA; Microsoft Research, Redmond, WA 98052 USA; Caltech, Pasadena, CA 91125 USA",
        "aff_domain": "caltech.edu;caltech.edu;microsoft.com;caltech.edu",
        "email": "caltech.edu;caltech.edu;microsoft.com;caltech.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "California Institute of Technology;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.caltech.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "Caltech;MSR",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Pasadena;Redmond",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b1a6c3a5a9",
        "title": "Regularization of Neural Networks using DropConnect",
        "site": "https://proceedings.mlr.press/v28/wan13.html",
        "author": "Li Wan; Matthew Zeiler; Sixin Zhang; Yann Le Cun; Rob Fergus",
        "abstract": "We introduce DropConnect, a generalization of DropOut, for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recoginition benchmarks can be obtained by aggregating multiple DropConnect-trained models.",
        "bibtex": "@InProceedings{pmlr-v28-wan13,\n  title = \t {Regularization of Neural Networks using DropConnect},\n  author = \t {Wan, Li and Zeiler, Matthew and Zhang, Sixin and Le Cun, Yann and Fergus, Rob},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1058--1066},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/wan13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/wan13.html},\n  abstract = \t {We introduce DropConnect, a generalization of DropOut, for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recoginition benchmarks can be obtained by aggregating multiple DropConnect-trained models.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/wan13.pdf",
        "supp": "",
        "pdf_size": 737968,
        "gs_citation": 3524,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10803576202345658680&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Dept. of Computer Science, Courant Institute of Mathematical Science, New York University; Dept. of Computer Science, Courant Institute of Mathematical Science, New York University; Dept. of Computer Science, Courant Institute of Mathematical Science, New York University; Dept. of Computer Science, Courant Institute of Mathematical Science, New York University; Dept. of Computer Science, Courant Institute of Mathematical Science, New York University",
        "aff_domain": "cs.nyu.edu;cs.nyu.edu;cs.nyu.edu;cs.nyu.edu;cs.nyu.edu",
        "email": "cs.nyu.edu;cs.nyu.edu;cs.nyu.edu;cs.nyu.edu;cs.nyu.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "New York University",
        "aff_unique_dep": "Dept. of Computer Science",
        "aff_unique_url": "https://www.nyu.edu",
        "aff_unique_abbr": "NYU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "New York",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0e0066453b",
        "title": "Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization",
        "site": "https://proceedings.mlr.press/v28/jaggi13.html",
        "author": "Martin Jaggi",
        "abstract": "We provide stronger and more general primal-dual convergence results for Frank-Wolfe-type algorithms (a.k.a. conditional gradient) for constrained convex optimization, enabled by a simple framework of duality gap certificates. Our analysis also holds if the linear subproblems are only solved approximately (as well as if the gradients are inexact), and is proven to be worst-case optimal in the sparsity of the obtained solutions.    On the application side, this allows us to unify a large variety of existing sparse greedy methods, in particular for optimization over convex hulls of an atomic set, even if those sets can only be approximated, including sparse (or structured sparse) vectors or matrices, low-rank matrices, permutation matrices, or max-norm bounded matrices.    We present a new general framework for convex optimization over matrix factorizations, where every Frank-Wolfe iteration will consist of a low-rank update, and discuss the broad application areas of this approach.",
        "bibtex": "@InProceedings{pmlr-v28-jaggi13,\n  title = \t {Revisiting {Frank-Wolfe}: Projection-Free Sparse Convex Optimization},\n  author = \t {Jaggi, Martin},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {427--435},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/jaggi13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/jaggi13.html},\n  abstract = \t {We provide stronger and more general primal-dual convergence results for Frank-Wolfe-type algorithms (a.k.a. conditional gradient) for constrained convex optimization, enabled by a simple framework of duality gap certificates. Our analysis also holds if the linear subproblems are only solved approximately (as well as if the gradients are inexact), and is proven to be worst-case optimal in the sparsity of the obtained solutions.    On the application side, this allows us to unify a large variety of existing sparse greedy methods, in particular for optimization over convex hulls of an atomic set, even if those sets can only be approximated, including sparse (or structured sparse) vectors or matrices, low-rank matrices, permutation matrices, or max-norm bounded matrices.    We present a new general framework for convex optimization over matrix factorizations, where every Frank-Wolfe iteration will consist of a low-rank update, and discuss the broad application areas of this approach.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/jaggi13.pdf",
        "supp": "",
        "pdf_size": 910364,
        "gs_citation": 1724,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8256725870545384707&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "CMAP, \u00b4Ecole Polytechnique, Palaiseau, France",
        "aff_domain": "cmap.polytechnique.fr",
        "email": "cmap.polytechnique.fr",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Ecole Polytechnique",
        "aff_unique_dep": "CMAP",
        "aff_unique_url": "https://www.ecolepolytechnique.fr",
        "aff_unique_abbr": "Polytechnique",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Palaiseau",
        "aff_country_unique_index": "0",
        "aff_country_unique": "France"
    },
    {
        "id": "37cb1dd61f",
        "title": "Revisiting the Nystrom method for improved large-scale machine learning",
        "site": "https://proceedings.mlr.press/v28/gittens13.html",
        "author": "Alex Gittens; Michael Mahoney",
        "abstract": "We reconsider randomized algorithms for the low-rank approximation of SPSD matrices such as Laplacian and kernel matrices that arise in data analysis and machine learning applications.    Our main results consist of an empirical evaluation of the performance quality and running time of sampling and projection methods on a diverse suite of SPSD matrices. Our results highlight complementary aspects of sampling versus projection methods, and they point to differences between uniform and nonuniform sampling methods based on leverage scores.    We complement our empirical results with a suite of worst-case theoretical bounds for both random sampling and random projection methods. These bounds are qualitatively superior to existing bounds\u2014 e.g., improved additive-error bounds for spectral and Frobenius norm error and relative-error bounds for trace norm error.",
        "bibtex": "@InProceedings{pmlr-v28-gittens13,\n  title = \t {Revisiting the Nystrom method for improved large-scale machine learning},\n  author = \t {Gittens, Alex and Mahoney, Michael},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {567--575},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/gittens13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/gittens13.html},\n  abstract = \t {We reconsider randomized algorithms for the low-rank approximation of SPSD matrices such as Laplacian and kernel matrices that arise in data analysis and machine learning applications.    Our main results consist of an empirical evaluation of the performance quality and running time of sampling and projection methods on a diverse suite of SPSD matrices. Our results highlight complementary aspects of sampling versus projection methods, and they point to differences between uniform and nonuniform sampling methods based on leverage scores.    We complement our empirical results with a suite of worst-case theoretical bounds for both random sampling and random projection methods. These bounds are qualitatively superior to existing bounds\u2014 e.g., improved additive-error bounds for spectral and Frobenius norm error and relative-error bounds for trace norm error.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/gittens13.pdf",
        "supp": "",
        "pdf_size": 454057,
        "gs_citation": 529,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13252607169074816129&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Dept. of Applied and Computational Math, CalTech, Pasadena CA 91125 USA; Dept. of Mathematics, Stanford University, Stanford, CA 9430 USA",
        "aff_domain": "caltech.edu;cs.stanford.edu",
        "email": "caltech.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "California Institute of Technology;Stanford University",
        "aff_unique_dep": "Department of Applied and Computational Mathematics;Dept. of Mathematics",
        "aff_unique_url": "https://www.caltech.edu;https://www.stanford.edu",
        "aff_unique_abbr": "CalTech;Stanford",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Pasadena;Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3ceed296b0",
        "title": "Riemannian Similarity Learning",
        "site": "https://proceedings.mlr.press/v28/cheng13.html",
        "author": "Li Cheng",
        "abstract": "We consider a similarity-score based paradigm to address scenarios where either the class labels are only partially revealed during learning, or the training and testing data are drawn from heterogeneous sources. The learning problem is subsequently formulated as optimization over a bilinear form of fixed rank. Our paradigm bears similarity to metric learning, where the major difference lies in its aim of learning a rectangular similarity matrix, instead of a proper metric. We tackle this problem in a Riemannian optimization framework. In particular, we consider its applications in pairwise-based action recognition, and cross-domain image-based object recognition. In both applications, the proposed algorithm produces competitive performance on respective benchmark datasets.",
        "bibtex": "@InProceedings{pmlr-v28-cheng13,\n  title = \t {Riemannian Similarity Learning},\n  author = \t {Cheng, Li},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {540--548},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/cheng13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/cheng13.html},\n  abstract = \t {We consider a similarity-score based paradigm to address scenarios where either the class labels are only partially revealed during learning, or the training and testing data are drawn from heterogeneous sources. The learning problem is subsequently formulated as optimization over a bilinear form of fixed rank. Our paradigm bears similarity to metric learning, where the major difference lies in its aim of learning a rectangular similarity matrix, instead of a proper metric. We tackle this problem in a Riemannian optimization framework. In particular, we consider its applications in pairwise-based action recognition, and cross-domain image-based object recognition. In both applications, the proposed algorithm produces competitive performance on respective benchmark datasets.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/cheng13.pdf",
        "supp": "",
        "pdf_size": 368103,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7528529916203425829&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Bioinformatics Institute, A*STAR, Singapore+School of Computing, National University of Singapore, Singapore",
        "aff_domain": "bii.a-star.edu.sg",
        "email": "bii.a-star.edu.sg",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1",
        "aff_unique_norm": "Bioinformatics Institute;National University of Singapore",
        "aff_unique_dep": ";School of Computing",
        "aff_unique_url": "https://www.bii.a-star.edu.sg;https://www.nus.edu.sg",
        "aff_unique_abbr": "BII;NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "58bde6e991",
        "title": "Risk Bounds and Learning Algorithms for the Regression Approach to Structured Output Prediction",
        "site": "https://proceedings.mlr.press/v28/giguere13.html",
        "author": "S\u00e9bastien Gigu\u00e8re; Fran\u00e7ois Laviolette; Mario Marchand; Khadidja Sylla",
        "abstract": "We provide rigorous guarantees for the regression approach to structured output prediction. We show that the quadratic regression loss is a convex surrogate of the prediction loss when the output kernel satisfies some condition with respect to the prediction loss. We provide two upper bounds of the prediction risk that depend on the empirical quadratic risk of the predictor. The minimizer of the first bound is the predictor proposed by Cortes et al. (2007) while the minimizer of the second bound is a predictor that  has never been proposed so far. Both predictors are compared on practical tasks.",
        "bibtex": "@InProceedings{pmlr-v28-giguere13,\n  title = \t {Risk Bounds and Learning Algorithms for the Regression Approach to Structured Output Prediction},\n  author = \t {Gigu\u00e8re, S\u00e9bastien and Laviolette, Fran\u00e7ois and Marchand, Mario and Sylla, Khadidja},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {107--114},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/giguere13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/giguere13.html},\n  abstract = \t {We provide rigorous guarantees for the regression approach to structured output prediction. We show that the quadratic regression loss is a convex surrogate of the prediction loss when the output kernel satisfies some condition with respect to the prediction loss. We provide two upper bounds of the prediction risk that depend on the empirical quadratic risk of the predictor. The minimizer of the first bound is the predictor proposed by Cortes et al. (2007) while the minimizer of the second bound is a predictor that  has never been proposed so far. Both predictors are compared on practical tasks.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/giguere13.pdf",
        "supp": "",
        "pdf_size": 428834,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3828741820680738912&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "D\u00b4epartement d\u2019informatique et de g\u00b4enie logiciel, Universit\u00b4e Laval, Qu\u00b4ebec (QC), Canada, G1V-0A6; D\u00b4epartement d\u2019informatique et de g\u00b4enie logiciel, Universit\u00b4e Laval, Qu\u00b4ebec (QC), Canada, G1V-0A6; D\u00b4epartement d\u2019informatique et de g\u00b4enie logiciel, Universit\u00b4e Laval, Qu\u00b4ebec (QC), Canada, G1V-0A6; D\u00b4epartement d\u2019informatique et de g\u00b4enie logiciel, Universit\u00b4e Laval, Qu\u00b4ebec (QC), Canada, G1V-0A6",
        "aff_domain": "ulaval.ca;ift.ulaval.ca;ift.ulaval.ca;ulaval.ca",
        "email": "ulaval.ca;ift.ulaval.ca;ift.ulaval.ca;ulaval.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Universit\u00e9 Laval",
        "aff_unique_dep": "D\u00e9partement d\u2019informatique et de g\u00e9nie logiciel",
        "aff_unique_url": "https://www.ulaval.ca",
        "aff_unique_abbr": "UL",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Qu\u00e9bec",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "26e16c6879",
        "title": "Robust Regression on MapReduce",
        "site": "https://proceedings.mlr.press/v28/meng13b.html",
        "author": "Xiangrui Meng; Michael Mahoney",
        "abstract": "Although the MapReduce framework is now the \\emphde facto standard for   analyzing massive data sets, many algorithms (in particular, many   iterative algorithms popular in machine learning, optimization, and linear   algebra) are hard to fit into MapReduce.   Consider, \\emphe.g., the \\ell_p regression problem: given a matrix   A \u2208\\mathbbR^m \\times n and a vector b \u2208\\mathbbR^m, find a   vector x^* \u2208\\mathbbR^n that minimizes f(x) = \\|A x - b\\|_p.   The widely-used \\ell_2 regression, \\emphi.e., linear least-squares, is   known to be highly sensitive to outliers; and choosing p \u2208[1, 2) can   help improve robustness.  In this work, we propose an efficient algorithm for solving strongly   over-determined (m \u226bn) robust \\ell_p regression problems to moderate precision on   MapReduce.  Our empirical results on data up to the terabyte scale demonstrate   that our algorithm is a significant improvement over traditional iterative algorithms on MapReduce   for \\ell_1 regression, even for a fairly small number   of iterations.   In addition, our proposed interior-point cutting-plane method can also be   extended to solving more general convex problems on MapReduce.",
        "bibtex": "@InProceedings{pmlr-v28-meng13b,\n  title = \t {Robust Regression on MapReduce},\n  author = \t {Meng, Xiangrui and Mahoney, Michael},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {888--896},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/meng13b.pdf},\n  url = \t {https://proceedings.mlr.press/v28/meng13b.html},\n  abstract = \t {Although the MapReduce framework is now the \\emphde facto standard for   analyzing massive data sets, many algorithms (in particular, many   iterative algorithms popular in machine learning, optimization, and linear   algebra) are hard to fit into MapReduce.   Consider, \\emphe.g., the \\ell_p regression problem: given a matrix   A \u2208\\mathbbR^m \\times n and a vector b \u2208\\mathbbR^m, find a   vector x^* \u2208\\mathbbR^n that minimizes f(x) = \\|A x - b\\|_p.   The widely-used \\ell_2 regression, \\emphi.e., linear least-squares, is   known to be highly sensitive to outliers; and choosing p \u2208[1, 2) can   help improve robustness.  In this work, we propose an efficient algorithm for solving strongly   over-determined (m \u226bn) robust \\ell_p regression problems to moderate precision on   MapReduce.  Our empirical results on data up to the terabyte scale demonstrate   that our algorithm is a significant improvement over traditional iterative algorithms on MapReduce   for \\ell_1 regression, even for a fairly small number   of iterations.   In addition, our proposed interior-point cutting-plane method can also be   extended to solving more general convex problems on MapReduce.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/meng13b.pdf",
        "supp": "",
        "pdf_size": 437257,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13163744712755495231&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "LinkedIn Corporation, 2029 Stierlin Ct, Mountain View, CA 94043; Department of Mathematics, Stanford University, Stanford, CA 94305",
        "aff_domain": "linkedin.com;cs.stanford.edu",
        "email": "linkedin.com;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "LinkedIn Corporation;Stanford University",
        "aff_unique_dep": ";Department of Mathematics",
        "aff_unique_url": "https://www.linkedin.com;https://www.stanford.edu",
        "aff_unique_abbr": "LinkedIn;Stanford",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Mountain View;Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7d8039f878",
        "title": "Robust Sparse Regression under Adversarial Corruption",
        "site": "https://proceedings.mlr.press/v28/chen13h.html",
        "author": "Yudong Chen; Constantine Caramanis; Shie Mannor",
        "abstract": "We consider high dimensional sparse regression with arbitrary \u2013 possibly, severe or coordinated \u2013 errors in the covariates matrix. We are interested in understanding how many corruptions we can tolerate, while identifying the correct support. To the best of our knowledge, neither standard outlier rejection techniques, nor recently developed robust regression algorithms (that focus only on corrupted response variables), nor recent algorithms for dealing with stochastic noise or erasures, can provide guarantees on support recovery. As we show, neither can the natural brute force algorithm that takes exponential time to find the subset of data and support columns, that yields the smallest regression error.     We explore the power of a simple idea: replace the essential linear algebraic calculation \u2013 the inner product \u2013 with a robust counterpart that cannot be greatly affected by a controlled number of arbitrarily corrupted points: the trimmed inner product. We consider three popular algorithms in the uncorrupted setting: Thresholding Regression, Lasso, and the Dantzig selector, and show that the counterparts obtained using the trimmed inner product are provably robust.",
        "bibtex": "@InProceedings{pmlr-v28-chen13h,\n  title = \t {Robust Sparse Regression under Adversarial Corruption},\n  author = \t {Chen, Yudong and Caramanis, Constantine and Mannor, Shie},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {774--782},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/chen13h.pdf},\n  url = \t {https://proceedings.mlr.press/v28/chen13h.html},\n  abstract = \t {We consider high dimensional sparse regression with arbitrary \u2013 possibly, severe or coordinated \u2013 errors in the covariates matrix. We are interested in understanding how many corruptions we can tolerate, while identifying the correct support. To the best of our knowledge, neither standard outlier rejection techniques, nor recently developed robust regression algorithms (that focus only on corrupted response variables), nor recent algorithms for dealing with stochastic noise or erasures, can provide guarantees on support recovery. As we show, neither can the natural brute force algorithm that takes exponential time to find the subset of data and support columns, that yields the smallest regression error.     We explore the power of a simple idea: replace the essential linear algebraic calculation \u2013 the inner product \u2013 with a robust counterpart that cannot be greatly affected by a controlled number of arbitrarily corrupted points: the trimmed inner product. We consider three popular algorithms in the uncorrupted setting: Thresholding Regression, Lasso, and the Dantzig selector, and show that the counterparts obtained using the trimmed inner product are provably robust.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/chen13h.pdf",
        "supp": "",
        "pdf_size": 535487,
        "gs_citation": 181,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16917278469968626008&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX 78712; Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX 78712; Department of Electrical Engineering, Technion, Haifa 32000, Israel",
        "aff_domain": "utexas.edu;utexas.edu;ee.technion.ac.il",
        "email": "utexas.edu;utexas.edu;ee.technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Texas at Austin;Technion",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;Department of Electrical Engineering",
        "aff_unique_url": "https://www.utexas.edu;https://www.technion.ac.il",
        "aff_unique_abbr": "UT Austin;Technion",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Austin;Haifa",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United States;Israel"
    },
    {
        "id": "eba959633c",
        "title": "Robust Structural Metric Learning",
        "site": "https://proceedings.mlr.press/v28/lim13.html",
        "author": "Daryl Lim; Gert Lanckriet; Brian McFee",
        "abstract": "Metric learning algorithms produce a linear transformation of data which is optimized for a prediction task, such as nearest-neighbor classification or ranking.  However, when the input data contains a large portion of non-informative features, existing methods fail to identify the relevant features, and performance degrades accordingly. In this paper, we present an efficient and robust structural metric learning algorithm which enforces group sparsity on the learned transformation, while optimizing for structured ranking output prediction.  Experiments on synthetic and real datasets demonstrate that the proposed method outperforms previous methods in both high- and low-noise settings.",
        "bibtex": "@InProceedings{pmlr-v28-lim13,\n  title = \t {Robust Structural Metric Learning},\n  author = \t {Lim, Daryl and Lanckriet, Gert and McFee, Brian},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {615--623},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/lim13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/lim13.html},\n  abstract = \t {Metric learning algorithms produce a linear transformation of data which is optimized for a prediction task, such as nearest-neighbor classification or ranking.  However, when the input data contains a large portion of non-informative features, existing methods fail to identify the relevant features, and performance degrades accordingly. In this paper, we present an efficient and robust structural metric learning algorithm which enforces group sparsity on the learned transformation, while optimizing for structured ranking output prediction.  Experiments on synthetic and real datasets demonstrate that the proposed method outperforms previous methods in both high- and low-noise settings.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/lim13.pdf",
        "supp": "",
        "pdf_size": 533011,
        "gs_citation": 121,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11821848859804652898&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Electrical and Computer Engineering, University of California, San Diego, CA 92093 USA; Center for Jazz Studies, Columbia University, New York, NY 10027 USA; Department of Electrical and Computer Engineering, University of California, San Diego, CA 92093 USA",
        "aff_domain": "ucsd.edu;columbia.edu;ece.ucsd.edu",
        "email": "ucsd.edu;columbia.edu;ece.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of California, San Diego;Columbia University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;Center for Jazz Studies",
        "aff_unique_url": "https://www.ucsd.edu;https://www.columbia.edu",
        "aff_unique_abbr": "UCSD;Columbia",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "San Diego;New York",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6587bb0346",
        "title": "Robust and Discriminative Self-Taught Learning",
        "site": "https://proceedings.mlr.press/v28/wang13g.html",
        "author": "Hua Wang; Feiping Nie; Heng Huang",
        "abstract": "The lack of training data is a common challenge in many machine learning problems, which is often tackled by semi-supervised learning methods or transfer learning methods. The former requires unlabeled images from the same distribution as the labeled ones and the latter leverages labeled images from related homogenous tasks. However, these restrictions often cannot be satisfied. To address this, we propose a novel robust and discriminative self-taught learning approach to utilize any unlabeled data without the above restrictions. Our new approach employs a robust loss function to learn the dictionary, and enforces the structured sparse regularization to automatically select the optimal dictionary basis vectors and incorporate the supervision information contained in the labeled data. We derive an efficient iterative algorithm to solve the optimization problem and rigorously prove its convergence. Promising results in extensive experiments have validated the proposed approach.",
        "bibtex": "@InProceedings{pmlr-v28-wang13g,\n  title = \t {Robust and Discriminative Self-Taught Learning},\n  author = \t {Wang, Hua and Nie, Feiping and Huang, Heng},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {298--306},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/wang13g.pdf},\n  url = \t {https://proceedings.mlr.press/v28/wang13g.html},\n  abstract = \t {The lack of training data is a common challenge in many machine learning problems, which is often tackled by semi-supervised learning methods or transfer learning methods. The former requires unlabeled images from the same distribution as the labeled ones and the latter leverages labeled images from related homogenous tasks. However, these restrictions often cannot be satisfied. To address this, we propose a novel robust and discriminative self-taught learning approach to utilize any unlabeled data without the above restrictions. Our new approach employs a robust loss function to learn the dictionary, and enforces the structured sparse regularization to automatically select the optimal dictionary basis vectors and incorporate the supervision information contained in the labeled data. We derive an efficient iterative algorithm to solve the optimization problem and rigorously prove its convergence. Promising results in extensive experiments have validated the proposed approach.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/wang13g.pdf",
        "supp": "",
        "pdf_size": 985773,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17692276916185793677&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Colorado School of Mines; The University of Texas at Arlington; The University of Texas at Arlington",
        "aff_domain": "gmail.com;gmail.com;uta.edu",
        "email": "gmail.com;gmail.com;uta.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Colorado School of Mines;University of Texas at Arlington",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.mines.edu;https://www.uta.edu",
        "aff_unique_abbr": "CSM;UTA",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Arlington",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c1120573d7",
        "title": "Rounding Methods for Discrete Linear Classification",
        "site": "https://proceedings.mlr.press/v28/chevaleyre13.html",
        "author": "Yann Chevaleyre; Fr\u00e9d\u00e9erick Koriche; Jean-daniel Zucker",
        "abstract": "Learning discrete linear functions is a notoriously difficult challenge. In this paper, the learning task is cast as combinatorial optimization problem: given a set of positive and negative feature vectors in the Euclidean space, the goal is to find a discrete linear function that minimizes the cumulative hinge loss of this training set. Since this problem is NP-hard, we propose two simple rounding algorithms that discretize the fractional solution of the problem. Generalization bounds are derived for two important classes of binary-weighted linear functions, by establishing the Rademacher complexity of these classes and proving approximation bounds for rounding methods. These methods are compared on both synthetic and real-world data.",
        "bibtex": "@InProceedings{pmlr-v28-chevaleyre13,\n  title = \t {Rounding Methods for Discrete Linear Classification},\n  author = \t {Chevaleyre, Yann and Koriche, Fr\u00e9d\u00e9erick and Zucker, Jean-daniel},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {651--659},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/chevaleyre13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/chevaleyre13.html},\n  abstract = \t {Learning discrete linear functions is a notoriously difficult challenge. In this paper, the learning task is cast as combinatorial optimization problem: given a set of positive and negative feature vectors in the Euclidean space, the goal is to find a discrete linear function that minimizes the cumulative hinge loss of this training set. Since this problem is NP-hard, we propose two simple rounding algorithms that discretize the fractional solution of the problem. Generalization bounds are derived for two important classes of binary-weighted linear functions, by establishing the Rademacher complexity of these classes and proving approximation bounds for rounding methods. These methods are compared on both synthetic and real-world data.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/chevaleyre13.pdf",
        "supp": "",
        "pdf_size": 586145,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=416158554532613751&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "LIPN, CNRS UMR 7030, Universit\u00e9 Paris Nord; CRIL, CNRS UMR 8188, Universit\u00e9 d\u2019Artois; INSERM U872, Universit\u00e9 Pierre et Marie Curie",
        "aff_domain": "lipn.univ-paris13.fr;cril.fr;ird.fr",
        "email": "lipn.univ-paris13.fr;cril.fr;ird.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Universit\u00e9 Paris Nord;Universit\u00e9 d\u2019Artois;Universit\u00e9 Pierre et Marie Curie",
        "aff_unique_dep": "LIPN;CRIL, CNRS UMR 8188;INSERM U872",
        "aff_unique_url": "https://www.univ-paris13.fr;https://www.univ-artois.fr;https://www.upmc.fr",
        "aff_unique_abbr": "UP13;;UPMC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "ba725f1151",
        "title": "SADA: A General Framework to Support Robust Causation Discovery",
        "site": "https://proceedings.mlr.press/v28/cai13.html",
        "author": "Ruichu Cai; Zhenjie Zhang; Zhifeng Hao",
        "abstract": "Causality discovery without manipulation is considered a crucial problem to a variety of applications, such as genetic therapy. The state-of-the-art solutions, e.g. LiNGAM, return accurate results when the number of labeled samples is larger than the number of variables. These approaches are thus applicable only when large numbers of samples are available or the problem domain is sufficiently small. Motivated by the observations of the local sparsity properties on causal structures, we propose a general Split-and-Merge strategy, named SADA, to enhance the scalability of a wide class of causality discovery algorithms. SADA is able to accurately identify the causal variables, even when the sample size is significantly smaller than the number of variables. In SADA, the variables are partitioned into subsets, by finding cuts on the sparse probabilistic graphical model over the variables. By running mainstream causation discovery algorithms, e.g. LiNGAM, on the subproblems, complete causality can be reconstructed by combining all the partial results. SADA benefits from the recursive division technique, since each small subproblem generates more accurate result under the same number of samples. We theoretically prove that SADA always reduces the scale of problems without significant sacrifice on result accuracy, depending only on the local sparsity condition over the variables. Experiments on real-world datasets verify the improvements on scalability and accuracy by applying SADA on top of existing causation algorithms.",
        "bibtex": "@InProceedings{pmlr-v28-cai13,\n  title = \t {SADA: A General Framework to Support Robust Causation Discovery},\n  author = \t {Cai, Ruichu and Zhang, Zhenjie and Hao, Zhifeng},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {208--216},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/cai13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/cai13.html},\n  abstract = \t {Causality discovery without manipulation is considered a crucial problem to a variety of applications, such as genetic therapy. The state-of-the-art solutions, e.g. LiNGAM, return accurate results when the number of labeled samples is larger than the number of variables. These approaches are thus applicable only when large numbers of samples are available or the problem domain is sufficiently small. Motivated by the observations of the local sparsity properties on causal structures, we propose a general Split-and-Merge strategy, named SADA, to enhance the scalability of a wide class of causality discovery algorithms. SADA is able to accurately identify the causal variables, even when the sample size is significantly smaller than the number of variables. In SADA, the variables are partitioned into subsets, by finding cuts on the sparse probabilistic graphical model over the variables. By running mainstream causation discovery algorithms, e.g. LiNGAM, on the subproblems, complete causality can be reconstructed by combining all the partial results. SADA benefits from the recursive division technique, since each small subproblem generates more accurate result under the same number of samples. We theoretically prove that SADA always reduces the scale of problems without significant sacrifice on result accuracy, depending only on the local sparsity condition over the variables. Experiments on real-world datasets verify the improvements on scalability and accuracy by applying SADA on top of existing causation algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/cai13.pdf",
        "supp": "",
        "pdf_size": 202485,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2320591570489215306&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Faculty of Computer Science, Guangdong University of Technology + State Key Laboratory for Novel Software Technology, Nanjing University, P.R. China; Advanced Digital Sciences Center, Illinois at Singapore Pte. Ltd., Singapore; Faculty of Computer Science, Guangdong University of Technology, P.R. China",
        "aff_domain": "gmail.com;adsc.com.sg;gdut.edu.cn",
        "email": "gmail.com;adsc.com.sg;gdut.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;0",
        "aff_unique_norm": "Guangdong University of Technology;Nanjing University;Illinois at Singapore Pte. Ltd.",
        "aff_unique_dep": "Faculty of Computer Science;State Key Laboratory for Novel Software Technology;Advanced Digital Sciences Center",
        "aff_unique_url": "http://www.gdut.edu.cn;http://www.nju.edu.cn;https://www.illinoisatsingapore.org.sg",
        "aff_unique_abbr": ";Nanjing University;",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Singapore",
        "aff_country_unique_index": "0+0;1;0",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "99861275cc",
        "title": "Safe Policy Iteration",
        "site": "https://proceedings.mlr.press/v28/pirotta13.html",
        "author": "Matteo Pirotta; Marcello Restelli; Alessio Pecorino; Daniele Calandriello",
        "abstract": "This paper presents a study of the policy improvement step that can be usefully exploited by approximate policy-iteration algorithms.  When either the policy evaluation step or the policy improvement step returns an approximated result, the sequence of policies produced by policy iteration may not be monotonically increasing, and oscillations may occur.  To address this issue, we consider safe policy improvements, i.e., at each iteration we search for a policy that maximizes a lower bound to the policy improvement w.r.t. the current policy. When no improving policy can be found the algorithm stops.  We propose two safe policy-iteration algorithms that differ in the way the next policy is chosen w.r.t. the estimated greedy policy.  Besides being theoretically derived and discussed, the proposed algorithms are empirically evaluated and compared with state-of-the-art approaches on some chain-walk domains and on the Blackjack card game.",
        "bibtex": "@InProceedings{pmlr-v28-pirotta13,\n  title = \t {Safe Policy Iteration},\n  author = \t {Pirotta, Matteo and Restelli, Marcello and Pecorino, Alessio and Calandriello, Daniele},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {307--315},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/pirotta13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/pirotta13.html},\n  abstract = \t {This paper presents a study of the policy improvement step that can be usefully exploited by approximate policy-iteration algorithms.  When either the policy evaluation step or the policy improvement step returns an approximated result, the sequence of policies produced by policy iteration may not be monotonically increasing, and oscillations may occur.  To address this issue, we consider safe policy improvements, i.e., at each iteration we search for a policy that maximizes a lower bound to the policy improvement w.r.t. the current policy. When no improving policy can be found the algorithm stops.  We propose two safe policy-iteration algorithms that differ in the way the next policy is chosen w.r.t. the estimated greedy policy.  Besides being theoretically derived and discussed, the proposed algorithms are empirically evaluated and compared with state-of-the-art approaches on some chain-walk domains and on the Blackjack card game.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/pirotta13.pdf",
        "supp": "",
        "pdf_size": 437369,
        "gs_citation": 137,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17681390066007549999&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Dept. Elect., Inf., and Bioeng., Politecnico di Milano, piazza Leonardo da Vinci 32, I-20133, Milan, ITALY; Dept. Elect., Inf., and Bioeng., Politecnico di Milano, piazza Leonardo da Vinci 32, I-20133, Milan, ITALY; Dept. Elect., Inf., and Bioeng., Politecnico di Milano, piazza Leonardo da Vinci 32, I-20133, Milan, ITALY; Dept. Elect., Inf., and Bioeng., Politecnico di Milano, piazza Leonardo da Vinci 32, I-20133, Milan, ITALY",
        "aff_domain": "polimi.it;polimi.it;mail.polimi.it;mail.polimi.it",
        "email": "polimi.it;polimi.it;mail.polimi.it;mail.polimi.it",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Politecnico di Milano",
        "aff_unique_dep": "Department of Electrical, Information, and Bioengineering",
        "aff_unique_url": "https://www.polimi.it",
        "aff_unique_abbr": "Polimi",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Milan",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "efa0df7eb4",
        "title": "Safe Screening of Non-Support Vectors in Pathwise SVM Computation",
        "site": "https://proceedings.mlr.press/v28/ogawa13b.html",
        "author": "Kohei Ogawa; Yoshiki Suzuki; Ichiro Takeuchi",
        "abstract": "In this paper, we claim that some of the non-support vectors (non-SVs) that have no influence on the classifier can be screened out prior to the training phase in pathwise SVM computation scenario, in which one is asked to train a sequence (or path) of SVM classifiers for different regularization parameters. Based on a recently proposed framework so-called safe screening rule, we derive a rule for screening out non-SVs in advance, and discuss how we can exploit the advantage of the rule in pathwise SVM computation scenario. Experiments indicate that our approach often substantially reduce the total pathwise computation cost.",
        "bibtex": "@InProceedings{pmlr-v28-ogawa13b,\n  title = \t {Safe Screening of Non-Support Vectors in Pathwise SVM Computation},\n  author = \t {Ogawa, Kohei and Suzuki, Yoshiki and Takeuchi, Ichiro},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1382--1390},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/ogawa13b.pdf},\n  url = \t {https://proceedings.mlr.press/v28/ogawa13b.html},\n  abstract = \t {In this paper, we claim that some of the non-support vectors (non-SVs) that have no influence on the classifier can be screened out prior to the training phase in pathwise SVM computation scenario, in which one is asked to train a sequence (or path) of SVM classifiers for different regularization parameters. Based on a recently proposed framework so-called safe screening rule, we derive a rule for screening out non-SVs in advance, and discuss how we can exploit the advantage of the rule in pathwise SVM computation scenario. Experiments indicate that our approach often substantially reduce the total pathwise computation cost.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/ogawa13b.pdf",
        "supp": "",
        "pdf_size": 485207,
        "gs_citation": 145,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13291097155048666936&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Nagoya Institute of Technology, Nagoya, Japan; Nagoya Institute of Technology, Nagoya, Japan; Nagoya Institute of Technology, Nagoya, Japan",
        "aff_domain": "gmail.com;gmail.com;nitech.ac.jp",
        "email": "gmail.com;gmail.com;nitech.ac.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Nagoya Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nitech.ac.jp",
        "aff_unique_abbr": "NIT",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Nagoya",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "031c89e8ab",
        "title": "Saving Evaluation Time for the Decision Function in Boosting: Representation and Reordering Base Learner",
        "site": "https://proceedings.mlr.press/v28/sun13.html",
        "author": "Peng Sun; Jie Zhou",
        "abstract": "For a well trained Boosting classifier, we are interested in how to save the testing time, i.e., to make the decision without evaluating all the base learners. To address this problem, in previous work the base learners are sequentially calculated and early stopping is allowed if the decision function has been confident enough to output its value. In such a chain structure, the order of base learners is critical: better order can lead to less evaluation time.    In this paper, we present a novel method for ordering. We base our discussion on the data structure representing Boosting\u2019s decision function. Viewing the decision function a boolean expression, we propose a Binary Valued Tree for its representation. As a secondary contribution, such a representation unifies the work by previous researchers and helps devise new representation. Also, its connection to Binary Decision Diagram(BDD) is discussed.",
        "bibtex": "@InProceedings{pmlr-v28-sun13,\n  title = \t {Saving Evaluation Time for the Decision Function in Boosting: Representation and Reordering Base Learner},\n  author = \t {Sun, Peng and Zhou, Jie},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {933--941},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/sun13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/sun13.html},\n  abstract = \t {For a well trained Boosting classifier, we are interested in how to save the testing time, i.e., to make the decision without evaluating all the base learners. To address this problem, in previous work the base learners are sequentially calculated and early stopping is allowed if the decision function has been confident enough to output its value. In such a chain structure, the order of base learners is critical: better order can lead to less evaluation time.    In this paper, we present a novel method for ordering. We base our discussion on the data structure representing Boosting\u2019s decision function. Viewing the decision function a boolean expression, we propose a Binary Valued Tree for its representation. As a secondary contribution, such a representation unifies the work by previous researchers and helps devise new representation. Also, its connection to Binary Decision Diagram(BDD) is discussed.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/sun13.pdf",
        "supp": "",
        "pdf_size": 907913,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16466612192405202289&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Tsinghua National Laboratory for Information Science and Technology(TNList), Department of Automation, Tsinghua University, Beijing 100084, China; Tsinghua National Laboratory for Information Science and Technology(TNList), Department of Automation, Tsinghua University, Beijing 100084, China",
        "aff_domain": "mails.tsinghua.edu.edu;tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.edu;tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Tsinghua University",
        "aff_unique_dep": "Department of Automation",
        "aff_unique_url": "https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "Tsinghua",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "d77785e995",
        "title": "Scalable Optimization of Neighbor Embedding for Visualization",
        "site": "https://proceedings.mlr.press/v28/yang13b.html",
        "author": "Zhirong Yang; Jaakko Peltonen; Samuel Kaski",
        "abstract": "Neighbor embedding (NE) methods have found their use in data visualization but are limited in big data analysis tasks due to their O(n^2) complexity for n data samples. We demonstrate that the obvious approach of subsampling produces inferior results and propose a generic approximated optimization technique that reduces the NE optimization cost to O(n log n). The technique is based on realizing that in visualization the embedding space is necessarily very low-dimensional (2D or 3D), and hence efficient approximations developed for n-body force calculations can be applied. In gradient-based NE algorithms the gradient for an individual point decomposes into \u201cforces\u201d exerted by the other points. The contributions of close-by points need to be computed individually but far-away points can be approximated by their \u201ccenter of mass\u201d, rapidly computable by applying a recursive decomposition of the visualization space into quadrants. The new algorithm brings a significant speed-up for medium-size data, and brings \u201cbig data\u201d within reach of visualization.",
        "bibtex": "@InProceedings{pmlr-v28-yang13b,\n  title = \t {Scalable Optimization of Neighbor Embedding for Visualization},\n  author = \t {Yang, Zhirong and Peltonen, Jaakko and Kaski, Samuel},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {127--135},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/yang13b.pdf},\n  url = \t {https://proceedings.mlr.press/v28/yang13b.html},\n  abstract = \t {Neighbor embedding (NE) methods have found their use in data visualization but are limited in big data analysis tasks due to their O(n^2) complexity for n data samples. We demonstrate that the obvious approach of subsampling produces inferior results and propose a generic approximated optimization technique that reduces the NE optimization cost to O(n log n). The technique is based on realizing that in visualization the embedding space is necessarily very low-dimensional (2D or 3D), and hence efficient approximations developed for n-body force calculations can be applied. In gradient-based NE algorithms the gradient for an individual point decomposes into \u201cforces\u201d exerted by the other points. The contributions of close-by points need to be computed individually but far-away points can be approximated by their \u201ccenter of mass\u201d, rapidly computable by applying a recursive decomposition of the visualization space into quadrants. The new algorithm brings a significant speed-up for medium-size data, and brings \u201cbig data\u201d within reach of visualization.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/yang13b.pdf",
        "supp": "",
        "pdf_size": 2393091,
        "gs_citation": 93,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=413897364736771323&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Information and Computer Science, Aalto University, Finland; Department of Information and Computer Science, Aalto University, Finland + Helsinki Institute for Information Technology HIIT, Aalto University and University of Helsinki; Department of Information and Computer Science, Aalto University, Finland + Helsinki Institute for Information Technology HIIT, Aalto University and University of Helsinki",
        "aff_domain": "aalto.fi;aalto.fi;aalto.fi",
        "email": "aalto.fi;aalto.fi;aalto.fi",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+0;0+0",
        "aff_unique_norm": "Aalto University",
        "aff_unique_dep": "Department of Information and Computer Science",
        "aff_unique_url": "https://www.aalto.fi",
        "aff_unique_abbr": "Aalto",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0+0",
        "aff_country_unique": "Finland"
    },
    {
        "id": "e027a13f0b",
        "title": "Scalable Simple Random Sampling and Stratified Sampling",
        "site": "https://proceedings.mlr.press/v28/meng13a.html",
        "author": "Xiangrui Meng",
        "abstract": "Analyzing data sets of billions of records has now become a regular task in    many companies and institutions.    In the statistical analysis of those massive data sets, sampling generally    plays a very important role.    In this work, we describe a scalable simple random sampling algorithm, named    ScaSRS, which uses probabilistic thresholds to decide on the fly whether to    accept, reject, or wait-list an item independently of others.    We prove, with high probability, it succeeds and needs only O(\\sqrtk)    storage, where k is the sample size.    ScaSRS extends naturally to a scalable stratified sampling algorithm, which is    favorable for heterogeneous data sets.    The proposed algorithms, when implemented in MapReduce, can effectively reduce    the size of intermediate output and greatly improve load balancing.    Empirical evaluation on large-scale data sets clearly demonstrates their    superiority.",
        "bibtex": "@InProceedings{pmlr-v28-meng13a,\n  title = \t {Scalable Simple Random Sampling and Stratified Sampling},\n  author = \t {Meng, Xiangrui},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {531--539},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/meng13a.pdf},\n  url = \t {https://proceedings.mlr.press/v28/meng13a.html},\n  abstract = \t {Analyzing data sets of billions of records has now become a regular task in    many companies and institutions.    In the statistical analysis of those massive data sets, sampling generally    plays a very important role.    In this work, we describe a scalable simple random sampling algorithm, named    ScaSRS, which uses probabilistic thresholds to decide on the fly whether to    accept, reject, or wait-list an item independently of others.    We prove, with high probability, it succeeds and needs only O(\\sqrtk)    storage, where k is the sample size.    ScaSRS extends naturally to a scalable stratified sampling algorithm, which is    favorable for heterogeneous data sets.    The proposed algorithms, when implemented in MapReduce, can effectively reduce    the size of intermediate output and greatly improve load balancing.    Empirical evaluation on large-scale data sets clearly demonstrates their    superiority.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/meng13a.pdf",
        "supp": "",
        "pdf_size": 346086,
        "gs_citation": 245,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11672774684782803642&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "LinkedIn Corporation, 2029 Stierlin Court, Mountain View, CA 94043, USA",
        "aff_domain": "linkedin.com",
        "email": "linkedin.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "LinkedIn Corporation",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.linkedin.com",
        "aff_unique_abbr": "LinkedIn",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "fec7c13b39",
        "title": "Scale Invariant Conditional Dependence Measures",
        "site": "https://proceedings.mlr.press/v28/jreddi13.html",
        "author": "Sashank J Reddi; Barnabas Poczos",
        "abstract": "In this paper we develop new dependence and conditional dependence measures and provide their estimators. An attractive property of these measures and estimators is that they are invariant to any monotone increasing transformations of the random variables, which is important in many applications including feature selection. Under certain conditions we show the consistency of these estimators, derive upper bounds on their convergence rates, and show that the estimators do not suffer from the curse of dimensionality. However, when the conditions are less restrictive, we derive a lower bound which proves that in the worst case the convergence can be arbitrarily slow similarly to some other estimators. Numerical illustrations demonstrate the applicability of our method.",
        "bibtex": "@InProceedings{pmlr-v28-jreddi13,\n  title = \t {Scale Invariant Conditional Dependence Measures},\n  author = \t {J Reddi, Sashank and Poczos, Barnabas},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1355--1363},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/jreddi13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/jreddi13.html},\n  abstract = \t {In this paper we develop new dependence and conditional dependence measures and provide their estimators. An attractive property of these measures and estimators is that they are invariant to any monotone increasing transformations of the random variables, which is important in many applications including feature selection. Under certain conditions we show the consistency of these estimators, derive upper bounds on their convergence rates, and show that the estimators do not suffer from the curse of dimensionality. However, when the conditions are less restrictive, we derive a lower bound which proves that in the worst case the convergence can be arbitrarily slow similarly to some other estimators. Numerical illustrations demonstrate the applicability of our method.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/jreddi13.pdf",
        "supp": "",
        "pdf_size": 439111,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3775091004595264493&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Machine Learning Department, School of Computer Science, Carnegie Mellon University; Machine Learning Department, School of Computer Science, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Machine Learning Department",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "91cdf5b338",
        "title": "Scaling Multidimensional Gaussian Processes using Projected Additive Approximations",
        "site": "https://proceedings.mlr.press/v28/gilboa13.html",
        "author": "Elad Gilboa; Yunus Saat\u00e7i; John Cunningham; Elad Gilboa",
        "abstract": "Exact Gaussian Process (GP) regression has O(N^3) runtime for data size N, making it intractable for large N. Advances in GP scaling have not been extended to the multidimensional input setting, despite the preponderance of multidimensional applications.   This paper introduces and tests a novel method of projected additive approximation to multidimensional GPs. We thoroughly illustrate the power of this method on several datasets, achieving close performance to the naive Full GP at orders of magnitude less cost.",
        "bibtex": "@InProceedings{pmlr-v28-gilboa13,\n  title = \t {Scaling Multidimensional {G}aussian Processes using Projected Additive Approximations},\n  author = \t {Gilboa, Elad and Saat\u00e7i, Yunus and Cunningham, John and Gilboa, Elad},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {454--461},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/gilboa13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/gilboa13.html},\n  abstract = \t {Exact Gaussian Process (GP) regression has O(N^3) runtime for data size N, making it intractable for large N. Advances in GP scaling have not been extended to the multidimensional input setting, despite the preponderance of multidimensional applications.   This paper introduces and tests a novel method of projected additive approximation to multidimensional GPs. We thoroughly illustrate the power of this method on several datasets, achieving close performance to the naive Full GP at orders of magnitude less cost.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/gilboa13.pdf",
        "supp": "",
        "pdf_size": 143564,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15344993193623712629&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "4750eb599e",
        "title": "Scaling the Indian Buffet Process via Submodular Maximization",
        "site": "https://proceedings.mlr.press/v28/reed13.html",
        "author": "Colorado Reed; Ghahramani Zoubin",
        "abstract": "Inference for latent feature models is inherently difficult as the inference space grows exponentially with the size of the input data and number of latent features. In this work, we use Kurihara & Wellings (2008)\u2019s maximization-expectation framework to perform approximate MAP inference for linear-Gaussian latent feature models with an Indian Buffet Process (IBP) prior. This formulation yields a submodular function of the features that corresponds to a lower bound on the model evidence. By adding a constant to this function, we obtain a nonnegative submodular function that can be maximized via a greedy algorithm that obtains at least a 1/3-approximation to the optimal solution. Our inference method scales linearly with the size of the input data, and we show the efficacy of our method on the largest datasets currently analyzed using an IBP model.",
        "bibtex": "@InProceedings{pmlr-v28-reed13,\n  title = \t {Scaling the Indian Buffet Process via Submodular Maximization},\n  author = \t {Reed, Colorado and Zoubin, Ghahramani},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1013--1021},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/reed13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/reed13.html},\n  abstract = \t {Inference for latent feature models is inherently difficult as the inference space grows exponentially with the size of the input data and number of latent features. In this work, we use Kurihara & Wellings (2008)\u2019s maximization-expectation framework to perform approximate MAP inference for linear-Gaussian latent feature models with an Indian Buffet Process (IBP) prior. This formulation yields a submodular function of the features that corresponds to a lower bound on the model evidence. By adding a constant to this function, we obtain a nonnegative submodular function that can be maximized via a greedy algorithm that obtains at least a 1/3-approximation to the optimal solution. Our inference method scales linearly with the size of the input data, and we show the efficacy of our method on the largest datasets currently analyzed using an IBP model. }\n}",
        "pdf": "http://proceedings.mlr.press/v28/reed13.pdf",
        "supp": "",
        "pdf_size": 534491,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16824129998209688918&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Engineering Department, Cambridge University, Cambridge UK; Engineering Department, Cambridge University, Cambridge UK",
        "aff_domain": "cam.ac.uk;eng.cam.ac.uk",
        "email": "cam.ac.uk;eng.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "Engineering Department",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "76cf7dab2a",
        "title": "Selective sampling algorithms for cost-sensitive multiclass prediction",
        "site": "https://proceedings.mlr.press/v28/agarwal13.html",
        "author": "Alekh Agarwal",
        "abstract": "In this paper, we study the problem of active learning for cost-sensitive multiclass classification. We propose selective  sampling algorithms, which process the data in a streaming fashion,  querying only a subset of the labels. For these algorithms, we analyze the regret and label complexity when the labels are generated according to a generalized linear model. We establish that the gains of active learning over passive learning can range from none to exponentially large, based on a natural notion of margin. We also  present a safety guarantee to guard against model mismatch. Numerical  simulations show that our algorithms indeed obtain a low regret with a  small number of queries.",
        "bibtex": "@InProceedings{pmlr-v28-agarwal13,\n  title = \t {Selective sampling algorithms for cost-sensitive multiclass prediction},\n  author = \t {Agarwal, Alekh},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1220--1228},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/agarwal13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/agarwal13.html},\n  abstract = \t {In this paper, we study the problem of active learning for cost-sensitive multiclass classification. We propose selective  sampling algorithms, which process the data in a streaming fashion,  querying only a subset of the labels. For these algorithms, we analyze the regret and label complexity when the labels are generated according to a generalized linear model. We establish that the gains of active learning over passive learning can range from none to exponentially large, based on a natural notion of margin. We also  present a safety guarantee to guard against model mismatch. Numerical  simulations show that our algorithms indeed obtain a low regret with a  small number of queries.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/agarwal13.pdf",
        "supp": "",
        "pdf_size": 840085,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8067924652640644755&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Microsoft Research, New York NY",
        "aff_domain": "microsoft.com",
        "email": "microsoft.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Microsoft Research",
        "aff_unique_url": "https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "MSR",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "New York",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5a67b4caef",
        "title": "Semi-supervised Clustering by Input Pattern Assisted Pairwise Similarity Matrix Completion",
        "site": "https://proceedings.mlr.press/v28/yi13.html",
        "author": "Jinfeng Yi; Lijun Zhang; Rong Jin; Qi Qian; Anil Jain",
        "abstract": "Many semi-supervised clustering algorithms have been proposed to improve the clustering accuracy by effectively exploring the available side information that is usually in the form of pairwise constraints. Despite the progress, there are two main shortcomings of the existing semi-supervised clustering algorithms. First, they have to deal with non-convex optimization problems, leading to clustering results that are sensitive to the initialization. Second, none of these algorithms is equipped with theoretical guarantee regarding the clustering performance. We address these limitations by developing a framework for semi-supervised clustering based on \\it input pattern assisted matrix completion. The key idea is to cast clustering into a matrix completion problem, and solve it efficiently by exploiting the correlation between input patterns and cluster assignments. Our analysis shows that under appropriate conditions, only O(\\log n) pairwise constraints are needed to accurately recover the true cluster partition. We verify the effectiveness of the proposed algorithm by comparing it to the state-of-the-art semi-supervised clustering algorithms on several benchmark datasets.",
        "bibtex": "@InProceedings{pmlr-v28-yi13,\n  title = \t {Semi-supervised Clustering by Input Pattern Assisted Pairwise Similarity Matrix Completion},\n  author = \t {Yi, Jinfeng and Zhang, Lijun and Jin, Rong and Qian, Qi and Jain, Anil},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1400--1408},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/yi13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/yi13.html},\n  abstract = \t {Many semi-supervised clustering algorithms have been proposed to improve the clustering accuracy by effectively exploring the available side information that is usually in the form of pairwise constraints. Despite the progress, there are two main shortcomings of the existing semi-supervised clustering algorithms. First, they have to deal with non-convex optimization problems, leading to clustering results that are sensitive to the initialization. Second, none of these algorithms is equipped with theoretical guarantee regarding the clustering performance. We address these limitations by developing a framework for semi-supervised clustering based on \\it input pattern assisted matrix completion. The key idea is to cast clustering into a matrix completion problem, and solve it efficiently by exploiting the correlation between input patterns and cluster assignments. Our analysis shows that under appropriate conditions, only O(\\log n) pairwise constraints are needed to accurately recover the true cluster partition. We verify the effectiveness of the proposed algorithm by comparing it to the state-of-the-art semi-supervised clustering algorithms on several benchmark datasets.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/yi13.pdf",
        "supp": "",
        "pdf_size": 407343,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3232444769353944000&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science and Engineering, Michigan State University, East Lansing, MI 48824 USA; Department of Computer Science and Engineering, Michigan State University, East Lansing, MI 48824 USA; Department of Computer Science and Engineering, Michigan State University, East Lansing, MI 48824 USA; Department of Computer Science and Engineering, Michigan State University, East Lansing, MI 48824 USA; Department of Computer Science and Engineering, Michigan State University, East Lansing, MI 48824 USA",
        "aff_domain": "msu.edu;msu.edu;cse.msu.edu;msu.edu;cse.msu.edu",
        "email": "msu.edu;msu.edu;cse.msu.edu;msu.edu;cse.msu.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Michigan State University",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.msu.edu",
        "aff_unique_abbr": "MSU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "East Lansing",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "303eeadc8a",
        "title": "Sequential Bayesian Search",
        "site": "https://proceedings.mlr.press/v28/wen13.html",
        "author": "Zheng Wen; Branislav Kveton; Brian Eriksson; Sandilya Bhamidipati",
        "abstract": "Millions of people search daily for movies, music, and books on the Internet. Unfortunately, non-personalized exploration of items can result in an infeasible number of costly interaction steps. We study the problem of efficient, repeated interactive search. In this problem, the user is navigated to the items of interest through a series of options and our objective is to learn a better search policy from past interactions with the user. We propose an efficient learning algorithm for solving the problem, sequential Bayesian search (SBS), and prove that it is Bayesian optimal. We also analyze the algorithm from the frequentist point of view and show that its regret is sublinear in the number of searches. Finally, we evaluate our method on a real-world movie discovery problem and show that it performs nearly optimally as the number of searches increases.",
        "bibtex": "@InProceedings{pmlr-v28-wen13,\n  title = \t {Sequential {B}ayesian Search},\n  author = \t {Wen, Zheng and Kveton, Branislav and Eriksson, Brian and Bhamidipati, Sandilya},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {226--234},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/wen13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/wen13.html},\n  abstract = \t {Millions of people search daily for movies, music, and books on the Internet. Unfortunately, non-personalized exploration of items can result in an infeasible number of costly interaction steps. We study the problem of efficient, repeated interactive search. In this problem, the user is navigated to the items of interest through a series of options and our objective is to learn a better search policy from past interactions with the user. We propose an efficient learning algorithm for solving the problem, sequential Bayesian search (SBS), and prove that it is Bayesian optimal. We also analyze the algorithm from the frequentist point of view and show that its regret is sublinear in the number of searches. Finally, we evaluate our method on a real-world movie discovery problem and show that it performs nearly optimally as the number of searches increases.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/wen13.pdf",
        "supp": "",
        "pdf_size": 556723,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16346267180798786793&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Electrical Engineering Department, Stanford University, Stanford, CA 94305, USA; Technicolor Research Center, 735 Emerson St, Palo Alto, CA 94301, USA; Technicolor Research Center, 735 Emerson St, Palo Alto, CA 94301, USA; Technicolor Research Center, 735 Emerson St, Palo Alto, CA 94301, USA",
        "aff_domain": "stanford.edu;technicolor.com;technicolor.com;technicolor.com",
        "email": "stanford.edu;technicolor.com;technicolor.com;technicolor.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Stanford University;Technicolor Research Center",
        "aff_unique_dep": "Electrical Engineering Department;",
        "aff_unique_url": "https://www.stanford.edu;https://www.technicolor.com/en",
        "aff_unique_abbr": "Stanford;",
        "aff_campus_unique_index": "0;1;1;1",
        "aff_campus_unique": "Stanford;Palo Alto",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6ace4fb70a",
        "title": "Sharp Generalization Error Bounds for Randomly-projected Classifiers",
        "site": "https://proceedings.mlr.press/v28/durrant13.html",
        "author": "Robert Durrant; Ata Kaban",
        "abstract": "We derive sharp bounds on the generalization error of a generic linear classifier trained by   empirical risk minimization on randomly-projected data. We make no restrictive assumptions   (such as sparsity or separability) on the data: Instead we use the fact that, in a classification setting, the question of interest is really \u2018what is the effect of random projection on the predicted class labels?\u2019 and we therefore derive the exact probability of \u2018label flipping\u2019 under Gaussian random projection in order to quantify this effect precisely in our bounds.",
        "bibtex": "@InProceedings{pmlr-v28-durrant13,\n  title = \t {Sharp Generalization Error Bounds for Randomly-projected Classifiers},\n  author = \t {Durrant, Robert and Kaban, Ata},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {693--701},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/durrant13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/durrant13.html},\n  abstract = \t {We derive sharp bounds on the generalization error of a generic linear classifier trained by   empirical risk minimization on randomly-projected data. We make no restrictive assumptions   (such as sparsity or separability) on the data: Instead we use the fact that, in a classification setting, the question of interest is really \u2018what is the effect of random projection on the predicted class labels?\u2019 and we therefore derive the exact probability of \u2018label flipping\u2019 under Gaussian random projection in order to quantify this effect precisely in our bounds.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/durrant13.pdf",
        "supp": "",
        "pdf_size": 250664,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14722905101505679123&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "School of Computer Science, University of Birmingham, Edgbaston, UK, B15 2TT; School of Computer Science, University of Birmingham, Edgbaston, UK, B15 2TT",
        "aff_domain": "cs.bham.ac.uk;cs.bham.ac.uk",
        "email": "cs.bham.ac.uk;cs.bham.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Birmingham",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.birmingham.ac.uk",
        "aff_unique_abbr": "UoB",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Edgbaston",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "274947af87",
        "title": "Simple Sparsification Improves Sparse Denoising Autoencoders in Denoising Highly Corrupted Images",
        "site": "https://proceedings.mlr.press/v28/cho13.html",
        "author": "Kyunghyun Cho",
        "abstract": "Recently Burger et al. (2012) and Xie et al. (2012) proposed to use a denoising autoencoder (DAE) for denoising noisy images. They showed that a plain, deep DAE can denoise noisy images as well as the conventional methods such as BM3D and KSVD. Both of them approached image denoising by denoising small, image patches of a larger image and combining them to form a clean image. In this setting, it is usual to use the encoder of the DAE to obtain the latent representation and subsequently apply the decoder to get the clean patch. We propose that a simple sparsification of the latent representation found by the encoder improves denoising performance, when the DAE was trained with sparsity regularization. The experiments confirm that the proposed sparsification indeed helps both denoising a small image patch and denoising a larger image consisting of those patches. Furthermore, it is found out that the proposed method improves even classification performance when test samples are corrupted with noise.",
        "bibtex": "@InProceedings{pmlr-v28-cho13,\n  title = \t {Simple Sparsification Improves Sparse Denoising Autoencoders in Denoising Highly Corrupted Images},\n  author = \t {Cho, Kyunghyun},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {432--440},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/cho13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/cho13.html},\n  abstract = \t {Recently Burger et al. (2012) and Xie et al. (2012) proposed to use a denoising autoencoder (DAE) for denoising noisy images. They showed that a plain, deep DAE can denoise noisy images as well as the conventional methods such as BM3D and KSVD. Both of them approached image denoising by denoising small, image patches of a larger image and combining them to form a clean image. In this setting, it is usual to use the encoder of the DAE to obtain the latent representation and subsequently apply the decoder to get the clean patch. We propose that a simple sparsification of the latent representation found by the encoder improves denoising performance, when the DAE was trained with sparsity regularization. The experiments confirm that the proposed sparsification indeed helps both denoising a small image patch and denoising a larger image consisting of those patches. Furthermore, it is found out that the proposed method improves even classification performance when test samples are corrupted with noise.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/cho13.pdf",
        "supp": "",
        "pdf_size": 488212,
        "gs_citation": 92,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14097142124672908388&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Information and Computer Science, Aalto University School of Science, Finland",
        "aff_domain": "aalto.fi",
        "email": "aalto.fi",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Aalto University",
        "aff_unique_dep": "Department of Information and Computer Science",
        "aff_unique_url": "https://www.aalto.fi",
        "aff_unique_abbr": "Aalto",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Finland"
    },
    {
        "id": "b650693481",
        "title": "Smooth Operators",
        "site": "https://proceedings.mlr.press/v28/grunewalder13.html",
        "author": "Steffen Grunewalder; Gretton Arthur; John Shawe-Taylor",
        "abstract": "We develop a generic approach to form smooth versions of basic mathematical operations like multiplication, composition, change of measure, and conditional expectation, among others.   Operations which result in functions outside the reproducing kernel Hilbert space (such as the product of two RKHS functions) are approximated via a natural cost function, such that the solution is guaranteed to be in the targeted RKHS. This approximation problem is reduced to a regression problem using an adjoint trick, and solved in a vector-valued RKHS, consisting of continuous, linear, smooth operators which map from an input, real-valued RKHS to the desired target RKHS. Important constraints, such as an almost everywhere positive density, can be enforced or approximated naturally in this framework, using convex constraints on the operators. Finally, smooth operators can be composed to accomplish more complex machine learning tasks, such as the sum rule and kernelized approximate Bayesian inference, where state-of-the-art convergence rates are obtained.",
        "bibtex": "@InProceedings{pmlr-v28-grunewalder13,\n  title = \t {Smooth Operators},\n  author = \t {Grunewalder, Steffen and Arthur, Gretton and Shawe-Taylor, John},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1184--1192},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/grunewalder13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/grunewalder13.html},\n  abstract = \t {We develop a generic approach to form smooth versions of basic mathematical operations like multiplication, composition, change of measure, and conditional expectation, among others.   Operations which result in functions outside the reproducing kernel Hilbert space (such as the product of two RKHS functions) are approximated via a natural cost function, such that the solution is guaranteed to be in the targeted RKHS. This approximation problem is reduced to a regression problem using an adjoint trick, and solved in a vector-valued RKHS, consisting of continuous, linear, smooth operators which map from an input, real-valued RKHS to the desired target RKHS. Important constraints, such as an almost everywhere positive density, can be enforced or approximated naturally in this framework, using convex constraints on the operators. Finally, smooth operators can be composed to accomplish more complex machine learning tasks, such as the sum rule and kernelized approximate Bayesian inference, where state-of-the-art convergence rates are obtained.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/grunewalder13.pdf",
        "supp": "",
        "pdf_size": 369804,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=467157777889027110&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "6ad3b3c89e",
        "title": "Smooth Sparse Coding via Marginal Regression for Learning Sparse Representations",
        "site": "https://proceedings.mlr.press/v28/balasubramanian13.html",
        "author": "Krishnakumar Balasubramanian; Kai Yu; Guy Lebanon",
        "abstract": "We propose and analyze a novel framework for learning sparse representations, based on two statistical techniques: kernel smoothing and marginal regression. The proposed approach provides a flexible framework for incorporating feature similarity or temporal information present in data sets, via nonparametric kernel smoothing. We provide generalization bounds for dictionary learning  using smooth sparse coding and show how the sample complexity depends on the L1 norm of kernel function used. Furthermore, we propose using marginal regression for obtaining sparse codes, which significantly improves the  speed and allows one to scale to large dictionary sizes easily. We demonstrate the advantages of the proposed approach, both in terms of accuracy and speed by extensive experimentation on several real data sets. In addition, we demonstrate how the proposed approach could be used for improving semisupervised sparse coding.",
        "bibtex": "@InProceedings{pmlr-v28-balasubramanian13,\n  title = \t {Smooth Sparse Coding via Marginal Regression for Learning Sparse Representations},\n  author = \t {Balasubramanian, Krishnakumar and Yu, Kai and Lebanon, Guy},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {289--297},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/balasubramanian13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/balasubramanian13.html},\n  abstract = \t {We propose and analyze a novel framework for learning sparse representations, based on two statistical techniques: kernel smoothing and marginal regression. The proposed approach provides a flexible framework for incorporating feature similarity or temporal information present in data sets, via nonparametric kernel smoothing. We provide generalization bounds for dictionary learning  using smooth sparse coding and show how the sample complexity depends on the L1 norm of kernel function used. Furthermore, we propose using marginal regression for obtaining sparse codes, which significantly improves the  speed and allows one to scale to large dictionary sizes easily. We demonstrate the advantages of the proposed approach, both in terms of accuracy and speed by extensive experimentation on several real data sets. In addition, we demonstrate how the proposed approach could be used for improving semisupervised sparse coding.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/balasubramanian13.pdf",
        "supp": "",
        "pdf_size": 228556,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11844209239288974717&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "College of Computing, Georgia Institute of Technology; Baidu Inc.; College of Computing, Georgia Institute of Technology",
        "aff_domain": "gatech.edu;baidu.com;cc.gatech.edu",
        "email": "gatech.edu;baidu.com;cc.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Georgia Institute of Technology;Baidu",
        "aff_unique_dep": "College of Computing;Baidu Inc.",
        "aff_unique_url": "https://www.gatech.edu;https://www.baidu.com",
        "aff_unique_abbr": "Georgia Tech;Baidu",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Atlanta;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "a54296cb30",
        "title": "Solving Continuous POMDPs: Value Iteration with Incremental Learning of an Efficient Space Representation",
        "site": "https://proceedings.mlr.press/v28/brechtel13.html",
        "author": "Sebastian Brechtel; Tobias Gindele; R\u00fcdiger Dillmann",
        "abstract": "Discrete POMDPs of medium complexity can be approximately solved in reasonable time. However, most applications have a continuous and thus uncountably infinite state space. We propose the novel concept of learning a discrete representation of the continuous state space to solve the integrals in continuous POMDPs efficiently and generalize sparse calculations over the continuous space. The representation is iteratively refined as part of a novel Value Iteration step and does not depend on prior knowledge. Consistency for the learned generalization is asserted by a self-correction algorithm. The presented concept is implemented for continuous state and observation spaces based on Monte Carlo approximation to allow for arbitrary POMDP models. In an experimental comparison it yields higher values in significantly shorter time than state of the art algorithms and solves higher-dimensional problems.",
        "bibtex": "@InProceedings{pmlr-v28-brechtel13,\n  title = \t {Solving Continuous POMDPs: Value Iteration with Incremental Learning of an Efficient Space Representation},\n  author = \t {Brechtel, Sebastian and Gindele, Tobias and Dillmann, R\u00fcdiger},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {370--378},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/brechtel13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/brechtel13.html},\n  abstract = \t {Discrete POMDPs of medium complexity can be approximately solved in reasonable time. However, most applications have a continuous and thus uncountably infinite state space. We propose the novel concept of learning a discrete representation of the continuous state space to solve the integrals in continuous POMDPs efficiently and generalize sparse calculations over the continuous space. The representation is iteratively refined as part of a novel Value Iteration step and does not depend on prior knowledge. Consistency for the learned generalization is asserted by a self-correction algorithm. The presented concept is implemented for continuous state and observation spaces based on Monte Carlo approximation to allow for arbitrary POMDP models. In an experimental comparison it yields higher values in significantly shorter time than state of the art algorithms and solves higher-dimensional problems.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/brechtel13.pdf",
        "supp": "",
        "pdf_size": 1958692,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7755650172653003944&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Karlsruhe Institute of Technology; Karlsruhe Institute of Technology; Karlsruhe Institute of Technology",
        "aff_domain": "kit.edu;kit.edu;kit.edu",
        "email": "kit.edu;kit.edu;kit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Karlsruhe Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.kit.edu",
        "aff_unique_abbr": "KIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "3759d18304",
        "title": "Sparse Gaussian Conditional Random Fields: Algorithms, Theory, and Application to Energy Forecasting",
        "site": "https://proceedings.mlr.press/v28/wytock13.html",
        "author": "Matt Wytock; Zico Kolter",
        "abstract": "This paper considers the sparse Gaussian conditional random field, a discriminative extension of sparse inverse covariance estimation, where we use convex methods to learn a high-dimensional conditional distribution of outputs given inputs. The model has been proposed by multiple researchers within the past year, yet previous papers have been substantially limited in their analysis of the method and in the ability to solve large-scale problems.  In this paper, we make three contributions: 1) we develop a second-order active-set method which is several orders of magnitude faster that previously proposed optimization approaches for this problem 2) we analyze the model from a theoretical standpoint, improving upon past bounds with convergence rates that depend logarithmically on the data dimension, and 3) we apply the method to large-scale energy forecasting problems, demonstrating state-of-the-art performance on two real-world tasks.",
        "bibtex": "@InProceedings{pmlr-v28-wytock13,\n  title = \t {Sparse Gaussian Conditional Random Fields: Algorithms, Theory, and Application to Energy Forecasting},\n  author = \t {Wytock, Matt and Kolter, Zico},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1265--1273},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/wytock13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/wytock13.html},\n  abstract = \t {This paper considers the sparse Gaussian conditional random field, a discriminative extension of sparse inverse covariance estimation, where we use convex methods to learn a high-dimensional conditional distribution of outputs given inputs. The model has been proposed by multiple researchers within the past year, yet previous papers have been substantially limited in their analysis of the method and in the ability to solve large-scale problems.  In this paper, we make three contributions: 1) we develop a second-order active-set method which is several orders of magnitude faster that previously proposed optimization approaches for this problem 2) we analyze the model from a theoretical standpoint, improving upon past bounds with convergence rates that depend logarithmically on the data dimension, and 3) we apply the method to large-scale energy forecasting problems, demonstrating state-of-the-art performance on two real-world tasks.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/wytock13.pdf",
        "supp": "",
        "pdf_size": 370357,
        "gs_citation": 130,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3685327827391564974&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Carnegie Mellon University, Pittsburgh PA; Carnegie Mellon University, Pittsburgh PA",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "924cfa9ae1",
        "title": "Sparse PCA through Low-rank Approximations",
        "site": "https://proceedings.mlr.press/v28/papailiopoulos13.html",
        "author": "Dimitris Papailiopoulos; Alexandros Dimakis; Stavros Korokythakis",
        "abstract": "We introduce a novel algorithm that computes the k-sparse principal component of a positive semidefinite matrix A.  Our algorithm is combinatorial and operates by examining a discrete set of special vectors lying in a low-dimensional eigen-subspace of A.  We obtain provable approximation guarantees that depend on the spectral profile of the matrix: the faster the eigenvalue decay, the better the quality of our approximation.  For example, if the eigenvalues of A follow a power-law decay, we obtain a polynomial-time approximation   algorithm for any desired accuracy.   We implement our algorithm and test it on multiple artificial and real data sets. Due to   a feature elimination step, it is possible to perform sparse PCA on data sets consisting of millions of entries in a few minutes.   Our experimental evaluation shows that our scheme is nearly optimal   while finding very sparse vectors.  We compare to the prior state of the art and show that our scheme matches or outperforms previous algorithms   in all tested data sets.",
        "bibtex": "@InProceedings{pmlr-v28-papailiopoulos13,\n  title = \t {Sparse PCA through Low-rank Approximations},\n  author = \t {Papailiopoulos, Dimitris and Dimakis, Alexandros and Korokythakis, Stavros},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {747--755},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/papailiopoulos13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/papailiopoulos13.html},\n  abstract = \t {We introduce a novel algorithm that computes the k-sparse principal component of a positive semidefinite matrix A.  Our algorithm is combinatorial and operates by examining a discrete set of special vectors lying in a low-dimensional eigen-subspace of A.  We obtain provable approximation guarantees that depend on the spectral profile of the matrix: the faster the eigenvalue decay, the better the quality of our approximation.  For example, if the eigenvalues of A follow a power-law decay, we obtain a polynomial-time approximation   algorithm for any desired accuracy.   We implement our algorithm and test it on multiple artificial and real data sets. Due to   a feature elimination step, it is possible to perform sparse PCA on data sets consisting of millions of entries in a few minutes.   Our experimental evaluation shows that our scheme is nearly optimal   while finding very sparse vectors.  We compare to the prior state of the art and show that our scheme matches or outperforms previous algorithms   in all tested data sets. }\n}",
        "pdf": "http://proceedings.mlr.press/v28/papailiopoulos13.pdf",
        "supp": "",
        "pdf_size": 535063,
        "gs_citation": 94,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18010789333807145099&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Electrical and Computer Engineering, the University of Texas at Austin, TX, USA; Department of Electrical and Computer Engineering, the University of Texas at Austin, TX, USA; Stochastic Technologies",
        "aff_domain": "utexas.edu;austin.utexas.edu;stochastictechnologies.com",
        "email": "utexas.edu;austin.utexas.edu;stochastictechnologies.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Texas at Austin;Stochastic Technologies",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;",
        "aff_unique_url": "https://www.utexas.edu;",
        "aff_unique_abbr": "UT Austin;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Austin;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "4ce8ce2d08",
        "title": "Sparse Uncorrelated Linear Discriminant Analysis",
        "site": "https://proceedings.mlr.press/v28/zhang13.html",
        "author": "Xiaowei Zhang; Delin Chu",
        "abstract": "In this paper, we develop a novel approach for sparse uncorrelated linear discriminant analysis (ULDA). Our proposal is based on characterization of all solutions of the generalized ULDA. We incorporate sparsity into the ULDA transformation by seeking the solution with minimum \\ell_1-norm from all minimum dimension solutions of the generalized ULDA. The problem is then formulated as a \\ell_1-minimization problem and is solved by accelerated linearized Bregman method. Experiments on high-dimensional gene expression data demonstrate that our approach not only computes extremely sparse solutions but also performs well in classification. Experimental results also show that our approach can help for data visualization in low-dimensional space.",
        "bibtex": "@InProceedings{pmlr-v28-zhang13,\n  title = \t {Sparse Uncorrelated Linear Discriminant Analysis},\n  author = \t {Zhang, Xiaowei and Chu, Delin},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {45--52},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/zhang13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/zhang13.html},\n  abstract = \t {In this paper, we develop a novel approach for sparse uncorrelated linear discriminant analysis (ULDA). Our proposal is based on characterization of all solutions of the generalized ULDA. We incorporate sparsity into the ULDA transformation by seeking the solution with minimum \\ell_1-norm from all minimum dimension solutions of the generalized ULDA. The problem is then formulated as a \\ell_1-minimization problem and is solved by accelerated linearized Bregman method. Experiments on high-dimensional gene expression data demonstrate that our approach not only computes extremely sparse solutions but also performs well in classification. Experimental results also show that our approach can help for data visualization in low-dimensional space.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/zhang13.pdf",
        "supp": "",
        "pdf_size": 398629,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4466593598462691537&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Mathematics, National University of Singapore, Singapore 119076; Department of Mathematics, National University of Singapore, Singapore 119076",
        "aff_domain": "nus.edu.sg;nus.edu.sg",
        "email": "nus.edu.sg;nus.edu.sg",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "National University of Singapore",
        "aff_unique_dep": "Department of Mathematics",
        "aff_unique_url": "https://www.nus.edu.sg",
        "aff_unique_abbr": "NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "47759b6064",
        "title": "Sparse coding for multitask and transfer learning",
        "site": "https://proceedings.mlr.press/v28/maurer13.html",
        "author": "Andreas Maurer; Massi Pontil; Bernardino Romera-Paredes",
        "abstract": "We investigate the use of sparse coding and dictionary learning in the context of multitask and transfer learning. The central assumption of our learning method is that the tasks parameters are well approximated by sparse linear combinations of the atoms of a dictionary on a high or infinite dimensional space. This assumption, together with the large quantity of available data in the multitask and transfer learning settings, allows a principled choice of the dictionary. We provide bounds on the generalization error of this approach, for both settings. Numerical experiments on one synthetic and two real datasets show the advantage of our method over single task learning, a previous method based on orthogonal and dense representation of the tasks and a related method learning task grouping.",
        "bibtex": "@InProceedings{pmlr-v28-maurer13,\n  title = \t {Sparse coding for multitask and transfer learning},\n  author = \t {Maurer, Andreas and Pontil, Massi and Romera-Paredes, Bernardino},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {343--351},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/maurer13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/maurer13.html},\n  abstract = \t {We investigate the use of sparse coding and dictionary learning in the context of multitask and transfer learning. The central assumption of our learning method is that the tasks parameters are well approximated by sparse linear combinations of the atoms of a dictionary on a high or infinite dimensional space. This assumption, together with the large quantity of available data in the multitask and transfer learning settings, allows a principled choice of the dictionary. We provide bounds on the generalization error of this approach, for both settings. Numerical experiments on one synthetic and two real datasets show the advantage of our method over single task learning, a previous method based on orthogonal and dense representation of the tasks and a related method learning task grouping.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/maurer13.pdf",
        "supp": "",
        "pdf_size": 216661,
        "gs_citation": 223,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11657119000168665434&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Adalbertstrasse 55, D-80799, Munchen, Germany; Department of Computer Science and Centre for Computational Statistics and Machine Learning, University College London, Malet Place, London WC1E 6BT, UK; Department of Computer Science and UCL Interactive Centre, University College London, Malet Place, London WC1E 6BT, UK",
        "aff_domain": "andreas-maurer.eu;cs.ucl.ac.uk;ucl.ac.uk",
        "email": "andreas-maurer.eu;cs.ucl.ac.uk;ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Technical University of Munich;University College London",
        "aff_unique_dep": ";Department of Computer Science",
        "aff_unique_url": "https://www.tum.de;https://www.ucl.ac.uk",
        "aff_unique_abbr": "TUM;UCL",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Munich;London",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Germany;United Kingdom"
    },
    {
        "id": "20936af51d",
        "title": "Sparse projections onto the simplex",
        "site": "https://proceedings.mlr.press/v28/kyrillidis13.html",
        "author": "Anastasios Kyrillidis; Stephen Becker; Volkan Cevher; Christoph Koch",
        "abstract": "Most learning methods with rank or sparsity constraints use convex relaxations, which lead to optimization with the nuclear norm or the \\ell_1-norm. However, several important learning applications cannot benefit from this approach as they feature these convex norms as constraints in addition to the non-convex rank and sparsity constraints. In this setting, we derive efficient sparse projections onto the simplex and its extension, and illustrate how to use them to solve high-dimensional learning problems in quantum tomography, sparse density estimation and portfolio selection with non-convex constraints.",
        "bibtex": "@InProceedings{pmlr-v28-kyrillidis13,\n  title = \t {Sparse projections onto the simplex},\n  author = \t {Kyrillidis, Anastasios and Becker, Stephen and Cevher, Volkan and Koch, Christoph},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {235--243},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/kyrillidis13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/kyrillidis13.html},\n  abstract = \t {Most learning methods with rank or sparsity constraints use convex relaxations, which lead to optimization with the nuclear norm or the \\ell_1-norm. However, several important learning applications cannot benefit from this approach as they feature these convex norms as constraints in addition to the non-convex rank and sparsity constraints. In this setting, we derive efficient sparse projections onto the simplex and its extension, and illustrate how to use them to solve high-dimensional learning problems in quantum tomography, sparse density estimation and portfolio selection with non-convex constraints.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/kyrillidis13.pdf",
        "supp": "",
        "pdf_size": 793008,
        "gs_citation": 106,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3489334766042670156&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "EPFL; UPMC; EPFL; EPFL",
        "aff_domain": "epfl.ch;upmc.fr;epfl.ch;epfl.ch",
        "email": "epfl.ch;upmc.fr;epfl.ch;epfl.ch",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "EPFL;Universit\u00e9 Pierre et Marie Curie",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.epfl.ch;https://www.upmc.fr",
        "aff_unique_abbr": "EPFL;UPMC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "Switzerland;France"
    },
    {
        "id": "60f3df6629",
        "title": "Sparsity-Based Generalization Bounds for Predictive Sparse Coding",
        "site": "https://proceedings.mlr.press/v28/mehta13.html",
        "author": "Nishant Mehta; Alexander Gray",
        "abstract": "The goal of predictive sparse coding is to learn a representation of examples as sparse linear combinations of elements from a dictionary, such that a learned hypothesis linear in the new representation performs well on a predictive task. Predictive sparse coding has demonstrated impressive performance on a variety of supervised tasks, but its generalization properties have not been studied. We establish the first generalization error bounds for predictive sparse coding, in the overcomplete setting, where the number of features k exceeds the original dimensionality d. The learning bound decays as (sqrt(d k/m)) with respect to d, k, and the size m of the training sample. It depends intimately on stability properties of the learned sparse encoder, as measured on the training sample. Consequently, we also present a fundamental stability result for the LASSO, a result that characterizes the stability of the sparse codes with respect to dictionary perturbations.",
        "bibtex": "@InProceedings{pmlr-v28-mehta13,\n  title = \t {Sparsity-Based Generalization Bounds for Predictive Sparse Coding},\n  author = \t {Mehta, Nishant and Gray, Alexander},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {36--44},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/mehta13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/mehta13.html},\n  abstract = \t {The goal of predictive sparse coding is to learn a representation of examples as sparse linear combinations of elements from a dictionary, such that a learned hypothesis linear in the new representation performs well on a predictive task. Predictive sparse coding has demonstrated impressive performance on a variety of supervised tasks, but its generalization properties have not been studied. We establish the first generalization error bounds for predictive sparse coding, in the overcomplete setting, where the number of features k exceeds the original dimensionality d. The learning bound decays as (sqrt(d k/m)) with respect to d, k, and the size m of the training sample. It depends intimately on stability properties of the learned sparse encoder, as measured on the training sample. Consequently, we also present a fundamental stability result for the LASSO, a result that characterizes the stability of the sparse codes with respect to dictionary perturbations.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/mehta13.pdf",
        "supp": "",
        "pdf_size": 306705,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9226194333005683494&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "College of Computing, Georgia Institute of Technology, Atlanta, GA 30332, USA; College of Computing, Georgia Institute of Technology, Atlanta, GA 30332, USA",
        "aff_domain": "cc.gatech.edu;cc.gatech.edu",
        "email": "cc.gatech.edu;cc.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Georgia Institute of Technology",
        "aff_unique_dep": "College of Computing",
        "aff_unique_url": "https://www.gatech.edu",
        "aff_unique_abbr": "Georgia Tech",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Atlanta",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "eee4fc7604",
        "title": "Spectral Compressed Sensing via Structured Matrix Completion",
        "site": "https://proceedings.mlr.press/v28/chen13g.html",
        "author": "Yuxin Chen; Yuejie Chi",
        "abstract": "The paper studies the problem of recovering a spectrally sparse object from a small number of time domain samples. Specifically, the object of interest with ambient dimension n is assumed to be a mixture of r complex multi-dimensional sinusoids, while the underlying frequencies can assume any value in the unit disk. Conventional compressed sensing paradigms suffer from the \\em basis mismatch issue when imposing a discrete dictionary on the Fourier representation. To address this problem,  we develop a novel nonparametric algorithm, called enhanced matrix completion (EMaC), based on structured matrix completion. The algorithm starts by converting the data into a low-rank enhanced form with multi-fold Hankel structure, then attempts recovery via nuclear norm minimization. Under mild incoherence conditions, EMaC allows perfect recovery as soon as the number of samples exceeds the order of \\mathcalO(r\\log^2 n). We also show that, in many instances, accurate completion of a low-rank multi-fold Hankel matrix is possible when the number of observed entries is proportional to the information theoretical limits (except for a logarithmic gap). The robustness of EMaC against bounded noise and its applicability to super resolution are further demonstrated by numerical experiments.",
        "bibtex": "@InProceedings{pmlr-v28-chen13g,\n  title = \t {Spectral Compressed Sensing via Structured Matrix Completion},\n  author = \t {Chen, Yuxin and Chi, Yuejie},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {414--422},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/chen13g.pdf},\n  url = \t {https://proceedings.mlr.press/v28/chen13g.html},\n  abstract = \t {The paper studies the problem of recovering a spectrally sparse object from a small number of time domain samples. Specifically, the object of interest with ambient dimension n is assumed to be a mixture of r complex multi-dimensional sinusoids, while the underlying frequencies can assume any value in the unit disk. Conventional compressed sensing paradigms suffer from the \\em basis mismatch issue when imposing a discrete dictionary on the Fourier representation. To address this problem,  we develop a novel nonparametric algorithm, called enhanced matrix completion (EMaC), based on structured matrix completion. The algorithm starts by converting the data into a low-rank enhanced form with multi-fold Hankel structure, then attempts recovery via nuclear norm minimization. Under mild incoherence conditions, EMaC allows perfect recovery as soon as the number of samples exceeds the order of \\mathcalO(r\\log^2 n). We also show that, in many instances, accurate completion of a low-rank multi-fold Hankel matrix is possible when the number of observed entries is proportional to the information theoretical limits (except for a logarithmic gap). The robustness of EMaC against bounded noise and its applicability to super resolution are further demonstrated by numerical experiments.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/chen13g.pdf",
        "supp": "",
        "pdf_size": 403189,
        "gs_citation": 379,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8517192265760564915&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 20,
        "aff": "Department of Electrical Engineering, Stanford University, Stanford, CA 94305, USA; Electrical and Computer Engineering, The Ohio State University, Columbus, OH 43210, USA",
        "aff_domain": "stanford.edu;ece.osu.edu",
        "email": "stanford.edu;ece.osu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Stanford University;Ohio State University",
        "aff_unique_dep": "Department of Electrical Engineering;Electrical and Computer Engineering",
        "aff_unique_url": "https://www.stanford.edu;https://www.osu.edu",
        "aff_unique_abbr": "Stanford;OSU",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Stanford;Columbus",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5d0c6ac047",
        "title": "Spectral Experts for Estimating Mixtures of Linear Regressions",
        "site": "https://proceedings.mlr.press/v28/tejasvichaganty13.html",
        "author": "Arun Tejasvi Chaganty; Percy Liang",
        "abstract": "Discriminative latent-variable models are typically learned using EM or gradient-based optimization, which suffer from local optima.  In this paper, we develop a new computationally efficient and provably consistent estimator for the mixture of linear regressions, a simple instance of discriminative latent-variable models.  Our approach relies on a low-rank linear regression to recover a symmetric tensor, which can be factorized into the parameters using the tensor power method.  We prove rates of convergence for our estimator and provide an empirical evaluation illustrating its strengths relative to local optimization (EM).",
        "bibtex": "@InProceedings{pmlr-v28-tejasvichaganty13,\n  title = \t {Spectral Experts for Estimating Mixtures of Linear Regressions},\n  author = \t {Tejasvi Chaganty, Arun and Liang, Percy},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1040--1048},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/tejasvichaganty13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/tejasvichaganty13.html},\n  abstract = \t {Discriminative latent-variable models are typically learned using EM or gradient-based optimization, which suffer from local optima.  In this paper, we develop a new computationally efficient and provably consistent estimator for the mixture of linear regressions, a simple instance of discriminative latent-variable models.  Our approach relies on a low-rank linear regression to recover a symmetric tensor, which can be factorized into the parameters using the tensor power method.  We prove rates of convergence for our estimator and provide an empirical evaluation illustrating its strengths relative to local optimization (EM).  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/tejasvichaganty13.pdf",
        "supp": "",
        "pdf_size": 691908,
        "gs_citation": 171,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4798975462638489414&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Stanford University; Stanford University",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "65dd466b80",
        "title": "Spectral Learning of Hidden Markov Models from Dynamic and Static Data",
        "site": "https://proceedings.mlr.press/v28/huang13.html",
        "author": "Tzu-Kuo Huang; Jeff Schneider",
        "abstract": "We develop spectral learning algorithms for Hidden Markov Models  that learn not only from time series, or dynamic data but also  static data drawn independently from the HMM\u2019s stationary distribution.  This is motivated by the fact that static, orderless snapshots are usually easier to obtain than time series in quite a few dynamic modeling tasks. Building on existing spectral learning algorithms, our methods solve convex optimization problems minimizing squared loss on the dynamic data plus a regularization term on the static data. Experiments on synthetic and  real human activities data demonstrate better prediction by the proposed method than existing spectral algorithms.",
        "bibtex": "@InProceedings{pmlr-v28-huang13,\n  title = \t {Spectral Learning of Hidden Markov Models from Dynamic and Static Data},\n  author = \t {Huang, Tzu-Kuo and Schneider, Jeff},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {630--638},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/huang13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/huang13.html},\n  abstract = \t {We develop spectral learning algorithms for Hidden Markov Models  that learn not only from time series, or dynamic data but also  static data drawn independently from the HMM\u2019s stationary distribution.  This is motivated by the fact that static, orderless snapshots are usually easier to obtain than time series in quite a few dynamic modeling tasks. Building on existing spectral learning algorithms, our methods solve convex optimization problems minimizing squared loss on the dynamic data plus a regularization term on the static data. Experiments on synthetic and  real human activities data demonstrate better prediction by the proposed method than existing spectral algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/huang13.pdf",
        "supp": "",
        "pdf_size": 314808,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9584460003923715308&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Machine Learning Department, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Machine Learning Department",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ca83c809bc",
        "title": "Squared-loss Mutual Information Regularization: A Novel Information-theoretic Approach to Semi-supervised Learning",
        "site": "https://proceedings.mlr.press/v28/niu13.html",
        "author": "Gang Niu; Wittawat Jitkrittum; Bo Dai; Hirotaka Hachiya; Masashi Sugiyama",
        "abstract": "We propose squared-loss mutual information regularization (SMIR) for multi-class probabilistic classification, following the information maximization principle. SMIR is convex under mild conditions and thus improves the nonconvexity of mutual information regularization. It offers all of the following four abilities to semi-supervised algorithms: Analytical solution, out-of-sample/multi-class classification, and probabilistic output. Furthermore, novel generalization error bounds are derived. Experiments show SMIR compares favorably with state-of-the-art methods.",
        "bibtex": "@InProceedings{pmlr-v28-niu13,\n  title = \t {Squared-loss Mutual Information Regularization: A Novel Information-theoretic Approach to Semi-supervised Learning},\n  author = \t {Niu, Gang and Jitkrittum, Wittawat and Dai, Bo and Hachiya, Hirotaka and Sugiyama, Masashi},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {10--18},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/niu13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/niu13.html},\n  abstract = \t {We propose squared-loss mutual information regularization (SMIR) for multi-class probabilistic classification, following the information maximization principle. SMIR is convex under mild conditions and thus improves the nonconvexity of mutual information regularization. It offers all of the following four abilities to semi-supervised algorithms: Analytical solution, out-of-sample/multi-class classification, and probabilistic output. Furthermore, novel generalization error bounds are derived. Experiments show SMIR compares favorably with state-of-the-art methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/niu13.pdf",
        "supp": "",
        "pdf_size": 517666,
        "gs_citation": 70,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6251990112729145262&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Science, Tokyo Institute of Technology, Tokyo, 152-8552, Japan; Department of Computer Science, Tokyo Institute of Technology, Tokyo, 152-8552, Japan + College of Computing, Georgia Institute of Technology, Atlanta, GA 30332, USA; College of Computing, Georgia Institute of Technology, Atlanta, GA 30332, USA; Department of Computer Science, Tokyo Institute of Technology, Tokyo, 152-8552, Japan; Department of Computer Science, Tokyo Institute of Technology, Tokyo, 152-8552, Japan",
        "aff_domain": "sg.cs.titech.ac.jp;gmail.com;gatech.edu;gmail.com;cs.titech.ac.jp",
        "email": "sg.cs.titech.ac.jp;gmail.com;gatech.edu;gmail.com;cs.titech.ac.jp",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;1;0;0",
        "aff_unique_norm": "Tokyo Institute of Technology;Georgia Institute of Technology",
        "aff_unique_dep": "Department of Computer Science;College of Computing",
        "aff_unique_url": "https://www.titech.ac.jp;https://www.gatech.edu",
        "aff_unique_abbr": "Titech;Georgia Tech",
        "aff_campus_unique_index": "0;0+1;1;0;0",
        "aff_campus_unique": "Tokyo;Atlanta",
        "aff_country_unique_index": "0;0+1;1;0;0",
        "aff_country_unique": "Japan;United States"
    },
    {
        "id": "e084fa5f66",
        "title": "Stability and Hypothesis Transfer Learning",
        "site": "https://proceedings.mlr.press/v28/kuzborskij13.html",
        "author": "Ilja Kuzborskij; Francesco Orabona",
        "abstract": "We consider the transfer learning scenario, where the learner does not have access to the source domain directly, but rather operates on the basis of hypotheses induced from it \u2013 the Hypothesis Transfer Learning (HTL) problem. Particularly, we conduct a theoretical analysis of HTL by considering the algorithmic stability of a class of HTL algorithms based on Regularized Least Squares with biased regularization. We show that the relatedness of source and target domains accelerates the convergence of the Leave-One-Out error to the generalization error, thus enabling the use of the Leave-One-Out error to find the optimal transfer parameters, even in the presence of a small training set. In case of unrelated domains we also suggest a theoretically principled way to prevent negative transfer, so that in the limit we recover the performance of the algorithm not using any knowledge from the source domain.",
        "bibtex": "@InProceedings{pmlr-v28-kuzborskij13,\n  title = \t {Stability and Hypothesis Transfer Learning},\n  author = \t {Kuzborskij, Ilja and Orabona, Francesco},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {942--950},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/kuzborskij13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/kuzborskij13.html},\n  abstract = \t {We consider the transfer learning scenario, where the learner does not have access to the source domain directly, but rather operates on the basis of hypotheses induced from it \u2013 the Hypothesis Transfer Learning (HTL) problem. Particularly, we conduct a theoretical analysis of HTL by considering the algorithmic stability of a class of HTL algorithms based on Regularized Least Squares with biased regularization. We show that the relatedness of source and target domains accelerates the convergence of the Leave-One-Out error to the generalization error, thus enabling the use of the Leave-One-Out error to find the optimal transfer parameters, even in the presence of a small training set. In case of unrelated domains we also suggest a theoretically principled way to prevent negative transfer, so that in the limit we recover the performance of the algorithm not using any knowledge from the source domain.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/kuzborskij13.pdf",
        "supp": "",
        "pdf_size": 326200,
        "gs_citation": 218,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5642234637780837688&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Idiap Research Institute, Switzerland + \u00b4Ecole Polytechnique F\u00b4 ed\u00b4 erale de Lausanne (EPFL), Switzerland; Toyota Technological Institute at Chicago, USA",
        "aff_domain": "idiap.ch;orabona.com",
        "email": "idiap.ch;orabona.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "Idiap Research Institute;EPFL;Toyota Technological Institute at Chicago",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.idiap.ch;https://www.epfl.ch;https://www.tti-chicago.org",
        "aff_unique_abbr": "Idiap;EPFL;TTI Chicago",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Chicago",
        "aff_country_unique_index": "0+0;1",
        "aff_country_unique": "Switzerland;United States"
    },
    {
        "id": "fedbd5d312",
        "title": "Stable Coactive Learning via Perturbation",
        "site": "https://proceedings.mlr.press/v28/raman13.html",
        "author": "Karthik Raman; Thorsten Joachims; Pannaga Shivaswamy; Tobias Schnabel",
        "abstract": "Coactive Learning is a model of interaction between a learning system (e.g. search engine) and its human users, wherein the system learns from (typically implicit) user feedback during operational use. User feedback takes the form of preferences, and recent work has introduced online algorithms that learn from this weak feedback. However, we show that these algorithms can be unstable and ineffective in real-world settings where biases and noise in the feedback are significant. In this paper, we propose the first coactive learning algorithm that can learn robustly despite bias and noise. In particular, we explore how presenting users with slightly perturbed objects (e.g., rankings) can stabilize the learning process. We theoretically validate the algorithm by proving bounds on the average regret. We also provide extensive empirical evidence on benchmarks and from a live search engine user study, showing that the new algorithm substantially outperforms existing methods.",
        "bibtex": "@InProceedings{pmlr-v28-raman13,\n  title = \t {Stable Coactive Learning via Perturbation},\n  author = \t {Raman, Karthik and Joachims, Thorsten and Shivaswamy, Pannaga and Schnabel, Tobias},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {837--845},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/raman13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/raman13.html},\n  abstract = \t {Coactive Learning is a model of interaction between a learning system (e.g. search engine) and its human users, wherein the system learns from (typically implicit) user feedback during operational use. User feedback takes the form of preferences, and recent work has introduced online algorithms that learn from this weak feedback. However, we show that these algorithms can be unstable and ineffective in real-world settings where biases and noise in the feedback are significant. In this paper, we propose the first coactive learning algorithm that can learn robustly despite bias and noise. In particular, we explore how presenting users with slightly perturbed objects (e.g., rankings) can stabilize the learning process. We theoretically validate the algorithm by proving bounds on the average regret. We also provide extensive empirical evidence on benchmarks and from a live search engine user study, showing that the new algorithm substantially outperforms existing methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/raman13.pdf",
        "supp": "",
        "pdf_size": 1009292,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13552990587550333548&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computer Science, Cornell University, Ithaca, NY, USA; Department of Computer Science, Cornell University, Ithaca, NY, USA; AT&T Research, San Francisco, CA, USA; Fachbereich Informatik, Universitaet Stuttgart, Stuttgart, Germany",
        "aff_domain": "cs.cornell.edu;cs.cornell.edu;research.att.com;cornell.edu",
        "email": "cs.cornell.edu;cs.cornell.edu;research.att.com;cornell.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "Cornell University;AT&T Research;Universitaet Stuttgart",
        "aff_unique_dep": "Department of Computer Science;;Fachbereich Informatik",
        "aff_unique_url": "https://www.cornell.edu;https://www.research.att.com;https://www.uni-stuttgart.de",
        "aff_unique_abbr": "Cornell;AT&T;",
        "aff_campus_unique_index": "0;0;1;2",
        "aff_campus_unique": "Ithaca;San Francisco;Stuttgart",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "United States;Germany"
    },
    {
        "id": "eabec6ed4f",
        "title": "Stochastic Alternating Direction Method of Multipliers",
        "site": "https://proceedings.mlr.press/v28/ouyang13.html",
        "author": "Hua Ouyang; Niao He; Long Tran; Alexander Gray",
        "abstract": "The Alternating Direction Method of Multipliers (ADMM) has received lots of attention recently due to the tremendous demand from large-scale and data-distributed machine learning applications. In this paper, we present a stochastic setting for optimization problems with non-smooth composite objective functions. To solve this problem, we propose a stochastic ADMM algorithm. Our algorithm applies to a more general class of convex and nonsmooth objective functions, beyond the smooth and separable least squares loss used in lasso. We also demonstrate the rates of convergence for our algorithm under various structural assumptions of the stochastic function: O(1/\\sqrtt) for convex functions and O(\\log t/t) for strongly convex functions. Compared to previous literature, we establish the convergence rate of ADMM for convex problems in terms of both the objective value and the feasibility violation. A novel application named Graph-Guided SVM is proposed to demonstrate the usefulness of our algorithm.",
        "bibtex": "@InProceedings{pmlr-v28-ouyang13,\n  title = \t {Stochastic Alternating Direction Method of Multipliers},\n  author = \t {Ouyang, Hua and He, Niao and Tran, Long and Gray, Alexander},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {80--88},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/ouyang13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/ouyang13.html},\n  abstract = \t {The Alternating Direction Method of Multipliers (ADMM) has received lots of attention recently due to the tremendous demand from large-scale and data-distributed machine learning applications. In this paper, we present a stochastic setting for optimization problems with non-smooth composite objective functions. To solve this problem, we propose a stochastic ADMM algorithm. Our algorithm applies to a more general class of convex and nonsmooth objective functions, beyond the smooth and separable least squares loss used in lasso. We also demonstrate the rates of convergence for our algorithm under various structural assumptions of the stochastic function: O(1/\\sqrtt) for convex functions and O(\\log t/t) for strongly convex functions. Compared to previous literature, we establish the convergence rate of ADMM for convex problems in terms of both the objective value and the feasibility violation. A novel application named Graph-Guided SVM is proposed to demonstrate the usefulness of our algorithm.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/ouyang13.pdf",
        "supp": "",
        "pdf_size": 195772,
        "gs_citation": 368,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2528599853796397483&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "School of Computational Science and Engineering, Georgia Tech; H. Milton Stewart School of Industrial and Systems Engineering, Georgia Tech; H. Milton Stewart School of Industrial and Systems Engineering, Georgia Tech; School of Computational Science and Engineering, Georgia Tech",
        "aff_domain": "cc.gatech.edu;isye.gatech.edu;gatech.edu;cc.gatech.edu",
        "email": "cc.gatech.edu;isye.gatech.edu;gatech.edu;cc.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Georgia Institute of Technology",
        "aff_unique_dep": "School of Computational Science and Engineering",
        "aff_unique_url": "https://www.gatech.edu",
        "aff_unique_abbr": "Georgia Tech",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Atlanta;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7cd5745541",
        "title": "Stochastic Gradient Descent for Non-smooth Optimization: Convergence Results and Optimal Averaging Schemes",
        "site": "https://proceedings.mlr.press/v28/shamir13.html",
        "author": "Ohad Shamir; Tong Zhang",
        "abstract": "Stochastic Gradient Descent (SGD) is one of the simplest and most popular stochastic optimization methods. While it has already been theoretically studied for decades, the classical analysis usually required non-trivial smoothness assumptions, which do not apply to many modern applications of SGD with non-smooth objective functions such as support vector machines.  In this paper, we investigate the performance of SGD \\emphwithout such smoothness assumptions, as well as a running average scheme to convert the SGD iterates to a solution with optimal optimization accuracy. In this framework, we prove that after T rounds, the suboptimality of the \\emphlast SGD iterate scales as O(\\log(T)/\\sqrtT) for non-smooth convex objective functions, and O(\\log(T)/T) in the non-smooth strongly convex case. To the best of our knowledge, these are the first bounds of this kind, and almost match the minimax-optimal rates obtainable by appropriate averaging schemes. We also propose a new and simple averaging scheme, which not only attains optimal rates, but can also be easily computed on-the-fly (in contrast, the suffix averaging scheme proposed in \\citetRakhShaSri12arxiv is not as simple to implement). Finally, we provide some experimental illustrations.",
        "bibtex": "@InProceedings{pmlr-v28-shamir13,\n  title = \t {Stochastic Gradient Descent for Non-smooth Optimization: Convergence Results and Optimal Averaging Schemes},\n  author = \t {Shamir, Ohad and Zhang, Tong},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {71--79},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/shamir13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/shamir13.html},\n  abstract = \t {Stochastic Gradient Descent (SGD) is one of the simplest and most popular stochastic optimization methods. While it has already been theoretically studied for decades, the classical analysis usually required non-trivial smoothness assumptions, which do not apply to many modern applications of SGD with non-smooth objective functions such as support vector machines.  In this paper, we investigate the performance of SGD \\emphwithout such smoothness assumptions, as well as a running average scheme to convert the SGD iterates to a solution with optimal optimization accuracy. In this framework, we prove that after T rounds, the suboptimality of the \\emphlast SGD iterate scales as O(\\log(T)/\\sqrtT) for non-smooth convex objective functions, and O(\\log(T)/T) in the non-smooth strongly convex case. To the best of our knowledge, these are the first bounds of this kind, and almost match the minimax-optimal rates obtainable by appropriate averaging schemes. We also propose a new and simple averaging scheme, which not only attains optimal rates, but can also be easily computed on-the-fly (in contrast, the suffix averaging scheme proposed in \\citetRakhShaSri12arxiv is not as simple to implement). Finally, we provide some experimental illustrations.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/shamir13.pdf",
        "supp": "",
        "pdf_size": 331266,
        "gs_citation": 714,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15518308142955416640&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Microsoft Research, One Microsoft Way, Redmond, WA 98052, USA; Department of Statistics, Rutgers University, Piscataway NJ 08854, USA",
        "aff_domain": "microsoft.com;stat.rutgers.edu",
        "email": "microsoft.com;stat.rutgers.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Microsoft;Rutgers University",
        "aff_unique_dep": "Microsoft Research;Department of Statistics",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://www.rutgers.edu",
        "aff_unique_abbr": "MSR;Rutgers",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Redmond;Piscataway",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6af8df9e8c",
        "title": "Stochastic Simultaneous Optimistic Optimization",
        "site": "https://proceedings.mlr.press/v28/valko13.html",
        "author": "Michal Valko; Alexandra Carpentier; R\u00e9mi Munos",
        "abstract": "We study the problem of global maximization of a function f given a finite number of evaluations perturbed by noise. We consider a very weak assumption on the function, namely that it is locally smooth (in some precise sense) with respect to some semi-metric, around one of its global maxima. Compared to previous works on bandits in general spaces (Kleinberg et al., 2008; Bubeck et al., 2011a) our algorithm does not require the knowledge of this semi-metric. Our algorithm, StoSOO, follows an optimistic strategy to iteratively construct upper confidence bounds over the hierarchical partitions of the function domain to decide which point to sample next. A finite-time analysis of StoSOO shows that it performs almost as well as the best specifically-tuned algorithms even though the local smoothness of the function is not known.",
        "bibtex": "@InProceedings{pmlr-v28-valko13,\n  title = \t {Stochastic Simultaneous Optimistic Optimization},\n  author = \t {Valko, Michal and Carpentier, Alexandra and Munos, R\u00e9mi},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {19--27},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/valko13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/valko13.html},\n  abstract = \t {We study the problem of global maximization of a function f given a finite number of evaluations perturbed by noise. We consider a very weak assumption on the function, namely that it is locally smooth (in some precise sense) with respect to some semi-metric, around one of its global maxima. Compared to previous works on bandits in general spaces (Kleinberg et al., 2008; Bubeck et al., 2011a) our algorithm does not require the knowledge of this semi-metric. Our algorithm, StoSOO, follows an optimistic strategy to iteratively construct upper confidence bounds over the hierarchical partitions of the function domain to decide which point to sample next. A finite-time analysis of StoSOO shows that it performs almost as well as the best specifically-tuned algorithms even though the local smoothness of the function is not known.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/valko13.pdf",
        "supp": "",
        "pdf_size": 812469,
        "gs_citation": 142,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12670743573031663001&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 22,
        "aff": "INRIA Lille - Nord Europe, SequeL team, 40 avenue Halley 59650, Villeneuve d\u2019Ascq, France; Statistical Laboratory, CMS, Wilberforce Road, CB3 0WB, University of Cambridge, United Kingdom; INRIA Lille - Nord Europe, SequeL team, 40 avenue Halley 59650, Villeneuve d\u2019Ascq, France",
        "aff_domain": "inria.fr;statslab.cam.ac.uk;inria.fr",
        "email": "inria.fr;statslab.cam.ac.uk;inria.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "INRIA Lille - Nord Europe;University of Cambridge",
        "aff_unique_dep": "SequeL team;Statistical Laboratory",
        "aff_unique_url": "https://www.inria.fr/lille-nord-europe;https://www.cam.ac.uk",
        "aff_unique_abbr": "INRIA;Cambridge",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Lille;Cambridge",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "France;United Kingdom"
    },
    {
        "id": "962ee9fb7f",
        "title": "Stochastic k-Neighborhood Selection for Supervised and Unsupervised Learning",
        "site": "https://proceedings.mlr.press/v28/tarlow13.html",
        "author": "Daniel Tarlow; Kevin Swersky; Laurent Charlin; Ilya Sutskever; Rich Zemel",
        "abstract": "Neighborhood Components Analysis (NCA) is a popular method for  learning a distance metric to be used within a k-nearest neighbors  (kNN) classifier.    A key assumption built into the model is that each point  stochastically selects a single neighbor, which  makes the model well-justified only for kNN with k=1.  However, kNN classifiers with k>1 are more robust and usually   preferred in practice.     Here we present kNCA, which generalizes NCA by  learning distance metrics that are appropriate for  kNN with arbitrary k.  The main technical contribution is showing  how to efficiently compute and optimize the expected  accuracy of a kNN classifier.  We apply similar ideas in an unsupervised  setting to yield kSNE and ktSNE, generalizations of  Stochastic Neighbor Embedding (SNE, tSNE) that operate on  neighborhoods of size k, which provide an axis of control over  embeddings that allow for more homogeneous and interpretable regions.  Empirically, we show that kNCA often improves classification accuracy over  state of the art methods, produces qualitative  differences in the embeddings as k is varied, and is more robust with  respect to label noise.",
        "bibtex": "@InProceedings{pmlr-v28-tarlow13,\n  title = \t {Stochastic k-Neighborhood Selection for Supervised and Unsupervised Learning},\n  author = \t {Tarlow, Daniel and Swersky, Kevin and Charlin, Laurent and Sutskever, Ilya and Zemel, Rich},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {199--207},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/tarlow13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/tarlow13.html},\n  abstract = \t {Neighborhood Components Analysis (NCA) is a popular method for  learning a distance metric to be used within a k-nearest neighbors  (kNN) classifier.    A key assumption built into the model is that each point  stochastically selects a single neighbor, which  makes the model well-justified only for kNN with k=1.  However, kNN classifiers with k>1 are more robust and usually   preferred in practice.     Here we present kNCA, which generalizes NCA by  learning distance metrics that are appropriate for  kNN with arbitrary k.  The main technical contribution is showing  how to efficiently compute and optimize the expected  accuracy of a kNN classifier.  We apply similar ideas in an unsupervised  setting to yield kSNE and ktSNE, generalizations of  Stochastic Neighbor Embedding (SNE, tSNE) that operate on  neighborhoods of size k, which provide an axis of control over  embeddings that allow for more homogeneous and interpretable regions.  Empirically, we show that kNCA often improves classification accuracy over  state of the art methods, produces qualitative  differences in the embeddings as k is varied, and is more robust with  respect to label noise. }\n}",
        "pdf": "http://proceedings.mlr.press/v28/tarlow13.pdf",
        "supp": "",
        "pdf_size": 2086240,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3161604095948689376&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Microsoft Research Cambridge; University of Toronto; University of Toronto; Google Inc.; University of Toronto",
        "aff_domain": "microsoft.com;cs.toronto.edu;cs.toronto.edu;google.com;cs.toronto.edu",
        "email": "microsoft.com;cs.toronto.edu;cs.toronto.edu;google.com;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;2;1",
        "aff_unique_norm": "Microsoft;University of Toronto;Google",
        "aff_unique_dep": "Microsoft Research;;Google",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/group/microsoft-research-cambridge;https://www.utoronto.ca;https://www.google.com",
        "aff_unique_abbr": "MSR Cambridge;U of T;Google",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Cambridge;;Mountain View",
        "aff_country_unique_index": "0;1;1;2;1",
        "aff_country_unique": "United Kingdom;Canada;United States"
    },
    {
        "id": "33fd1b32dc",
        "title": "Strict Monotonicity of Sum of Squares Error  and Normalized Cut in the Lattice of Clusterings",
        "site": "https://proceedings.mlr.press/v28/rebagliati13.html",
        "author": "Nicola Rebagliati",
        "abstract": "Sum of Squares Error and Normalized Cut are two widely used clustering functional. It is known their minimum values are monotone with respect to the input number of clusters and this monotonicity does not allow for a simple automatic selection of a correct number of clusters. Here we study monotonicity not just on the minimizers but on the entire clustering lattice. We show the value of Sum of Squares Error is strictly monotone under the strict refinement relation of clusterings and we obtain data-dependent bounds on the difference between the value of a clustering and one of its refinements. Using analogous techniques we show the value of Normalized Cut is strictly anti-monotone. These results imply that even if we restrict our solutions to form a chain of clustering, like the one we get from hierarchical algorithms, we cannot rely on the functional values in order to choose the number of clusters. By using these results we get some data-dependent bounds on the difference of the values of any two clusterings.",
        "bibtex": "@InProceedings{pmlr-v28-rebagliati13,\n  title = \t {Strict Monotonicity of Sum of Squares Error  and Normalized Cut in the Lattice of Clusterings},\n  author = \t {Rebagliati, Nicola},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {163--171},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/rebagliati13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/rebagliati13.html},\n  abstract = \t {Sum of Squares Error and Normalized Cut are two widely used clustering functional. It is known their minimum values are monotone with respect to the input number of clusters and this monotonicity does not allow for a simple automatic selection of a correct number of clusters. Here we study monotonicity not just on the minimizers but on the entire clustering lattice. We show the value of Sum of Squares Error is strictly monotone under the strict refinement relation of clusterings and we obtain data-dependent bounds on the difference between the value of a clustering and one of its refinements. Using analogous techniques we show the value of Normalized Cut is strictly anti-monotone. These results imply that even if we restrict our solutions to form a chain of clustering, like the one we get from hierarchical algorithms, we cannot rely on the functional values in order to choose the number of clusters. By using these results we get some data-dependent bounds on the difference of the values of any two clusterings.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/rebagliati13.pdf",
        "supp": "",
        "pdf_size": 353116,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16640084296883822180&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "VTT Technical Research Centre of Finland",
        "aff_domain": "gmail.com",
        "email": "gmail.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "VTT Technical Research Centre of Finland",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.vtt.fi",
        "aff_unique_abbr": "VTT",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Finland"
    },
    {
        "id": "0096e36671",
        "title": "Structure Discovery in Nonparametric Regression through Compositional Kernel Search",
        "site": "https://proceedings.mlr.press/v28/duvenaud13.html",
        "author": "David Duvenaud; James Lloyd; Roger Grosse; Joshua Tenenbaum; Ghahramani Zoubin",
        "abstract": "Despite its importance, choosing the structural form of the kernel in nonparametric regression remains a black art. We define a space of kernel structures which are built compositionally by adding and multiplying a small number of base kernels. We present a method for searching over this space of structures which mirrors the scientific discovery process. The learned structures can often decompose functions into interpretable components and enable long-range extrapolation on time-series datasets. Our structure search method outperforms many widely used kernels and kernel combination methods on a variety of prediction tasks.",
        "bibtex": "@InProceedings{pmlr-v28-duvenaud13,\n  title = \t {Structure Discovery in Nonparametric Regression through Compositional Kernel Search},\n  author = \t {Duvenaud, David and Lloyd, James and Grosse, Roger and Tenenbaum, Joshua and Zoubin, Ghahramani},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1166--1174},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/duvenaud13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/duvenaud13.html},\n  abstract = \t {Despite its importance, choosing the structural form of the kernel in nonparametric regression remains a black art. We define a space of kernel structures which are built compositionally by adding and multiplying a small number of base kernels. We present a method for searching over this space of structures which mirrors the scientific discovery process. The learned structures can often decompose functions into interpretable components and enable long-range extrapolation on time-series datasets. Our structure search method outperforms many widely used kernels and kernel combination methods on a variety of prediction tasks.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/duvenaud13.pdf",
        "supp": "",
        "pdf_size": 1475273,
        "gs_citation": 678,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15167247395625717679&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "U. Cambridge\u2020; U. Cambridge\u2020; MIT\u2021; MIT\u2021; U. Cambridge\u2020",
        "aff_domain": "cam.ac.uk;cam.ac.uk;mit.edu;mit.edu;eng.cam.ac.uk",
        "email": "cam.ac.uk;cam.ac.uk;mit.edu;mit.edu;eng.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;1;0",
        "aff_unique_norm": "University of Cambridge;Massachusetts Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cam.ac.uk;https://web.mit.edu",
        "aff_unique_abbr": "Cambridge;MIT",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0;1;1;0",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "4da7c60a15",
        "title": "Subproblem-Tree Calibration: A Unified Approach to Max-Product Message Passing",
        "site": "https://proceedings.mlr.press/v28/wang13b.html",
        "author": "Huayan Wang; Koller Daphne",
        "abstract": "Max-product (max-sum) message passing algorithms are widely used for MAP inference in MRFs. It has many variants sharing a common flavor of passing \"messages\" over some graph-object. Recent advances revealed that its convergent versions (such as MPLP, MSD, TRW-S) can be viewed as performing block coordinate descent (BCD) in a dual objective. That is, each BCD step achieves dual-optimal w.r.t. a block of dual variables (messages), thereby decreases the dual objective monotonically. However, most existing algorithms are limited to updating blocks selected in rather restricted ways. In this paper, we show a \"unified\" message passing algorithm that: (a) subsumes MPLP, MSD, and TRW-S as special cases when applied to their respective choices of dual objective and blocks, and (b) is able to perform BCD under much more flexible choices of blocks (including very large blocks) as well as the dual objective itself (that arise from an arbitrary dual decomposition).",
        "bibtex": "@InProceedings{pmlr-v28-wang13b,\n  title = \t {Subproblem-Tree Calibration: A Unified Approach to Max-Product Message Passing},\n  author = \t {Wang, Huayan and Daphne, Koller},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {190--198},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/wang13b.pdf},\n  url = \t {https://proceedings.mlr.press/v28/wang13b.html},\n  abstract = \t {Max-product (max-sum) message passing algorithms are widely used for MAP inference in MRFs. It has many variants sharing a common flavor of passing \"messages\" over some graph-object. Recent advances revealed that its convergent versions (such as MPLP, MSD, TRW-S) can be viewed as performing block coordinate descent (BCD) in a dual objective. That is, each BCD step achieves dual-optimal w.r.t. a block of dual variables (messages), thereby decreases the dual objective monotonically. However, most existing algorithms are limited to updating blocks selected in rather restricted ways. In this paper, we show a \"unified\" message passing algorithm that: (a) subsumes MPLP, MSD, and TRW-S as special cases when applied to their respective choices of dual objective and blocks, and (b) is able to perform BCD under much more flexible choices of blocks (including very large blocks) as well as the dual objective itself (that arise from an arbitrary dual decomposition).}\n}",
        "pdf": "http://proceedings.mlr.press/v28/wang13b.pdf",
        "supp": "",
        "pdf_size": 569588,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10636872308137436569&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Computer Science Department, Stanford University, Palo Alto, CA 94305 USA; Computer Science Department, Stanford University, Palo Alto, CA 94305 USA",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Palo Alto",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e737b9466c",
        "title": "Subtle Topic Models and Discovering Subtly Manifested Software Concerns Automatically",
        "site": "https://proceedings.mlr.press/v28/das13.html",
        "author": "Mrinal Das; Suparna Bhattacharya; Chiranjib Bhattacharyya; Gopinath Kanchi",
        "abstract": "In a recent pioneering approach LDA was used to discover cross cutting concerns(CCC) automatically from software codebases. LDA though successful in detecting prominent concerns, fails to detect many useful CCCs including ones that may be heavily executed but elude discovery because they do not have a strong prevalence in source-code. We pose this problem as that of discovering topics that rarely occur in individual documents, which we will refer to as subtle topics. Recently an interesting approach, namely focused topic models(FTM) was proposed for detecting rare topics. FTM, though successful in detecting topics which occur prominently in very few documents, is unable to detect subtle topics. Discovering subtle topics thus remains an important open problem. To address this issue we propose subtle topic models(STM). STM uses a generalized stick breaking process(GSBP) as a prior for defining multiple distributions over topics. This hierarchical structure on topics allows STM to discover rare topics beyond the capabilities of FTM. The associated inference is non-standard and is solved by exploiting the relationship between GSBP and generalized Dirichlet distribution. Empirical results show that STM is able to discover subtle CCC in two benchmark code-bases, a feat which is beyond the scope of existing topic models, thus demonstrating the potential of the model in automated concern discovery, a known difficult problem in Software Engineering. Furthermore it is observed that even in general text corpora STM outperforms the state of art in discovering subtle topics.",
        "bibtex": "@InProceedings{pmlr-v28-das13,\n  title = \t {Subtle Topic Models and Discovering Subtly Manifested Software Concerns Automatically},\n  author = \t {Das, Mrinal and Bhattacharya, Suparna and Bhattacharyya, Chiranjib and Kanchi, Gopinath},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {253--261},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/das13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/das13.html},\n  abstract = \t {In a recent pioneering approach LDA was used to discover cross cutting concerns(CCC) automatically from software codebases. LDA though successful in detecting prominent concerns, fails to detect many useful CCCs including ones that may be heavily executed but elude discovery because they do not have a strong prevalence in source-code. We pose this problem as that of discovering topics that rarely occur in individual documents, which we will refer to as subtle topics. Recently an interesting approach, namely focused topic models(FTM) was proposed for detecting rare topics. FTM, though successful in detecting topics which occur prominently in very few documents, is unable to detect subtle topics. Discovering subtle topics thus remains an important open problem. To address this issue we propose subtle topic models(STM). STM uses a generalized stick breaking process(GSBP) as a prior for defining multiple distributions over topics. This hierarchical structure on topics allows STM to discover rare topics beyond the capabilities of FTM. The associated inference is non-standard and is solved by exploiting the relationship between GSBP and generalized Dirichlet distribution. Empirical results show that STM is able to discover subtle CCC in two benchmark code-bases, a feat which is beyond the scope of existing topic models, thus demonstrating the potential of the model in automated concern discovery, a known difficult problem in Software Engineering. Furthermore it is observed that even in general text corpora STM outperforms the state of art in discovering subtle topics.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/das13.pdf",
        "supp": "",
        "pdf_size": 208015,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15887306777887791862&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India; IBM Research-India + Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India; Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India; Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India",
        "aff_domain": "csa.iisc.ernet.in;in.ibm.com;csa.iisc.ernet.in;csa.iisc.ernet.in",
        "email": "csa.iisc.ernet.in;in.ibm.com;csa.iisc.ernet.in;csa.iisc.ernet.in",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;0;0",
        "aff_unique_norm": "Indian Institute of Science;IBM",
        "aff_unique_dep": "Department of Computer Science and Automation;IBM Research",
        "aff_unique_url": "https://www.iisc.ac.in;https://www.ibm.com/research/in",
        "aff_unique_abbr": "IISc;IBM Research-India",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Bangalore;",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "5d0b447022",
        "title": "Taming the Curse of Dimensionality: Discrete Integration by Hashing and Optimization",
        "site": "https://proceedings.mlr.press/v28/ermon13.html",
        "author": "Stefano Ermon; Carla Gomes; Ashish Sabharwal; Bart Selman",
        "abstract": "Integration is affected by the curse of dimensionality and quickly becomes intractable as the dimensionality of the problem grows. We propose a randomized algorithm that, with high probability, gives a constant-factor approximation of a general discrete integral defined over an exponentially large set. This algorithm relies on solving only a small number of instances of a discrete combinatorial optimization problem subject to randomly generated parity constraints used as a hash function. As an application, we demonstrate that with a small number of MAP queries we can efficiently approximate the partition function of discrete graphical models, which can in turn be used, for instance, for marginal computation or model selection.",
        "bibtex": "@InProceedings{pmlr-v28-ermon13,\n  title = \t {Taming the Curse of Dimensionality: Discrete Integration by Hashing and Optimization},\n  author = \t {Ermon, Stefano and Gomes, Carla and Sabharwal, Ashish and Selman, Bart},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {334--342},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/ermon13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/ermon13.html},\n  abstract = \t {Integration is affected by the curse of dimensionality and quickly becomes intractable as the dimensionality of the problem grows. We propose a randomized algorithm that, with high probability, gives a constant-factor approximation of a general discrete integral defined over an exponentially large set. This algorithm relies on solving only a small number of instances of a discrete combinatorial optimization problem subject to randomly generated parity constraints used as a hash function. As an application, we demonstrate that with a small number of MAP queries we can efficiently approximate the partition function of discrete graphical models, which can in turn be used, for instance, for marginal computation or model selection.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/ermon13.pdf",
        "supp": "",
        "pdf_size": 483442,
        "gs_citation": 158,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15904972283351517607&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Dept. of Computer Science, Cornell University, Ithaca NY 14853, U.S.A.; Dept. of Computer Science, Cornell University, Ithaca NY 14853, U.S.A.; IBM Watson Research Center, Yorktown Heights, NY 10598, U.S.A.; Dept. of Computer Science, Cornell University, Ithaca NY 14853, U.S.A.",
        "aff_domain": "cs.cornell.edu;cs.cornell.edu;us.ibm.com;cs.cornell.edu",
        "email": "cs.cornell.edu;cs.cornell.edu;us.ibm.com;cs.cornell.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Cornell University;IBM",
        "aff_unique_dep": "Department of Computer Science;IBM Watson Research Center",
        "aff_unique_url": "https://www.cornell.edu;https://www.ibm.com/watson",
        "aff_unique_abbr": "Cornell;IBM Watson",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Ithaca;Yorktown Heights",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "fbf9fd11d4",
        "title": "Temporal Difference Methods for the Variance of the Reward To Go",
        "site": "https://proceedings.mlr.press/v28/tamar13.html",
        "author": "Aviv Tamar; Dotan Di Castro; Shie Mannor",
        "abstract": "In this paper we extend temporal difference policy evaluation algorithms to performance criteria that include the variance of the cumulative reward. Such criteria are useful for risk management, and are important in domains such as finance and process control. We propose variants of both TD(0) and LSTD(\u03bb) with linear function approximation, prove their convergence, and demonstrate their utility in a 4-dimensional continuous state space problem.",
        "bibtex": "@InProceedings{pmlr-v28-tamar13,\n  title = \t {Temporal Difference Methods for the Variance of the Reward To Go},\n  author = \t {Tamar, Aviv and Di Castro, Dotan and Mannor, Shie},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {495--503},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/tamar13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/tamar13.html},\n  abstract = \t {In this paper we extend temporal difference policy evaluation algorithms to performance criteria that include the variance of the cumulative reward. Such criteria are useful for risk management, and are important in domains such as finance and process control. We propose variants of both TD(0) and LSTD(\u03bb) with linear function approximation, prove their convergence, and demonstrate their utility in a 4-dimensional continuous state space problem.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/tamar13.pdf",
        "supp": "",
        "pdf_size": 500936,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16281031226557318032&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Electrical Engineering, The Technion - Israel Institute of Technology, Haifa, Israel 32000; Department of Electrical Engineering, The Technion - Israel Institute of Technology, Haifa, Israel 32000; Department of Electrical Engineering, The Technion - Israel Institute of Technology, Haifa, Israel 32000",
        "aff_domain": "tx.technion.ac.il;tx.technion.ac.il;ee.technion.ac.il",
        "email": "tx.technion.ac.il;tx.technion.ac.il;ee.technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Technion - Israel Institute of Technology",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.technion.ac.il",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Haifa",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "2b0e08aa72",
        "title": "Tensor Analyzers",
        "site": "https://proceedings.mlr.press/v28/tang13.html",
        "author": "Yichuan Tang; Ruslan Salakhutdinov; Geoffrey Hinton",
        "abstract": "Factor Analysis is a statistical method that seeks to explain linear variations in data by using unobserved latent variables. Due to its additive nature, it is not suitable for modeling data that is generated by multiple groups of latent factors which interact multiplicatively. In this paper, we introduce Tensor Analyzers which are a multilinear generalization of Factor Analyzers. We describe an efficient way of sampling from the posterior distribution over factor values and we demonstrate that these samples can be used in the EM algorithm for learning interesting mixture models of natural image patches. Tensor Analyzers can also accurately recognize a face under significant pose and illumination variations when given only one previous image of that face. We also show that Tensor Analyzers can be trained in an unsupervised, semi-supervised, or fully supervised settings.",
        "bibtex": "@InProceedings{pmlr-v28-tang13,\n  title = \t {Tensor Analyzers},\n  author = \t {Tang, Yichuan and Salakhutdinov, Ruslan and Hinton, Geoffrey},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {163--171},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/tang13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/tang13.html},\n  abstract = \t {Factor Analysis is a statistical method that seeks to explain linear variations in data by using unobserved latent variables. Due to its additive nature, it is not suitable for modeling data that is generated by multiple groups of latent factors which interact multiplicatively. In this paper, we introduce Tensor Analyzers which are a multilinear generalization of Factor Analyzers. We describe an efficient way of sampling from the posterior distribution over factor values and we demonstrate that these samples can be used in the EM algorithm for learning interesting mixture models of natural image patches. Tensor Analyzers can also accurately recognize a face under significant pose and illumination variations when given only one previous image of that face. We also show that Tensor Analyzers can be trained in an unsupervised, semi-supervised, or fully supervised settings.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/tang13.pdf",
        "supp": "",
        "pdf_size": 2095036,
        "gs_citation": 77,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2088491428224991733&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Computer Science, University of Toronto. Toronto, Ontario, Canada; Department of Computer Science, University of Toronto. Toronto, Ontario, Canada; Department of Computer Science, University of Toronto. Toronto, Ontario, Canada",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "53398ecf3c",
        "title": "That was fast! Speeding up NN search of high dimensional distributions.",
        "site": "https://proceedings.mlr.press/v28/coviello13.html",
        "author": "Emanuele Coviello; Adeel Mumtaz; Antoni Chan; Gert Lanckriet",
        "abstract": "We present a data structure for fast nearest neighbor retrieval of generative models of documents based on KL divergence.  Our data structure, which shares some similarity with Bregman Ball Trees, consists of a hierarchical partition of a database,   and uses a novel branch and bound methodology for search.  The main technical contribution of the paper is a   novel and efficient algorithm  for deciding whether to explore nodes during backtracking, based on a variational approximation.  This reduces the number of computations per node, and overcomes the limitations of Bregman Ball Trees on high dimensional data.  In addition, our strategy is applicable also to probability distributions with hidden state variables, and is not limited to regular exponential family distributions.    Experiments demonstrate substantial speed-ups over both Bregman  Ball Trees and over brute force search, on both moderate and high dimensional histogram data.  In addition, experiments on linear dynamical systems demonstrate the flexibility of our approach to latent variable models.",
        "bibtex": "@InProceedings{pmlr-v28-coviello13,\n  title = \t {That was fast! Speeding up NN search of high dimensional distributions.},\n  author = \t {Coviello, Emanuele and Mumtaz, Adeel and Chan, Antoni and Lanckriet, Gert},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {468--476},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/coviello13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/coviello13.html},\n  abstract = \t {We present a data structure for fast nearest neighbor retrieval of generative models of documents based on KL divergence.  Our data structure, which shares some similarity with Bregman Ball Trees, consists of a hierarchical partition of a database,   and uses a novel branch and bound methodology for search.  The main technical contribution of the paper is a   novel and efficient algorithm  for deciding whether to explore nodes during backtracking, based on a variational approximation.  This reduces the number of computations per node, and overcomes the limitations of Bregman Ball Trees on high dimensional data.  In addition, our strategy is applicable also to probability distributions with hidden state variables, and is not limited to regular exponential family distributions.    Experiments demonstrate substantial speed-ups over both Bregman  Ball Trees and over brute force search, on both moderate and high dimensional histogram data.  In addition, experiments on linear dynamical systems demonstrate the flexibility of our approach to latent variable models.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/coviello13.pdf",
        "supp": "",
        "pdf_size": 427526,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6850393626715220015&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "University of California, San Diego; City University of Hong Kong; City University of Hong Kong; University of California, San Diego",
        "aff_domain": "ucsd.edu;gmail.com;cityu.edu.hk;ece.ucsd.edu",
        "email": "ucsd.edu;gmail.com;cityu.edu.hk;ece.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "University of California, San Diego;City University of Hong Kong",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucsd.edu;https://www.cityu.edu.hk",
        "aff_unique_abbr": "UCSD;CityU",
        "aff_campus_unique_index": "0;1;1;0",
        "aff_campus_unique": "San Diego;Hong Kong SAR",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "e0520db640",
        "title": "The Bigraphical Lasso",
        "site": "https://proceedings.mlr.press/v28/kalaitzis13.html",
        "author": "Alfredo Kalaitzis; John Lafferty; Neil D. Lawrence; Shuheng Zhou",
        "abstract": "The i.i.d. assumption in machine learning is endemic, but often flawed. Complex data sets exhibit partial correlations between both instances and features. A model specifying both types of correlation can have a number of parameters that scales quadratically with the number of features and data points. We introduce the bigraphical lasso, an estimator for precision matrices of matrix-normals based on the Cartesian product of graphs. A prominent product in spectral graph theory, this structure has appealing properties for regression, enhanced sparsity and interpretability. To deal with the parameter explosion we introduce L1 penalties and fit the model through a flip-flop algorithm that results in a linear number of lasso regressions.",
        "bibtex": "@InProceedings{pmlr-v28-kalaitzis13,\n  title = \t {The Bigraphical Lasso},\n  author = \t {Kalaitzis, Alfredo and Lafferty, John and Lawrence, Neil D. and Zhou, Shuheng},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1229--1237},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/kalaitzis13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/kalaitzis13.html},\n  abstract = \t {The i.i.d. assumption in machine learning is endemic, but often flawed. Complex data sets exhibit partial correlations between both instances and features. A model specifying both types of correlation can have a number of parameters that scales quadratically with the number of features and data points. We introduce the bigraphical lasso, an estimator for precision matrices of matrix-normals based on the Cartesian product of graphs. A prominent product in spectral graph theory, this structure has appealing properties for regression, enhanced sparsity and interpretability. To deal with the parameter explosion we introduce L1 penalties and fit the model through a flip-flop algorithm that results in a linear number of lasso regressions.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/kalaitzis13.pdf",
        "supp": "",
        "pdf_size": 1886609,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13945771376897475023&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Statistical Science, University College London; Department of Statistics, University of Chicago; Department of Computer Science, University of She\ufb03eld; Department of Statistics, University of Michigan",
        "aff_domain": "ucl.ac.uk;galton.uchicago.edu;sheffield.ac.uk;umich.edu",
        "email": "ucl.ac.uk;galton.uchicago.edu;sheffield.ac.uk;umich.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "University College London;University of Chicago;University of Sheffield;University of Michigan",
        "aff_unique_dep": "Department of Statistical Science;Department of Statistics;Department of Computer Science;Department of Statistics",
        "aff_unique_url": "https://www.ucl.ac.uk;https://www.uchicago.edu;https://www.sheffield.ac.uk;https://www.umich.edu",
        "aff_unique_abbr": "UCL;UChicago;Sheffield;UM",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "London;;Ann Arbor",
        "aff_country_unique_index": "0;1;0;1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "2a1cd70f90",
        "title": "The Cross-Entropy Method Optimizes for Quantiles",
        "site": "https://proceedings.mlr.press/v28/goschin13.html",
        "author": "Sergiu Goschin; Ari Weinstein; Michael Littman",
        "abstract": "Cross-entropy optimization (CE) has proven to be a powerful tool for search in control environments. In the basic scheme, a distribution over proposed solutions is repeatedly adapted by evaluating a sample of solutions and refocusing the distribution on a percentage of those with the highest scores.  We show that, in the kind of noisy evaluation environments that are common in decision-making domains, this percentage-based refocusing does not optimize the expected utility of solutions, but instead a quantile metric. We provide a variant of CE (Proportional CE) that effectively optimizes the expected value. We show using variants of established noisy environments that Proportional CE can be used in place of CE and can improve solution quality.",
        "bibtex": "@InProceedings{pmlr-v28-goschin13,\n  title = \t {The Cross-Entropy Method Optimizes for Quantiles},\n  author = \t {Goschin, Sergiu and Weinstein, Ari and Littman, Michael},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1193--1201},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/goschin13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/goschin13.html},\n  abstract = \t {Cross-entropy optimization (CE) has proven to be a powerful tool for search in control environments. In the basic scheme, a distribution over proposed solutions is repeatedly adapted by evaluating a sample of solutions and refocusing the distribution on a percentage of those with the highest scores.  We show that, in the kind of noisy evaluation environments that are common in decision-making domains, this percentage-based refocusing does not optimize the expected utility of solutions, but instead a quantile metric. We provide a variant of CE (Proportional CE) that effectively optimizes the expected value. We show using variants of established noisy environments that Proportional CE can be used in place of CE and can improve solution quality.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/goschin13.pdf",
        "supp": "",
        "pdf_size": 804956,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1888593015014666361&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Rutgers University, Piscataway, NJ 08854 USA; Rutgers University, Piscataway, NJ 08854 USA; Brown University, Providence, RI 02912 USA",
        "aff_domain": "cs.rutgers.edu;cs.rutgers.edu;cs.brown.edu",
        "email": "cs.rutgers.edu;cs.rutgers.edu;cs.brown.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Rutgers University;Brown University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.rutgers.edu;https://www.brown.edu",
        "aff_unique_abbr": "Rutgers;Brown",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Piscataway;Providence",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f0747cd63c",
        "title": "The Extended Parameter Filter",
        "site": "https://proceedings.mlr.press/v28/bugraerol13.html",
        "author": "Yusuf Bugra Erol; Lei Li; Bharath Ramsundar; Russell Stuart",
        "abstract": "The parameters of temporal models, such as dynamic Bayesian networks, may be modelled in a Bayesian context as static or atemporal variables that influence transition probabilities at every time step. Particle filters fail for models that include such variables, while methods that use Gibbs sampling of parameter variables may incur a per-sample cost that grows linearly with the length of the observation sequence. Storvik devised a method for incremental computation of exact sufficient statistics that, for some cases, reduces the per-sample cost to a constant.  In this paper, we demonstrate a connection between Storvik\u2019s filter and a Kalman filter in parameter space and establish more general conditions under which Storvik\u2019s filter works. Drawing on an analogy to the extended Kalman filter, we develop and analyze, both theoretically and experimentally, a Taylor approximation to the parameter posterior that allows Storvik\u2019s method to be applied to a broader class of models. Our experiments on both synthetic examples and real applications show improvement over existing methods.",
        "bibtex": "@InProceedings{pmlr-v28-bugraerol13,\n  title = \t {The Extended Parameter Filter},\n  author = \t {Bugra Erol, Yusuf and Li, Lei and Ramsundar, Bharath and Stuart, Russell},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1103--1111},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/bugraerol13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/bugraerol13.html},\n  abstract = \t {The parameters of temporal models, such as dynamic Bayesian networks, may be modelled in a Bayesian context as static or atemporal variables that influence transition probabilities at every time step. Particle filters fail for models that include such variables, while methods that use Gibbs sampling of parameter variables may incur a per-sample cost that grows linearly with the length of the observation sequence. Storvik devised a method for incremental computation of exact sufficient statistics that, for some cases, reduces the per-sample cost to a constant.  In this paper, we demonstrate a connection between Storvik\u2019s filter and a Kalman filter in parameter space and establish more general conditions under which Storvik\u2019s filter works. Drawing on an analogy to the extended Kalman filter, we develop and analyze, both theoretically and experimentally, a Taylor approximation to the parameter posterior that allows Storvik\u2019s method to be applied to a broader class of models. Our experiments on both synthetic examples and real applications show improvement over existing methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/bugraerol13.pdf",
        "supp": "",
        "pdf_size": 1068486,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15855549786585957662&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 25,
        "aff": "EECS Department, University of California, Berkeley; EECS Department, University of California, Berkeley; Computer Science Department, Stanford University; EECS Department, University of California, Berkeley",
        "aff_domain": "eecs.berkeley.edu;cs.berkeley.edu;stanford.edu;cs.berkeley.edu",
        "email": "eecs.berkeley.edu;cs.berkeley.edu;stanford.edu;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of California, Berkeley;Stanford University",
        "aff_unique_dep": "EECS Department;Computer Science Department",
        "aff_unique_url": "https://www.berkeley.edu;https://www.stanford.edu",
        "aff_unique_abbr": "UC Berkeley;Stanford",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Berkeley;Stanford",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "728070c475",
        "title": "The Most Generative Maximum Margin Bayesian Networks",
        "site": "https://proceedings.mlr.press/v28/peharz13.html",
        "author": "Robert Peharz; Sebastian Tschiatschek; Franz Pernkopf",
        "abstract": "Although discriminative learning in graphical models generally improves classification results, the generative semantics of the model are compromised.  In this paper, we introduce a novel approach of hybrid generative-discriminative learning for Bayesian networks.  We use an SVM-type large margin formulation for discriminative training, introducing a likelihood-weighted \\ell^1-norm for the SVM-norm-penalization.  This simultaneously optimizes the data likelihood and therefore partly maintains the generative character of the model.  For many network structures, our method can be formulated as a convex problem, guaranteeing a globally optimal solution.  In terms of classification, the resulting models outperform state-of-the art generative and discriminative learning methods for Bayesian networks, and are comparable with linear and kernelized SVMs.  Furthermore, the models achieve likelihoods close to the maximum likelihood solution and show robust behavior in classification experiments with missing features.",
        "bibtex": "@InProceedings{pmlr-v28-peharz13,\n  title = \t {The Most Generative Maximum Margin Bayesian Networks},\n  author = \t {Peharz, Robert and Tschiatschek, Sebastian and Pernkopf, Franz},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {235--243},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/peharz13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/peharz13.html},\n  abstract = \t {Although discriminative learning in graphical models generally improves classification results, the generative semantics of the model are compromised.  In this paper, we introduce a novel approach of hybrid generative-discriminative learning for Bayesian networks.  We use an SVM-type large margin formulation for discriminative training, introducing a likelihood-weighted \\ell^1-norm for the SVM-norm-penalization.  This simultaneously optimizes the data likelihood and therefore partly maintains the generative character of the model.  For many network structures, our method can be formulated as a convex problem, guaranteeing a globally optimal solution.  In terms of classification, the resulting models outperform state-of-the art generative and discriminative learning methods for Bayesian networks, and are comparable with linear and kernelized SVMs.  Furthermore, the models achieve likelihoods close to the maximum likelihood solution and show robust behavior in classification experiments with missing features.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/peharz13.pdf",
        "supp": "",
        "pdf_size": 288374,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1174284432349150734&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "Signal Processing and Speech Communication Laboratory, Graz University of Technology; Signal Processing and Speech Communication Laboratory, Graz University of Technology; Signal Processing and Speech Communication Laboratory, Graz University of Technology",
        "aff_domain": "tugraz.at;tugraz.at;tugraz.at",
        "email": "tugraz.at;tugraz.at;tugraz.at",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Graz University of Technology",
        "aff_unique_dep": "Signal Processing and Speech Communication Laboratory",
        "aff_unique_url": "https://www.tugraz.at",
        "aff_unique_abbr": "TUGraz",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Graz",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Austria"
    },
    {
        "id": "61c9319fef",
        "title": "The Pairwise Piecewise-Linear Embedding for Efficient Non-Linear Classification",
        "site": "https://proceedings.mlr.press/v28/pele13.html",
        "author": "Ofir Pele; Ben Taskar; Amir Globerson; Michael Werman",
        "abstract": "Linear classiffers are much faster to learn and test than non-linear ones. On the other hand, non-linear kernels offer improved performance, albeit at the increased cost of training kernel classiffers. To use non-linear mappings with efficient linear learning algorithms, explicit embeddings that approximate popular kernels have recently been proposed. However, the embedding process itself is often costly and the results are usually less accurate than kernel methods. In this work we propose a non-linear feature map that is both very efficient, but at the same time highly expressive. The method is based on discretization and interpolation of individual features values and feature pairs. The discretization allows us to model different regions of the feature space separately, while the interpolation preserves the original continuous values. Using this embedding is strictly more general than a linear model and as efficient as the second-order polynomial explicit feature map. An extensive empirical evaluation shows that our method consistently signiffcantly outperforms other methods, including a wide range of kernels. This is in contrast to other proposed embeddings that were faster than kernel methods, but with lower accuracy.",
        "bibtex": "@InProceedings{pmlr-v28-pele13,\n  title = \t {The Pairwise Piecewise-Linear Embedding for Efficient Non-Linear Classification},\n  author = \t {Pele, Ofir and Taskar, Ben and Globerson, Amir and Werman, Michael},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {205--213},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/pele13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/pele13.html},\n  abstract = \t {Linear classiffers are much faster to learn and test than non-linear ones. On the other hand, non-linear kernels offer improved performance, albeit at the increased cost of training kernel classiffers. To use non-linear mappings with efficient linear learning algorithms, explicit embeddings that approximate popular kernels have recently been proposed. However, the embedding process itself is often costly and the results are usually less accurate than kernel methods. In this work we propose a non-linear feature map that is both very efficient, but at the same time highly expressive. The method is based on discretization and interpolation of individual features values and feature pairs. The discretization allows us to model different regions of the feature space separately, while the interpolation preserves the original continuous values. Using this embedding is strictly more general than a linear model and as efficient as the second-order polynomial explicit feature map. An extensive empirical evaluation shows that our method consistently signiffcantly outperforms other methods, including a wide range of kernels. This is in contrast to other proposed embeddings that were faster than kernel methods, but with lower accuracy.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/pele13.pdf",
        "supp": "",
        "pdf_size": 697205,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2714411979497329127&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "University of Pennsylvania, Department of Computer and Information Science, Philadelphia, PA 19104 USA; University of Pennsylvania, Department of Computer and Information Science, Philadelphia, PA 19104 USA; The Hebrew University of Jerusalem, School of Computer Science, Jerusalem 91904 Israel; The Hebrew University of Jerusalem, School of Computer Science, Jerusalem 91904 Israel",
        "aff_domain": "cis.upenn.edu;cis.upenn.edu;cs.huji.ac.il;cs.huji.ac.il",
        "email": "cis.upenn.edu;cis.upenn.edu;cs.huji.ac.il;cs.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;1",
        "aff_unique_norm": "University of Pennsylvania;Hebrew University of Jerusalem",
        "aff_unique_dep": "Department of Computer and Information Science;School of Computer Science",
        "aff_unique_url": "https://www.upenn.edu;http://www.huji.ac.il",
        "aff_unique_abbr": "UPenn;HUJI",
        "aff_campus_unique_index": "0;0;1;1",
        "aff_campus_unique": "Philadelphia;Jerusalem",
        "aff_country_unique_index": "0;0;1;1",
        "aff_country_unique": "United States;Israel"
    },
    {
        "id": "3f4eb50e0a",
        "title": "The Sample-Complexity of General Reinforcement Learning",
        "site": "https://proceedings.mlr.press/v28/lattimore13.html",
        "author": "Tor Lattimore; Marcus Hutter; Peter Sunehag",
        "abstract": "We study the sample-complexity of reinforcement learning in a general setting without  assuming ergodicity or finiteness of the environment. Instead, we define a topology  on the space of environments and show that  if an environment class is compact with respect to this topology then finite sample-complexity bounds are possible and give an  algorithm achieving these bounds. We also  show the existence of environment classes  that are non-compact where finite sample-complexity bounds are not achievable. A  lower bound is presented that matches the  upper bound except for logarithmic factors.",
        "bibtex": "@InProceedings{pmlr-v28-lattimore13,\n  title = \t {The Sample-Complexity of General Reinforcement Learning},\n  author = \t {Lattimore, Tor and Hutter, Marcus and Sunehag, Peter},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {28--36},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/lattimore13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/lattimore13.html},\n  abstract = \t {We study the sample-complexity of reinforcement learning in a general setting without  assuming ergodicity or finiteness of the environment. Instead, we define a topology  on the space of environments and show that  if an environment class is compact with respect to this topology then finite sample-complexity bounds are possible and give an  algorithm achieving these bounds. We also  show the existence of environment classes  that are non-compact where finite sample-complexity bounds are not achievable. A  lower bound is presented that matches the  upper bound except for logarithmic factors.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/lattimore13.pdf",
        "supp": "",
        "pdf_size": 403008,
        "gs_citation": 79,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15988533019306116129&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Australian National University; Australian National University; Australian National University",
        "aff_domain": "anu.edu.au;anu.edu.au;anu.edu.au",
        "email": "anu.edu.au;anu.edu.au;anu.edu.au",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Australian National University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.anu.edu.au",
        "aff_unique_abbr": "ANU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "a5618b9762",
        "title": "The lasso, persistence, and cross-validation",
        "site": "https://proceedings.mlr.press/v28/homrighausen13.html",
        "author": "Darren Homrighausen; Daniel McDonald",
        "abstract": "During the last fifteen years, the lasso procedure has been the target of a substantial amount of theoretical and applied research. Correspondingly, many results are known about its behavior for a fixed or optimally chosen smoothing parameter (given up to unknown constants). Much less, however, is known about the lasso\u2019s behavior when the smoothing parameter is chosen in a data dependent way. To this end, we give the first result about the risk consistency of lasso when the smoothing parameter is chosen via cross-validation. We consider the high-dimensional setting wherein the number of predictors p=n^\u03b1, \u03b1>0 grows with the number of observations.",
        "bibtex": "@InProceedings{pmlr-v28-homrighausen13,\n  title = \t {The lasso, persistence, and cross-validation},\n  author = \t {Homrighausen, Darren and McDonald, Daniel},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1031--1039},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/homrighausen13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/homrighausen13.html},\n  abstract = \t {During the last fifteen years, the lasso procedure has been the target of a substantial amount of theoretical and applied research. Correspondingly, many results are known about its behavior for a fixed or optimally chosen smoothing parameter (given up to unknown constants). Much less, however, is known about the lasso\u2019s behavior when the smoothing parameter is chosen in a data dependent way. To this end, we give the first result about the risk consistency of lasso when the smoothing parameter is chosen via cross-validation. We consider the high-dimensional setting wherein the number of predictors p=n^\u03b1, \u03b1>0 grows with the number of observations.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/homrighausen13.pdf",
        "supp": "",
        "pdf_size": 352825,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8695301473000705077&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Statistics, Colorado State University, Fort Collins, CO 80523; Department of Statistics, Indiana University, Bloomington, IN 47408",
        "aff_domain": "stat.colostate.edu;indiana.edu",
        "email": "stat.colostate.edu;indiana.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Colorado State University;Indiana University",
        "aff_unique_dep": "Department of Statistics;Department of Statistics",
        "aff_unique_url": "https://www.colostate.edu;https://www.indiana.edu",
        "aff_unique_abbr": "CSU;IU",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Fort Collins;Bloomington",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "62333039dd",
        "title": "Thompson Sampling for Contextual Bandits with Linear Payoffs",
        "site": "https://proceedings.mlr.press/v28/agrawal13.html",
        "author": "Shipra Agrawal; Navin Goyal",
        "abstract": "Thompson Sampling is one of the oldest heuristics for multi-armed bandit problems. It is a randomized algorithm based on Bayesian ideas, and has recently generated significant interest after several studies demonstrated it to have better empirical performance compared to the state of the art methods. However, many questions regarding its theoretical performance remained open. In this paper, we design and analyze Thompson Sampling algorithm for the stochastic contextual multi-armed bandit problem with linear payoff functions, when the contexts are provided by an adaptive adversary. This is among the most important and widely studied version of the contextual bandits problem. We prove a high probability regret bound of \\tildeO(\\fracd\\sqrt\u03b5\\sqrtT^1+\u03b5) in time T for any \u03b5\u2208(0,1), where d is the dimension of each context vector and \u03b5is a parameter used by the algorithm. Our results provide the first theoretical guarantees for the contextual version of Thompson Sampling, and are close to the lower bound of \u03a9(\\sqrtdT) for this problem. This essentially solves the COLT open problem of Chapelle and Li [COLT 2012] regarding regret bounds for Thompson Sampling for contextual bandits problem with linear payoff functions.     Our version of Thompson sampling uses Gaussian prior and Gaussian likelihood function. Our novel martingale-based analysis techniques also allow easy extensions to the use of other distributions, satisfying certain general conditions.",
        "bibtex": "@InProceedings{pmlr-v28-agrawal13,\n  title = \t {Thompson Sampling for Contextual Bandits with Linear Payoffs},\n  author = \t {Agrawal, Shipra and Goyal, Navin},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {127--135},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/agrawal13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/agrawal13.html},\n  abstract = \t {Thompson Sampling is one of the oldest heuristics for multi-armed bandit problems. It is a randomized algorithm based on Bayesian ideas, and has recently generated significant interest after several studies demonstrated it to have better empirical performance compared to the state of the art methods. However, many questions regarding its theoretical performance remained open. In this paper, we design and analyze Thompson Sampling algorithm for the stochastic contextual multi-armed bandit problem with linear payoff functions, when the contexts are provided by an adaptive adversary. This is among the most important and widely studied version of the contextual bandits problem. We prove a high probability regret bound of \\tildeO(\\fracd\\sqrt\u03b5\\sqrtT^1+\u03b5) in time T for any \u03b5\u2208(0,1), where d is the dimension of each context vector and \u03b5is a parameter used by the algorithm. Our results provide the first theoretical guarantees for the contextual version of Thompson Sampling, and are close to the lower bound of \u03a9(\\sqrtdT) for this problem. This essentially solves the COLT open problem of Chapelle and Li [COLT 2012] regarding regret bounds for Thompson Sampling for contextual bandits problem with linear payoff functions.     Our version of Thompson sampling uses Gaussian prior and Gaussian likelihood function. Our novel martingale-based analysis techniques also allow easy extensions to the use of other distributions, satisfying certain general conditions.   }\n}",
        "pdf": "http://proceedings.mlr.press/v28/agrawal13.pdf",
        "supp": "",
        "pdf_size": 335380,
        "gs_citation": 1339,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16506820398491305928&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Microsoft Research India; Microsoft Research India",
        "aff_domain": "microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Microsoft Research India",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/group/microsoft-research-india",
        "aff_unique_abbr": "MSR India",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "fb0d67880c",
        "title": "Thurstonian Boltzmann Machines: Learning from Multiple Inequalities",
        "site": "https://proceedings.mlr.press/v28/tran13.html",
        "author": "Truyen Tran; Dinh Phung; Svetha Venkatesh",
        "abstract": "We introduce Thurstonian Boltzmann Machines (TBM), a unified architecture that can naturally incorporate a wide range of data inputs at the same time. Our motivation rests in the Thurstonian view that many discrete data types can be considered as being generated from a subset of underlying latent continuous variables, and in the observation that each realisation of a discrete type imposes certain inequalities on those variables. Thus learning and inference in TBM reduce to making sense of a set of inequalities. Our proposed TBM naturally supports the following types: Gaussian, intervals, censored, binary, categorical, muticategorical, ordinal, (in)-complete rank with and without ties. We demonstrate the versatility and capacity of the proposed model on three applications of very different natures; namely handwritten digit recognition, collaborative filtering and complex social survey analysis.",
        "bibtex": "@InProceedings{pmlr-v28-tran13,\n  title = \t {Thurstonian {B}oltzmann Machines: Learning from Multiple Inequalities},\n  author = \t {Tran, Truyen and Phung, Dinh and Venkatesh, Svetha},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {46--54},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/tran13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/tran13.html},\n  abstract = \t {We introduce Thurstonian Boltzmann Machines (TBM), a unified architecture that can naturally incorporate a wide range of data inputs at the same time. Our motivation rests in the Thurstonian view that many discrete data types can be considered as being generated from a subset of underlying latent continuous variables, and in the observation that each realisation of a discrete type imposes certain inequalities on those variables. Thus learning and inference in TBM reduce to making sense of a set of inequalities. Our proposed TBM naturally supports the following types: Gaussian, intervals, censored, binary, categorical, muticategorical, ordinal, (in)-complete rank with and without ties. We demonstrate the versatility and capacity of the proposed model on three applications of very different natures; namely handwritten digit recognition, collaborative filtering and complex social survey analysis.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/tran13.pdf",
        "supp": "",
        "pdf_size": 1144615,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8392130159507412574&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Center for Pattern Recognition and Data Analytics (PRaDA), Deakin University, Australia+Institute for Multi-sensor Processing & Content Analysis (IMPCA), Curtin University, Australia; Center for Pattern Recognition and Data Analytics (PRaDA), Deakin University, Australia; Center for Pattern Recognition and Data Analytics (PRaDA), Deakin University, Australia",
        "aff_domain": "deakin.edu.au;deakin.edu.au;deakin.edu.au",
        "email": "deakin.edu.au;deakin.edu.au;deakin.edu.au",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "Deakin University;Curtin University",
        "aff_unique_dep": "Center for Pattern Recognition and Data Analytics (PRaDA);Institute for Multi-sensor Processing & Content Analysis (IMPCA)",
        "aff_unique_url": "https://www.deakin.edu.au;https://www.curtin.edu.au",
        "aff_unique_abbr": "Deakin;Curtin",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "c75c388d7e",
        "title": "Top-down particle filtering for Bayesian decision trees",
        "site": "https://proceedings.mlr.press/v28/lakshminarayanan13.html",
        "author": "Balaji Lakshminarayanan; Daniel Roy; Yee Whye Teh",
        "abstract": "Decision tree learning is a popular approach for classification and regression in machine learning and statistics, and Bayesian formulations - which introduce a prior distribution over decision trees, and formulate learning as posterior inference given data - have been shown to produce competitive performance. Unlike classic decision tree learning algorithms like ID3, C4.5 and CART, which work in a top-down manner, existing Bayesian algorithms produce an approximation to the posterior distribution by evolving a",
        "bibtex": "@InProceedings{pmlr-v28-lakshminarayanan13,\n  title = \t {Top-down particle filtering for Bayesian decision trees},\n  author = \t {Lakshminarayanan, Balaji and Roy, Daniel and Whye Teh, Yee},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {280--288},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/lakshminarayanan13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/lakshminarayanan13.html},\n  abstract = \t {Decision tree learning is a popular approach for classification and regression in machine learning and statistics, and Bayesian formulations - which introduce a prior distribution over decision trees, and formulate learning as posterior inference given data - have been shown to produce competitive performance. Unlike classic decision tree learning algorithms like ID3, C4.5 and CART, which work in a top-down manner, existing Bayesian algorithms produce an approximation to the posterior distribution by evolving a",
        "pdf": "http://proceedings.mlr.press/v28/lakshminarayanan13.pdf",
        "supp": "",
        "pdf_size": 637824,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6233339373776023578&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Gatsby Unit, CSML, University College London; University of Cambridge; Department of Statistics, University of Oxford",
        "aff_domain": "gatsby.ucl.ac.uk;eng.cam.ac.uk;stats.ox.ac.uk",
        "email": "gatsby.ucl.ac.uk;eng.cam.ac.uk;stats.ox.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University College London;University of Cambridge;University of Oxford",
        "aff_unique_dep": "Gatsby Unit, CSML;;Department of Statistics",
        "aff_unique_url": "https://www.ucl.ac.uk;https://www.cam.ac.uk;https://www.ox.ac.uk",
        "aff_unique_abbr": "UCL;Cambridge;Oxford",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "London;Cambridge;Oxford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "21aae1de72",
        "title": "Top-k Selection based on Adaptive Sampling of Noisy Preferences",
        "site": "https://proceedings.mlr.press/v28/busa-fekete13.html",
        "author": "Robert Busa-Fekete; Balazs Szorenyi; Weiwei Cheng; Paul Weng; Eyke Huellermeier",
        "abstract": "We consider the problem of reliably selecting an optimal subset of fixed size from a given set of choice alternatives, based on noisy information about the quality of these alternatives. Problems of similar kind have been tackled by means of adaptive sampling schemes called racing algorithms. However, in contrast to existing approaches, we do not assume that each alternative is characterized by a real-valued random variable, and that samples are taken from the corresponding distributions. Instead, we only assume that alternatives can be compared in terms of pairwise preferences. We propose and formally analyze a general preference-based racing algorithm that we instantiate with three specific ranking procedures and corresponding sampling schemes. Experiments with real and synthetic data are presented to show the efficiency of our approach.",
        "bibtex": "@InProceedings{pmlr-v28-busa-fekete13,\n  title = \t {Top-k Selection based on Adaptive Sampling of Noisy Preferences},\n  author = \t {Busa-Fekete, Robert and Szorenyi, Balazs and Cheng, Weiwei and Weng, Paul and Huellermeier, Eyke},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1094--1102},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/busa-fekete13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/busa-fekete13.html},\n  abstract = \t {We consider the problem of reliably selecting an optimal subset of fixed size from a given set of choice alternatives, based on noisy information about the quality of these alternatives. Problems of similar kind have been tackled by means of adaptive sampling schemes called racing algorithms. However, in contrast to existing approaches, we do not assume that each alternative is characterized by a real-valued random variable, and that samples are taken from the corresponding distributions. Instead, we only assume that alternatives can be compared in terms of pairwise preferences. We propose and formally analyze a general preference-based racing algorithm that we instantiate with three specific ranking procedures and corresponding sampling schemes. Experiments with real and synthetic data are presented to show the efficiency of our approach.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/busa-fekete13.pdf",
        "supp": "",
        "pdf_size": 434537,
        "gs_citation": 97,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17534951515950834189&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 24,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "0418cf936e",
        "title": "Topic Discovery through Data Dependent and Random Projections",
        "site": "https://proceedings.mlr.press/v28/ding13.html",
        "author": "Weicong Ding; Mohammad Hossein Rohban; Prakash Ishwar; Venkatesh Saligrama",
        "abstract": "We present algorithms for topic modeling based on the geometry of cross-document word-frequency patterns. This perspective gains significance under the so called separability condition. This is a condition on existence of novel-words that are unique to each topic. We present a suite of highly efficient algorithms with provable guarantees based on data-dependent and random projections to identify novel words and associated topics. Our key insight here is that the maximum and minimum values of cross-document frequency patterns projected along any direction are associated with novel words. While our sample complexity bounds for topic recovery are similar to the state-of-art, the computational complexity of our random projection scheme scales linearly with the number of documents and the number of words per document. We present several experiments on synthetic and realworld datasets to demonstrate qualitative and quantitative merits of our scheme.",
        "bibtex": "@InProceedings{pmlr-v28-ding13,\n  title = \t {Topic Discovery through Data Dependent and Random Projections},\n  author = \t {Ding, Weicong and Hossein Rohban, Mohammad and Ishwar, Prakash and Saligrama, Venkatesh},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1202--1210},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/ding13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/ding13.html},\n  abstract = \t {We present algorithms for topic modeling based on the geometry of cross-document word-frequency patterns. This perspective gains significance under the so called separability condition. This is a condition on existence of novel-words that are unique to each topic. We present a suite of highly efficient algorithms with provable guarantees based on data-dependent and random projections to identify novel words and associated topics. Our key insight here is that the maximum and minimum values of cross-document frequency patterns projected along any direction are associated with novel words. While our sample complexity bounds for topic recovery are similar to the state-of-art, the computational complexity of our random projection scheme scales linearly with the number of documents and the number of words per document. We present several experiments on synthetic and realworld datasets to demonstrate qualitative and quantitative merits of our scheme.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/ding13.pdf",
        "supp": "",
        "pdf_size": 309202,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15122066441512019197&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Electrical & Computer Engineering, Boston University, Boston, MA, USA; Department of Electrical & Computer Engineering, Boston University, Boston, MA, USA; Department of Electrical & Computer Engineering, Boston University, Boston, MA, USA; Department of Electrical & Computer Engineering, Boston University, Boston, MA, USA",
        "aff_domain": "bu.edu;bu.edu;bu.edu;bu.edu",
        "email": "bu.edu;bu.edu;bu.edu;bu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Boston University",
        "aff_unique_dep": "Department of Electrical & Computer Engineering",
        "aff_unique_url": "https://www.bu.edu",
        "aff_unique_abbr": "BU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Boston",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6bce33b977",
        "title": "Topic Model Diagnostics: Assessing Domain Relevance via Topical Alignment",
        "site": "https://proceedings.mlr.press/v28/chuang13.html",
        "author": "Jason Chuang; Sonal Gupta; Christopher Manning; Jeffrey Heer",
        "abstract": "The use of topic models to analyze domain-specific texts often requires manual validation of the latent topics to ensure they are meaningful. We introduce a framework to support large-scale assessment of topical relevance. We measure the correspondence between a set of latent topics and a set of reference concepts to quantify four types of topical misalignment: junk, fused, missing, and repeated topics. Our analysis compares 10,000 topic model variants to 200 expert-provided domain concepts, and demonstrates how our framework can inform choices of model parameters, inference algorithms, and intrinsic measures of topical quality.",
        "bibtex": "@InProceedings{pmlr-v28-chuang13,\n  title = \t {Topic Model Diagnostics: Assessing Domain Relevance via Topical Alignment},\n  author = \t {Chuang, Jason and Gupta, Sonal and Manning, Christopher and Heer, Jeffrey},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {612--620},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/chuang13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/chuang13.html},\n  abstract = \t {The use of topic models to analyze domain-specific texts often requires manual validation of the latent topics to ensure they are meaningful. We introduce a framework to support large-scale assessment of topical relevance. We measure the correspondence between a set of latent topics and a set of reference concepts to quantify four types of topical misalignment: junk, fused, missing, and repeated topics. Our analysis compares 10,000 topic model variants to 200 expert-provided domain concepts, and demonstrates how our framework can inform choices of model parameters, inference algorithms, and intrinsic measures of topical quality.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/chuang13.pdf",
        "supp": "",
        "pdf_size": 4776484,
        "gs_citation": 172,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16487762282646986016&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Stanford University, 353 Serra Mall, Stanford, CA 94305 USA; Stanford University, 353 Serra Mall, Stanford, CA 94305 USA; Stanford University, 353 Serra Mall, Stanford, CA 94305 USA; Stanford University, 353 Serra Mall, Stanford, CA 94305 USA",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "21f545a5cb",
        "title": "Toward Optimal Stratification for Stratified Monte-Carlo Integration",
        "site": "https://proceedings.mlr.press/v28/carpentier13.html",
        "author": "Alexandra Carpentier; R\u00e9mi Munos",
        "abstract": "We consider the problem of adaptive stratified sampling for Monte Carlo integration of a function, given a finite number of function evaluations perturbed by noise. Here we address the problem of adapting simultaneously the number of samples into each stratum and the stratification itself. We show a tradeoff in the size of the partitioning. On the one hand it is important to refine the partition in areas where the observation noise or the function are heterogeneous in order to reduce this variability. But on the other hand, a too refined stratification makes it harder to assign the samples according to a near-optimal (oracle) allocation strategy. In this paper we provide an algorithm \\em Monte-Carlo Upper-Lower Confidence Bound that selects online, among a large class of partitions, the partition that provides a near-optimal trade-off, and allocates the samples almost optimally on this partition.",
        "bibtex": "@InProceedings{pmlr-v28-carpentier13,\n  title = \t {Toward Optimal Stratification for Stratified Monte-Carlo Integration},\n  author = \t {Carpentier, Alexandra and Munos, R\u00e9mi},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {28--36},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/carpentier13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/carpentier13.html},\n  abstract = \t {We consider the problem of adaptive stratified sampling for Monte Carlo integration of a function, given a finite number of function evaluations perturbed by noise. Here we address the problem of adapting simultaneously the number of samples into each stratum and the stratification itself. We show a tradeoff in the size of the partitioning. On the one hand it is important to refine the partition in areas where the observation noise or the function are heterogeneous in order to reduce this variability. But on the other hand, a too refined stratification makes it harder to assign the samples according to a near-optimal (oracle) allocation strategy. In this paper we provide an algorithm \\em Monte-Carlo Upper-Lower Confidence Bound that selects online, among a large class of partitions, the partition that provides a near-optimal trade-off, and allocates the samples almost optimally on this partition.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/carpentier13.pdf",
        "supp": "",
        "pdf_size": 440666,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=952490113970392097&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Statistical Laboratory, Center for Mathematical Sciences, Wilberforce Road, CB3 0WB Cambridge, United Kingdom; INRIA Lille - Nord Europe, Parc Scienti\ufb01que de la Haute-Borne, 40 Avenue Halley, 59650 Villeneuve dAscq, France",
        "aff_domain": "statslab.cam.ac.uk;inria.fr",
        "email": "statslab.cam.ac.uk;inria.fr",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Cambridge;INRIA Lille - Nord Europe",
        "aff_unique_dep": "Center for Mathematical Sciences;",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.inria.fr/lille-nord-europe",
        "aff_unique_abbr": "Cambridge;INRIA",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Cambridge;Lille",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United Kingdom;France"
    },
    {
        "id": "a4de61e0a1",
        "title": "Transition Matrix Estimation in High Dimensional Time Series",
        "site": "https://proceedings.mlr.press/v28/han13a.html",
        "author": "Fang Han; Han Liu",
        "abstract": "In this paper, we propose a new method in estimating transition matrices of high dimensional vector autoregressive (VAR) models. Here the data are assumed to come from a stationary Gaussian VAR time series. By formulating the problem as a linear program, we provide a new approach to conduct inference on such models. In theory, under a doubly asymptotic framework in which both the sample size T and dimensionality d of the time series can increase, we provide explicit rates of convergence between the estimator and the population transition matrix under different matrix norms. Our results show that the spectral norm of the transition matrix plays a pivotal role in determining the final rates of convergence. This is the first work analyzing the estimation of transition matrices under a high dimensional doubly asymptotic framework. Experiments are conducted on both synthetic and real-world stock data to demonstrate the effectiveness of the proposed method compared with the existing methods. The results of this paper have broad impact on different applications, including finance, genomics, and brain imaging.",
        "bibtex": "@InProceedings{pmlr-v28-han13a,\n  title = \t {Transition Matrix Estimation in High Dimensional Time Series},\n  author = \t {Han, Fang and Liu, Han},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {172--180},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/han13a.pdf},\n  url = \t {https://proceedings.mlr.press/v28/han13a.html},\n  abstract = \t {In this paper, we propose a new method in estimating transition matrices of high dimensional vector autoregressive (VAR) models. Here the data are assumed to come from a stationary Gaussian VAR time series. By formulating the problem as a linear program, we provide a new approach to conduct inference on such models. In theory, under a doubly asymptotic framework in which both the sample size T and dimensionality d of the time series can increase, we provide explicit rates of convergence between the estimator and the population transition matrix under different matrix norms. Our results show that the spectral norm of the transition matrix plays a pivotal role in determining the final rates of convergence. This is the first work analyzing the estimation of transition matrices under a high dimensional doubly asymptotic framework. Experiments are conducted on both synthetic and real-world stock data to demonstrate the effectiveness of the proposed method compared with the existing methods. The results of this paper have broad impact on different applications, including finance, genomics, and brain imaging.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/han13a.pdf",
        "supp": "",
        "pdf_size": 388050,
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4636380837207670466&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Johns Hopkins University; Princeton University",
        "aff_domain": "jhsph.edu;princeton.edu",
        "email": "jhsph.edu;princeton.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Johns Hopkins University;Princeton University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.jhu.edu;https://www.princeton.edu",
        "aff_unique_abbr": "JHU;Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "84696f1dcb",
        "title": "Tree-Independent Dual-Tree Algorithms",
        "site": "https://proceedings.mlr.press/v28/curtin13.html",
        "author": "Ryan Curtin; William March; Parikshit Ram; David Anderson; Alexander Gray; Charles Isbell",
        "abstract": "Dual-tree algorithms are a widely used class of branch-and-bound algorithms.  Unfortunately, developing dual-tree algorithms for use with different trees and problems is often complex and burdensome.  We introduce a four-part logical split: the tree, the traversal, the point-to-point base case, and the pruning rule.  We provide a meta-algorithm which allows development of dual-tree algorithms in a tree-independent manner and easy extension to entirely new types of trees.  Representations are provided  for five common algorithms; for k-nearest neighbor search, this leads to a novel, tighter pruning bound. The meta-algorithm also allows  straightforward extensions to massively parallel settings.",
        "bibtex": "@InProceedings{pmlr-v28-curtin13,\n  title = \t {Tree-Independent Dual-Tree Algorithms},\n  author = \t {Curtin, Ryan and March, William and Ram, Parikshit and Anderson, David and Gray, Alexander and Isbell, Charles},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {1435--1443},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/curtin13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/curtin13.html},\n  abstract = \t {Dual-tree algorithms are a widely used class of branch-and-bound algorithms.  Unfortunately, developing dual-tree algorithms for use with different trees and problems is often complex and burdensome.  We introduce a four-part logical split: the tree, the traversal, the point-to-point base case, and the pruning rule.  We provide a meta-algorithm which allows development of dual-tree algorithms in a tree-independent manner and easy extension to entirely new types of trees.  Representations are provided  for five common algorithms; for k-nearest neighbor search, this leads to a novel, tighter pruning bound. The meta-algorithm also allows  straightforward extensions to massively parallel settings.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/curtin13.pdf",
        "supp": "",
        "pdf_size": 359576,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7682390263548290656&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology",
        "aff_domain": "cc.gatech.edu;gatech.edu;gatech.edu;gatech.edu;cc.gatech.edu;cc.gatech.edu",
        "email": "cc.gatech.edu;gatech.edu;gatech.edu;gatech.edu;cc.gatech.edu;cc.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Georgia Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.gatech.edu",
        "aff_unique_abbr": "Georgia Tech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f43ab3cae1",
        "title": "Two-Sided Exponential Concentration Bounds for Bayes Error Rate and Shannon Entropy",
        "site": "https://proceedings.mlr.press/v28/honorio13.html",
        "author": "Jean Honorio; Jaakkola Tommi",
        "abstract": "We provide a method that approximates the Bayes error rate and the Shannon entropy with high probability. The Bayes error rate approximation makes possible to build a classifier that polynomially approaches Bayes error rate. The Shannon entropy approximation provides provable performance guarantees for learning trees and Bayesian networks from continuous variables. Our results rely on some reasonable regularity conditions of the unknown probability distributions, and apply to bounded as well as unbounded variables.",
        "bibtex": "@InProceedings{pmlr-v28-honorio13,\n  title = \t {Two-Sided Exponential Concentration Bounds for Bayes Error Rate and Shannon Entropy},\n  author = \t {Honorio, Jean and Tommi, Jaakkola},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {459--467},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/honorio13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/honorio13.html},\n  abstract = \t {We provide a method that approximates the Bayes error rate and the Shannon entropy with high probability. The Bayes error rate approximation makes possible to build a classifier that polynomially approaches Bayes error rate. The Shannon entropy approximation provides provable performance guarantees for learning trees and Bayesian networks from continuous variables. Our results rely on some reasonable regularity conditions of the unknown probability distributions, and apply to bounded as well as unbounded variables.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/honorio13.pdf",
        "supp": "",
        "pdf_size": 354914,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14254262231356008960&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "CSAIL, MIT, Cambridge, MA 02139, USA; CSAIL, MIT, Cambridge, MA 02139, USA",
        "aff_domain": "csail.mit.edu;csail.mit.edu",
        "email": "csail.mit.edu;csail.mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.csail.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9645dff822",
        "title": "Unfolding Latent Tree Structures using 4th Order Tensors",
        "site": "https://proceedings.mlr.press/v28/ishteva13.html",
        "author": "Mariya Ishteva; Haesun Park; Le Song",
        "abstract": "Discovering the latent structure from many observed variables is an important yet challenging learning task. Existing approaches for discovering latent structures often require the unknown number of hidden states as an input. In this paper, we propose a quartet based approach which is agnostic to this number. The key contribution is a novel rank characterization of the tensor associated with the marginal distribution of a quartet. This characterization allows us to design a nuclear norm based test for resolving quartet relations. We then use the quartet test as a subroutine in a divide-and-conquer algorithm for recovering the latent tree structure. Under mild conditions, the algorithm is consistent and its error probability decays exponentially with increasing sample size. We demonstrate that the proposed approach compares favorably to alternatives. In a real world stock dataset, it also discovers meaningful groupings of variables, and produces a model that fits the data better.",
        "bibtex": "@InProceedings{pmlr-v28-ishteva13,\n  title = \t {Unfolding Latent Tree Structures using 4th Order Tensors},\n  author = \t {Ishteva, Mariya and Park, Haesun and Song, Le},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {316--324},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/ishteva13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/ishteva13.html},\n  abstract = \t {Discovering the latent structure from many observed variables is an important yet challenging learning task. Existing approaches for discovering latent structures often require the unknown number of hidden states as an input. In this paper, we propose a quartet based approach which is agnostic to this number. The key contribution is a novel rank characterization of the tensor associated with the marginal distribution of a quartet. This characterization allows us to design a nuclear norm based test for resolving quartet relations. We then use the quartet test as a subroutine in a divide-and-conquer algorithm for recovering the latent tree structure. Under mild conditions, the algorithm is consistent and its error probability decays exponentially with increasing sample size. We demonstrate that the proposed approach compares favorably to alternatives. In a real world stock dataset, it also discovers meaningful groupings of variables, and produces a model that fits the data better.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/ishteva13.pdf",
        "supp": "",
        "pdf_size": 682321,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4555010171820542800&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "ELEC, Vrije Universiteit Brussel, 1050 Brussels, Belgium; College of Computing, Georgia Institute of Technology, Atlanta, GA 30332, USA; College of Computing, Georgia Institute of Technology, Atlanta, GA 30332, USA",
        "aff_domain": "vub.ac.be;cc.gatech.edu;cc.gatech.edu",
        "email": "vub.ac.be;cc.gatech.edu;cc.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Vrije Universiteit Brussel;Georgia Institute of Technology",
        "aff_unique_dep": "ELEC;College of Computing",
        "aff_unique_url": "https://www.vub.be;https://www.gatech.edu",
        "aff_unique_abbr": "VUB;Georgia Tech",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Brussels;Atlanta",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Belgium;United States"
    },
    {
        "id": "e7515bdc6c",
        "title": "Vanishing Component Analysis",
        "site": "https://proceedings.mlr.press/v28/livni13.html",
        "author": "Roi Livni; David Lehavi; Sagi Schein; Hila Nachliely; Shai Shalev-Shwartz; Amir Globerson",
        "abstract": "The vanishing ideal of a set of n points S, is the set of all polynomials that attain the value of zero on all the points in S. Such ideals can be compactly represented using a small set of polynomials known as generators of the ideal. Here we describe and analyze an efficient procedure that constructs a set of generators of a vanishing ideal. Our procedure is numerically stable, and can be used to find approximately vanishing polynomials.  The resulting polynomials capture nonlinear structure in data, and can for example be used within supervised learning. Empirical comparison with kernel methods show that our method constructs more compact classifiers with comparable accuracy.",
        "bibtex": "@InProceedings{pmlr-v28-livni13,\n  title = \t {Vanishing Component Analysis},\n  author = \t {Livni, Roi and Lehavi, David and Schein, Sagi and Nachliely, Hila and Shalev-Shwartz, Shai and Globerson, Amir},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {597--605},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/livni13.pdf},\n  url = \t {https://proceedings.mlr.press/v28/livni13.html},\n  abstract = \t {The vanishing ideal of a set of n points S, is the set of all polynomials that attain the value of zero on all the points in S. Such ideals can be compactly represented using a small set of polynomials known as generators of the ideal. Here we describe and analyze an efficient procedure that constructs a set of generators of a vanishing ideal. Our procedure is numerically stable, and can be used to find approximately vanishing polynomials.  The resulting polynomials capture nonlinear structure in data, and can for example be used within supervised learning. Empirical comparison with kernel methods show that our method constructs more compact classifiers with comparable accuracy.  }\n}",
        "pdf": "http://proceedings.mlr.press/v28/livni13.pdf",
        "supp": "",
        "pdf_size": 377649,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5682301862972019889&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "644f15be77",
        "title": "\\proptoSVM for Learning with Label Proportions",
        "site": "https://proceedings.mlr.press/v28/yu13a.html",
        "author": "Felix Yu; Dong Liu; Sanjiv Kumar; Jebara Tony; Shih-Fu Chang",
        "abstract": "We study the problem of learning with label proportions in which the training data is provided in groups and only the proportion of each class in each group is known. We propose a new method called proportion-SVM, or \\proptoSVM, which explicitly models the latent unknown instance labels together with the known group label proportions in a large-margin framework. Unlike the existing works, our approach avoids making restrictive assumptions about the data. The \\proptoSVM model leads to a non-convex integer programming problem. In order to solve it efficiently, we propose two algorithms: one based on simple alternating optimization and the other based on a convex relaxation. Extensive experiments on standard datasets show that \\proptoSVM outperforms the state-of-the-art, especially for larger group sizes.",
        "bibtex": "@InProceedings{pmlr-v28-yu13a,\n  title = \t {$\\propto$SVM for Learning with Label Proportions},\n  author = \t {Yu, Felix and Liu, Dong and Kumar, Sanjiv and Tony, Jebara and Chang, Shih-Fu},\n  booktitle = \t {Proceedings of the 30th International Conference on Machine Learning},\n  pages = \t {504--512},\n  year = \t {2013},\n  editor = \t {Dasgupta, Sanjoy and McAllester, David},\n  volume = \t {28},\n  number =       {3},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Atlanta, Georgia, USA},\n  month = \t {17--19 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v28/yu13a.pdf},\n  url = \t {https://proceedings.mlr.press/v28/yu13a.html},\n  abstract = \t {We study the problem of learning with label proportions in which the training data is provided in groups and only the proportion of each class in each group is known. We propose a new method called proportion-SVM, or \\proptoSVM, which explicitly models the latent unknown instance labels together with the known group label proportions in a large-margin framework. Unlike the existing works, our approach avoids making restrictive assumptions about the data. The \\proptoSVM model leads to a non-convex integer programming problem. In order to solve it efficiently, we propose two algorithms: one based on simple alternating optimization and the other based on a convex relaxation. Extensive experiments on standard datasets show that \\proptoSVM outperforms the state-of-the-art, especially for larger group sizes.}\n}",
        "pdf": "http://proceedings.mlr.press/v28/yu13a.pdf",
        "supp": "",
        "pdf_size": 502951,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17396090325309326914&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Columbia University; Columbia University; Google Research; Columbia University; Columbia University",
        "aff_domain": "ee.columbia.edu;ee.columbia.edu;google.com;cs.columbia.edu;cs.columbia.edu",
        "email": "ee.columbia.edu;ee.columbia.edu;google.com;cs.columbia.edu;cs.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "Columbia University;Google",
        "aff_unique_dep": ";Google Research",
        "aff_unique_url": "https://www.columbia.edu;https://research.google",
        "aff_unique_abbr": "Columbia;Google Research",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    }
]