[
    {
        "id": "e7a11335f2",
        "title": "(Near) Dimension Independent Risk Bounds for Differentially Private Learning",
        "site": "https://proceedings.mlr.press/v32/jain14.html",
        "author": "Prateek Jain; Abhradeep Guha Thakurta",
        "abstract": "In this paper, we study the problem of differentially private risk minimization where the goal is to provide differentially private algorithms that have small excess risk. In particular we address the following open problem: \\emphIs it possible to design computationally efficient differentially private risk minimizers with  excess risk bounds that do not explicitly depend on dimensionality (p) and do not require  structural assumptions like restricted strong convexity?  In this paper, we answer the question in the affirmative for a variant of the well-known  \\emphoutput and \\emphobjective perturbation algorithms [Chaudhuri et al., 2011]. In particular, we show that  in generalized linear model, variants of both output and objective perturbation algorithms have no \\em explicit dependence on p. Our results assume that the underlying loss function is a 1-Lipschitz convex function and we show that the excess risk depends only on  L_2 norm of the true risk minimizer and that of training points.  Next, we present a novel privacy preserving algorithm for risk minimization over simplex in the generalized linear model, where the loss function is  a doubly differentiable convex function. Assuming that the training points have bounded L_\u221e-norm, our algorithm provides risk bound that has only \\em logarithmic dependence on p. We also apply our technique to the online learning setting and obtain a regret bound with similar logarithmic dependence on p. In contrast, the existing differentially private online learning methods incur O(\\sqrtp)  dependence.",
        "bibtex": "@InProceedings{pmlr-v32-jain14,\n  title = \t {(Near) Dimension Independent Risk Bounds for Differentially Private Learning},\n  author = \t {Jain, Prateek and Thakurta, Abhradeep Guha},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {476--484},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/jain14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/jain14.html},\n  abstract = \t {In this paper, we study the problem of differentially private risk minimization where the goal is to provide differentially private algorithms that have small excess risk. In particular we address the following open problem: \\emphIs it possible to design computationally efficient differentially private risk minimizers with  excess risk bounds that do not explicitly depend on dimensionality (p) and do not require  structural assumptions like restricted strong convexity?  In this paper, we answer the question in the affirmative for a variant of the well-known  \\emphoutput and \\emphobjective perturbation algorithms [Chaudhuri et al., 2011]. In particular, we show that  in generalized linear model, variants of both output and objective perturbation algorithms have no \\em explicit dependence on p. Our results assume that the underlying loss function is a 1-Lipschitz convex function and we show that the excess risk depends only on  L_2 norm of the true risk minimizer and that of training points.  Next, we present a novel privacy preserving algorithm for risk minimization over simplex in the generalized linear model, where the loss function is  a doubly differentiable convex function. Assuming that the training points have bounded L_\u221e-norm, our algorithm provides risk bound that has only \\em logarithmic dependence on p. We also apply our technique to the online learning setting and obtain a regret bound with similar logarithmic dependence on p. In contrast, the existing differentially private online learning methods incur O(\\sqrtp)  dependence.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/jain14.pdf",
        "supp": "",
        "pdf_size": 233594,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9083944043221948723&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Microsoft Research; Stanford University + Microsoft Research",
        "aff_domain": "microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0",
        "aff_unique_norm": "Microsoft;Stanford University",
        "aff_unique_dep": "Microsoft Research;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://www.stanford.edu",
        "aff_unique_abbr": "MSR;Stanford",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c325cfd96f",
        "title": "A Bayesian Framework for Online Classifier Ensemble",
        "site": "https://proceedings.mlr.press/v32/bai14.html",
        "author": "Qinxun Bai; Henry Lam; Stan Sclaroff",
        "abstract": "We propose a Bayesian framework for recursively estimating the classifier weights in online learning of a classifier ensemble. In contrast with past methods, such as stochastic gradient descent or online boosting, our framework estimates the weights in terms of evolving posterior distributions. For a specified class of loss functions, we show that it is possible to formulate a suitably defined likelihood function and hence use the posterior distribution as an approximation to the global empirical loss minimizer. If the stream of training data is sampled from a stationary process, we can also show that our framework admits a superior rate of convergence to the expected loss minimizer than is possible with standard stochastic gradient descent. In experiments with real-world datasets, our formulation often performs better than online boosting algorithms.",
        "bibtex": "@InProceedings{pmlr-v32-bai14,\n  title = \t {A Bayesian Framework for Online Classifier Ensemble},\n  author = \t {Bai, Qinxun and Lam, Henry and Sclaroff, Stan},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1584--1592},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/bai14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/bai14.html},\n  abstract = \t {We propose a Bayesian framework for recursively estimating the classifier weights in online learning of a classifier ensemble. In contrast with past methods, such as stochastic gradient descent or online boosting, our framework estimates the weights in terms of evolving posterior distributions. For a specified class of loss functions, we show that it is possible to formulate a suitably defined likelihood function and hence use the posterior distribution as an approximation to the global empirical loss minimizer. If the stream of training data is sampled from a stationary process, we can also show that our framework admits a superior rate of convergence to the expected loss minimizer than is possible with standard stochastic gradient descent. In experiments with real-world datasets, our formulation often performs better than online boosting algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/bai14.pdf",
        "supp": "",
        "pdf_size": 192392,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2834595109422337802&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Science, Boston University, Boston, MA 02215 USA; Department of Mathematics and Statistics, Boston University, Boston, MA 02215 USA; Department of Computer Science, Boston University, Boston, MA 02215 USA",
        "aff_domain": "CS.BU.EDU;BU.EDU;CS.BU.EDU",
        "email": "CS.BU.EDU;BU.EDU;CS.BU.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Boston University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.bu.edu",
        "aff_unique_abbr": "BU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Boston",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "11c45ba64c",
        "title": "A Bayesian Wilcoxon signed-rank test based on the Dirichlet process",
        "site": "https://proceedings.mlr.press/v32/benavoli14.html",
        "author": "Alessio Benavoli; Giorgio Corani; Francesca Mangili; Marco Zaffalon; Fabrizio Ruggeri",
        "abstract": "Bayesian methods are ubiquitous in machine learning.  Nevertheless, the analysis of empirical results is typically   performed  by frequentist tests. This implies dealing with  null hypothesis significance tests and  p-values, even though the   shortcomings of such methods are well known.   We propose  a nonparametric Bayesian version of the Wilcoxon   signed-rank test using a Dirichlet process (DP) based prior.  We address in two different ways the problem of how to choose  the   infinite dimensional parameter that characterizes the DP.   The proposed  test has all the traditional strengths of the Bayesian   approach; for instance, unlike the frequentist tests,   it allows verifying the null hypothesis, not only rejecting it, and   taking decision which minimize the expected loss.  Moreover, one of the solutions proposed to model the infinitedimensional parameter of the DP, allows isolating instances in which the traditional frequentist test is guessing at random.   We show results dealing with the comparison of two classifiers using real and simulated data.",
        "bibtex": "@InProceedings{pmlr-v32-benavoli14,\n  title = \t {A Bayesian Wilcoxon signed-rank test based on the Dirichlet process},\n  author = \t {Benavoli, Alessio and Corani, Giorgio and Mangili, Francesca and Zaffalon, Marco and Ruggeri, Fabrizio},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1026--1034},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/benavoli14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/benavoli14.html},\n  abstract = \t {Bayesian methods are ubiquitous in machine learning.  Nevertheless, the analysis of empirical results is typically   performed  by frequentist tests. This implies dealing with  null hypothesis significance tests and  p-values, even though the   shortcomings of such methods are well known.   We propose  a nonparametric Bayesian version of the Wilcoxon   signed-rank test using a Dirichlet process (DP) based prior.  We address in two different ways the problem of how to choose  the   infinite dimensional parameter that characterizes the DP.   The proposed  test has all the traditional strengths of the Bayesian   approach; for instance, unlike the frequentist tests,   it allows verifying the null hypothesis, not only rejecting it, and   taking decision which minimize the expected loss.  Moreover, one of the solutions proposed to model the infinitedimensional parameter of the DP, allows isolating instances in which the traditional frequentist test is guessing at random.   We show results dealing with the comparison of two classifiers using real and simulated data.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/benavoli14.pdf",
        "supp": "",
        "pdf_size": 171690,
        "gs_citation": 140,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7647998582348358970&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "IPG IDSIA, Manno, Switzerland; IPG IDSIA, Manno, Switzerland; IPG IDSIA, Manno, Switzerland; IPG IDSIA, Manno, Switzerland; CNR IMATI, Milano, Italy",
        "aff_domain": "idsia.ch;idsia.ch;idsia.ch;idsia.ch;mi.imati.cnr.it",
        "email": "idsia.ch;idsia.ch;idsia.ch;idsia.ch;mi.imati.cnr.it",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "IDSIA;CNR IMATI",
        "aff_unique_dep": "Institute of Cognitive Science and Technology;",
        "aff_unique_url": "https://www.idsia.ch/;https://www.imati.cnr.it",
        "aff_unique_abbr": "IDSIA;",
        "aff_campus_unique_index": "0;0;0;0;1",
        "aff_campus_unique": "Manno;Milano",
        "aff_country_unique_index": "0;0;0;0;1",
        "aff_country_unique": "Switzerland;Italy"
    },
    {
        "id": "53a94cce48",
        "title": "A Clockwork RNN",
        "site": "https://proceedings.mlr.press/v32/koutnik14.html",
        "author": "Jan Koutnik; Klaus Greff; Faustino Gomez; Juergen Schmidhuber",
        "abstract": "Sequence prediction and classification are ubiquitous and challenging problems in machine learning that can require identifying complex dependencies between temporally distant inputs. Recurrent Neural Networks (RNNs) have the ability, in theory, to cope with these temporal dependencies by virtue of the short-term memory implemented by their recurrent (feedback) connections. However, in practice they are difficult to train successfully when long-term memory is  required.    This paper introduces a simple, yet powerful modification to the  simple RNN (SRN) architecture, the Clockwork RNN (CW-RNN), in which the hidden layer is partitioned into separate modules, each processing inputs at its own temporal granularity, making computations only at its prescribed clock rate.    Rather than making the standard RNN models more complex, CW-RNN  reduces the number of SRN parameters, improves the performance  significantly in the tasks tested, and speeds up the network evaluation.    The network is demonstrated in preliminary experiments involving three tasks: audio signal generation, TIMIT spoken word classification,  where it outperforms both SRN and LSTM networks, and online handwriting recognition, where it outperforms SRNs.",
        "bibtex": "@InProceedings{pmlr-v32-koutnik14,\n  title = \t {A Clockwork RNN},\n  author = \t {Koutnik, Jan and Greff, Klaus and Gomez, Faustino and Schmidhuber, Juergen},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1863--1871},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/koutnik14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/koutnik14.html},\n  abstract = \t {Sequence prediction and classification are ubiquitous and challenging problems in machine learning that can require identifying complex dependencies between temporally distant inputs. Recurrent Neural Networks (RNNs) have the ability, in theory, to cope with these temporal dependencies by virtue of the short-term memory implemented by their recurrent (feedback) connections. However, in practice they are difficult to train successfully when long-term memory is  required.    This paper introduces a simple, yet powerful modification to the  simple RNN (SRN) architecture, the Clockwork RNN (CW-RNN), in which the hidden layer is partitioned into separate modules, each processing inputs at its own temporal granularity, making computations only at its prescribed clock rate.    Rather than making the standard RNN models more complex, CW-RNN  reduces the number of SRN parameters, improves the performance  significantly in the tasks tested, and speeds up the network evaluation.    The network is demonstrated in preliminary experiments involving three tasks: audio signal generation, TIMIT spoken word classification,  where it outperforms both SRN and LSTM networks, and online handwriting recognition, where it outperforms SRNs.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/koutnik14.pdf",
        "supp": "",
        "pdf_size": 549712,
        "gs_citation": 734,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2407089308575049376&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "IDSIA, USI&SUPSI, Manno-Lugano, CH-6928, Switzerland; IDSIA, USI&SUPSI, Manno-Lugano, CH-6928, Switzerland; IDSIA, USI&SUPSI, Manno-Lugano, CH-6928, Switzerland; IDSIA, USI&SUPSI, Manno-Lugano, CH-6928, Switzerland",
        "aff_domain": "IDSIA.CH;IDSIA.CH;IDSIA.CH;IDSIA.CH",
        "email": "IDSIA.CH;IDSIA.CH;IDSIA.CH;IDSIA.CH",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "IDSIA",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Manno-Lugano",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "6b0bd5348d",
        "title": "A Compilation Target for Probabilistic Programming Languages",
        "site": "https://proceedings.mlr.press/v32/paige14.html",
        "author": "Brooks Paige; Frank Wood",
        "abstract": "Forward inference techniques such as sequential Monte Carlo and particle Markov chain Monte Carlo for probabilistic programming can be implemented in any programming language by creative use of standardized operating system functionality including processes, forking, mutexes, and shared memory.   Exploiting this we have defined, developed, and tested a probabilistic programming language intermediate representation language we call probabilistic C, which itself can be compiled to machine code by standard compilers and linked to operating system libraries yielding an efficient, scalable, portable probabilistic programming compilation target.  This opens up a new hardware and systems research path for optimizing probabilistic programming systems.",
        "bibtex": "@InProceedings{pmlr-v32-paige14,\n  title = \t {A Compilation Target for Probabilistic Programming Languages},\n  author = \t {Paige, Brooks and Wood, Frank},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1935--1943},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/paige14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/paige14.html},\n  abstract = \t {Forward inference techniques such as sequential Monte Carlo and particle Markov chain Monte Carlo for probabilistic programming can be implemented in any programming language by creative use of standardized operating system functionality including processes, forking, mutexes, and shared memory.   Exploiting this we have defined, developed, and tested a probabilistic programming language intermediate representation language we call probabilistic C, which itself can be compiled to machine code by standard compilers and linked to operating system libraries yielding an efficient, scalable, portable probabilistic programming compilation target.  This opens up a new hardware and systems research path for optimizing probabilistic programming systems.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/paige14.pdf",
        "supp": "",
        "pdf_size": 892594,
        "gs_citation": 88,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13156637826181001596&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "University of Oxford, Department of Engineering Science, Oxford, UK; University of Oxford, Department of Engineering Science, Oxford, UK",
        "aff_domain": "ROBOTS.OX.AC.UK;ROBOTS.OX.AC.UK",
        "email": "ROBOTS.OX.AC.UK;ROBOTS.OX.AC.UK",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Oxford",
        "aff_unique_dep": "Department of Engineering Science",
        "aff_unique_url": "https://www.ox.ac.uk",
        "aff_unique_abbr": "Oxford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Oxford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "e17995de2f",
        "title": "A Consistent Histogram Estimator for Exchangeable Graph Models",
        "site": "https://proceedings.mlr.press/v32/chan14.html",
        "author": "Stanley Chan; Edoardo Airoldi",
        "abstract": "Exchangeable graph models (ExGM) subsume a number of popular network models. The mathematical object that characterizes an ExGM is  termed a graphon. Finding scalable estimators of graphons, provably consistent, remains an open issue. In this paper, we propose a histogram estimator of a graphon that is provably consistent and numerically efficient. The proposed estimator is based on a sorting-and-smoothing (SAS) algorithm, which first sorts the empirical degree of a graph, then smooths the sorted graph using total variation minimization. The consistency of the SAS algorithm is proved by leveraging  sparsity concepts from compressed sensing.",
        "bibtex": "@InProceedings{pmlr-v32-chan14,\n  title = \t {A Consistent Histogram Estimator for Exchangeable Graph Models},\n  author = \t {Chan, Stanley and Airoldi, Edoardo},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {208--216},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/chan14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/chan14.html},\n  abstract = \t {Exchangeable graph models (ExGM) subsume a number of popular network models. The mathematical object that characterizes an ExGM is  termed a graphon. Finding scalable estimators of graphons, provably consistent, remains an open issue. In this paper, we propose a histogram estimator of a graphon that is provably consistent and numerically efficient. The proposed estimator is based on a sorting-and-smoothing (SAS) algorithm, which first sorts the empirical degree of a graph, then smooths the sorted graph using total variation minimization. The consistency of the SAS algorithm is proved by leveraging  sparsity concepts from compressed sensing.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/chan14.pdf",
        "supp": "",
        "pdf_size": 1093281,
        "gs_citation": 136,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4033775662606955201&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "School of Engineering and Applied Sciences, Harvard University, Cambridge, MA 02138, USA; Department of Statistics, Harvard University, Cambridge, MA 02138, USA",
        "aff_domain": "SEAS.HARVARD.EDU;FAS.HARVARD.EDU",
        "email": "SEAS.HARVARD.EDU;FAS.HARVARD.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Harvard University",
        "aff_unique_dep": "School of Engineering and Applied Sciences",
        "aff_unique_url": "https://www.harvard.edu",
        "aff_unique_abbr": "Harvard",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c4a7590e00",
        "title": "A Convergence Rate Analysis for LogitBoost, MART and Their Variant",
        "site": "https://proceedings.mlr.press/v32/sunc14.html",
        "author": "Peng Sun; Tong Zhang; Jie Zhou",
        "abstract": "LogitBoost, MART and their variant can be viewed as additive tree regression using logistic loss and boosting style optimization. We analyze their convergence rates based on a new weak learnability formulation. We show that it has O(\\frac1T) rate when using gradient descent only, while a linear rate is achieved when using Newton descent. Moreover, introducing Newton descent when growing the trees, as LogitBoost does, leads to a faster linear rate. Empirical results on UCI datasets support our analysis.",
        "bibtex": "@InProceedings{pmlr-v32-sunc14,\n  title = \t {A Convergence Rate Analysis for LogitBoost, MART and Their Variant},\n  author = \t {Sun, Peng and Zhang, Tong and Zhou, Jie},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1251--1259},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/sunc14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/sunc14.html},\n  abstract = \t {LogitBoost, MART and their variant can be viewed as additive tree regression using logistic loss and boosting style optimization. We analyze their convergence rates based on a new weak learnability formulation. We show that it has O(\\frac1T) rate when using gradient descent only, while a linear rate is achieved when using Newton descent. Moreover, introducing Newton descent when growing the trees, as LogitBoost does, leads to a faster linear rate. Empirical results on UCI datasets support our analysis.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/sunc14.pdf",
        "supp": "",
        "pdf_size": 432433,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5646843122814594908&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Tsinghua National Laboratory for Information Science and Technology(TNList), Department of Automation, Tsinghua University, Beijing 100084, China; Baidu Inc., Beijing, China+Department of Statistics, Rutgers University, NJ, USA; Tsinghua National Laboratory for Information Science and Technology(TNList), Department of Automation, Tsinghua University, Beijing 100084, China",
        "aff_domain": "mails.tsinghua.edu.cn;stat.rutgers.edu;tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn;stat.rutgers.edu;tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2;0",
        "aff_unique_norm": "Tsinghua University;Baidu;Rutgers University",
        "aff_unique_dep": "Department of Automation;Baidu Inc.;Department of Statistics",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.baidu.com;https://www.rutgers.edu",
        "aff_unique_abbr": "Tsinghua;Baidu;Rutgers",
        "aff_campus_unique_index": "0;0+1;0",
        "aff_campus_unique": "Beijing;New Brunswick",
        "aff_country_unique_index": "0;0+1;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "f999497fb5",
        "title": "A Deep Semi-NMF Model for Learning Hidden Representations",
        "site": "https://proceedings.mlr.press/v32/trigeorgis14.html",
        "author": "George Trigeorgis; Konstantinos Bousmalis; Stefanos Zafeiriou; Bjoern Schuller",
        "abstract": "Semi-NMF is a matrix factorization technique that learns a low-dimensional representation of a dataset that lends itself to a clustering interpretation. It is possible that the mapping between this new representation and our original features contains rather complex hierarchical information with implicit lower-level hidden attributes, that classical one level clustering methodologies can not interpret. In this work we propose a novel model, Deep Semi-NMF, that is able to learn such hidden representations that allow themselves to an interpretation of clustering  according to different, unknown attributes of a given dataset. We show that by doing so, our model is able to learn low-dimensional representations that are better suited for clustering, outperforming Semi-NMF, but also other NMF variants.",
        "bibtex": "@InProceedings{pmlr-v32-trigeorgis14,\n  title = \t {A Deep Semi-NMF Model for Learning Hidden Representations},\n  author = \t {Trigeorgis, George and Bousmalis, Konstantinos and Zafeiriou, Stefanos and Schuller, Bjoern},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1692--1700},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/trigeorgis14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/trigeorgis14.html},\n  abstract = \t {Semi-NMF is a matrix factorization technique that learns a low-dimensional representation of a dataset that lends itself to a clustering interpretation. It is possible that the mapping between this new representation and our original features contains rather complex hierarchical information with implicit lower-level hidden attributes, that classical one level clustering methodologies can not interpret. In this work we propose a novel model, Deep Semi-NMF, that is able to learn such hidden representations that allow themselves to an interpretation of clustering  according to different, unknown attributes of a given dataset. We show that by doing so, our model is able to learn low-dimensional representations that are better suited for clustering, outperforming Semi-NMF, but also other NMF variants.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/trigeorgis14.pdf",
        "supp": "",
        "pdf_size": 1270190,
        "gs_citation": 241,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8314841548122995996&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computing, Imperial College London, United Kingdom; Department of Computing, Imperial College London, United Kingdom; Department of Computing, Imperial College London, United Kingdom; Department of Computing, Imperial College London, United Kingdom",
        "aff_domain": "IMPERIAL.AC.UK;IMPERIAL.AC.UK;IMPERIAL.AC.UK;IMPERIAL.AC.UK",
        "email": "IMPERIAL.AC.UK;IMPERIAL.AC.UK;IMPERIAL.AC.UK;IMPERIAL.AC.UK",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Imperial College London",
        "aff_unique_dep": "Department of Computing",
        "aff_unique_url": "https://www.imperial.ac.uk",
        "aff_unique_abbr": "Imperial",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "c40ee0c5c9",
        "title": "A Deep and Tractable Density Estimator",
        "site": "https://proceedings.mlr.press/v32/uria14.html",
        "author": "Benigno Uria; Iain Murray; Hugo Larochelle",
        "abstract": "The Neural Autoregressive Distribution Estimator (NADE) and its real-valued version RNADE are competitive density models of multidimensional data across a variety of domains. These models use a fixed, arbitrary ordering of the data  dimensions. One can easily condition on variables at the beginning of the ordering, and marginalize out variables at the end of the ordering, however other inference tasks require approximate inference. In this work we introduce an efficient procedure to simultaneously train a NADE model for each possible ordering of the variables, by sharing parameters across all these models. We can thus use the most convenient model for each inference task at hand, and ensembles of such models with different orderings are immediately available. Moreover, unlike the original NADE, our training procedure scales to deep models. Empirically, ensembles of Deep NADE models obtain state of the art density estimation performance.",
        "bibtex": "@InProceedings{pmlr-v32-uria14,\n  title = \t {A Deep and Tractable Density Estimator},\n  author = \t {Uria, Benigno and Murray, Iain and Larochelle, Hugo},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {467--475},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/uria14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/uria14.html},\n  abstract = \t {The Neural Autoregressive Distribution Estimator (NADE) and its real-valued version RNADE are competitive density models of multidimensional data across a variety of domains. These models use a fixed, arbitrary ordering of the data  dimensions. One can easily condition on variables at the beginning of the ordering, and marginalize out variables at the end of the ordering, however other inference tasks require approximate inference. In this work we introduce an efficient procedure to simultaneously train a NADE model for each possible ordering of the variables, by sharing parameters across all these models. We can thus use the most convenient model for each inference task at hand, and ensembles of such models with different orderings are immediately available. Moreover, unlike the original NADE, our training procedure scales to deep models. Empirically, ensembles of Deep NADE models obtain state of the art density estimation performance.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/uria14.pdf",
        "supp": "",
        "pdf_size": 633789,
        "gs_citation": 213,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8034738930540433063&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "School of Informatics, University of Edinburgh; School of Informatics, University of Edinburgh; D\u00b4epartement d\u2019informatique, Universit \u00b4e de Sherbrooke",
        "aff_domain": "ED.AC.UK;ED.AC.UK;USHERBROOKE.CA",
        "email": "ED.AC.UK;ED.AC.UK;USHERBROOKE.CA",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Edinburgh;Universit u00e9 de Sherbrooke",
        "aff_unique_dep": "School of Informatics;D u00e9partement d\u2019informatique",
        "aff_unique_url": "https://www.ed.ac.uk;https://www.usherbrooke.ca",
        "aff_unique_abbr": "Edinburgh;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Edinburgh;",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United Kingdom;Canada"
    },
    {
        "id": "f05b3e3417",
        "title": "A Discriminative Latent Variable Model for Online Clustering",
        "site": "https://proceedings.mlr.press/v32/samdani14.html",
        "author": "Rajhans Samdani; Kai-Wei Chang; Dan Roth",
        "abstract": "This paper presents a latent variable structured prediction model for discriminative supervised clustering of items called the Latent Left-linking Model (L3M). We present an online clustering algorithm for L3M based on a feature-based item similarity function. We provide a learning framework for estimating the similarity function and present a fast stochastic gradient-based learning technique. In our experiments on coreference resolution and document clustering, L3 M outperforms several existing online as well as batch supervised clustering techniques.",
        "bibtex": "@InProceedings{pmlr-v32-samdani14,\n  title = \t {A Discriminative Latent Variable Model for Online Clustering},\n  author = \t {Samdani, Rajhans and Chang, Kai-Wei and Roth, Dan},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1--9},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/samdani14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/samdani14.html},\n  abstract = \t {This paper presents a latent variable structured prediction model for discriminative supervised clustering of items called the Latent Left-linking Model (L3M). We present an online clustering algorithm for L3M based on a feature-based item similarity function. We provide a learning framework for estimating the similarity function and present a fast stochastic gradient-based learning technique. In our experiments on coreference resolution and document clustering, L3 M outperforms several existing online as well as batch supervised clustering techniques.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/samdani14.pdf",
        "supp": "",
        "pdf_size": 346120,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13642082475690876441&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Google Research; University of Illinois; University of Illinois",
        "aff_domain": "GOOGLE.COM;ILLINOIS.EDU;ILLINOIS.EDU",
        "email": "GOOGLE.COM;ILLINOIS.EDU;ILLINOIS.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Google;University of Illinois",
        "aff_unique_dep": "Google Research;",
        "aff_unique_url": "https://research.google;https://www.illinois.edu",
        "aff_unique_abbr": "Google Research;UIUC",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c0e5cebc89",
        "title": "A Divide-and-Conquer Solver for Kernel Support Vector Machines",
        "site": "https://proceedings.mlr.press/v32/hsieha14.html",
        "author": "Cho-Jui Hsieh; Si Si; Inderjit Dhillon",
        "abstract": "The kernel support vector machine (SVM) is one of the most widely used classification methods; however, the amount of computation required becomes the bottleneck when facing millions of samples. In this paper, we propose and analyze a novel divide-and-conquer solver for kernel SVMs (DC-SVM). In the division step, we partition the kernel SVM problem into smaller subproblems by clustering the data, so that each subproblem can be solved independently and efficiently. We show theoretically that the support vectors identified by the subproblem solution are likely to be support vectors of the entire kernel SVM problem, provided that the problem is partitioned appropriately by kernel clustering. In the conquer step, the local solutions from the subproblems are used to initialize a global coordinate descent solver, which converges quickly as suggested by our analysis. By extending this idea, we develop a multilevel Divide-and-Conquer SVM algorithm with adaptive clustering and early prediction strategy, which outperforms state-of-the-art methods in terms of training speed, testing accuracy, and memory usage. As an example, on the covtype dataset with half-a-million samples, DC-SVM is 7 times faster than LIBSVM in obtaining the exact SVM solution (to within 10^-6 relative error) which achieves 96.15% prediction accuracy. Moreover, with our proposed early prediction strategy, DC-SVM achieves about 96% accuracy in only 12 minutes, which is more than 100 times faster than LIBSVM.",
        "bibtex": "@InProceedings{pmlr-v32-hsieha14,\n  title = \t {A Divide-and-Conquer Solver for Kernel Support Vector Machines},\n  author = \t {Hsieh, Cho-Jui and Si, Si and Dhillon, Inderjit},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {566--574},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/hsieha14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/hsieha14.html},\n  abstract = \t {The kernel support vector machine (SVM) is one of the most widely used classification methods; however, the amount of computation required becomes the bottleneck when facing millions of samples. In this paper, we propose and analyze a novel divide-and-conquer solver for kernel SVMs (DC-SVM). In the division step, we partition the kernel SVM problem into smaller subproblems by clustering the data, so that each subproblem can be solved independently and efficiently. We show theoretically that the support vectors identified by the subproblem solution are likely to be support vectors of the entire kernel SVM problem, provided that the problem is partitioned appropriately by kernel clustering. In the conquer step, the local solutions from the subproblems are used to initialize a global coordinate descent solver, which converges quickly as suggested by our analysis. By extending this idea, we develop a multilevel Divide-and-Conquer SVM algorithm with adaptive clustering and early prediction strategy, which outperforms state-of-the-art methods in terms of training speed, testing accuracy, and memory usage. As an example, on the covtype dataset with half-a-million samples, DC-SVM is 7 times faster than LIBSVM in obtaining the exact SVM solution (to within 10^-6 relative error) which achieves 96.15% prediction accuracy. Moreover, with our proposed early prediction strategy, DC-SVM achieves about 96% accuracy in only 12 minutes, which is more than 100 times faster than LIBSVM.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/hsieha14.pdf",
        "supp": "",
        "pdf_size": 334545,
        "gs_citation": 223,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17649357951714509227&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, The University of Texas, Austin, TX 78721, USA; Department of Computer Science, The University of Texas, Austin, TX 78721, USA; Department of Computer Science, The University of Texas, Austin, TX 78721, USA",
        "aff_domain": "CS.UTEXAS.EDU;CS.UTEXAS.EDU;CS.UTEXAS.EDU",
        "email": "CS.UTEXAS.EDU;CS.UTEXAS.EDU;CS.UTEXAS.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "323fdf4f79",
        "title": "A Highly Scalable Parallel Algorithm for Isotropic Total Variation Models",
        "site": "https://proceedings.mlr.press/v32/wangb14.html",
        "author": "Jie Wang; Qingyang Li; Sen Yang; Wei Fan; Peter Wonka; Jieping Ye",
        "abstract": "Total variation (TV) models are among the most popular and successful tools in signal processing. However, due to the complex nature of the TV term, it is challenging to efficiently compute a solution for large-scale problems. State-of-the-art algorithms that are based on the alternating direction method of multipliers (ADMM)  often involve solving large-size linear systems. In this paper, we propose a highly scalable parallel algorithm for TV models that is based on a novel decomposition strategy of the problem domain. As a result, the TV models can be decoupled into a set of small and independent subproblems, which admit closed form solutions. This makes our approach particularly suitable for parallel implementation. Our algorithm is guaranteed to converge to its global minimum. With N variables and n_p processes, the time complexity is O(N/(\u03b5n_p)) to reach an epsilon-optimal solution. Extensive experiments demonstrate that our approach outperforms existing state-of-the-art algorithms, especially in dealing with high-resolution, mega-size images.",
        "bibtex": "@InProceedings{pmlr-v32-wangb14,\n  title = \t {A Highly Scalable Parallel Algorithm for Isotropic Total Variation Models},\n  author = \t {Wang, Jie and Li, Qingyang and Yang, Sen and Fan, Wei and Wonka, Peter and Ye, Jieping},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {235--243},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/wangb14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/wangb14.html},\n  abstract = \t {Total variation (TV) models are among the most popular and successful tools in signal processing. However, due to the complex nature of the TV term, it is challenging to efficiently compute a solution for large-scale problems. State-of-the-art algorithms that are based on the alternating direction method of multipliers (ADMM)  often involve solving large-size linear systems. In this paper, we propose a highly scalable parallel algorithm for TV models that is based on a novel decomposition strategy of the problem domain. As a result, the TV models can be decoupled into a set of small and independent subproblems, which admit closed form solutions. This makes our approach particularly suitable for parallel implementation. Our algorithm is guaranteed to converge to its global minimum. With N variables and n_p processes, the time complexity is O(N/(\u03b5n_p)) to reach an epsilon-optimal solution. Extensive experiments demonstrate that our approach outperforms existing state-of-the-art algorithms, especially in dealing with high-resolution, mega-size images.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/wangb14.pdf",
        "supp": "",
        "pdf_size": 1618914,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2509486549108238357&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Arizona State University; Arizona State University; Arizona State University; Huawei Noahs Ark Lab; King Abdullah University of Science and Technology + Arizona State University; Arizona State University",
        "aff_domain": "asu.edu;asu.edu;asu.edu;huawei.com;gmail.com;asu.edu",
        "email": "asu.edu;asu.edu;asu.edu;huawei.com;gmail.com;asu.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;2+0;0",
        "aff_unique_norm": "Arizona State University;Huawei;King Abdullah University of Science and Technology",
        "aff_unique_dep": ";Noahs Ark Lab;",
        "aff_unique_url": "https://www.asu.edu;https://www.huawei.com;https://www.kast.kau.edu.sa",
        "aff_unique_abbr": "ASU;Huawei;KAUST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;2+0;0",
        "aff_country_unique": "United States;China;Saudi Arabia"
    },
    {
        "id": "7b0a4bb1e0",
        "title": "A Kernel Independence Test for Random Processes",
        "site": "https://proceedings.mlr.press/v32/chwialkowski14.html",
        "author": "Kacper Chwialkowski; Arthur Gretton",
        "abstract": "A non-parametric approach to the problem of testing the independence of two random processes is developed.  The test statistic is the Hilbert-Schmidt Independence Criterion (HSIC), which was used previously in testing independence for i.i.d. pairs of variables. The asymptotic behaviour of HSIC is established when computed from samples drawn from random processes. It is shown that earlier bootstrap procedures which worked in the i.i.d. case will fail for random processes, and an alternative consistent estimate of the p-values is proposed. Tests on artificial data and real-world forex data indicate that the new test procedure discovers dependence which is missed by linear approaches, while the earlier bootstrap procedure returns an elevated number of false positives.",
        "bibtex": "@InProceedings{pmlr-v32-chwialkowski14,\n  title = \t {A Kernel Independence Test for Random Processes},\n  author = \t {Chwialkowski, Kacper and Gretton, Arthur},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1422--1430},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/chwialkowski14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/chwialkowski14.html},\n  abstract = \t {A non-parametric approach to the problem of testing the independence of two random processes is developed.  The test statistic is the Hilbert-Schmidt Independence Criterion (HSIC), which was used previously in testing independence for i.i.d. pairs of variables. The asymptotic behaviour of HSIC is established when computed from samples drawn from random processes. It is shown that earlier bootstrap procedures which worked in the i.i.d. case will fail for random processes, and an alternative consistent estimate of the p-values is proposed. Tests on artificial data and real-world forex data indicate that the new test procedure discovers dependence which is missed by linear approaches, while the earlier bootstrap procedure returns an elevated number of false positives.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/chwialkowski14.pdf",
        "supp": "",
        "pdf_size": 640244,
        "gs_citation": 89,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6992339197033090311&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "University College London, Computer Science Department; University College London, Gatsby Computational Neuroscience Unit",
        "aff_domain": "gmail.com;gmail.com",
        "email": "gmail.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University College London",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.ucl.ac.uk",
        "aff_unique_abbr": "UCL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "e7738ecafd",
        "title": "A PAC-Bayesian bound for Lifelong Learning",
        "site": "https://proceedings.mlr.press/v32/pentina14.html",
        "author": "Anastasia Pentina; Christoph Lampert",
        "abstract": "Transfer learning has received a lot of attention in the machine learning community over the last years, and several effective algorithms have been developed. However, relatively little is known about their theoretical properties, especially in the setting of lifelong learning, where the goal is to transfer information to tasks for which no data have been observed so far.     In this work we study lifelong learning from a theoretical perspective. Our main result is a PAC-Bayesian generalization bound that offers a unified view on existing paradigms for transfer learning, such as the transfer of parameters or the transfer of low-dimensional representations. We also use the bound to derive two principled lifelong learning algorithms, and we show that these yield results comparable with existing methods.",
        "bibtex": "@InProceedings{pmlr-v32-pentina14,\n  title = \t {A PAC-Bayesian bound for Lifelong Learning},\n  author = \t {Pentina, Anastasia and Lampert, Christoph},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {991--999},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/pentina14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/pentina14.html},\n  abstract = \t {Transfer learning has received a lot of attention in the machine learning community over the last years, and several effective algorithms have been developed. However, relatively little is known about their theoretical properties, especially in the setting of lifelong learning, where the goal is to transfer information to tasks for which no data have been observed so far.     In this work we study lifelong learning from a theoretical perspective. Our main result is a PAC-Bayesian generalization bound that offers a unified view on existing paradigms for transfer learning, such as the transfer of parameters or the transfer of low-dimensional representations. We also use the bound to derive two principled lifelong learning algorithms, and we show that these yield results comparable with existing methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/pentina14.pdf",
        "supp": "",
        "pdf_size": 682044,
        "gs_citation": 239,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5407951846097465959&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 13,
        "aff": "IST Austria (Institute of Science and Technology Austria); IST Austria (Institute of Science and Technology Austria)",
        "aff_domain": "IST.AC.AT;IST.AC.AT",
        "email": "IST.AC.AT;IST.AC.AT",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Institute of Science and Technology Austria",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ist.ac.at",
        "aff_unique_abbr": "IST Austria",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Austria"
    },
    {
        "id": "c938db8bfb",
        "title": "A Physics-Based Model Prior for Object-Oriented MDPs",
        "site": "https://proceedings.mlr.press/v32/scholz14.html",
        "author": "Jonathan Scholz; Martin Levihn; Charles Isbell; David Wingate",
        "abstract": "One of the key challenges in using reinforcement learning in robotics is the need for models that capture natural world structure. There are, methods that formalize multi-object dynamics using relational representations, but these methods are not sufficiently compact for  real-world robotics. We present a physics-based approach that exploits modern simulation tools to efficiently parameterize physical dynamics.  Our results show that this representation can result in much faster learning, by virtue of its strong but appropriate inductive bias in  physical environments.",
        "bibtex": "@InProceedings{pmlr-v32-scholz14,\n  title = \t {A Physics-Based Model Prior for Object-Oriented MDPs},\n  author = \t {Scholz, Jonathan and Levihn, Martin and Isbell, Charles and Wingate, David},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1089--1097},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/scholz14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/scholz14.html},\n  abstract = \t {One of the key challenges in using reinforcement learning in robotics is the need for models that capture natural world structure. There are, methods that formalize multi-object dynamics using relational representations, but these methods are not sufficiently compact for  real-world robotics. We present a physics-based approach that exploits modern simulation tools to efficiently parameterize physical dynamics.  Our results show that this representation can result in much faster learning, by virtue of its strong but appropriate inductive bias in  physical environments.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/scholz14.pdf",
        "supp": "",
        "pdf_size": 1541959,
        "gs_citation": 79,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6218042605355830778&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 4,
        "aff": "Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology; Massachusetts Institute of Technology",
        "aff_domain": "GATECH.EDU;GATECH.EDU;GATECH.EDU;MIT.EDU",
        "email": "GATECH.EDU;GATECH.EDU;GATECH.EDU;MIT.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Georgia Institute of Technology;Massachusetts Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.gatech.edu;https://web.mit.edu",
        "aff_unique_abbr": "Georgia Tech;MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "691a83cffd",
        "title": "A Single-Pass Algorithm for Efficiently Recovering Sparse Cluster Centers of High-dimensional Data",
        "site": "https://proceedings.mlr.press/v32/yib14.html",
        "author": "Jinfeng Yi; Lijun Zhang; Jun Wang; Rong Jin; Anil Jain",
        "abstract": "Learning a statistical model for high-dimensional data is an important topic in machine learning. Although this problem has been well studied in the supervised setting, little is known about its unsupervised counterpart. In this work, we focus on the problem of clustering high-dimensional data with sparse centers.  In particular, we address the following open question in unsupervised learning: \u201cis it possible to reliably cluster high-dimensional data when the number of samples is smaller than the data dimensionality?\" We develop an efficient clustering algorithm that is able to estimate sparse cluster centers with a single pass over the data. Our theoretical analysis shows that the proposed algorithm is able to accurately recover cluster centers with only O(s\\log d) number of samples (data points), provided all the cluster centers are s-sparse vectors in a d dimensional space. Experimental results verify both the effectiveness and efficiency of the proposed clustering algorithm compared to the state-of-the-art algorithms on several benchmark datasets.",
        "bibtex": "@InProceedings{pmlr-v32-yib14,\n  title = \t {A Single-Pass Algorithm for Efficiently Recovering Sparse Cluster Centers of High-dimensional Data},\n  author = \t {Yi, Jinfeng and Zhang, Lijun and Wang, Jun and Jin, Rong and Jain, Anil},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {658--666},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/yib14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/yib14.html},\n  abstract = \t {Learning a statistical model for high-dimensional data is an important topic in machine learning. Although this problem has been well studied in the supervised setting, little is known about its unsupervised counterpart. In this work, we focus on the problem of clustering high-dimensional data with sparse centers.  In particular, we address the following open question in unsupervised learning: \u201cis it possible to reliably cluster high-dimensional data when the number of samples is smaller than the data dimensionality?\" We develop an efficient clustering algorithm that is able to estimate sparse cluster centers with a single pass over the data. Our theoretical analysis shows that the proposed algorithm is able to accurately recover cluster centers with only O(s\\log d) number of samples (data points), provided all the cluster centers are s-sparse vectors in a d dimensional space. Experimental results verify both the effectiveness and efficiency of the proposed clustering algorithm compared to the state-of-the-art algorithms on several benchmark datasets.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/yib14.pdf",
        "supp": "",
        "pdf_size": 349836,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13785143606465953639&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "IBM Thomas J. Watson Research Center, Yorktown Heights, NY 10598, USA; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China; IBM Thomas J. Watson Research Center, Yorktown Heights, NY 10598, USA; Department of Computer Science and Engineering, Michigan State University, East Lansing, MI 48824 USA; Department of Computer Science and Engineering, Michigan State University, East Lansing, MI 48824 USA",
        "aff_domain": "US.IBM.COM;LAMDA.NJU.EDU.CN;US.IBM.COM;CSE.MSU.EDU;CSE.MSU.EDU",
        "email": "US.IBM.COM;LAMDA.NJU.EDU.CN;US.IBM.COM;CSE.MSU.EDU;CSE.MSU.EDU",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;2;2",
        "aff_unique_norm": "IBM;Nanjing University;Michigan State University",
        "aff_unique_dep": "IBM Thomas J. Watson Research Center;National Key Laboratory for Novel Software Technology;Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.ibm.com/research/watson;http://www.nju.edu.cn;https://www.msu.edu",
        "aff_unique_abbr": "IBM Watson;Nanjing U;MSU",
        "aff_campus_unique_index": "0;1;0;2;2",
        "aff_campus_unique": "Yorktown Heights;Nanjing;East Lansing",
        "aff_country_unique_index": "0;1;0;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "aafc94133c",
        "title": "A Statistical Convergence Perspective of Algorithms for Rank Aggregation from Pairwise Data",
        "site": "https://proceedings.mlr.press/v32/rajkumar14.html",
        "author": "Arun Rajkumar; Shivani Agarwal",
        "abstract": "There has been much interest recently in the problem of rank aggregation from pairwise data. A natural question that arises is: under what sorts of statistical assumptions do various rank aggregation algorithms converge to an \u2018optimal\u2019 ranking? In this paper, we consider this question in a natural setting where pairwise comparisons are drawn randomly and independently from some underlying probability distribution. We first show that, under a \u2018time-reversibility\u2019 or Bradley-Terry-Luce (BTL) condition on the distribution generating the outcomes of the pairwise comparisons, the rank centrality (PageRank) and least squares (HodgeRank) algorithms both converge to an optimal ranking. Next, we show that a matrix version of the Borda count algorithm, and more surprisingly, an algorithm which performs maximal likelihood estimation under a BTL assumption, both converge to an optimal ranking under a \u2018low-noise\u2019 condition that is strictly more general than BTL. Finally, we propose a new SVM-based algorithm for rank aggregation from pairwise data, and show that this converges to an optimal ranking under an even more general condition that we term \u2018generalized low-noise\u2019. In all cases, we provide explicit sample complexity bounds for exact recovery of an optimal ranking. Our experiments confirm our theoretical findings and help to shed light on the statistical behavior of various rank aggregation algorithms.",
        "bibtex": "@InProceedings{pmlr-v32-rajkumar14,\n  title = \t {A Statistical Convergence Perspective of Algorithms for Rank Aggregation from Pairwise Data},\n  author = \t {Rajkumar, Arun and Agarwal, Shivani},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {118--126},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/rajkumar14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/rajkumar14.html},\n  abstract = \t {There has been much interest recently in the problem of rank aggregation from pairwise data. A natural question that arises is: under what sorts of statistical assumptions do various rank aggregation algorithms converge to an \u2018optimal\u2019 ranking? In this paper, we consider this question in a natural setting where pairwise comparisons are drawn randomly and independently from some underlying probability distribution. We first show that, under a \u2018time-reversibility\u2019 or Bradley-Terry-Luce (BTL) condition on the distribution generating the outcomes of the pairwise comparisons, the rank centrality (PageRank) and least squares (HodgeRank) algorithms both converge to an optimal ranking. Next, we show that a matrix version of the Borda count algorithm, and more surprisingly, an algorithm which performs maximal likelihood estimation under a BTL assumption, both converge to an optimal ranking under a \u2018low-noise\u2019 condition that is strictly more general than BTL. Finally, we propose a new SVM-based algorithm for rank aggregation from pairwise data, and show that this converges to an optimal ranking under an even more general condition that we term \u2018generalized low-noise\u2019. In all cases, we provide explicit sample complexity bounds for exact recovery of an optimal ranking. Our experiments confirm our theoretical findings and help to shed light on the statistical behavior of various rank aggregation algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/rajkumar14.pdf",
        "supp": "",
        "pdf_size": 472767,
        "gs_citation": 202,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11644890105542872364&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Indian Institute of Science, Bangalore 560012, INDIA; Indian Institute of Science, Bangalore 560012, INDIA",
        "aff_domain": "CSA.IISC.ERNET.IN;CSA.IISC.ERNET.IN",
        "email": "CSA.IISC.ERNET.IN;CSA.IISC.ERNET.IN",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indian Institute of Science",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iisc.ac.in",
        "aff_unique_abbr": "IISc",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Bangalore",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "bcae721027",
        "title": "A Statistical Perspective on Algorithmic Leveraging",
        "site": "https://proceedings.mlr.press/v32/ma14.html",
        "author": "Ping Ma; Michael Mahoney; Bin Yu",
        "abstract": "One popular method for dealing with large-scale data sets is sampling. Using the empirical statistical leverage scores as an importance sampling distribution, the method of algorithmic leveraging samples and rescales rows/columns of data matrices to reduce the data size before performing computations on the subproblem. Existing work has focused on algorithmic issues, but none of it addresses statistical aspects of this method.  Here, we provide an effective framework to evaluate the statistical properties of algorithmic leveraging in the context of estimating parameters in a linear regression model.   In particular, for several versions of leverage-based sampling, we derive results for the bias and variance, both conditional and unconditional on the observed data. We show that from the statistical perspective of bias and variance, neither leverage-based sampling nor uniform sampling dominates the other. This result is particularly striking, given the well-known result that, from the algorithmic perspective of worst-case analysis, leverage-based sampling provides uniformly superior worst-case algorithmic results, when compared with uniform sampling. Based on these theoretical results, we propose and analyze two new leveraging algorithms: one constructs a smaller least-squares problem with \u201cshrinked\u201d leverage scores (SLEV), and the other solves a smaller and unweighted (or biased) least-squares problem (LEVUNW). The empirical results indicate that our theory is a good predictor of practical performance of existing and new leverage-based algorithms and that the new algorithms achieve improved performance.",
        "bibtex": "@InProceedings{pmlr-v32-ma14,\n  title = \t {A Statistical Perspective on Algorithmic Leveraging},\n  author = \t {Ma, Ping and Mahoney, Michael and Yu, Bin},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {91--99},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/ma14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/ma14.html},\n  abstract = \t {One popular method for dealing with large-scale data sets is sampling. Using the empirical statistical leverage scores as an importance sampling distribution, the method of algorithmic leveraging samples and rescales rows/columns of data matrices to reduce the data size before performing computations on the subproblem. Existing work has focused on algorithmic issues, but none of it addresses statistical aspects of this method.  Here, we provide an effective framework to evaluate the statistical properties of algorithmic leveraging in the context of estimating parameters in a linear regression model.   In particular, for several versions of leverage-based sampling, we derive results for the bias and variance, both conditional and unconditional on the observed data. We show that from the statistical perspective of bias and variance, neither leverage-based sampling nor uniform sampling dominates the other. This result is particularly striking, given the well-known result that, from the algorithmic perspective of worst-case analysis, leverage-based sampling provides uniformly superior worst-case algorithmic results, when compared with uniform sampling. Based on these theoretical results, we propose and analyze two new leveraging algorithms: one constructs a smaller least-squares problem with \u201cshrinked\u201d leverage scores (SLEV), and the other solves a smaller and unweighted (or biased) least-squares problem (LEVUNW). The empirical results indicate that our theory is a good predictor of practical performance of existing and new leverage-based algorithms and that the new algorithms achieve improved performance.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/ma14.pdf",
        "supp": "",
        "pdf_size": 362468,
        "gs_citation": 457,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15760958566514078228&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "Department of Statistics, University of Georgia; International Computer Science Institute and Dept. of Statistics, University of California at Berkeley; Departments of Statistics and EECS, University of California at Berkeley",
        "aff_domain": "uga.edu;icsi.berkeley.edu;stat.berkeley.edu",
        "email": "uga.edu;icsi.berkeley.edu;stat.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Georgia;University of California, Berkeley",
        "aff_unique_dep": "Department of Statistics;Dept. of Statistics",
        "aff_unique_url": "https://www.uga.edu;https://www.berkeley.edu",
        "aff_unique_abbr": "UGA;UC Berkeley",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ab10e045ab",
        "title": "A Unified Framework for Consistency of Regularized Loss Minimizers",
        "site": "https://proceedings.mlr.press/v32/honorio14.html",
        "author": "Jean Honorio; Tommi Jaakkola",
        "abstract": "We characterize a family of regularized loss minimization problems that satisfy three properties: scaled uniform convergence, super-norm regularization, and norm-loss monotonicity. We show several theoretical guarantees within this framework, including loss consistency, norm consistency, sparsistency (i.e. support recovery) as well as sign consistency. A number of regularization problems can be shown to fall within our framework and we provide several examples. Our results can be seen as a concise summary of existing guarantees but we also extend them to new settings. Our formulation enables us to assume very little about the hypothesis class, data distribution, the loss, or the regularization. In particular, many of our results do not require a bounded hypothesis class, or identically distributed samples. Similarly, we do not assume boundedness, convexity or smoothness of the loss nor the regularizer. We only assume approximate optimality of the empirical minimizer. In terms of recovery, in contrast to existing results, our sparsistency and sign consistency results do not require knowledge of the sub-differential of the objective function.",
        "bibtex": "@InProceedings{pmlr-v32-honorio14,\n  title = \t {A Unified Framework for Consistency of Regularized Loss Minimizers},\n  author = \t {Honorio, Jean and Jaakkola, Tommi},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {136--144},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/honorio14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/honorio14.html},\n  abstract = \t {We characterize a family of regularized loss minimization problems that satisfy three properties: scaled uniform convergence, super-norm regularization, and norm-loss monotonicity. We show several theoretical guarantees within this framework, including loss consistency, norm consistency, sparsistency (i.e. support recovery) as well as sign consistency. A number of regularization problems can be shown to fall within our framework and we provide several examples. Our results can be seen as a concise summary of existing guarantees but we also extend them to new settings. Our formulation enables us to assume very little about the hypothesis class, data distribution, the loss, or the regularization. In particular, many of our results do not require a bounded hypothesis class, or identically distributed samples. Similarly, we do not assume boundedness, convexity or smoothness of the loss nor the regularizer. We only assume approximate optimality of the empirical minimizer. In terms of recovery, in contrast to existing results, our sparsistency and sign consistency results do not require knowledge of the sub-differential of the objective function.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/honorio14.pdf",
        "supp": "",
        "pdf_size": 509048,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10714245010296334968&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "CSAIL, MIT, Cambridge, MA 02139, USA; CSAIL, MIT, Cambridge, MA 02139, USA",
        "aff_domain": "CSAIL.MIT.EDU;CSAIL.MIT.EDU",
        "email": "CSAIL.MIT.EDU;CSAIL.MIT.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.csail.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9170592a10",
        "title": "A Unifying View of Representer Theorems",
        "site": "https://proceedings.mlr.press/v32/argyriou14.html",
        "author": "Andreas Argyriou; Francesco Dinuzzo",
        "abstract": "It is known that the solution of regularization and interpolation problems with Hilbertian penalties can be expressed as a linear combination of the data. This very useful property, called the representer theorem, has been widely studied and applied to machine learning problems. Analogous optimality conditions have appeared in other contexts, notably in matrix regularization.  In this paper we propose a unified view, which generalizes the concept of representer theorems and extends necessary and sufficient conditions for such theorems to hold. Our main result shows a close connection between representer theorems and certain classes of regularization penalties, which we call orthomonotone functions.  This result not only subsumes previous representer theorems as  special cases but also yields a new class of optimality conditions, which goes beyond the classical linear combination of the data.  Moreover, orthomonotonicity   provides a useful criterion for testing whether a representer theorem  holds for a specific regularization problem.",
        "bibtex": "@InProceedings{pmlr-v32-argyriou14,\n  title = \t {A Unifying View of Representer Theorems},\n  author = \t {Argyriou, Andreas and Dinuzzo, Francesco},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {748--756},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/argyriou14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/argyriou14.html},\n  abstract = \t {It is known that the solution of regularization and interpolation problems with Hilbertian penalties can be expressed as a linear combination of the data. This very useful property, called the representer theorem, has been widely studied and applied to machine learning problems. Analogous optimality conditions have appeared in other contexts, notably in matrix regularization.  In this paper we propose a unified view, which generalizes the concept of representer theorems and extends necessary and sufficient conditions for such theorems to hold. Our main result shows a close connection between representer theorems and certain classes of regularization penalties, which we call orthomonotone functions.  This result not only subsumes previous representer theorems as  special cases but also yields a new class of optimality conditions, which goes beyond the classical linear combination of the data.  Moreover, orthomonotonicity   provides a useful criterion for testing whether a representer theorem  holds for a specific regularization problem.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/argyriou14.pdf",
        "supp": "",
        "pdf_size": 168006,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=343045130348741035&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "\u00b4Ecole Centrale Paris, Center for Visual Computing; IBM Research, Dublin + Max Planck Institute for Intelligent Systems, T\u00a8ubingen",
        "aff_domain": "ecp.fr;ie.ibm.com",
        "email": "ecp.fr;ie.ibm.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2",
        "aff_unique_norm": "Ecole Centrale Paris;IBM;Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": "Center for Visual Computing;IBM Research;",
        "aff_unique_url": "https://www.ecp.fr;https://www.ibm.com/research;https://www.mpi-is.mpg.de",
        "aff_unique_abbr": "ECP;IBM;MPI-IS",
        "aff_campus_unique_index": "1+2",
        "aff_campus_unique": ";Dublin;T\u00fcbingen",
        "aff_country_unique_index": "0;1+2",
        "aff_country_unique": "France;Ireland;Germany"
    },
    {
        "id": "a01022f8d8",
        "title": "A new Q(lambda) with interim forward view and Monte Carlo equivalence",
        "site": "https://proceedings.mlr.press/v32/sutton14.html",
        "author": "Rich Sutton; Ashique Rupam Mahmood; Doina Precup; Hado Hasselt",
        "abstract": "Q-learning, the most popular of reinforcement learning algorithms, has always included an extension to eligibility traces to enable more rapid learning and improved asymptotic performance on non-Markov problems. The lambda parameter smoothly shifts on-policy algorithms such as TD(lambda) and Sarsa(lambda) from a pure bootstrapping form (lambda=0) to a pure Monte Carlo form (lambda=1). In off-policy algorithms, including Q(lambda), GQ(lambda), and off-policy LSTD(lambda), the lambda parameter is intended to play the same role, but does not; on every exploratory action these algorithms bootstrap regardless of the value of lambda, and as a result they fail to approximate Monte Carlo learning when lambda=1. It may seem that this is inevitable for any online off-policy algorithm; if updates are made on each step on which the target policy is followed, then how could just the right updates be \u2018un-made\u2019 upon deviation from the target policy? In this paper, we introduce a new version of Q(lambda) that does exactly that, without significantly increased algorithmic complexity. En route to our new Q(lambda), we introduce a new derivation technique based on the forward-view/backward-view analysis familiar from TD(lambda) but extended to apply at every time step rather than only at the end of episodes. We apply this technique to derive first a new off-policy version of TD(lambda), called PTD(lambda), and then our new Q(lambda), called PQ(lambda).",
        "bibtex": "@InProceedings{pmlr-v32-sutton14,\n  title = \t {A new Q(lambda) with interim forward view and Monte Carlo equivalence},\n  author = \t {Sutton, Rich and Mahmood, Ashique Rupam and Precup, Doina and Hasselt, Hado},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {568--576},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/sutton14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/sutton14.html},\n  abstract = \t {Q-learning, the most popular of reinforcement learning algorithms, has always included an extension to eligibility traces to enable more rapid learning and improved asymptotic performance on non-Markov problems. The lambda parameter smoothly shifts on-policy algorithms such as TD(lambda) and Sarsa(lambda) from a pure bootstrapping form (lambda=0) to a pure Monte Carlo form (lambda=1). In off-policy algorithms, including Q(lambda), GQ(lambda), and off-policy LSTD(lambda), the lambda parameter is intended to play the same role, but does not; on every exploratory action these algorithms bootstrap regardless of the value of lambda, and as a result they fail to approximate Monte Carlo learning when lambda=1. It may seem that this is inevitable for any online off-policy algorithm; if updates are made on each step on which the target policy is followed, then how could just the right updates be \u2018un-made\u2019 upon deviation from the target policy? In this paper, we introduce a new version of Q(lambda) that does exactly that, without significantly increased algorithmic complexity. En route to our new Q(lambda), we introduce a new derivation technique based on the forward-view/backward-view analysis familiar from TD(lambda) but extended to apply at every time step rather than only at the end of episodes. We apply this technique to derive first a new off-policy version of TD(lambda), called PTD(lambda), and then our new Q(lambda), called PQ(lambda).}\n}",
        "pdf": "http://proceedings.mlr.press/v32/sutton14.pdf",
        "supp": "",
        "pdf_size": 5326607,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16887275669316308906&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Reinforcement Learning and Arti\ufb01cial Intelligence Laboratory, University of Alberta, Edmonton, AB T6G 2E8 Canada; Reinforcement Learning and Arti\ufb01cial Intelligence Laboratory, University of Alberta, Edmonton, AB T6G 2E8 Canada; School of Computer Science, McGill University, Montr \u00b4eal, QC H3A 0G4 Canada; Reinforcement Learning and Arti\ufb01cial Intelligence Laboratory, University of Alberta, Edmonton, AB T6G 2E8 Canada",
        "aff_domain": "cs.ualberta.ca;cs.ualberta.ca;cs.mcgill.ca;cs.ualberta.ca",
        "email": "cs.ualberta.ca;cs.ualberta.ca;cs.mcgill.ca;cs.ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Alberta;McGill University",
        "aff_unique_dep": "Reinforcement Learning and Arti\ufb01cial Intelligence Laboratory;School of Computer Science",
        "aff_unique_url": "https://www.ualberta.ca;https://www.mcgill.ca",
        "aff_unique_abbr": "UAlberta;McGill",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Edmonton;Montr\u00e9al",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "acd051eb04",
        "title": "A reversible infinite HMM using normalised random measures",
        "site": "https://proceedings.mlr.press/v32/knowles14.html",
        "author": "David Knowles; Zoubin Ghahramani; Konstantina Palla",
        "abstract": "We present a nonparametric prior over reversible Markov chains. We use completely random measures, specifically gamma processes, to construct a countably infinite graph with weighted edges.  By enforcing symmetry to make the edges undirected we define a prior over random walks on graphs that results in a reversible Markov chain. The resulting prior over infinite transition matrices is closely related to the hierarchical Dirichlet process but enforces reversibility. A reinforcement scheme has recently been proposed with similar properties, but the de Finetti measure is not well characterised. We take the alternative approach of explicitly constructing the mixing measure, which allows more straightforward and efficient inference at the cost of no longer having a closed form predictive distribution. We use our process to construct a reversible infinite HMM which we apply to two real datasets, one from epigenomics and one ion channel recording.",
        "bibtex": "@InProceedings{pmlr-v32-knowles14,\n  title = \t {A reversible infinite HMM using normalised random measures},\n  author = \t {Knowles, David and Ghahramani, Zoubin and Palla, Konstantina},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1998--2006},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/knowles14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/knowles14.html},\n  abstract = \t {We present a nonparametric prior over reversible Markov chains. We use completely random measures, specifically gamma processes, to construct a countably infinite graph with weighted edges.  By enforcing symmetry to make the edges undirected we define a prior over random walks on graphs that results in a reversible Markov chain. The resulting prior over infinite transition matrices is closely related to the hierarchical Dirichlet process but enforces reversibility. A reinforcement scheme has recently been proposed with similar properties, but the de Finetti measure is not well characterised. We take the alternative approach of explicitly constructing the mixing measure, which allows more straightforward and efficient inference at the cost of no longer having a closed form predictive distribution. We use our process to construct a reversible infinite HMM which we apply to two real datasets, one from epigenomics and one ion channel recording.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/knowles14.pdf",
        "supp": "",
        "pdf_size": 463103,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3367584001554045851&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "University of Cambridge, Trumpington Street, CB2 1PZ; Stanford University, 353 Serra Mall, CA 94305-9025; University of Cambridge, Trumpington Street, CB2 1PZ",
        "aff_domain": "cam.ac.uk;cs.stanford.edu;eng.cam.ac.uk",
        "email": "cam.ac.uk;cs.stanford.edu;eng.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Cambridge;Stanford University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.stanford.edu",
        "aff_unique_abbr": "Cambridge;Stanford",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Cambridge;Stanford",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "9eaa2abb05",
        "title": "Accelerated Proximal Stochastic Dual Coordinate Ascent for Regularized Loss Minimization",
        "site": "https://proceedings.mlr.press/v32/shalev-shwartz14.html",
        "author": "Shai Shalev-Shwartz; Tong Zhang",
        "abstract": "We introduce a proximal version of the stochastic dual coordinate ascent method and show how to accelerate the method using an inner-outer iteration procedure. We analyze the runtime of the framework and obtain rates that improve state-of-the-art results for various key machine learning optimization problems including SVM,   logistic regression, ridge regression, Lasso, and multiclass SVM. Experiments validate our theoretical findings.",
        "bibtex": "@InProceedings{pmlr-v32-shalev-shwartz14,\n  title = \t {Accelerated Proximal Stochastic Dual Coordinate Ascent for Regularized Loss Minimization},\n  author = \t {Shalev-Shwartz, Shai and Zhang, Tong},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {64--72},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/shalev-shwartz14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/shalev-shwartz14.html},\n  abstract = \t {We introduce a proximal version of the stochastic dual coordinate ascent method and show how to accelerate the method using an inner-outer iteration procedure. We analyze the runtime of the framework and obtain rates that improve state-of-the-art results for various key machine learning optimization problems including SVM,   logistic regression, ridge regression, Lasso, and multiclass SVM. Experiments validate our theoretical findings.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/shalev-shwartz14.pdf",
        "supp": "",
        "pdf_size": 290549,
        "gs_citation": 523,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12521975291081883484&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "School of Computer Science and Engineering, The Hebrew University, Jerusalem, Israel; Department of Statistics, Rutgers University, NJ, USA + Baidu Inc., Beijing, China",
        "aff_domain": "CS.HUJI.AC.IL;RCI.RUTGERS.EDU",
        "email": "CS.HUJI.AC.IL;RCI.RUTGERS.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2",
        "aff_unique_norm": "Hebrew University;Rutgers University;Baidu",
        "aff_unique_dep": "School of Computer Science and Engineering;Department of Statistics;Baidu Inc.",
        "aff_unique_url": "http://www.huji.ac.il;https://www.rutgers.edu;https://www.baidu.com",
        "aff_unique_abbr": "HUJI;Rutgers;Baidu",
        "aff_campus_unique_index": "0;1+2",
        "aff_campus_unique": "Jerusalem;New Brunswick;Beijing",
        "aff_country_unique_index": "0;1+2",
        "aff_country_unique": "Israel;United States;China"
    },
    {
        "id": "7b983c0a48",
        "title": "Active Detection via Adaptive Submodularity",
        "site": "https://proceedings.mlr.press/v32/chena14.html",
        "author": "Yuxin Chen; Hiroaki Shioi; Cesar Fuentes Montesinos; Lian Pin Koh; Serge Wich; Andreas Krause",
        "abstract": "Efficient detection of multiple object instances is one of the fundamental challenges in computer vision. For certain object categories, even the best automatic systems are yet unable to produce high-quality detection results, and fully manual annotation would be an expensive process. How can detection algorithms interplay with human expert annotators? To make the best use of scarce (human) labeling resources, one needs to decide when to invoke the expert, such that the best possible performance can be achieved while requiring a minimum amount of supervision.   In this paper, we propose a principled approach to active object detection, and show that for a rich class of base detectors algorithms, one can derive a natural sequential decision problem for deciding when to invoke expert supervision. We further show that the objective function satisfies adaptive submodularity, which allows us to derive strong performance guarantees for our algorithm. We demonstrate the proposed algorithm on three real-world tasks, including a problem for biodiversity monitoring from micro UAVs in the Sumatra rain forest. Our results show that active detection not only outperforms its passive counterpart; for certain tasks, it also works significantly better than straightforward application of existing active learning techniques. To the best of our knowledge, our approach is the first to rigorously address the active detection problem from both empirical and theoretical perspectives.",
        "bibtex": "@InProceedings{pmlr-v32-chena14,\n  title = \t {Active Detection via Adaptive Submodularity},\n  author = \t {Chen, Yuxin and Shioi, Hiroaki and Montesinos, Cesar Fuentes and Koh, Lian Pin and Wich, Serge and Krause, Andreas},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {55--63},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/chena14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/chena14.html},\n  abstract = \t {Efficient detection of multiple object instances is one of the fundamental challenges in computer vision. For certain object categories, even the best automatic systems are yet unable to produce high-quality detection results, and fully manual annotation would be an expensive process. How can detection algorithms interplay with human expert annotators? To make the best use of scarce (human) labeling resources, one needs to decide when to invoke the expert, such that the best possible performance can be achieved while requiring a minimum amount of supervision.   In this paper, we propose a principled approach to active object detection, and show that for a rich class of base detectors algorithms, one can derive a natural sequential decision problem for deciding when to invoke expert supervision. We further show that the objective function satisfies adaptive submodularity, which allows us to derive strong performance guarantees for our algorithm. We demonstrate the proposed algorithm on three real-world tasks, including a problem for biodiversity monitoring from micro UAVs in the Sumatra rain forest. Our results show that active detection not only outperforms its passive counterpart; for certain tasks, it also works significantly better than straightforward application of existing active learning techniques. To the best of our knowledge, our approach is the first to rigorously address the active detection problem from both empirical and theoretical perspectives.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/chena14.pdf",
        "supp": "",
        "pdf_size": 2421875,
        "gs_citation": 66,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11540326347011666480&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "ETH Z\u00fcrich, Z\u00fcrich, Switzerland; ETH Z\u00fcrich, Z\u00fcrich, Switzerland + The University of Tokyo, Tokyo, Japan; ETH Z\u00fcrich, Z\u00fcrich, Switzerland; ETH Z\u00fcrich, Z\u00fcrich, Switzerland; Liverpool John Moores University, Liverpool, United Kingdom; ETH Z\u00fcrich, Z\u00fcrich, Switzerland",
        "aff_domain": "inf.ethz.ch;space.rcast.u-tokyo.ac.jp;student.ethz.ch;env.ethz.ch;yahoo.com;ethz.ch",
        "email": "inf.ethz.ch;space.rcast.u-tokyo.ac.jp;student.ethz.ch;env.ethz.ch;yahoo.com;ethz.ch",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;0;0;2;0",
        "aff_unique_norm": "ETH Zurich;University of Tokyo;Liverpool John Moores University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ethz.ch;https://www.u-tokyo.ac.jp;https://www.ljmu.ac.uk",
        "aff_unique_abbr": "ETHZ;UTokyo;LJMU",
        "aff_campus_unique_index": "0;0+1;0;0;2;0",
        "aff_campus_unique": "Z\u00fcrich;Tokyo;Liverpool",
        "aff_country_unique_index": "0;0+1;0;0;2;0",
        "aff_country_unique": "Switzerland;Japan;United Kingdom"
    },
    {
        "id": "19dd8b7c49",
        "title": "Active Learning of Parameterized Skills",
        "site": "https://proceedings.mlr.press/v32/silva14.html",
        "author": "Bruno Da Silva; George Konidaris; Andrew Barto",
        "abstract": "We introduce a method for actively learning parameterized skills. Parameterized skills are flexible behaviors that can solve any task drawn from a distribution of parameterized reinforcement learning problems. Approaches to learning such skills have been proposed, but limited attention has been given to identifying which training tasks allow for rapid skill acquisition. We construct a non-parametric Bayesian model of skill performance and derive analytical expressions for a novel acquisition criterion capable of identifying tasks that maximize expected improvement in skill performance. We also introduce a spatiotemporal kernel tailored for non-stationary skill performance models. The proposed method is agnostic to policy and skill representation and scales independently of task dimensionality. We evaluate it on a non-linear simulated catapult control problem over arbitrarily mountainous terrains.",
        "bibtex": "@InProceedings{pmlr-v32-silva14,\n  title = \t {Active Learning of Parameterized Skills},\n  author = \t {Silva, Bruno Da and Konidaris, George and Barto, Andrew},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1737--1745},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/silva14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/silva14.html},\n  abstract = \t {We introduce a method for actively learning parameterized skills. Parameterized skills are flexible behaviors that can solve any task drawn from a distribution of parameterized reinforcement learning problems. Approaches to learning such skills have been proposed, but limited attention has been given to identifying which training tasks allow for rapid skill acquisition. We construct a non-parametric Bayesian model of skill performance and derive analytical expressions for a novel acquisition criterion capable of identifying tasks that maximize expected improvement in skill performance. We also introduce a spatiotemporal kernel tailored for non-stationary skill performance models. The proposed method is agnostic to policy and skill representation and scales independently of task dimensionality. We evaluate it on a non-linear simulated catapult control problem over arbitrarily mountainous terrains.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/silva14.pdf",
        "supp": "",
        "pdf_size": 1323176,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13691574226403606137&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "School of Computer Science, University of Massachusetts Amherst, MA 01003; Computer Science and Artificial Intelligence Lab, MIT, Cambridge, MA 02139; School of Computer Science, University of Massachusetts Amherst, MA 01003",
        "aff_domain": "CS.UMASS.EDU;CSAIL.MIT.EDU;CS.UMASS.EDU",
        "email": "CS.UMASS.EDU;CSAIL.MIT.EDU;CS.UMASS.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Massachusetts Amherst;Massachusetts Institute of Technology",
        "aff_unique_dep": "School of Computer Science;Computer Science and Artificial Intelligence Lab",
        "aff_unique_url": "https://www.umass.edu;https://web.mit.edu",
        "aff_unique_abbr": "UMass Amherst;MIT",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Amherst;Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "992711eb31",
        "title": "Active Transfer Learning under Model Shift",
        "site": "https://proceedings.mlr.press/v32/wangi14.html",
        "author": "Xuezhi Wang; Tzu-Kuo Huang; Jeff Schneider",
        "abstract": "Transfer learning algorithms are used when one has sufficient training data for one supervised learning task (the source task) but only very limited training data for a second task (the target task) that is similar but not identical to the first.  These algorithms use varying assumptions about the similarity between the tasks to carry information from the source to the target task.  Common assumptions are that only certain specific marginal or conditional distributions have changed while all else remains the same. Alternatively, if one has only the target task, but also has the ability to choose a limited amount of additional training data to collect, then active learning algorithms are used to make choices which will most improve performance on the target task. These algorithms may be combined into active transfer learning, but previous efforts have had to apply the two methods in sequence or use restrictive transfer assumptions.    We propose two transfer learning algorithms that allow changes in all marginal and conditional distributions but assume the changes are smooth in order to achieve transfer between the tasks.  We then propose an active learning algorithm for the second method that yields a combined active transfer learning algorithm.  We demonstrate the algorithms on synthetic functions and a real-world task on estimating the yield of vineyards from images of the grapes.",
        "bibtex": "@InProceedings{pmlr-v32-wangi14,\n  title = \t {Active Transfer Learning under Model Shift},\n  author = \t {Wang, Xuezhi and Huang, Tzu-Kuo and Schneider, Jeff},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1305--1313},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/wangi14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/wangi14.html},\n  abstract = \t {Transfer learning algorithms are used when one has sufficient training data for one supervised learning task (the source task) but only very limited training data for a second task (the target task) that is similar but not identical to the first.  These algorithms use varying assumptions about the similarity between the tasks to carry information from the source to the target task.  Common assumptions are that only certain specific marginal or conditional distributions have changed while all else remains the same. Alternatively, if one has only the target task, but also has the ability to choose a limited amount of additional training data to collect, then active learning algorithms are used to make choices which will most improve performance on the target task. These algorithms may be combined into active transfer learning, but previous efforts have had to apply the two methods in sequence or use restrictive transfer assumptions.    We propose two transfer learning algorithms that allow changes in all marginal and conditional distributions but assume the changes are smooth in order to achieve transfer between the tasks.  We then propose an active learning algorithm for the second method that yields a combined active transfer learning algorithm.  We demonstrate the algorithms on synthetic functions and a real-world task on estimating the yield of vineyards from images of the grapes.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/wangi14.pdf",
        "supp": "",
        "pdf_size": 1112332,
        "gs_citation": 105,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15275397802756467201&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Computer Science Department, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Pittsburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "32685816a1",
        "title": "Adaptive Monte Carlo via Bandit Allocation",
        "site": "https://proceedings.mlr.press/v32/neufeld14.html",
        "author": "James Neufeld; Andras Gyorgy; Csaba Szepesvari; Dale Schuurmans",
        "abstract": "We consider the problem of sequentially choosing between a set of unbiased Monte Carlo estimators to minimize the mean-squared-error (MSE) of a final combined estimate. By reducing this task to a stochastic multi-armed bandit problem, we show that well developed allocation strategies can be used to achieve an MSE that approaches that of the best estimator chosen in retrospect. We then extend these developments to a scenario where alternative estimators have different, possibly stochastic, costs. The outcome is a new set of adaptive Monte Carlo strategies that provide stronger guarantees than previous approaches while offering practical advantages.",
        "bibtex": "@InProceedings{pmlr-v32-neufeld14,\n  title = \t {Adaptive Monte Carlo via Bandit Allocation},\n  author = \t {Neufeld, James and Gyorgy, Andras and Szepesvari, Csaba and Schuurmans, Dale},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1944--1952},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/neufeld14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/neufeld14.html},\n  abstract = \t {We consider the problem of sequentially choosing between a set of unbiased Monte Carlo estimators to minimize the mean-squared-error (MSE) of a final combined estimate. By reducing this task to a stochastic multi-armed bandit problem, we show that well developed allocation strategies can be used to achieve an MSE that approaches that of the best estimator chosen in retrospect. We then extend these developments to a scenario where alternative estimators have different, possibly stochastic, costs. The outcome is a new set of adaptive Monte Carlo strategies that provide stronger guarantees than previous approaches while offering practical advantages.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/neufeld14.pdf",
        "supp": "",
        "pdf_size": 1453801,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9018769540097370155&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Computing Science, University of Alberta, Edmonton, AB, Canada T6G 2E8; Department of Computing Science, University of Alberta, Edmonton, AB, Canada T6G 2E8; Department of Computing Science, University of Alberta, Edmonton, AB, Canada T6G 2E8; Department of Computing Science, University of Alberta, Edmonton, AB, Canada T6G 2E8",
        "aff_domain": "UALBERTA.CA;UALBERTA.CA;UALBERTA.CA;UALBERTA.CA",
        "email": "UALBERTA.CA;UALBERTA.CA;UALBERTA.CA;UALBERTA.CA",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "Department of Computing Science",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Edmonton",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "1a9a1f9281",
        "title": "Adaptivity and Optimism: An Improved Exponentiated Gradient Algorithm",
        "site": "https://proceedings.mlr.press/v32/steinhardtb14.html",
        "author": "Jacob Steinhardt; Percy Liang",
        "abstract": "We present an adaptive variant of the exponentiated gradient algorithm. Leveraging the optimistic learning framework of Rakhlin & Sridharan (2012), we obtain regret bounds that in the learning from experts setting depend on the variance and path length of the best expert, improving on results by Hazan & Kale (2008) and Chiang et al. (2012), and resolving an open problem posed by Kale (2012). Our techniques naturally extend to matrix-valued loss functions, where we present an adaptive matrix exponentiated gradient algorithm. To obtain the optimal regret bound in the matrix case, we generalize the Follow-the-Regularized-Leader algorithm to vector-valued payoffs, which may be of independent interest.",
        "bibtex": "@InProceedings{pmlr-v32-steinhardtb14,\n  title = \t {Adaptivity and Optimism: An Improved Exponentiated Gradient Algorithm},\n  author = \t {Steinhardt, Jacob and Liang, Percy},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1593--1601},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/steinhardtb14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/steinhardtb14.html},\n  abstract = \t {We present an adaptive variant of the exponentiated gradient algorithm. Leveraging the optimistic learning framework of Rakhlin & Sridharan (2012), we obtain regret bounds that in the learning from experts setting depend on the variance and path length of the best expert, improving on results by Hazan & Kale (2008) and Chiang et al. (2012), and resolving an open problem posed by Kale (2012). Our techniques naturally extend to matrix-valued loss functions, where we present an adaptive matrix exponentiated gradient algorithm. To obtain the optimal regret bound in the matrix case, we generalize the Follow-the-Regularized-Leader algorithm to vector-valued payoffs, which may be of independent interest.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/steinhardtb14.pdf",
        "supp": "",
        "pdf_size": 348279,
        "gs_citation": 91,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12613525883968164464&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Stanford University; Stanford University",
        "aff_domain": "CS.STANFORD.EDU;CS.STANFORD.EDU",
        "email": "CS.STANFORD.EDU;CS.STANFORD.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2c5c10cd12",
        "title": "Admixture of Poisson MRFs: A Topic Model with Word Dependencies",
        "site": "https://proceedings.mlr.press/v32/inouye14.html",
        "author": "David Inouye; Pradeep Ravikumar; Inderjit Dhillon",
        "abstract": "This paper introduces a new topic model based on an admixture of Poisson Markov Random Fields (APM), which can model dependencies between words as opposed to previous independent topic models such as PLSA (Hofmann, 1999), LDA (Blei et al., 2003) or SAM (Reisinger et al., 2010). We propose a class of admixture models that generalizes previous topic models and show an equivalence between the conditional distribution of LDA and independent Poissons\u2014suggesting that APM subsumes the modeling power of LDA. We present a tractable method for estimating the parameters of an APM based on the pseudo log-likelihood and demonstrate the benefits of APM over previous models by preliminary qualitative and quantitative experiments.",
        "bibtex": "@InProceedings{pmlr-v32-inouye14,\n  title = \t {Admixture of Poisson MRFs: A Topic Model with Word Dependencies},\n  author = \t {Inouye, David and Ravikumar, Pradeep and Dhillon, Inderjit},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {683--691},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/inouye14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/inouye14.html},\n  abstract = \t {This paper introduces a new topic model based on an admixture of Poisson Markov Random Fields (APM), which can model dependencies between words as opposed to previous independent topic models such as PLSA (Hofmann, 1999), LDA (Blei et al., 2003) or SAM (Reisinger et al., 2010). We propose a class of admixture models that generalizes previous topic models and show an equivalence between the conditional distribution of LDA and independent Poissons\u2014suggesting that APM subsumes the modeling power of LDA. We present a tractable method for estimating the parameters of an APM based on the pseudo log-likelihood and demonstrate the benefits of APM over previous models by preliminary qualitative and quantitative experiments.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/inouye14.pdf",
        "supp": "",
        "pdf_size": 373677,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16700332971925100300&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Dept. of Computer Science, University of Texas, Austin, TX 78712, USA; Dept. of Computer Science, University of Texas, Austin, TX 78712, USA; Dept. of Computer Science, University of Texas, Austin, TX 78712, USA",
        "aff_domain": "CS.UTEXAS.EDU;CS.UTEXAS.EDU;CS.UTEXAS.EDU",
        "email": "CS.UTEXAS.EDU;CS.UTEXAS.EDU;CS.UTEXAS.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "aa928edcc9",
        "title": "Affinity Weighted Embedding",
        "site": "https://proceedings.mlr.press/v32/weston14.html",
        "author": "Jason Weston; Ron Weiss; Hector Yee",
        "abstract": "Supervised linear embedding models like Wsabie (Weston et al., 2011) and supervised semantic indexing (Bai et al., 2010) have proven successful at ranking, recommendation and annotation tasks. However, despite being scalable to large datasets they do not take full advantage of the extra data due to their linear nature, and we believe they typically underfit. We propose a new class of models which aim to provide improved performance while retaining many of the benefits of the existing class of embedding models. Our approach works by reweighting each component of the embedding of features and labels with a potentially nonlinear affinity function. We describe several variants of the family, and show  its usefulness on several datasets.",
        "bibtex": "@InProceedings{pmlr-v32-weston14,\n  title = \t {Affinity Weighted Embedding},\n  author = \t {Weston, Jason and Weiss, Ron and Yee, Hector},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1215--1223},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/weston14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/weston14.html},\n  abstract = \t {Supervised linear embedding models like Wsabie (Weston et al., 2011) and supervised semantic indexing (Bai et al., 2010) have proven successful at ranking, recommendation and annotation tasks. However, despite being scalable to large datasets they do not take full advantage of the extra data due to their linear nature, and we believe they typically underfit. We propose a new class of models which aim to provide improved performance while retaining many of the benefits of the existing class of embedding models. Our approach works by reweighting each component of the embedding of features and labels with a potentially nonlinear affinity function. We describe several variants of the family, and show  its usefulness on several datasets.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/weston14.pdf",
        "supp": "",
        "pdf_size": 292616,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4087675190852891129&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "Google Inc, New York, NY, USA; Google Inc, New York, NY, USA; Google Inc, San Bruno, CA, USA",
        "aff_domain": "GOOGLE.COM;GOOGLE.COM;GOOGLE.COM",
        "email": "GOOGLE.COM;GOOGLE.COM;GOOGLE.COM",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google",
        "aff_unique_url": "https://www.google.com",
        "aff_unique_abbr": "Google",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "New York;San Bruno",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1ecc5547c2",
        "title": "Aggregating  Ordinal Labels from Crowds by Minimax Conditional Entropy",
        "site": "https://proceedings.mlr.press/v32/zhouc14.html",
        "author": "Dengyong Zhou; Qiang Liu; John Platt; Christopher Meek",
        "abstract": "We propose a method to aggregate noisy ordinal labels collected from a crowd of workers or annotators.  Eliciting ordinal labels is important in tasks such as judging web search quality and consumer satisfaction. Our method is  motivated by the observation that workers usually have difficulty distinguishing between two adjacent ordinal classes whereas distinguishing between two classes which are far away from each other is much easier. We develop the method  through  minimax conditional entropy subject to constraints which encode this observation. Empirical  evaluations on real datasets demonstrate significant improvements over existing methods.",
        "bibtex": "@InProceedings{pmlr-v32-zhouc14,\n  title = \t {Aggregating  Ordinal Labels from Crowds by Minimax Conditional Entropy},\n  author = \t {Zhou, Dengyong and Liu, Qiang and Platt, John and Meek, Christopher},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {262--270},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/zhouc14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/zhouc14.html},\n  abstract = \t {We propose a method to aggregate noisy ordinal labels collected from a crowd of workers or annotators.  Eliciting ordinal labels is important in tasks such as judging web search quality and consumer satisfaction. Our method is  motivated by the observation that workers usually have difficulty distinguishing between two adjacent ordinal classes whereas distinguishing between two classes which are far away from each other is much easier. We develop the method  through  minimax conditional entropy subject to constraints which encode this observation. Empirical  evaluations on real datasets demonstrate significant improvements over existing methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/zhouc14.pdf",
        "supp": "",
        "pdf_size": 155542,
        "gs_citation": 133,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17671768502632530&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Microsoft Research, Redmond, WA 98052; University of California, Irvine, CA 92697; Microsoft Research, Redmond, WA 98052; Microsoft Research, Redmond, WA 98052",
        "aff_domain": "MICROSOFT.COM;UCI.EDU;MICROSOFT.COM;MICROSOFT.COM",
        "email": "MICROSOFT.COM;UCI.EDU;MICROSOFT.COM;MICROSOFT.COM",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Microsoft;University of California, Irvine",
        "aff_unique_dep": "Microsoft Research;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://www.uci.edu",
        "aff_unique_abbr": "MSR;UCI",
        "aff_campus_unique_index": "0;1;0;0",
        "aff_campus_unique": "Redmond;Irvine",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "13aa2ee3e5",
        "title": "Agnostic Bayesian Learning of Ensembles",
        "site": "https://proceedings.mlr.press/v32/lacoste14.html",
        "author": "Alexandre Lacoste; Mario Marchand; Fran\u00e7ois Laviolette; Hugo Larochelle",
        "abstract": "We propose a method for producing ensembles of predictors based on holdout estimations of their generalization performances. This approach uses a prior directly on the performance of predictors taken from a finite set of candidates and attempts to infer which one is best. Using Bayesian inference, we can thus obtain a posterior that represents our uncertainty about that choice and construct a weighted ensemble of predictors accordingly. This approach has the advantage of not requiring that the predictors be probabilistic themselves, can deal with arbitrary measures of performance and does not assume that the data was actually generated from any of the predictors in the ensemble. Since the problem of finding the best (as opposed to the true) predictor among a class is known as agnostic PAC-learning, we refer to our method as agnostic Bayesian learning. We also propose a method to address the case where the performance estimate is obtained from k-fold cross validation. While being efficient and easily adjustable to any loss function, our experiments confirm that the agnostic Bayes approach is state of the art compared to common baselines such as model selection based on k-fold cross-validation or a linear combination of predictor outputs.",
        "bibtex": "@InProceedings{pmlr-v32-lacoste14,\n  title = \t {Agnostic Bayesian Learning of Ensembles},\n  author = \t {Lacoste, Alexandre and Marchand, Mario and Laviolette, Fran\u00e7ois and Larochelle, Hugo},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {611--619},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/lacoste14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/lacoste14.html},\n  abstract = \t {We propose a method for producing ensembles of predictors based on holdout estimations of their generalization performances. This approach uses a prior directly on the performance of predictors taken from a finite set of candidates and attempts to infer which one is best. Using Bayesian inference, we can thus obtain a posterior that represents our uncertainty about that choice and construct a weighted ensemble of predictors accordingly. This approach has the advantage of not requiring that the predictors be probabilistic themselves, can deal with arbitrary measures of performance and does not assume that the data was actually generated from any of the predictors in the ensemble. Since the problem of finding the best (as opposed to the true) predictor among a class is known as agnostic PAC-learning, we refer to our method as agnostic Bayesian learning. We also propose a method to address the case where the performance estimate is obtained from k-fold cross validation. While being efficient and easily adjustable to any loss function, our experiments confirm that the agnostic Bayes approach is state of the art compared to common baselines such as model selection based on k-fold cross-validation or a linear combination of predictor outputs.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/lacoste14.pdf",
        "supp": "",
        "pdf_size": 321404,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3120586296812741468&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "D\u00b4epartement d\u2019informatique et de g \u00b4enie logiciel, Universit \u00b4e Laval, Qu \u00b4ebec, Canada, G1K-7P4; D\u00b4epartement d\u2019informatique, Universit \u00b4e de Sherbrooke, Qu \u00b4ebec, Canada, J1K-2R1; D\u00b4epartement d\u2019informatique et de g \u00b4enie logiciel, Universit \u00b4e Laval, Qu \u00b4ebec, Canada, G1K-7P4; D\u00b4epartement d\u2019informatique et de g \u00b4enie logiciel, Universit \u00b4e Laval, Qu \u00b4ebec, Canada, G1K-7P4",
        "aff_domain": "ulaval.ca;usherbrooke.ca;ift.ulaval.ca;ift.ulaval.ca",
        "email": "ulaval.ca;usherbrooke.ca;ift.ulaval.ca;ift.ulaval.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Universit\u00e9 Laval;Universit\u00e9 de Sherbrooke",
        "aff_unique_dep": "D\u00e9partement d\u2019informatique et de g\u00e9nie logiciel;D\u00e9partement d\u2019informatique",
        "aff_unique_url": "https://www.ulaval.ca;https://www.usherbrooke.ca",
        "aff_unique_abbr": "UL;UdeS",
        "aff_campus_unique_index": "0;1;0;0",
        "aff_campus_unique": "Qu\u00e9bec;Sherbrooke",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "63be2d62d8",
        "title": "Alternating Minimization for Mixed Linear Regression",
        "site": "https://proceedings.mlr.press/v32/yia14.html",
        "author": "Xinyang Yi; Constantine Caramanis; Sujay Sanghavi",
        "abstract": "Mixed linear regression involves the recovery of two (or more) unknown vectors from unlabeled linear measurements; that is, where each sample comes from exactly one of the vectors, but we do not know which one. It is a classic problem, and the natural and empirically most popular approach to its solution has been the EM algorithm. As in other settings, this is prone to bad local minima; however, each iteration is very fast (alternating between guessing labels, and solving with those labels).    In this paper we provide a new initialization procedure for EM, based on finding the leading two eigenvectors of an appropriate matrix. We then show that with this, a re-sampled version of the EM algorithm provably converges to the correct vectors, under natural assumptions on the sampling distribution, and with nearly optimal (unimprovable) sample complexity. This provides not only the first characterization of EM\u2019s performance, but also much lower sample complexity as compared to both standard (randomly initialized) EM, and other methods for this problem.",
        "bibtex": "@InProceedings{pmlr-v32-yia14,\n  title = \t {Alternating Minimization for Mixed Linear Regression},\n  author = \t {Yi, Xinyang and Caramanis, Constantine and Sanghavi, Sujay},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {613--621},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/yia14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/yia14.html},\n  abstract = \t {Mixed linear regression involves the recovery of two (or more) unknown vectors from unlabeled linear measurements; that is, where each sample comes from exactly one of the vectors, but we do not know which one. It is a classic problem, and the natural and empirically most popular approach to its solution has been the EM algorithm. As in other settings, this is prone to bad local minima; however, each iteration is very fast (alternating between guessing labels, and solving with those labels).    In this paper we provide a new initialization procedure for EM, based on finding the leading two eigenvectors of an appropriate matrix. We then show that with this, a re-sampled version of the EM algorithm provably converges to the correct vectors, under natural assumptions on the sampling distribution, and with nearly optimal (unimprovable) sample complexity. This provides not only the first characterization of EM\u2019s performance, but also much lower sample complexity as compared to both standard (randomly initialized) EM, and other methods for this problem.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/yia14.pdf",
        "supp": "",
        "pdf_size": 377158,
        "gs_citation": 173,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5385115120044542372&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX, 78712; Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX, 78712; Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX, 78712",
        "aff_domain": "UTEXAS.EDU;MAIL.UTEXAS.EDU;MAIL.UTEXAS.EDU",
        "email": "UTEXAS.EDU;MAIL.UTEXAS.EDU;MAIL.UTEXAS.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a641a48378",
        "title": "An Adaptive Accelerated Proximal Gradient Method and its Homotopy Continuation for Sparse Optimization",
        "site": "https://proceedings.mlr.press/v32/lin14.html",
        "author": "Qihang Lin; Lin Xiao",
        "abstract": "We first propose an adaptive accelerated proximal gradient(APG) method for minimizing strongly convex composite functions with unknown convexity parameters. This method incorporates a restarting scheme to automatically estimate the strong convexity parameter and achieves a nearly optimal iteration complexity. Then we consider the \u21131-regularized least-squares (\u21131-LS) problem in the high-dimensional setting. Although such an objective function is not strongly convex, it has restricted strong convexity over sparse vectors. We exploit this property by combining the adaptive  APG method with a homotopy continuation scheme, which generates a sparse solution path towards optimality. This method obtains a global linear rate of convergence and its overall iteration complexity has a weaker dependency on the restricted condition number than previous work.",
        "bibtex": "@InProceedings{pmlr-v32-lin14,\n  title = \t {An Adaptive Accelerated Proximal Gradient Method and its Homotopy Continuation for Sparse Optimization},\n  author = \t {Lin, Qihang and Xiao, Lin},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {73--81},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/lin14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/lin14.html},\n  abstract = \t {We first propose an adaptive accelerated proximal gradient(APG) method for minimizing strongly convex composite functions with unknown convexity parameters. This method incorporates a restarting scheme to automatically estimate the strong convexity parameter and achieves a nearly optimal iteration complexity. Then we consider the \u21131-regularized least-squares (\u21131-LS) problem in the high-dimensional setting. Although such an objective function is not strongly convex, it has restricted strong convexity over sparse vectors. We exploit this property by combining the adaptive  APG method with a homotopy continuation scheme, which generates a sparse solution path towards optimality. This method obtains a global linear rate of convergence and its overall iteration complexity has a weaker dependency on the restricted condition number than previous work.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/lin14.pdf",
        "supp": "",
        "pdf_size": 337337,
        "gs_citation": 118,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6035819365871776486&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "The University of Iowa, Iowa City, IA 52245 USA; Microsoft Research, Redmond, WA 98052 USA",
        "aff_domain": "UIOWA.EDU;MICROSOFT.COM",
        "email": "UIOWA.EDU;MICROSOFT.COM",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Iowa;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.uiowa.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "UIowa;MSR",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Iowa City;Redmond",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f3544a26e0",
        "title": "An Analysis of State-Relevance Weights and Sampling Distributions on L1-Regularized Approximate Linear Programming Approximation Accuracy",
        "site": "https://proceedings.mlr.press/v32/taylor14.html",
        "author": "Gavin Taylor; Connor Geer; David Piekut",
        "abstract": "Recent interest in the use of L_1 regularization in the use of value function approximation includes Petrik et al.\u2019s introduction of L_1-Regularized Approximate Linear Programming (RALP).  RALP is unique among L_1-regularized approaches in that it approximates the optimal value function using off-policy samples.  Additionally, it produces policies which outperform those of previous methods, such as LSPI.  RALP\u2019s value function approximation quality is affected heavily by the choice of state-relevance weights in the objective function of the linear program, and by the distribution from which samples are drawn; however, there has been no discussion of these considerations in the previous literature.  In this paper, we discuss and explain the effects of choices in the state-relevance weights and sampling distribution on approximation quality, using both theoretical and experimental illustrations.  The results provide insight not only onto these effects, but also provide intuition into the types of MDPs which are especially well suited for approximation with RALP.",
        "bibtex": "@InProceedings{pmlr-v32-taylor14,\n  title = \t {An Analysis of State-Relevance Weights and Sampling Distributions on L1-Regularized Approximate Linear Programming Approximation Accuracy},\n  author = \t {Taylor, Gavin and Geer, Connor and Piekut, David},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {451--459},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/taylor14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/taylor14.html},\n  abstract = \t {Recent interest in the use of L_1 regularization in the use of value function approximation includes Petrik et al.\u2019s introduction of L_1-Regularized Approximate Linear Programming (RALP).  RALP is unique among L_1-regularized approaches in that it approximates the optimal value function using off-policy samples.  Additionally, it produces policies which outperform those of previous methods, such as LSPI.  RALP\u2019s value function approximation quality is affected heavily by the choice of state-relevance weights in the objective function of the linear program, and by the distribution from which samples are drawn; however, there has been no discussion of these considerations in the previous literature.  In this paper, we discuss and explain the effects of choices in the state-relevance weights and sampling distribution on approximation quality, using both theoretical and experimental illustrations.  The results provide insight not only onto these effects, but also provide intuition into the types of MDPs which are especially well suited for approximation with RALP.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/taylor14.pdf",
        "supp": "",
        "pdf_size": 393229,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6770995518433528557&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "United States Naval Academy, 572M Holloway Rd., Stop 9F, Annapolis, MD 21402-5002; United States Naval Academy, 572M Holloway Rd., Stop 9F, Annapolis, MD 21402-5002; United States Naval Academy, 572M Holloway Rd., Stop 9F, Annapolis, MD 21402-5002",
        "aff_domain": "USNA.EDU; ; ",
        "email": "USNA.EDU; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "United States Naval Academy",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.usna.edu",
        "aff_unique_abbr": "USNA",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Annapolis",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "49772dbca3",
        "title": "An Asynchronous Parallel Stochastic Coordinate Descent Algorithm",
        "site": "https://proceedings.mlr.press/v32/liud14.html",
        "author": "Ji Liu; Steve Wright; Christopher Re; Victor Bittorf; Srikrishna Sridhar",
        "abstract": "We describe an asynchronous parallel stochastic coordinate descent algorithm for minimizing smooth unconstrained or separably constrained functions. The method achieves a linear convergence rate on functions that satisfy an essential strong convexity property and a sublinear rate (1/K) on general convex functions. Near-linear speedup on a multicore system can be expected if the number of processors is O(n^1/2) in unconstrained optimization and O(n^1/4) in the separable-constrained case, where n is the number of variables. We  describe results from implementation on 40-core processors.",
        "bibtex": "@InProceedings{pmlr-v32-liud14,\n  title = \t {An Asynchronous Parallel Stochastic Coordinate Descent Algorithm},\n  author = \t {Liu, Ji and Wright, Steve and Re, Christopher and Bittorf, Victor and Sridhar, Srikrishna},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {469--477},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/liud14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/liud14.html},\n  abstract = \t {We describe an asynchronous parallel stochastic coordinate descent algorithm for minimizing smooth unconstrained or separably constrained functions. The method achieves a linear convergence rate on functions that satisfy an essential strong convexity property and a sublinear rate (1/K) on general convex functions. Near-linear speedup on a multicore system can be expected if the number of processors is O(n^1/2) in unconstrained optimization and O(n^1/4) in the separable-constrained case, where n is the number of variables. We  describe results from implementation on 40-core processors.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/liud14.pdf",
        "supp": "",
        "pdf_size": 324387,
        "gs_citation": 455,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16025456280965499308&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "Department of Computer Sciences, University of Wisconsin-Madison; Department of Computer Sciences, University of Wisconsin-Madison; Department of Computer Science, Stanford University; Department of Computer Sciences, University of Wisconsin-Madison; Department of Computer Sciences, University of Wisconsin-Madison",
        "aff_domain": "CS.WISC.EDU;CS.WISC.EDU;STANFORD.EDU;CS.WISC.EDU;CS.WISC.EDU",
        "email": "CS.WISC.EDU;CS.WISC.EDU;STANFORD.EDU;CS.WISC.EDU;CS.WISC.EDU",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "University of Wisconsin-Madison;Stanford University",
        "aff_unique_dep": "Department of Computer Sciences;Department of Computer Science",
        "aff_unique_url": "https://www.wisc.edu;https://www.stanford.edu",
        "aff_unique_abbr": "UW-Madison;Stanford",
        "aff_campus_unique_index": "0;0;1;0;0",
        "aff_campus_unique": "Madison;Stanford",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f0e97e8aa1",
        "title": "An Efficient Approach for Assessing Hyperparameter Importance",
        "site": "https://proceedings.mlr.press/v32/hutter14.html",
        "author": "Frank Hutter; Holger Hoos; Kevin Leyton-Brown",
        "abstract": "The performance of many machine learning methods depends critically on hyperparameter settings. Sophisticated Bayesian optimization methods have recently achieved considerable successes in optimizing these hyperparameters, in several cases surpassing the performance of human experts. However, blind reliance on such methods can leave end users without insight into the relative importance of different hyperparameters and their interactions. This paper describes efficient methods that can be used to gain such insight, leveraging random forest models fit on the data already gathered by Bayesian optimization. We first introduce a novel, linear-time algorithm for computing marginals of random forest predictions and then show how to leverage these predictions within a functional ANOVA framework, to quantify the importance of both single hyperparameters and of interactions between hyperparameters. We conducted experiments with prominent machine learning frameworks and state-of-the-art solvers for combinatorial problems. We show that our methods provide insight into the relationship between hyperparameter settings and performance, and demonstrate that\u2014even in very high-dimensional cases\u2014most performance variation is attributable to just a few hyperparameters.",
        "bibtex": "@InProceedings{pmlr-v32-hutter14,\n  title = \t {An Efficient Approach for Assessing Hyperparameter Importance},\n  author = \t {Hutter, Frank and Hoos, Holger and Leyton-Brown, Kevin},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {754--762},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/hutter14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/hutter14.html},\n  abstract = \t {The performance of many machine learning methods depends critically on hyperparameter settings. Sophisticated Bayesian optimization methods have recently achieved considerable successes in optimizing these hyperparameters, in several cases surpassing the performance of human experts. However, blind reliance on such methods can leave end users without insight into the relative importance of different hyperparameters and their interactions. This paper describes efficient methods that can be used to gain such insight, leveraging random forest models fit on the data already gathered by Bayesian optimization. We first introduce a novel, linear-time algorithm for computing marginals of random forest predictions and then show how to leverage these predictions within a functional ANOVA framework, to quantify the importance of both single hyperparameters and of interactions between hyperparameters. We conducted experiments with prominent machine learning frameworks and state-of-the-art solvers for combinatorial problems. We show that our methods provide insight into the relationship between hyperparameter settings and performance, and demonstrate that\u2014even in very high-dimensional cases\u2014most performance variation is attributable to just a few hyperparameters.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/hutter14.pdf",
        "supp": "",
        "pdf_size": 583808,
        "gs_citation": 692,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2341371337005807501&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff": "University of Freiburg, Freiburg, GERMANY; University of British Columbia, Vancouver, CANADA; University of British Columbia, Vancouver, CANADA",
        "aff_domain": "INFORMATIK.UNI-FREIBURG.DE;CS.UBC.CA;CS.UBC.CA",
        "email": "INFORMATIK.UNI-FREIBURG.DE;CS.UBC.CA;CS.UBC.CA",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Freiburg;University of British Columbia",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uni-freiburg.de;https://www.ubc.ca",
        "aff_unique_abbr": "UoF;UBC",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Freiburg;Vancouver",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Germany;Canada"
    },
    {
        "id": "40f1211589",
        "title": "An Information Geometry of Statistical Manifold Learning",
        "site": "https://proceedings.mlr.press/v32/suna14.html",
        "author": "Ke Sun; St\u00e9phane Marchand-Maillet",
        "abstract": "Manifold learning seeks low-dimensional representations of high-dimensional data. The main tactics have been exploring the geometry in an input data space and an output embedding space. We develop a manifold learning theory in a hypothesis space consisting of models. A model means a specific instance of a collection of points, e.g., the input data collectively or the output embedding collectively. The semi-Riemannian metric of this hypothesis space is uniquely derived in closed form based on the information geometry of probability distributions. There, manifold learning is interpreted as a trajectory of intermediate models. The volume of a continuous region reveals an amount of information. It can be measured to define model complexity and embedding quality. This provides deep unified perspectives of manifold learning theory.",
        "bibtex": "@InProceedings{pmlr-v32-suna14,\n  title = \t {An Information Geometry of Statistical Manifold Learning},\n  author = \t {Sun, Ke and Marchand-Maillet, St\u00e9phane},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1--9},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/suna14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/suna14.html},\n  abstract = \t {Manifold learning seeks low-dimensional representations of high-dimensional data. The main tactics have been exploring the geometry in an input data space and an output embedding space. We develop a manifold learning theory in a hypothesis space consisting of models. A model means a specific instance of a collection of points, e.g., the input data collectively or the output embedding collectively. The semi-Riemannian metric of this hypothesis space is uniquely derived in closed form based on the information geometry of probability distributions. There, manifold learning is interpreted as a trajectory of intermediate models. The volume of a continuous region reveals an amount of information. It can be measured to define model complexity and embedding quality. This provides deep unified perspectives of manifold learning theory.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/suna14.pdf",
        "supp": "",
        "pdf_size": 555835,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6687718262657872232&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Viper Group, Computer Vision & Multimedia Laboratory, University of Geneva, Switzerland; Viper Group, Computer Vision & Multimedia Laboratory, University of Geneva, Switzerland",
        "aff_domain": "UNIGE.CH;UNIGE.CH",
        "email": "UNIGE.CH;UNIGE.CH",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Geneva",
        "aff_unique_dep": "Computer Vision & Multimedia Laboratory",
        "aff_unique_url": "https://www.unige.ch",
        "aff_unique_abbr": "UniGE",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "5a0e360b65",
        "title": "Anomaly Ranking as Supervised Bipartite Ranking",
        "site": "https://proceedings.mlr.press/v32/clemencon14.html",
        "author": "Stephan Cl\u00e9men\u00e7on; Sylvain Robbiano",
        "abstract": "The Mass Volume (MV) curve is a visual  tool to evaluate the performance of a scoring  function with regard to its capacity to rank  data in the same order as the underlying density function. Anomaly ranking refers to the  unsupervised learning task which consists in  building a scoring function, based on unlabeled data, with a MV curve as low as possible at any point. In this paper, it is proved  that, in the case where the data generating probability distribution has compact support, anomaly ranking is equivalent to (supervised) bipartite ranking, where the goal is  to discriminate between the underlying probability distribution and the uniform distribution with same support. In this situation, the  MV curve can be then seen as a simple transform of the corresponding ROC curve. Exploiting this view, we then show how to use  bipartite ranking algorithms, possibly combined with random sampling, to solve the  MV curve minimization problem. Numerical experiments based on a variety of bipartite ranking algorithms well-documented in  the literature are displayed in order to illustrate the relevance of our approach.",
        "bibtex": "@InProceedings{pmlr-v32-clemencon14,\n  title = \t {Anomaly Ranking as Supervised Bipartite Ranking},\n  author = \t {Cl\u00e9men\u00e7on, Stephan and Robbiano, Sylvain},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {343--351},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/clemencon14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/clemencon14.html},\n  abstract = \t {The Mass Volume (MV) curve is a visual  tool to evaluate the performance of a scoring  function with regard to its capacity to rank  data in the same order as the underlying density function. Anomaly ranking refers to the  unsupervised learning task which consists in  building a scoring function, based on unlabeled data, with a MV curve as low as possible at any point. In this paper, it is proved  that, in the case where the data generating probability distribution has compact support, anomaly ranking is equivalent to (supervised) bipartite ranking, where the goal is  to discriminate between the underlying probability distribution and the uniform distribution with same support. In this situation, the  MV curve can be then seen as a simple transform of the corresponding ROC curve. Exploiting this view, we then show how to use  bipartite ranking algorithms, possibly combined with random sampling, to solve the  MV curve minimization problem. Numerical experiments based on a variety of bipartite ranking algorithms well-documented in  the literature are displayed in order to illustrate the relevance of our approach.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/clemencon14.pdf",
        "supp": "",
        "pdf_size": 371248,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12396809620901143656&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "LTCI UMR Telecom ParisTech/CNRS No. 5141, 46 rue Barrault, 75634 Paris Cedex, France + CIMFA V-Facultad de Ingeniera, Universidad de Valparaso, Valparaso, Chile; LTCI UMR Telecom ParisTech/CNRS No. 5141, 46 rue Barrault, 75634 Paris Cedex, France",
        "aff_domain": "telecom-paristech.fr;gmail.com",
        "email": "telecom-paristech.fr;gmail.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0",
        "aff_unique_norm": "Telecom ParisTech;Universidad de Valparaiso",
        "aff_unique_dep": "LTCI UMR;Facultad de Ingenieria",
        "aff_unique_url": "https://www.telecom-paris.fr;https://www.uv.cl/",
        "aff_unique_abbr": "Telecom ParisTech;",
        "aff_campus_unique_index": "0+1;0",
        "aff_campus_unique": "Paris;Valparaiso",
        "aff_country_unique_index": "0+1;0",
        "aff_country_unique": "France;Chile"
    },
    {
        "id": "5316983c0d",
        "title": "Anti-differentiating approximation algorithms:A case study with min-cuts, spectral, and flow",
        "site": "https://proceedings.mlr.press/v32/gleich14.html",
        "author": "David Gleich; Michael Mahoney",
        "abstract": "We formalize and illustrate the general concept of algorithmic anti-differentiation: given an algorithmic procedure, e.g., an approximation algorithm for which worst-case approximation guarantees are available or a heuristic that has been engineered to be practically-useful but for which a precise theoretical understanding is lacking, an algorithmic anti-derivative is a precise statement of an optimization problem that is exactly solved by that procedure. We explore this concept with a case study of approximation algorithms for finding locally-biased partitions in data graphs, demonstrating connections between min-cut objectives, a personalized version of the popular PageRank vector, and the highly effective \"push\" procedure for computing an approximation to personalized PageRank. We show, for example, that this latter algorithm solves (exactly, but implicitly) an l1-regularized l2-regression problem, a fact that helps to explain its excellent performance in practice. We expect that, when available, these implicit optimization problems will be critical for rationalizing and predicting the performance of many approximation algorithms on realistic data.",
        "bibtex": "@InProceedings{pmlr-v32-gleich14,\n  title = \t {Anti-differentiating approximation algorithms:A case study with min-cuts, spectral, and flow},\n  author = \t {Gleich, David and Mahoney, Michael},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1018--1025},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/gleich14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/gleich14.html},\n  abstract = \t {We formalize and illustrate the general concept of algorithmic anti-differentiation: given an algorithmic procedure, e.g., an approximation algorithm for which worst-case approximation guarantees are available or a heuristic that has been engineered to be practically-useful but for which a precise theoretical understanding is lacking, an algorithmic anti-derivative is a precise statement of an optimization problem that is exactly solved by that procedure. We explore this concept with a case study of approximation algorithms for finding locally-biased partitions in data graphs, demonstrating connections between min-cut objectives, a personalized version of the popular PageRank vector, and the highly effective \"push\" procedure for computing an approximation to personalized PageRank. We show, for example, that this latter algorithm solves (exactly, but implicitly) an l1-regularized l2-regression problem, a fact that helps to explain its excellent performance in practice. We expect that, when available, these implicit optimization problems will be critical for rationalizing and predicting the performance of many approximation algorithms on realistic data.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/gleich14.pdf",
        "supp": "",
        "pdf_size": 238981,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2666011731853601746&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Computer Science, Purdue University, West Lafayette, IN 47906; International Computer Science Institute and Dept. of Statistics, University of California at Berkeley, Berkeley, CA 94720",
        "aff_domain": "purdue.edu;icsi.berkeley.edu",
        "email": "purdue.edu;icsi.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Purdue University;University of California, Berkeley",
        "aff_unique_dep": "Computer Science;Dept. of Statistics",
        "aff_unique_url": "https://www.purdue.edu;https://www.berkeley.edu",
        "aff_unique_abbr": "Purdue;UC Berkeley",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "West Lafayette;Berkeley",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b2145a463c",
        "title": "Approximate Policy Iteration Schemes: A Comparison",
        "site": "https://proceedings.mlr.press/v32/scherrer14.html",
        "author": "Bruno Scherrer",
        "abstract": "We consider the infinite-horizon discounted optimal control problem  formalized by Markov Decision Processes. We focus on several  approximate variations of the Policy Iteration algorithm: Approximate Policy Iteration, Conservative Policy Iteration  (CPI), a natural adaptation of the Policy Search by  Dynamic Programming algorithm to the  infinite-horizon case (PSDP_\u221e), and the recently proposed  Non-Stationary Policy iteration (NSPI(m)). For all  algorithms, we describe performance bounds, and  make a comparison by paying a particular attention to the  concentrability constants involved, the number of iterations and the  memory required. Our analysis highlights the following points: 1) The  performance guarantee of CPI can be arbitrarily better than that of  API/API(\u03b1), but this comes at the cost of a  relative\u2014exponential in \\frac1\u03b5\u2014increase of the  number of iterations. 2) PSDP_\u221eenjoys the best of both worlds: its performance guarantee is similar to that of CPI, but within a number of iterations similar to that of API. 3) Contrary to API that  requires a constant memory, the memory needed by CPI and PSDP_\u221eis  proportional to their number of iterations, which may be problematic  when the discount factor \u03b3is close to 1 or the  approximation error \u03b5is close to 0; we show that  the NSPI(m) algorithm allows to make an overall trade-off between  memory and performance. Simulations with these schemes confirm our  analysis.",
        "bibtex": "@InProceedings{pmlr-v32-scherrer14,\n  title = \t {Approximate Policy Iteration Schemes: A Comparison},\n  author = \t {Scherrer, Bruno},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1314--1322},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/scherrer14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/scherrer14.html},\n  abstract = \t {We consider the infinite-horizon discounted optimal control problem  formalized by Markov Decision Processes. We focus on several  approximate variations of the Policy Iteration algorithm: Approximate Policy Iteration, Conservative Policy Iteration  (CPI), a natural adaptation of the Policy Search by  Dynamic Programming algorithm to the  infinite-horizon case (PSDP_\u221e), and the recently proposed  Non-Stationary Policy iteration (NSPI(m)). For all  algorithms, we describe performance bounds, and  make a comparison by paying a particular attention to the  concentrability constants involved, the number of iterations and the  memory required. Our analysis highlights the following points: 1) The  performance guarantee of CPI can be arbitrarily better than that of  API/API(\u03b1), but this comes at the cost of a  relative\u2014exponential in \\frac1\u03b5\u2014increase of the  number of iterations. 2) PSDP_\u221eenjoys the best of both worlds: its performance guarantee is similar to that of CPI, but within a number of iterations similar to that of API. 3) Contrary to API that  requires a constant memory, the memory needed by CPI and PSDP_\u221eis  proportional to their number of iterations, which may be problematic  when the discount factor \u03b3is close to 1 or the  approximation error \u03b5is close to 0; we show that  the NSPI(m) algorithm allows to make an overall trade-off between  memory and performance. Simulations with these schemes confirm our  analysis.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/scherrer14.pdf",
        "supp": "",
        "pdf_size": 865155,
        "gs_citation": 117,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9605603923054637996&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Inria, Villers-l `es-Nancy, F-54600, France+Universit \u00b4e de Lorraine, LORIA, UMR 7503, Vand\u0153uvre-l `es-Nancy, F-54506, France",
        "aff_domain": "INRIA.FR",
        "email": "INRIA.FR",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1",
        "aff_unique_norm": "INRIA;Universit\u00e9 de Lorraine",
        "aff_unique_dep": ";LORIA, UMR 7503",
        "aff_unique_url": "https://www.inria.fr;https://www.univ-lorraine.fr",
        "aff_unique_abbr": "Inria;",
        "aff_campus_unique_index": "0+1",
        "aff_campus_unique": "Villers-l\u00e8s-Nancy;Vand\u0153uvre-l\u00e8s-Nancy",
        "aff_country_unique_index": "0+0",
        "aff_country_unique": "France"
    },
    {
        "id": "09603f8ffa",
        "title": "Approximation Analysis of Stochastic Gradient Langevin Dynamics  by using Fokker-Planck Equation and Ito Process",
        "site": "https://proceedings.mlr.press/v32/satoa14.html",
        "author": "Issei Sato; Hiroshi Nakagawa",
        "abstract": "The stochastic gradient Langevin dynamics (SGLD) algorithm is appealing for large scale Bayesian learning.  The SGLD algorithm seamlessly transit stochastic optimization and Bayesian posterior sampling.  However, solid theories, such as convergence proof, have not been developed.  We theoretically analyze the SGLD algorithm with constant stepsize in two ways.  First, we show  by using the Fokker-Planck equation that the probability distribution of random variables generated by the SGLD algorithm converges to the Bayesian posterior.  Second, we analyze the convergence of the SGLD algorithm by using the Ito process, which reveals that the SGLD algorithm does not strongly but weakly converges.  This result indicates that the SGLD algorithm can be an approximation method for posterior averaging.",
        "bibtex": "@InProceedings{pmlr-v32-satoa14,\n  title = \t {Approximation Analysis of Stochastic Gradient Langevin Dynamics  by using Fokker-Planck Equation and Ito Process },\n  author = \t {Sato, Issei and Nakagawa, Hiroshi},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {982--990},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/satoa14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/satoa14.html},\n  abstract = \t {The stochastic gradient Langevin dynamics (SGLD) algorithm is appealing for large scale Bayesian learning.  The SGLD algorithm seamlessly transit stochastic optimization and Bayesian posterior sampling.  However, solid theories, such as convergence proof, have not been developed.  We theoretically analyze the SGLD algorithm with constant stepsize in two ways.  First, we show  by using the Fokker-Planck equation that the probability distribution of random variables generated by the SGLD algorithm converges to the Bayesian posterior.  Second, we analyze the convergence of the SGLD algorithm by using the Ito process, which reveals that the SGLD algorithm does not strongly but weakly converges.  This result indicates that the SGLD algorithm can be an approximation method for posterior averaging.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/satoa14.pdf",
        "supp": "",
        "pdf_size": 88751,
        "gs_citation": 99,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4383745065044089424&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "The University of Tokyo; The University of Tokyo",
        "aff_domain": "R.DL.ITC.U-TOKYO.AC.JP;DL.ITC.U-TOKYO.AC.JP",
        "email": "R.DL.ITC.U-TOKYO.AC.JP;DL.ITC.U-TOKYO.AC.JP",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Tokyo",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "UTokyo",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "b34c1074c5",
        "title": "Asymptotically consistent estimation of the number of change points in highly dependent time series",
        "site": "https://proceedings.mlr.press/v32/khaleghi14.html",
        "author": "Azadeh Khaleghi; Daniil Ryabko",
        "abstract": "The problem of change point estimation is considered in a general framework where the  data are generated by arbitrary unknown stationary ergodic process distributions. This means that the data may have  long-range dependencies of an arbitrary form. In this context the consistent estimation of the number of change points is provably impossible. A formulation is proposed which overcomes this obstacle:   it is possible to find the correct number of change points at the  expense of introducing the additional constraint that the correct number of  process distributions that generate the data is provided. This additional parameter has a natural interpretation  in many real-world applications.  It turns out that in this formulation change point estimation can be reduced to time series clustering. Based on this reduction, an algorithm is proposed that finds the number of change points and locates the changes.  This algorithm is shown to be asymptotically consistent.  The theoretical results are complemented with empirical evaluations.",
        "bibtex": "@InProceedings{pmlr-v32-khaleghi14,\n  title = \t {Asymptotically consistent estimation of the number of change points in highly dependent time series},\n  author = \t {Khaleghi, Azadeh and Ryabko, Daniil},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {539--547},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/khaleghi14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/khaleghi14.html},\n  abstract = \t {The problem of change point estimation is considered in a general framework where the  data are generated by arbitrary unknown stationary ergodic process distributions. This means that the data may have  long-range dependencies of an arbitrary form. In this context the consistent estimation of the number of change points is provably impossible. A formulation is proposed which overcomes this obstacle:   it is possible to find the correct number of change points at the  expense of introducing the additional constraint that the correct number of  process distributions that generate the data is provided. This additional parameter has a natural interpretation  in many real-world applications.  It turns out that in this formulation change point estimation can be reduced to time series clustering. Based on this reduction, an algorithm is proposed that finds the number of change points and locates the changes.  This algorithm is shown to be asymptotically consistent.  The theoretical results are complemented with empirical evaluations.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/khaleghi14.pdf",
        "supp": "",
        "pdf_size": 346663,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5783310012221878154&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "CBIO Mines ParisTech, INSERM U900, Institut Curie, FRANCE; SequeL-INRIA Lille - Nord Europe, FRANCE",
        "aff_domain": "CURIE.FR;INRIA.FR",
        "email": "CURIE.FR;INRIA.FR",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "MINES ParisTech;INRIA Lille - Nord Europe",
        "aff_unique_dep": "CBIO;SequeL",
        "aff_unique_url": "https://www.minesparistech.fr;https://www.inria.fr/lille-nord-europe",
        "aff_unique_abbr": "Mines ParisTech;INRIA",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Lille",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "2da7841631",
        "title": "Asynchronous Distributed ADMM for Consensus Optimization",
        "site": "https://proceedings.mlr.press/v32/zhange14.html",
        "author": "Ruiliang Zhang; James Kwok",
        "abstract": "Distributed optimization algorithms are highly attractive for solving big data problems. In particular, many machine learning problems can be formulated as the global consensus optimization problem, which can then be solved in a distributed manner by the alternating direction method of multipliers (ADMM) algorithm. However, this suffers from the straggler problem as its updates have to be synchronized. In this paper, we propose an asynchronous ADMM algorithm by using two conditions to control the asynchrony: partial barrier and bounded delay. The proposed algorithm has a simple structure and good convergence guarantees (its convergence rate can be reduced to that of its synchronous counterpart). Experiments on different distributed ADMM applications show that asynchrony reduces the time on network waiting, and achieves faster convergence than its synchronous counterpart in terms of the wall clock time.",
        "bibtex": "@InProceedings{pmlr-v32-zhange14,\n  title = \t {Asynchronous Distributed ADMM for Consensus Optimization},\n  author = \t {Zhang, Ruiliang and Kwok, James},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1701--1709},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/zhange14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/zhange14.html},\n  abstract = \t {Distributed optimization algorithms are highly attractive for solving big data problems. In particular, many machine learning problems can be formulated as the global consensus optimization problem, which can then be solved in a distributed manner by the alternating direction method of multipliers (ADMM) algorithm. However, this suffers from the straggler problem as its updates have to be synchronized. In this paper, we propose an asynchronous ADMM algorithm by using two conditions to control the asynchrony: partial barrier and bounded delay. The proposed algorithm has a simple structure and good convergence guarantees (its convergence rate can be reduced to that of its synchronous counterpart). Experiments on different distributed ADMM applications show that asynchrony reduces the time on network waiting, and achieves faster convergence than its synchronous counterpart in terms of the wall clock time.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/zhange14.pdf",
        "supp": "",
        "pdf_size": 493585,
        "gs_citation": 498,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12432768753791875532&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong; Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong",
        "aff_domain": "CSE.UST.HK;CSE.UST.HK",
        "email": "CSE.UST.HK;CSE.UST.HK",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.ust.hk",
        "aff_unique_abbr": "HKUST",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "4a77232b9c",
        "title": "Austerity in MCMC Land: Cutting the Metropolis-Hastings Budget",
        "site": "https://proceedings.mlr.press/v32/korattikara14.html",
        "author": "Anoop Korattikara; Yutian Chen; Max Welling",
        "abstract": "Can we make Bayesian posterior MCMC sampling more efficient when faced with very large datasets? We argue that computing the likelihood for N datapoints in the Metropolis-Hastings (MH) test to reach a single binary decision is computationally inefficient. We introduce an approximate MH rule based on a sequential hypothesis test that allows us to accept or reject samples with high confidence using only a fraction of the data required for the exact MH rule. While this method introduces an asymptotic bias, we show that this bias can be controlled and is more than offset by a decrease in variance due to our ability to draw more samples per unit of time.",
        "bibtex": "@InProceedings{pmlr-v32-korattikara14,\n  title = \t {Austerity in MCMC Land: Cutting the Metropolis-Hastings Budget},\n  author = \t {Korattikara, Anoop and Chen, Yutian and Welling, Max},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {181--189},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/korattikara14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/korattikara14.html},\n  abstract = \t {Can we make Bayesian posterior MCMC sampling more efficient when faced with very large datasets? We argue that computing the likelihood for N datapoints in the Metropolis-Hastings (MH) test to reach a single binary decision is computationally inefficient. We introduce an approximate MH rule based on a sequential hypothesis test that allows us to accept or reject samples with high confidence using only a fraction of the data required for the exact MH rule. While this method introduces an asymptotic bias, we show that this bias can be controlled and is more than offset by a decrease in variance due to our ability to draw more samples per unit of time.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/korattikara14.pdf",
        "supp": "",
        "pdf_size": 2478991,
        "gs_citation": 306,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16248314748252887021&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "School of Information & Computer Sciences, University of California, Irvine, CA 92617, USA; Department of Engineering, University of Cambridge, Cambridge CB2 1PZ, UK; Informatics Institute, University of Amsterdam, Science Park 904 1098 XH, Amsterdam, Netherlands",
        "aff_domain": "UCI.EDU;ENG.CAM.EDU;ICS.UCI.EDU",
        "email": "UCI.EDU;ENG.CAM.EDU;ICS.UCI.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of California, Irvine;University of Cambridge;University of Amsterdam",
        "aff_unique_dep": "School of Information & Computer Sciences;Department of Engineering;Informatics Institute",
        "aff_unique_url": "https://www.uci.edu;https://www.cam.ac.uk;https://www.uva.nl",
        "aff_unique_abbr": "UCI;Cambridge;UvA",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "Irvine;Cambridge;Amsterdam",
        "aff_country_unique_index": "0;1;2",
        "aff_country_unique": "United States;United Kingdom;Netherlands"
    },
    {
        "id": "859374ef5f",
        "title": "Automated inference of point of view from user interactions in collective intelligence venues",
        "site": "https://proceedings.mlr.press/v32/das14.html",
        "author": "Sanmay Das; Allen Lavoie",
        "abstract": "Empirical evaluation of trust and manipulation in large-scale collective intelligence processes is challenging. The datasets involved are too large for thorough manual study, and current automated options are limited. We introduce a statistical framework which classifies point of view based on user interactions. The framework works on Web-scale datasets and is applicable to a wide variety of collective intelligence processes. It enables principled study of such issues as manipulation, trustworthiness of information, and potential bias. We demonstrate the model\u2019s effectiveness in determining point of view on both synthetic data and a dataset of Wikipedia user interactions. We build a combined model of topics and points-of-view on the entire history of English Wikipedia, and show how it can be used to find potentially biased articles and visualize user interactions at a high level.",
        "bibtex": "@InProceedings{pmlr-v32-das14,\n  title = \t {Automated inference of point of view from user interactions in collective intelligence venues},\n  author = \t {Das, Sanmay and Lavoie, Allen},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {82--90},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/das14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/das14.html},\n  abstract = \t {Empirical evaluation of trust and manipulation in large-scale collective intelligence processes is challenging. The datasets involved are too large for thorough manual study, and current automated options are limited. We introduce a statistical framework which classifies point of view based on user interactions. The framework works on Web-scale datasets and is applicable to a wide variety of collective intelligence processes. It enables principled study of such issues as manipulation, trustworthiness of information, and potential bias. We demonstrate the model\u2019s effectiveness in determining point of view on both synthetic data and a dataset of Wikipedia user interactions. We build a combined model of topics and points-of-view on the entire history of English Wikipedia, and show how it can be used to find potentially biased articles and visualize user interactions at a high level.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/das14.pdf",
        "supp": "",
        "pdf_size": 2279739,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4894755342710432877&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Washington University in St. Louis; Washington University in St. Louis",
        "aff_domain": "seas.wustl.edu;wustl.edu",
        "email": "seas.wustl.edu;wustl.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Washington University in St. Louis",
        "aff_unique_dep": "",
        "aff_unique_url": "https://wustl.edu",
        "aff_unique_abbr": "WashU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "St. Louis",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "33cc604bf9",
        "title": "Bayesian Max-margin Multi-Task Learning with Data Augmentation",
        "site": "https://proceedings.mlr.press/v32/lic14.html",
        "author": "Chengtao Li; Jun Zhu; Jianfei Chen",
        "abstract": "Both max-margin and Bayesian methods have been extensively studied in multi-task learning, but have rarely been considered together. We present Bayesian max-margin multi-task learning, which conjoins the two schools of methods, thus allowing the discriminative max-margin methods to enjoy the great flexibility of Bayesian methods on incorporating rich prior information as well as performing nonparametric Bayesian feature learning with the latent dimensionality resolved from data. We develop Gibbs sampling algorithms by exploring data augmentation to deal with the non-smooth hinge loss. For nonparametric models, our algorithms do not need to make mean-field assumptions or truncated approximation. Empirical results demonstrate superior performance than competitors in both multi-task classification and regression.",
        "bibtex": "@InProceedings{pmlr-v32-lic14,\n  title = \t {Bayesian Max-margin Multi-Task Learning with Data Augmentation},\n  author = \t {Li, Chengtao and Zhu, Jun and Chen, Jianfei},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {415--423},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/lic14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/lic14.html},\n  abstract = \t {Both max-margin and Bayesian methods have been extensively studied in multi-task learning, but have rarely been considered together. We present Bayesian max-margin multi-task learning, which conjoins the two schools of methods, thus allowing the discriminative max-margin methods to enjoy the great flexibility of Bayesian methods on incorporating rich prior information as well as performing nonparametric Bayesian feature learning with the latent dimensionality resolved from data. We develop Gibbs sampling algorithms by exploring data augmentation to deal with the non-smooth hinge loss. For nonparametric models, our algorithms do not need to make mean-field assumptions or truncated approximation. Empirical results demonstrate superior performance than competitors in both multi-task classification and regression.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/lic14.pdf",
        "supp": "",
        "pdf_size": 540117,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17192088787500075477&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China; Dept. of Comp. Sci. & Tech, TNList Lab, State Key Lab of Intell. Tech & Sys, Tsinghua University, Beijing, China; Dept. of Comp. Sci. & Tech, TNList Lab, State Key Lab of Intell. Tech & Sys, Tsinghua University, Beijing, China",
        "aff_domain": "hotmail.com;mail.tsinghua.edu.cn;mails.tsinghua.edu.cn",
        "email": "hotmail.com;mail.tsinghua.edu.cn;mails.tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Tsinghua University",
        "aff_unique_dep": "Institute for Interdisciplinary Information Sciences",
        "aff_unique_url": "https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "Tsinghua",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "cf8aaf3220",
        "title": "Bayesian Nonparametric Multilevel Clustering with Group-Level Contexts",
        "site": "https://proceedings.mlr.press/v32/nguyenb14.html",
        "author": "Tien Vu Nguyen; Dinh Phung; Xuanlong Nguyen; Swetha Venkatesh; Hung Bui",
        "abstract": "We present a Bayesian nonparametric framework for multilevel clustering which utilizes group-level context information to simultaneously discover low-dimensional structures of the group contents and partitions groups into clusters. Using the Dirichlet process as the building block, our model constructs a product base-measure with a nested structure to accommodate content and context observations at multiple levels. The proposed model possesses properties that link the nested Dirichlet processes (nDP) and the Dirichlet process mixture models (DPM) in an interesting way: integrating out all contents results in the DPM over contexts, whereas integrating out group-speci\ufb01c contexts results in the nDP mixture over content variables. We provide a Polya-urn view of the model and an ef\ufb01cient collapsed Gibbs inference procedure. Extensive experiments on real-world datasets demonstrate the advantage of utilizing context information via our model in both text and image domains.",
        "bibtex": "@InProceedings{pmlr-v32-nguyenb14,\n  title = \t {Bayesian Nonparametric Multilevel Clustering with Group-Level Contexts},\n  author = \t {Nguyen, Tien Vu and Phung, Dinh and Nguyen, Xuanlong and Venkatesh, Swetha and Bui, Hung},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {288--296},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/nguyenb14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/nguyenb14.html},\n  abstract = \t {We present a Bayesian nonparametric framework for multilevel clustering which utilizes group-level context information to simultaneously discover low-dimensional structures of the group contents and partitions groups into clusters. Using the Dirichlet process as the building block, our model constructs a product base-measure with a nested structure to accommodate content and context observations at multiple levels. The proposed model possesses properties that link the nested Dirichlet processes (nDP) and the Dirichlet process mixture models (DPM) in an interesting way: integrating out all contents results in the DPM over contexts, whereas integrating out group-speci\ufb01c contexts results in the nDP mixture over content variables. We provide a Polya-urn view of the model and an ef\ufb01cient collapsed Gibbs inference procedure. Extensive experiments on real-world datasets demonstrate the advantage of utilizing context information via our model in both text and image domains.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/nguyenb14.pdf",
        "supp": "",
        "pdf_size": 485196,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15531067347907647379&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Center for Pattern Recognition and Data Analytics (PRaDA), Deakin University, Australia; Center for Pattern Recognition and Data Analytics (PRaDA), Deakin University, Australia; Department of Statistics, University of Michigan, Ann Arbor, USA; Center for Pattern Recognition and Data Analytics (PRaDA), Deakin University, Australia; Laboratory for Natural Language Understanding, Nuance Communications, Sunnyvale, USA",
        "aff_domain": "DEAKIN.EDU.AU;DEAKIN.EDU.AU;UMICH.EDU;DEAKIN.EDU.AU;GMAIL.COM",
        "email": "DEAKIN.EDU.AU;DEAKIN.EDU.AU;UMICH.EDU;DEAKIN.EDU.AU;GMAIL.COM",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0;2",
        "aff_unique_norm": "Deakin University;University of Michigan;Nuance Communications",
        "aff_unique_dep": "Center for Pattern Recognition and Data Analytics (PRaDA);Department of Statistics;Laboratory for Natural Language Understanding",
        "aff_unique_url": "https://www.deakin.edu.au;https://www.umich.edu;https://www.nuance.com",
        "aff_unique_abbr": "Deakin;UM;Nuance",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Ann Arbor;Sunnyvale",
        "aff_country_unique_index": "0;0;1;0;1",
        "aff_country_unique": "Australia;United States"
    },
    {
        "id": "20843d7136",
        "title": "Bayesian Optimization with Inequality Constraints",
        "site": "https://proceedings.mlr.press/v32/gardner14.html",
        "author": "Jacob Gardner; Matt Kusner;  Zhixiang; Kilian Weinberger; John Cunningham",
        "abstract": "Bayesian optimization is a powerful framework for minimizing expensive objective functions while using very few function evaluations.  It has been successfully applied to a variety of problems, including hyperparameter tuning and experimental design.  However, this framework has not been extended to the inequality-constrained optimization setting, particularly the setting in which evaluating feasibility is just as expensive as evaluating the objective.  Here we present constrained Bayesian optimization, which places a prior distribution on both the objective and the constraint functions.  We evaluate our method on simulated and real data, demonstrating that constrained Bayesian optimization can quickly find optimal and feasible points, even when small feasible regions cause standard methods to fail.",
        "bibtex": "@InProceedings{pmlr-v32-gardner14,\n  title = \t {Bayesian Optimization with Inequality Constraints},\n  author = \t {Gardner, Jacob and Kusner, Matt and Zhixiang,  and Weinberger, Kilian and Cunningham, John},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {937--945},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/gardner14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/gardner14.html},\n  abstract = \t {Bayesian optimization is a powerful framework for minimizing expensive objective functions while using very few function evaluations.  It has been successfully applied to a variety of problems, including hyperparameter tuning and experimental design.  However, this framework has not been extended to the inequality-constrained optimization setting, particularly the setting in which evaluating feasibility is just as expensive as evaluating the objective.  Here we present constrained Bayesian optimization, which places a prior distribution on both the objective and the constraint functions.  We evaluate our method on simulated and real data, demonstrating that constrained Bayesian optimization can quickly find optimal and feasible points, even when small feasible regions cause standard methods to fail.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/gardner14.pdf",
        "supp": "",
        "pdf_size": 8278316,
        "gs_citation": 685,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7914930628520199452&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Washington University in St. Louis; Washington University in St. Louis; Washington University in St. Louis; Washington University in St. Louis; Columbia University",
        "aff_domain": "WUSTL.EDU;WUSTL.EDU;CSE.WUSTL.EDU;WUSTL.EDU;COLUMBIA.EDU",
        "email": "WUSTL.EDU;WUSTL.EDU;CSE.WUSTL.EDU;WUSTL.EDU;COLUMBIA.EDU",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "Washington University in St. Louis;Columbia University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://wustl.edu;https://www.columbia.edu",
        "aff_unique_abbr": "WashU;Columbia",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "St. Louis;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "85f7fbe777",
        "title": "Beta Diffusion Trees",
        "site": "https://proceedings.mlr.press/v32/heaukulani14.html",
        "author": "Creighton Heaukulani; David Knowles; Zoubin Ghahramani",
        "abstract": "We define the beta diffusion tree, a random tree structure with a set of leaves that defines a collection of overlapping subsets of objects, known as a feature allocation. The generative process for the tree is defined in terms of particles (representing the objects) diffusing in some continuous space, analogously to the Dirichlet and Pitman-Yor diffusion trees (Neal, 2003b; Knowles & Ghahramani, 2011), both of which define tree structures over clusters of the particles. With the beta diffusion tree, however, multiple copies of a particle may exist and diffuse to multiple locations in the continuous space, resulting in (a random number of) possibly overlapping clusters of the objects. We demonstrate how to build a hierarchically-clustered factor analysis model with the beta diffusion tree and how to perform inference over the random tree structures with a Markov chain Monte Carlo algorithm. We conclude with several numerical experiments on missing data problems with data sets of gene expression arrays, international development statistics, and intranational socioeconomic measurements.",
        "bibtex": "@InProceedings{pmlr-v32-heaukulani14,\n  title = \t {Beta Diffusion Trees},\n  author = \t {Heaukulani, Creighton and Knowles, David and Ghahramani, Zoubin},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1809--1817},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/heaukulani14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/heaukulani14.html},\n  abstract = \t {We define the beta diffusion tree, a random tree structure with a set of leaves that defines a collection of overlapping subsets of objects, known as a feature allocation. The generative process for the tree is defined in terms of particles (representing the objects) diffusing in some continuous space, analogously to the Dirichlet and Pitman-Yor diffusion trees (Neal, 2003b; Knowles & Ghahramani, 2011), both of which define tree structures over clusters of the particles. With the beta diffusion tree, however, multiple copies of a particle may exist and diffuse to multiple locations in the continuous space, resulting in (a random number of) possibly overlapping clusters of the objects. We demonstrate how to build a hierarchically-clustered factor analysis model with the beta diffusion tree and how to perform inference over the random tree structures with a Markov chain Monte Carlo algorithm. We conclude with several numerical experiments on missing data problems with data sets of gene expression arrays, international development statistics, and intranational socioeconomic measurements.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/heaukulani14.pdf",
        "supp": "",
        "pdf_size": 450229,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9313484768488615381&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Cambridge, Department of Engineering, Cambridge, UK; Stanford University, Department of Computer Science, Stanford, CA, USA; University of Cambridge, Department of Engineering, Cambridge, UK",
        "aff_domain": "CAM.AC.UK;CS.STANFORD.EDU;ENG.CAM.AC.UK",
        "email": "CAM.AC.UK;CS.STANFORD.EDU;ENG.CAM.AC.UK",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Cambridge;Stanford University",
        "aff_unique_dep": "Department of Engineering;Department of Computer Science",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.stanford.edu",
        "aff_unique_abbr": "Cambridge;Stanford",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Cambridge;Stanford",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "e3d6a13976",
        "title": "Bias in Natural Actor-Critic Algorithms",
        "site": "https://proceedings.mlr.press/v32/thomas14.html",
        "author": "Philip Thomas",
        "abstract": "We show that several popular discounted reward natural actor-critics, including the popular NAC-LSTD and eNAC algorithms, do not generate unbiased estimates of the natural policy gradient as claimed. We derive the first unbiased discounted reward natural actor-critics using batch and iterative approaches to gradient estimation. We argue that the bias makes the existing algorithms more appropriate for the average reward setting. We also show that, when Sarsa(lambda) is guaranteed to converge to an optimal policy, the objective function used by natural actor-critics is concave, so policy gradient methods are guaranteed to converge to globally optimal policies as well.",
        "bibtex": "@InProceedings{pmlr-v32-thomas14,\n  title = \t {Bias in Natural Actor-Critic Algorithms},\n  author = \t {Thomas, Philip},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {441--448},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/thomas14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/thomas14.html},\n  abstract = \t {We show that several popular discounted reward natural actor-critics, including the popular NAC-LSTD and eNAC algorithms, do not generate unbiased estimates of the natural policy gradient as claimed. We derive the first unbiased discounted reward natural actor-critics using batch and iterative approaches to gradient estimation. We argue that the bias makes the existing algorithms more appropriate for the average reward setting. We also show that, when Sarsa(lambda) is guaranteed to converge to an optimal policy, the objective function used by natural actor-critics is concave, so policy gradient methods are guaranteed to converge to globally optimal policies as well.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/thomas14.pdf",
        "supp": "",
        "pdf_size": 385457,
        "gs_citation": 179,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4644387020703168864&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "School of Computer Science, University of Massachusetts, Amherst, MA 01002 USA",
        "aff_domain": "CS.UMASS.EDU",
        "email": "CS.UMASS.EDU",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Massachusetts Amherst",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.umass.edu",
        "aff_unique_abbr": "UMass Amherst",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Amherst",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6719243097",
        "title": "Boosting multi-step autoregressive forecasts",
        "site": "https://proceedings.mlr.press/v32/taieb14.html",
        "author": "Souhaib Ben Taieb; Rob Hyndman",
        "abstract": "Multi-step forecasts can be produced recursively by iterating a one-step model, or directly using a specific model for each horizon. Choosing between these two strategies is not an easy task since it involves a trade-off between bias and estimation variance over the forecast horizon. Using a nonlinear machine learning model makes the tradeoff even more difficult. To address this issue, we propose a new forecasting strategy which boosts traditional recursive linear forecasts with a direct strategy using a boosting autoregression procedure at each horizon. First, we investigate the performance of the proposed strategy in terms of bias and variance decomposition of the error using simulated time series. Then, we evaluate the proposed strategy on real-world time series from two forecasting competitions. Overall, we obtain excellent performance with respect to the standard forecasting strategies.",
        "bibtex": "@InProceedings{pmlr-v32-taieb14,\n  title = \t {Boosting multi-step autoregressive forecasts},\n  author = \t {Ben Taieb, Souhaib and Hyndman, Rob},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {109--117},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/taieb14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/taieb14.html},\n  abstract = \t {Multi-step forecasts can be produced recursively by iterating a one-step model, or directly using a specific model for each horizon. Choosing between these two strategies is not an easy task since it involves a trade-off between bias and estimation variance over the forecast horizon. Using a nonlinear machine learning model makes the tradeoff even more difficult. To address this issue, we propose a new forecasting strategy which boosts traditional recursive linear forecasts with a direct strategy using a boosting autoregression procedure at each horizon. First, we investigate the performance of the proposed strategy in terms of bias and variance decomposition of the error using simulated time series. Then, we evaluate the proposed strategy on real-world time series from two forecasting competitions. Overall, we obtain excellent performance with respect to the standard forecasting strategies.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/taieb14.pdf",
        "supp": "",
        "pdf_size": 386327,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17291943495764516223&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 14,
        "aff": "Machine Learning Group, Computer Science Department, Faculty of Sciences, Universit\u00e9 Libre de Bruxelles, Brussels, Belgium; Department of Econometrics and Business Statistics, Monash University, Clayton VIC 3800, Australia",
        "aff_domain": "ULB.AC.BE;MONASH.EDU",
        "email": "ULB.AC.BE;MONASH.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Universit\u00e9 Libre de Bruxelles;Monash University",
        "aff_unique_dep": "Computer Science Department;Department of Econometrics and Business Statistics",
        "aff_unique_url": "https://www.ulb.ac.be;https://www.monash.edu",
        "aff_unique_abbr": "ULB;Monash",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Brussels;Clayton",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Belgium;Australia"
    },
    {
        "id": "c0fd079335",
        "title": "Boosting with Online Binary Learners for the Multiclass Bandit Problem",
        "site": "https://proceedings.mlr.press/v32/chenb14.html",
        "author": "Shang-Tse Chen; Hsuan-Tien Lin; Chi-Jen Lu",
        "abstract": "We consider the problem of online multiclass prediction in the bandit setting. Compared with the full-information setting, in which the learner can receive the true label as feedback after making each prediction, the bandit setting assumes that the learner can only know the correctness of the predicted label. Because the bandit setting is more restricted, it is difficult to design good bandit learners and currently there are not many bandit learners. In this paper, we propose an approach that systematically converts existing online binary classifiers to promising bandit learners with strong theoretical guarantee. The approach matches the idea of boosting, which has been shown to be powerful for batch learning as well as online learning. In particular, we establish the weak-learning condition on the online binary classifiers, and show that the condition allows automatically constructing a bandit learner with arbitrary strength by combining several of those classifiers. Experimental results on several real-world data sets demonstrate the effectiveness of the proposed approach.",
        "bibtex": "@InProceedings{pmlr-v32-chenb14,\n  title = \t {Boosting with Online Binary Learners for the Multiclass Bandit Problem},\n  author = \t {Chen, Shang-Tse and Lin, Hsuan-Tien and Lu, Chi-Jen},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {342--350},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/chenb14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/chenb14.html},\n  abstract = \t {We consider the problem of online multiclass prediction in the bandit setting. Compared with the full-information setting, in which the learner can receive the true label as feedback after making each prediction, the bandit setting assumes that the learner can only know the correctness of the predicted label. Because the bandit setting is more restricted, it is difficult to design good bandit learners and currently there are not many bandit learners. In this paper, we propose an approach that systematically converts existing online binary classifiers to promising bandit learners with strong theoretical guarantee. The approach matches the idea of boosting, which has been shown to be powerful for batch learning as well as online learning. In particular, we establish the weak-learning condition on the online binary classifiers, and show that the condition allows automatically constructing a bandit learner with arbitrary strength by combining several of those classifiers. Experimental results on several real-world data sets demonstrate the effectiveness of the proposed approach.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/chenb14.pdf",
        "supp": "",
        "pdf_size": 294596,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6997596599542224184&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "School of Computer Science, Georgia Institute of Technology, Atlanta, GA; Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan; Institute of Information Science, Academia Sinica, Taipei, Taiwan",
        "aff_domain": "gatech.edu;csie.ntu.edu.tw;iis.sinica.edu.tw",
        "email": "gatech.edu;csie.ntu.edu.tw;iis.sinica.edu.tw",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Georgia Institute of Technology;National Taiwan University;Academia Sinica",
        "aff_unique_dep": "School of Computer Science;Department of Computer Science and Information Engineering;Institute of Information Science",
        "aff_unique_url": "https://www.gatech.edu;https://www.ntu.edu.tw;https://www.sinica.edu.tw",
        "aff_unique_abbr": "Georgia Tech;NTU;AS",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Atlanta;Taiwan",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "dcbcb12213",
        "title": "Buffer k-d Trees: Processing Massive Nearest Neighbor Queries on GPUs",
        "site": "https://proceedings.mlr.press/v32/gieseke14.html",
        "author": "Fabian Gieseke; Justin Heinermann; Cosmin Oancea; Christian Igel",
        "abstract": "We present a new approach for combining k-d trees and graphics processing units for nearest neighbor search. It is well known that a direct combination of these tools leads to a non-satisfying performance due to conditional computations and suboptimal memory accesses. To alleviate these problems, we propose a variant of the classical k-d tree data structure, called buffer k-d tree, which can be used to reorganize the search. Our experiments show that we can take advantage of both the hierarchical subdivision induced by k-d trees and the huge computational resources provided by today\u2019s many-core devices. We demonstrate the potential of our approach in astronomy, where hundreds of million nearest neighbor queries have to be processed.",
        "bibtex": "@InProceedings{pmlr-v32-gieseke14,\n  title = \t {Buffer k-d Trees: Processing Massive Nearest Neighbor Queries on GPUs},\n  author = \t {Gieseke, Fabian and Heinermann, Justin and Oancea, Cosmin and Igel, Christian},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {172--180},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/gieseke14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/gieseke14.html},\n  abstract = \t {We present a new approach for combining k-d trees and graphics processing units for nearest neighbor search. It is well known that a direct combination of these tools leads to a non-satisfying performance due to conditional computations and suboptimal memory accesses. To alleviate these problems, we propose a variant of the classical k-d tree data structure, called buffer k-d tree, which can be used to reorganize the search. Our experiments show that we can take advantage of both the hierarchical subdivision induced by k-d trees and the huge computational resources provided by today\u2019s many-core devices. We demonstrate the potential of our approach in astronomy, where hundreds of million nearest neighbor queries have to be processed.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/gieseke14.pdf",
        "supp": "",
        "pdf_size": 455669,
        "gs_citation": 122,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7399091864896915744&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Computer Science, University of Copenhagen; Department of Computing Science, University of Oldenburg; Department of Computer Science, University of Copenhagen; Department of Computer Science, University of Copenhagen",
        "aff_domain": "DIKU.DK;INFORMATIK.UNI-OLDENBURG.DE;DIKU.DK;DIKU.DK",
        "email": "DIKU.DK;INFORMATIK.UNI-OLDENBURG.DE;DIKU.DK;DIKU.DK",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "University of Copenhagen;University of Oldenburg",
        "aff_unique_dep": "Department of Computer Science;Department of Computing Science",
        "aff_unique_url": "https://www.ku.dk;https://www.uni-oldenburg.de",
        "aff_unique_abbr": "UCPH;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "Denmark;Germany"
    },
    {
        "id": "f5d44912a9",
        "title": "Circulant Binary Embedding",
        "site": "https://proceedings.mlr.press/v32/yub14.html",
        "author": "Felix Yu; Sanjiv Kumar; Yunchao Gong; Shih-Fu Chang",
        "abstract": "Binary embedding of high-dimensional data requires long codes to preserve the discriminative power of the input space. Traditional binary coding methods often suffer from very high computation and storage costs in such a scenario. To address this problem, we propose Circulant Binary Embedding (CBE) which generates binary codes by projecting the data with a circulant matrix. The circulant structure enables the use of Fast Fourier Transformation to speed up the computation. Compared to methods that use unstructured matrices, the proposed method improves the time complexity from \\mathcalO(d^2) to \\mathcalO(d\\logd), and the space complexity from \\mathcalO(d^2) to \\mathcalO(d) where d is the input dimensionality. We also propose a novel time-frequency alternating optimization to learn data-dependent circulant projections, which alternatively minimizes the objective in original and Fourier domains. We show by extensive experiments that the proposed approach gives much better performance than the state-of-the-art approaches for fixed time, and provides much faster computation with no performance degradation for fixed number of bits.",
        "bibtex": "@InProceedings{pmlr-v32-yub14,\n  title = \t {Circulant Binary Embedding},\n  author = \t {Yu, Felix and Kumar, Sanjiv and Gong, Yunchao and Chang, Shih-Fu},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {946--954},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/yub14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/yub14.html},\n  abstract = \t {Binary embedding of high-dimensional data requires long codes to preserve the discriminative power of the input space. Traditional binary coding methods often suffer from very high computation and storage costs in such a scenario. To address this problem, we propose Circulant Binary Embedding (CBE) which generates binary codes by projecting the data with a circulant matrix. The circulant structure enables the use of Fast Fourier Transformation to speed up the computation. Compared to methods that use unstructured matrices, the proposed method improves the time complexity from \\mathcalO(d^2) to \\mathcalO(d\\logd), and the space complexity from \\mathcalO(d^2) to \\mathcalO(d) where d is the input dimensionality. We also propose a novel time-frequency alternating optimization to learn data-dependent circulant projections, which alternatively minimizes the objective in original and Fourier domains. We show by extensive experiments that the proposed approach gives much better performance than the state-of-the-art approaches for fixed time, and provides much faster computation with no performance degradation for fixed number of bits.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/yub14.pdf",
        "supp": "",
        "pdf_size": 477456,
        "gs_citation": 182,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16502408016773772835&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Columbia University; Google Research; University of North Carolina at Chapel Hill; Columbia University",
        "aff_domain": "ee.columbia.edu;google.com;cs.unc.edu;ee.columbia.edu",
        "email": "ee.columbia.edu;google.com;cs.unc.edu;ee.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Columbia University;Google;University of North Carolina",
        "aff_unique_dep": ";Google Research;",
        "aff_unique_url": "https://www.columbia.edu;https://research.google;https://www.unc.edu",
        "aff_unique_abbr": "Columbia;Google Research;UNC",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Mountain View;Chapel Hill",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e8c988d763",
        "title": "Clustering in the Presence of Background Noise",
        "site": "https://proceedings.mlr.press/v32/ben-david14.html",
        "author": "Shai Ben-David; Nika Haghtalab",
        "abstract": "We address the problem of noise management in clustering algorithms. Namely, issues that arise when on top of some cluster structure the data also contains an unstructured set of points. We consider how clustering algorithms can be \u201crobustified\" so that they recover the cluster structure in spite of the unstructured part of the input. We introduce some quantitative measures of such robustness that take into account the strength of the embedded cluster structure as well was the mildness of the noise subset. We propose a simple and efficient method to turn any centroid-based clustering algorithm into a noise-robust one, and prove robustness guarantees for our method with respect to these measures. We also prove that more straightforward ways of \u201crobustifying\u201d clustering algorithms fail to achieve similar guarantees.",
        "bibtex": "@InProceedings{pmlr-v32-ben-david14,\n  title = \t {Clustering in the Presence of Background Noise},\n  author = \t {Ben-David, Shai and Haghtalab, Nika},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {280--288},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/ben-david14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/ben-david14.html},\n  abstract = \t {We address the problem of noise management in clustering algorithms. Namely, issues that arise when on top of some cluster structure the data also contains an unstructured set of points. We consider how clustering algorithms can be \u201crobustified\" so that they recover the cluster structure in spite of the unstructured part of the input. We introduce some quantitative measures of such robustness that take into account the strength of the embedded cluster structure as well was the mildness of the noise subset. We propose a simple and efficient method to turn any centroid-based clustering algorithm into a noise-robust one, and prove robustness guarantees for our method with respect to these measures. We also prove that more straightforward ways of \u201crobustifying\u201d clustering algorithms fail to achieve similar guarantees.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/ben-david14.pdf",
        "supp": "",
        "pdf_size": 363229,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9024500400607567275&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, ON N2L 3G1 CANADA; Computer Science Department, Carnegie Mellon University, Pittsburgh, PA 15213 USA",
        "aff_domain": "CS.UWATERLOO.CA;CMU.EDU",
        "email": "CS.UWATERLOO.CA;CMU.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Waterloo;Carnegie Mellon University",
        "aff_unique_dep": "David R. Cheriton School of Computer Science;Computer Science Department",
        "aff_unique_url": "https://uwaterloo.ca;https://www.cmu.edu",
        "aff_unique_abbr": "UWaterloo;CMU",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Waterloo;Pittsburgh",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "163d678090",
        "title": "Coding for Random Projections",
        "site": "https://proceedings.mlr.press/v32/lie14.html",
        "author": "Ping Li; Michael Mitzenmacher; Anshumali Shrivastava",
        "abstract": "The method of random projections has become  popular for large-scale applications in statistical learning, information retrieval, bio-informatics  and other applications.  Using a well-designed \\textbfcoding scheme for the projected data, which determines the number of bits needed for each projected value and how to allocate these bits, can significantly improve the effectiveness of the algorithm, in storage cost as well as computational speed.   In this paper, we study a number of simple coding schemes, focusing on the task of similarity estimation and on an application to training linear classifiers. We demonstrate that \\textbfuniform quantization outperforms the standard and influential method\u00a0\\citeProc:Datar_SCG04, which used a \\em window-and-random offset scheme. Indeed, we argue that in many cases coding with just a small number of bits suffices.  Furthermore, we also  develop a \\textbfnon-uniform 2-bit coding scheme that generally performs well in practice, as confirmed by our experiments on training linear support vector machines (SVM). Proofs and additional experiments  are available at \\em arXiv:1308.2218.      In the context of using coded random projections for \\textbfapproximate near neighbor search by building hash tables (\\em arXiv:1403.8144)\u00a0\\citeReport:RPCodeLSH2014, we show that the step of random offset in\u00a0\\citeProc:Datar_SCG04 is  again not needed  and may hurt the performance. Furthermore, we show that, unless the target similarity level is high, it usually suffices to use only 1 or 2 bits to code each hashed value for this task. Section\u00a0\\refsec_LSH presents some experimental results for LSH.",
        "bibtex": "@InProceedings{pmlr-v32-lie14,\n  title = \t {Coding for Random Projections},\n  author = \t {Li, Ping and Mitzenmacher, Michael and Shrivastava, Anshumali},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {676--684},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/lie14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/lie14.html},\n  abstract = \t {The method of random projections has become  popular for large-scale applications in statistical learning, information retrieval, bio-informatics  and other applications.  Using a well-designed \\textbfcoding scheme for the projected data, which determines the number of bits needed for each projected value and how to allocate these bits, can significantly improve the effectiveness of the algorithm, in storage cost as well as computational speed.   In this paper, we study a number of simple coding schemes, focusing on the task of similarity estimation and on an application to training linear classifiers. We demonstrate that \\textbfuniform quantization outperforms the standard and influential method\u00a0\\citeProc:Datar_SCG04, which used a \\em window-and-random offset scheme. Indeed, we argue that in many cases coding with just a small number of bits suffices.  Furthermore, we also  develop a \\textbfnon-uniform 2-bit coding scheme that generally performs well in practice, as confirmed by our experiments on training linear support vector machines (SVM). Proofs and additional experiments  are available at \\em arXiv:1308.2218.      In the context of using coded random projections for \\textbfapproximate near neighbor search by building hash tables (\\em arXiv:1403.8144)\u00a0\\citeReport:RPCodeLSH2014, we show that the step of random offset in\u00a0\\citeProc:Datar_SCG04 is  again not needed  and may hurt the performance. Furthermore, we show that, unless the target similarity level is high, it usually suffices to use only 1 or 2 bits to code each hashed value for this task. Section\u00a0\\refsec_LSH presents some experimental results for LSH.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/lie14.pdf",
        "supp": "",
        "pdf_size": 454882,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7092416128348074408&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Dept. of Statistics and Biostatistics, Dept. of Computer Science, Rutgers University, Piscataway, NJ 08854, USA; School of Engineering and Applied Sciences, Harvard University, Cambridge, MA 02138, USA; Dept. of Computer Science, Computing and Information Science, Cornell University, Ithaca, NY 14853, USA",
        "aff_domain": "stat.rutgers.edu;eecs.harvard.edu;cs.cornell.edu",
        "email": "stat.rutgers.edu;eecs.harvard.edu;cs.cornell.edu",
        "github": "",
        "project": "arXiv:1308.2218",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Rutgers University;Harvard University;Cornell University",
        "aff_unique_dep": "Dept. of Statistics and Biostatistics;School of Engineering and Applied Sciences;Dept. of Computer Science, Computing and Information Science",
        "aff_unique_url": "https://www.rutgers.edu;https://www.harvard.edu;https://www.cornell.edu",
        "aff_unique_abbr": "Rutgers;Harvard;Cornell",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "Piscataway;Cambridge;Ithaca",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e2d8989496",
        "title": "Coherent Matrix Completion",
        "site": "https://proceedings.mlr.press/v32/chenc14.html",
        "author": "Yudong Chen; Srinadh Bhojanapalli; Sujay Sanghavi; Rachel Ward",
        "abstract": "Matrix completion concerns the recovery of a low-rank matrix from a subset of its revealed entries, and nuclear norm minimization has emerged as an effective surrogate for this combinatorial problem.  Here, we show that nuclear norm minimization can recover an arbitrary n \\times n matrix of rank r from O(nr log^2(n)) revealed entries, provided that revealed entries are drawn proportionally to the local row and column coherences (closely related to leverage scores) of the underlying matrix.  Our results are order-optimal up to logarithmic factors, and extend existing results for nuclear norm minimization which require strong incoherence conditions on the types of matrices that can be recovered, due to assumed uniformly distributed revealed entries.  We further provide extensive numerical evidence that a proposed two-phase sampling algorithm can perform nearly as well as local-coherence sampling and without requiring a priori knowledge of the matrix coherence structure.  Finally, we apply our results to quantify how weighted nuclear norm minimization can improve on unweighted minimization given an arbitrary set of sampled entries.",
        "bibtex": "@InProceedings{pmlr-v32-chenc14,\n  title = \t {Coherent Matrix Completion},\n  author = \t {Chen, Yudong and Bhojanapalli, Srinadh and Sanghavi, Sujay and Ward, Rachel},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {674--682},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/chenc14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/chenc14.html},\n  abstract = \t {Matrix completion concerns the recovery of a low-rank matrix from a subset of its revealed entries, and nuclear norm minimization has emerged as an effective surrogate for this combinatorial problem.  Here, we show that nuclear norm minimization can recover an arbitrary n \\times n matrix of rank r from O(nr log^2(n)) revealed entries, provided that revealed entries are drawn proportionally to the local row and column coherences (closely related to leverage scores) of the underlying matrix.  Our results are order-optimal up to logarithmic factors, and extend existing results for nuclear norm minimization which require strong incoherence conditions on the types of matrices that can be recovered, due to assumed uniformly distributed revealed entries.  We further provide extensive numerical evidence that a proposed two-phase sampling algorithm can perform nearly as well as local-coherence sampling and without requiring a priori knowledge of the matrix coherence structure.  Finally, we apply our results to quantify how weighted nuclear norm minimization can improve on unweighted minimization given an arbitrary set of sampled entries.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/chenc14.pdf",
        "supp": "",
        "pdf_size": 2727545,
        "gs_citation": 151,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11106382674255707490&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "University of California, Berkeley, CA 94720, USA; The University of Texas at Austin, Austin, TX 78712, USA; The University of Texas at Austin, Austin, TX 78712, USA; The University of Texas at Austin, Austin, TX 78712, USA",
        "aff_domain": "utexas.edu;utexas.edu;mail.utexas.edu;math.utexas.edu",
        "email": "utexas.edu;utexas.edu;mail.utexas.edu;math.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "University of California, Berkeley;University of Texas at Austin",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.berkeley.edu;https://www.utexas.edu",
        "aff_unique_abbr": "UC Berkeley;UT Austin",
        "aff_campus_unique_index": "0;1;1;1",
        "aff_campus_unique": "Berkeley;Austin",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5b3f371b27",
        "title": "Cold-start Active Learning with Robust Ordinal Matrix Factorization",
        "site": "https://proceedings.mlr.press/v32/houlsby14.html",
        "author": "Neil Houlsby; Jose Miguel Hernandez-Lobato; Zoubin Ghahramani",
        "abstract": "We present a new matrix factorization model for rating data and a corresponding active learning strategy to address the cold-start problem. Cold-start is one of the most challenging tasks for recommender systems: what to recommend with new users or items for which one has little or no data. An approach is to use active learning to collect the most useful initial ratings. However, the performance of active learning depends strongly upon having accurate estimates of i) the uncertainty in model parameters and ii) the intrinsic noisiness of the data. To achieve these estimates we propose a heteroskedastic Bayesian model for ordinal matrix factorization. We also present a computationally efficient framework for Bayesian active learning with this type of complex probabilistic model. This algorithm successfully distinguishes between informative and noisy data points. Our model yields state-of-the-art predictive performance and, coupled with our active learning strategy, enables us to gain useful information in the cold-start setting from the very first active sample.",
        "bibtex": "@InProceedings{pmlr-v32-houlsby14,\n  title = \t {Cold-start Active Learning with Robust Ordinal Matrix Factorization},\n  author = \t {Houlsby, Neil and Hernandez-Lobato, Jose Miguel and Ghahramani, Zoubin},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {766--774},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/houlsby14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/houlsby14.html},\n  abstract = \t {We present a new matrix factorization model for rating data and a corresponding active learning strategy to address the cold-start problem. Cold-start is one of the most challenging tasks for recommender systems: what to recommend with new users or items for which one has little or no data. An approach is to use active learning to collect the most useful initial ratings. However, the performance of active learning depends strongly upon having accurate estimates of i) the uncertainty in model parameters and ii) the intrinsic noisiness of the data. To achieve these estimates we propose a heteroskedastic Bayesian model for ordinal matrix factorization. We also present a computationally efficient framework for Bayesian active learning with this type of complex probabilistic model. This algorithm successfully distinguishes between informative and noisy data points. Our model yields state-of-the-art predictive performance and, coupled with our active learning strategy, enables us to gain useful information in the cold-start setting from the very first active sample.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/houlsby14.pdf",
        "supp": "",
        "pdf_size": 580618,
        "gs_citation": 98,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14559706005039421375&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "University of Cambridge, Department of Engineering, Cambridge CB2 1PZ, UK; University of Cambridge, Department of Engineering, Cambridge CB2 1PZ, UK; University of Cambridge, Department of Engineering, Cambridge CB2 1PZ, UK",
        "aff_domain": "CAM.AC.UK;CAM.AC.UK;ENG.CAM.AC.UK",
        "email": "CAM.AC.UK;CAM.AC.UK;ENG.CAM.AC.UK",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "Department of Engineering",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "fcc53217bf",
        "title": "Combinatorial Partial Monitoring Game with Linear Feedback and Its Applications",
        "site": "https://proceedings.mlr.press/v32/lind14.html",
        "author": "Tian Lin; Bruno Abrahao; Robert Kleinberg; John Lui; Wei Chen",
        "abstract": "In online learning, a player chooses actions to play and receives reward and feedback from the environment with the goal of maximizing her reward over time. In this paper, we propose the model of combinatorial partial monitoring games with linear feedback, a model which simultaneously addresses limited feedback, infinite outcome space of the environment and exponentially large action space of the player. We present the Global Confidence Bound (GCB) algorithm, which integrates ideas from both combinatorial multi-armed bandits and finite partial monitoring games to handle all the above issues. GCB only requires feedback on a small set of actions and achieves O(T^\\frac23\\log T) distribution-independent regret and O(\\log T) distribution-dependent regret (the latter assuming unique optimal action), where T is the total time steps played. Moreover, the regret bounds only depend linearly on \\log |X| rather than |X|, where X is the action space. GCB isolates offline optimization tasks from online learning and avoids explicit enumeration of all actions in the online learning part. We demonstrate that our model and algorithm can be applied to a crowdsourcing application leading to both an efficient learning algorithm and low regret, and argue that they can be applied to a wide range of combinatorial applications constrained with limited feedback.",
        "bibtex": "@InProceedings{pmlr-v32-lind14,\n  title = \t {Combinatorial Partial Monitoring Game with Linear Feedback and Its Applications},\n  author = \t {Lin, Tian and Abrahao, Bruno and Kleinberg, Robert and Lui, John and Chen, Wei},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {901--909},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/lind14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/lind14.html},\n  abstract = \t {In online learning, a player chooses actions to play and receives reward and feedback from the environment with the goal of maximizing her reward over time. In this paper, we propose the model of combinatorial partial monitoring games with linear feedback, a model which simultaneously addresses limited feedback, infinite outcome space of the environment and exponentially large action space of the player. We present the Global Confidence Bound (GCB) algorithm, which integrates ideas from both combinatorial multi-armed bandits and finite partial monitoring games to handle all the above issues. GCB only requires feedback on a small set of actions and achieves O(T^\\frac23\\log T) distribution-independent regret and O(\\log T) distribution-dependent regret (the latter assuming unique optimal action), where T is the total time steps played. Moreover, the regret bounds only depend linearly on \\log |X| rather than |X|, where X is the action space. GCB isolates offline optimization tasks from online learning and avoids explicit enumeration of all actions in the online learning part. We demonstrate that our model and algorithm can be applied to a crowdsourcing application leading to both an efficient learning algorithm and low regret, and argue that they can be applied to a wide range of combinatorial applications constrained with limited feedback.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/lind14.pdf",
        "supp": "",
        "pdf_size": 349081,
        "gs_citation": 66,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5634808903774289480&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Tsinghua University, Beijing, China; Cornell University, Ithaca, NY 14850, USA; Cornell University, Ithaca, NY 14850, USA; The Chinese University of Hong Kong, Shatin, NT, Hong Kong; Microsoft Research, Beijing, China",
        "aff_domain": "MAILS.TSINGHUA.EDU.CN;CS.CORNELL.EDU;CS.CORNELL.EDU;CSE.CUHK.EDU.HK;MICROSOFT.COM",
        "email": "MAILS.TSINGHUA.EDU.CN;CS.CORNELL.EDU;CS.CORNELL.EDU;CSE.CUHK.EDU.HK;MICROSOFT.COM",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;2;3",
        "aff_unique_norm": "Tsinghua University;Cornell University;Chinese University of Hong Kong;Microsoft",
        "aff_unique_dep": ";;;Microsoft Research",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.cornell.edu;https://www.cuhk.edu.hk;https://www.microsoft.com/en-us/research/group/microsoft-research-asia",
        "aff_unique_abbr": "THU;Cornell;CUHK;MSR",
        "aff_campus_unique_index": "0;1;1;2;0",
        "aff_campus_unique": "Beijing;Ithaca;Hong Kong SAR",
        "aff_country_unique_index": "0;1;1;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "6c3f4c1c03",
        "title": "Communication-Efficient Distributed Optimization using an Approximate Newton-type Method",
        "site": "https://proceedings.mlr.press/v32/shamir14.html",
        "author": "Ohad Shamir; Nati Srebro; Tong Zhang",
        "abstract": "We present a novel Newton-type method for distributed optimization,  which is particularly well suited for stochastic optimization and  learning problems.  For quadratic objectives, the method enjoys a  linear rate of convergence which provably \\emphimproves with the  data size, requiring an essentially constant number of iterations  under reasonable assumptions.  We provide theoretical and empirical  evidence of the advantages of our method compared to other  approaches, such as one-shot parameter averaging and ADMM.",
        "bibtex": "@InProceedings{pmlr-v32-shamir14,\n  title = \t {Communication-Efficient Distributed Optimization using an Approximate Newton-type Method},\n  author = \t {Shamir, Ohad and Srebro, Nati and Zhang, Tong},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1000--1008},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/shamir14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/shamir14.html},\n  abstract = \t {We present a novel Newton-type method for distributed optimization,  which is particularly well suited for stochastic optimization and  learning problems.  For quadratic objectives, the method enjoys a  linear rate of convergence which provably \\emphimproves with the  data size, requiring an essentially constant number of iterations  under reasonable assumptions.  We provide theoretical and empirical  evidence of the advantages of our method compared to other  approaches, such as one-shot parameter averaging and ADMM.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/shamir14.pdf",
        "supp": "",
        "pdf_size": 341911,
        "gs_citation": 737,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13028554689829022708&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Science and Applied Mathematics, Weizmann Institute of Science, Rehovot, Israel; Toyota Technological Institute at Chicago and the Department of Computer Science, Technion, Haifa, Israel; Department of Statistics, Rutgers University, Piscataway NJ, USA + Baidu Inc., Beijing, China",
        "aff_domain": "weizmann.ac.il;ttic.edu;stat.rutgers.edu",
        "email": "weizmann.ac.il;ttic.edu;stat.rutgers.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2+3",
        "aff_unique_norm": "Weizmann Institute of Science;Toyota Technological Institute at Chicago;Rutgers University;Baidu",
        "aff_unique_dep": "Department of Computer Science and Applied Mathematics;Computer Science;Department of Statistics;Baidu Inc.",
        "aff_unique_url": "https://www.weizmann.ac.il;https://www.tti-chicago.org;https://www.rutgers.edu;https://www.baidu.com",
        "aff_unique_abbr": "Weizmann;TTI-Chicago;Rutgers;Baidu",
        "aff_campus_unique_index": "0;1;2+3",
        "aff_campus_unique": "Rehovot;Chicago;Piscataway;Beijing",
        "aff_country_unique_index": "0;1;1+2",
        "aff_country_unique": "Israel;United States;China"
    },
    {
        "id": "93c990baf9",
        "title": "Compact Random Feature Maps",
        "site": "https://proceedings.mlr.press/v32/hamid14.html",
        "author": "Raffay Hamid; Ying Xiao; Alex Gittens; Dennis Decoste",
        "abstract": "Kernel approximation using randomized feature maps has recently gained a lot of interest. In this work, we identify that previous approaches for polynomial kernel approximation create maps that are rank deficient, and therefore do not utilize the capacity of the projected feature space effectively. To address this challenge, we propose compact random feature maps (CRAFTMaps) to approximate polynomial kernels more concisely and accurately. We prove the error bounds of CRAFTMaps demonstrating their superior kernel reconstruction performance compared to the previous approximation schemes. We show how structured random matrices can be used to efficiently generate CRAFTMaps, and present a single-pass algorithm using CRAFTMaps to learn non-linear multi-class classifiers. We present experiments on multiple standard data-sets with performance competitive with state-of-the-art results.",
        "bibtex": "@InProceedings{pmlr-v32-hamid14,\n  title = \t {Compact Random Feature Maps},\n  author = \t {Hamid, Raffay and Xiao, Ying and Gittens, Alex and Decoste, Dennis},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {19--27},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/hamid14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/hamid14.html},\n  abstract = \t {Kernel approximation using randomized feature maps has recently gained a lot of interest. In this work, we identify that previous approaches for polynomial kernel approximation create maps that are rank deficient, and therefore do not utilize the capacity of the projected feature space effectively. To address this challenge, we propose compact random feature maps (CRAFTMaps) to approximate polynomial kernels more concisely and accurately. We prove the error bounds of CRAFTMaps demonstrating their superior kernel reconstruction performance compared to the previous approximation schemes. We show how structured random matrices can be used to efficiently generate CRAFTMaps, and present a single-pass algorithm using CRAFTMaps to learn non-linear multi-class classifiers. We present experiments on multiple standard data-sets with performance competitive with state-of-the-art results.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/hamid14.pdf",
        "supp": "",
        "pdf_size": 1789138,
        "gs_citation": 96,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1190916346984988371&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "eBay Research Laboratory; Georgia Institute of Technology + eBay Research Laboratory; eBay Research Laboratory; eBay Research Laboratory",
        "aff_domain": "CC.GATECH.EDU;GATECH.EDU;EBAY.COM;EBAY.COM",
        "email": "CC.GATECH.EDU;GATECH.EDU;EBAY.COM;EBAY.COM",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;0;0",
        "aff_unique_norm": "eBay;Georgia Institute of Technology",
        "aff_unique_dep": "eBay Research Laboratory;",
        "aff_unique_url": "https://www.ebayinc.com;https://www.gatech.edu",
        "aff_unique_abbr": "eBay;Georgia Tech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e05372a270",
        "title": "Composite Quantization for Approximate Nearest Neighbor Search",
        "site": "https://proceedings.mlr.press/v32/zhangd14.html",
        "author": "Ting Zhang; Chao Du; Jingdong Wang",
        "abstract": "This paper presents a novel compact coding approach, composite quantization, for approximate nearest neighbor search. The idea is to use the composition of several elements selected from the dictionaries to accurately approximate a vector and to represent the vector by a short code composed of the indices of the selected elements. To efficiently compute the approximate distance of a query to a database vector using the short code, we introduce an extra constraint, constant inter-dictionary-element-product, resulting in that  approximating the distance only using the distance of the query to each selected element is enough for nearest neighbor search. Experimental comparison with state-of-the-art algorithms over several benchmark datasets demonstrates the efficacy of the proposed approach.",
        "bibtex": "@InProceedings{pmlr-v32-zhangd14,\n  title = \t {Composite Quantization for Approximate Nearest Neighbor Search},\n  author = \t {Zhang, Ting and Du, Chao and Wang, Jingdong},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {838--846},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/zhangd14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/zhangd14.html},\n  abstract = \t {This paper presents a novel compact coding approach, composite quantization, for approximate nearest neighbor search. The idea is to use the composition of several elements selected from the dictionaries to accurately approximate a vector and to represent the vector by a short code composed of the indices of the selected elements. To efficiently compute the approximate distance of a query to a database vector using the short code, we introduce an extra constraint, constant inter-dictionary-element-product, resulting in that  approximating the distance only using the distance of the query to each selected element is enough for nearest neighbor search. Experimental comparison with state-of-the-art algorithms over several benchmark datasets demonstrates the efficacy of the proposed approach.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/zhangd14.pdf",
        "supp": "",
        "pdf_size": 248924,
        "gs_citation": 269,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17440651735303000857&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "University of Science and Technology of China, Hefei, P.R. China + Microsoft Research, Beijing, P.R. China; Tsinghua University, Beijing, P.R. China; Microsoft Research, Beijing, P.R. China",
        "aff_domain": "mail.ustc.edu.cn;gmail.com;microsoft.com",
        "email": "mail.ustc.edu.cn;gmail.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;1",
        "aff_unique_norm": "University of Science and Technology of China;Microsoft;Tsinghua University",
        "aff_unique_dep": ";Microsoft Research;",
        "aff_unique_url": "http://www.ustc.edu.cn;https://www.microsoft.com/en-us/research/group/microsoft-research-asia;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "USTC;MSR;THU",
        "aff_campus_unique_index": "0+1;1;1",
        "aff_campus_unique": "Hefei;Beijing",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "804ad51a30",
        "title": "Compositional Morphology for Word Representations and Language Modelling",
        "site": "https://proceedings.mlr.press/v32/botha14.html",
        "author": "Jan Botha; Phil Blunsom",
        "abstract": "This paper presents a scalable method for integrating compositional morphological representations into a vector-based probabilistic language model. Our approach is evaluated in the context of log-bilinear language models, rendered suitably efficient for implementation inside a machine translation decoder by factoring the vocabulary. We perform both intrinsic and extrinsic evaluations, presenting results on a range of languages which demonstrate that our model learns morphological representations that both perform well on word similarity tasks and lead to substantial reductions in perplexity. When used for translation into morphologically rich languages with large vocabularies, our models obtain improvements of up to 1.2 BLEU points relative to a baseline system using back-off n-gram models.",
        "bibtex": "@InProceedings{pmlr-v32-botha14,\n  title = \t {Compositional Morphology for Word Representations and Language Modelling},\n  author = \t {Botha, Jan and Blunsom, Phil},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1899--1907},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/botha14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/botha14.html},\n  abstract = \t {This paper presents a scalable method for integrating compositional morphological representations into a vector-based probabilistic language model. Our approach is evaluated in the context of log-bilinear language models, rendered suitably efficient for implementation inside a machine translation decoder by factoring the vocabulary. We perform both intrinsic and extrinsic evaluations, presenting results on a range of languages which demonstrate that our model learns morphological representations that both perform well on word similarity tasks and lead to substantial reductions in perplexity. When used for translation into morphologically rich languages with large vocabularies, our models obtain improvements of up to 1.2 BLEU points relative to a baseline system using back-off n-gram models.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/botha14.pdf",
        "supp": "",
        "pdf_size": 695237,
        "gs_citation": 313,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13147358520861020086&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "Department of Computer Science, University of Oxford, Oxford, OX1 3QD, UK; Department of Computer Science, University of Oxford, Oxford, OX1 3QD, UK",
        "aff_domain": "CS.OX.AC.UK;CS.OX.AC.UK",
        "email": "CS.OX.AC.UK;CS.OX.AC.UK",
        "github": "http://bothameister.github.io",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Oxford",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ox.ac.uk",
        "aff_unique_abbr": "Oxford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Oxford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "d8f93df15b",
        "title": "Computing Parametric Ranking Models via Rank-Breaking",
        "site": "https://proceedings.mlr.press/v32/soufiani14.html",
        "author": "Hossein Azari Soufiani; David Parkes; Lirong Xia",
        "abstract": "Rank breaking is a methodology introduced by Azari Soufiani et al. (2013a) for applying a Generalized Method of Moments (GMM) algorithm to the estimation of parametric ranking models. Breaking takes full rankings and breaks, or splits them up, into counts for pairs of alternatives that occur in particular positions (e.g., first place and second place, second place and third place). GMMs are of interest because they can achieve significant speed-up relative to maximum likelihood approaches and comparable statistical efficiency. We characterize the breakings for which the estimator is consistent for random utility models (RUMs) including Plackett-Luce and Normal-RUM, develop a general sufficient condition for a full breaking to be the only consistent breaking, and provide a trichotomy theorem in regard to single-edge breakings. Experimental results are presented to show the computational efficiency along with statistical performance of the proposed method.",
        "bibtex": "@InProceedings{pmlr-v32-soufiani14,\n  title = \t {Computing Parametric Ranking Models via Rank-Breaking},\n  author = \t {Soufiani, Hossein Azari and Parkes, David and Xia, Lirong},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {360--368},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/soufiani14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/soufiani14.html},\n  abstract = \t {Rank breaking is a methodology introduced by Azari Soufiani et al. (2013a) for applying a Generalized Method of Moments (GMM) algorithm to the estimation of parametric ranking models. Breaking takes full rankings and breaks, or splits them up, into counts for pairs of alternatives that occur in particular positions (e.g., first place and second place, second place and third place). GMMs are of interest because they can achieve significant speed-up relative to maximum likelihood approaches and comparable statistical efficiency. We characterize the breakings for which the estimator is consistent for random utility models (RUMs) including Plackett-Luce and Normal-RUM, develop a general sufficient condition for a full breaking to be the only consistent breaking, and provide a trichotomy theorem in regard to single-edge breakings. Experimental results are presented to show the computational efficiency along with statistical performance of the proposed method.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/soufiani14.pdf",
        "supp": "",
        "pdf_size": 436136,
        "gs_citation": 83,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1639071057951886163&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Harvard University; Harvard University; Rensselaer Polytechnic Institute",
        "aff_domain": "FAS.HARVARD.EDU;EECS.HARVARD.EDU;CS.RPI.EDU",
        "email": "FAS.HARVARD.EDU;EECS.HARVARD.EDU;CS.RPI.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Harvard University;Rensselaer Polytechnic Institute",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.harvard.edu;https://www.rpi.edu",
        "aff_unique_abbr": "Harvard;RPI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3d2c37a9ea",
        "title": "Concentration in unbounded metric spaces and algorithmic stability",
        "site": "https://proceedings.mlr.press/v32/kontorovicha14.html",
        "author": "Aryeh Kontorovich",
        "abstract": "We prove an extension of McDiarmid\u2019s inequality for metric spaces with unbounded diameter.  To this end, we introduce the notion of the \\em subgaussian diameter,  which is a distribution-dependent refinement of the metric diameter.  Our technique provides an alternative approach to that of Kutin and Niyogi\u2019s   method of weakly difference-bounded functions, and yields nontrivial,   dimension-free results in some interesting cases where the former does not.  As an application, we give apparently the first generalization bound in the  algorithmic stability setting that holds for unbounded loss functions.  This yields a novel risk bound for some regularized metric regression algorithms.  We give two extensions of the basic concentration result.  The first enables one to replace the independence assumption by appropriate strong mixing.  The second generalizes the subgaussian technique to other Orlicz norms.",
        "bibtex": "@InProceedings{pmlr-v32-kontorovicha14,\n  title = \t {Concentration in unbounded metric spaces and algorithmic stability},\n  author = \t {Kontorovich, Aryeh},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {28--36},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/kontorovicha14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/kontorovicha14.html},\n  abstract = \t {We prove an extension of McDiarmid\u2019s inequality for metric spaces with unbounded diameter.  To this end, we introduce the notion of the \\em subgaussian diameter,  which is a distribution-dependent refinement of the metric diameter.  Our technique provides an alternative approach to that of Kutin and Niyogi\u2019s   method of weakly difference-bounded functions, and yields nontrivial,   dimension-free results in some interesting cases where the former does not.  As an application, we give apparently the first generalization bound in the  algorithmic stability setting that holds for unbounded loss functions.  This yields a novel risk bound for some regularized metric regression algorithms.  We give two extensions of the basic concentration result.  The first enables one to replace the independence assumption by appropriate strong mixing.  The second generalizes the subgaussian technique to other Orlicz norms.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/kontorovicha14.pdf",
        "supp": "",
        "pdf_size": 243740,
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13667983621231938329&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, Ben-Gurion University",
        "aff_domain": "CS.BGU.AC.IL",
        "email": "CS.BGU.AC.IL",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Ben-Gurion University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.bgu.ac.il",
        "aff_unique_abbr": "BGU",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "4d4c213c9f",
        "title": "Concept Drift Detection Through Resampling",
        "site": "https://proceedings.mlr.press/v32/harel14.html",
        "author": "Maayan Harel; Shie Mannor; Ran El-Yaniv; Koby Crammer",
        "abstract": "Detecting changes in data-streams is an important part of enhancing learning quality in dynamic environments. We devise a procedure for detecting concept drifts in data-streams that relies on analyzing the empirical loss of learning algorithms. Our method is based on obtaining statistics from the loss distribution by reusing the data multiple times via resampling. We present theoretical guarantees for the proposed procedure based on the stability of the underlying learning algorithms. Experimental results show that the detection method has high recall and precision, and performs well in the presence of noise.",
        "bibtex": "@InProceedings{pmlr-v32-harel14,\n  title = \t {Concept Drift Detection Through Resampling},\n  author = \t {Harel, Maayan and Mannor, Shie and El-Yaniv, Ran and Crammer, Koby},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1009--1017},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/harel14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/harel14.html},\n  abstract = \t {Detecting changes in data-streams is an important part of enhancing learning quality in dynamic environments. We devise a procedure for detecting concept drifts in data-streams that relies on analyzing the empirical loss of learning algorithms. Our method is based on obtaining statistics from the loss distribution by reusing the data multiple times via resampling. We present theoretical guarantees for the proposed procedure based on the stability of the underlying learning algorithms. Experimental results show that the detection method has high recall and precision, and performs well in the presence of noise.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/harel14.pdf",
        "supp": "",
        "pdf_size": 329496,
        "gs_citation": 156,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8993996213599482750&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Technion - Israel Institute of Technology, Haifa, Israel; Technion - Israel Institute of Technology, Haifa, Israel; Technion - Israel Institute of Technology, Haifa, Israel; Technion - Israel Institute of Technology, Haifa, Israel",
        "aff_domain": "tx.technion.ac.il;ee.technion.ac.il;cs.technion.ac.il;ee.technion.ac.il",
        "email": "tx.technion.ac.il;ee.technion.ac.il;cs.technion.ac.il;ee.technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Technion - Israel Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.technion.ac.il",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Haifa",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "6df4acbe37",
        "title": "Condensed Filter Tree for Cost-Sensitive Multi-Label Classification",
        "site": "https://proceedings.mlr.press/v32/lia14.html",
        "author": "Chun-Liang Li; Hsuan-Tien Lin",
        "abstract": "Different real-world applications of multi-label classification often demand different evaluation criteria. We formalize this demand with a general setup, cost-sensitive multi-label classification (CSMLC), which takes  the evaluation criteria into account during learning. Nevertheless, most existing algorithms can only focus on optimizing a few specific evaluation criteria, and cannot systematically deal with different ones. In this paper, we propose a novel algorithm, called condensed filter tree (CFT), for optimizing any criteria in CSMLC. CFT is derived from reducing CSMLC to the famous filter tree algorithm for cost-sensitive multi-class classification via constructing the label powerset. We successfully cope with the difficulty of having exponentially many extended-classes within the powerset for representation, training and prediction by carefully designing the tree structure and focusing on the key nodes. Experimental results across many real-world datasets validate that CFT is competitive with special purpose algorithms on special criteria and reaches better performance on general criteria.",
        "bibtex": "@InProceedings{pmlr-v32-lia14,\n  title = \t {Condensed Filter Tree for Cost-Sensitive Multi-Label Classification},\n  author = \t {Li, Chun-Liang and Lin, Hsuan-Tien},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {423--431},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/lia14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/lia14.html},\n  abstract = \t {Different real-world applications of multi-label classification often demand different evaluation criteria. We formalize this demand with a general setup, cost-sensitive multi-label classification (CSMLC), which takes  the evaluation criteria into account during learning. Nevertheless, most existing algorithms can only focus on optimizing a few specific evaluation criteria, and cannot systematically deal with different ones. In this paper, we propose a novel algorithm, called condensed filter tree (CFT), for optimizing any criteria in CSMLC. CFT is derived from reducing CSMLC to the famous filter tree algorithm for cost-sensitive multi-class classification via constructing the label powerset. We successfully cope with the difficulty of having exponentially many extended-classes within the powerset for representation, training and prediction by carefully designing the tree structure and focusing on the key nodes. Experimental results across many real-world datasets validate that CFT is competitive with special purpose algorithms on special criteria and reaches better performance on general criteria.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/lia14.pdf",
        "supp": "",
        "pdf_size": 354547,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8624148478501431802&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science and Information Engineering, National Taiwan University; Department of Computer Science and Information Engineering, National Taiwan University",
        "aff_domain": "CSIE.NTU.EDU.TW;CSIE.NTU.EDU.TW",
        "email": "CSIE.NTU.EDU.TW;CSIE.NTU.EDU.TW",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "National Taiwan University",
        "aff_unique_dep": "Department of Computer Science and Information Engineering",
        "aff_unique_url": "https://www.ntu.edu.tw",
        "aff_unique_abbr": "NTU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Taiwan",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "5348a1e2ef",
        "title": "Consistency of Causal Inference under the Additive Noise Model",
        "site": "https://proceedings.mlr.press/v32/kpotufe14.html",
        "author": "Samory Kpotufe; Eleni Sgouritsa; Dominik Janzing; Bernhard Sch\u00f6lkopf",
        "abstract": "We analyze a family of methods for statistical  causal inference from sample under the so-called  Additive Noise Model. While most work  on the subject has concentrated on establishing  the soundness of the Additive Noise Model, the  statistical consistency of the resulting inference  methods has received little attention. We derive  general conditions under which the given family  of inference methods consistently infers the  causal direction in a nonparametric setting.",
        "bibtex": "@InProceedings{pmlr-v32-kpotufe14,\n  title = \t {Consistency of Causal Inference under the Additive Noise Model},\n  author = \t {Kpotufe, Samory and Sgouritsa, Eleni and Janzing, Dominik and Sch\u00f6lkopf, Bernhard},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {478--486},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/kpotufe14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/kpotufe14.html},\n  abstract = \t {We analyze a family of methods for statistical  causal inference from sample under the so-called  Additive Noise Model. While most work  on the subject has concentrated on establishing  the soundness of the Additive Noise Model, the  statistical consistency of the resulting inference  methods has received little attention. We derive  general conditions under which the given family  of inference methods consistently infers the  causal direction in a nonparametric setting.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/kpotufe14.pdf",
        "supp": "",
        "pdf_size": 307486,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18269698956495115540&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Toyota Technological Institute-Chicago; Max Planck Institute for Intelligent Systems; Max Planck Institute for Intelligent Systems; Max Planck Institute for Intelligent Systems",
        "aff_domain": "TTIC.EDU;TUEBINGEN.MPG.DE;TUEBINGEN.MPG.DE;TUEBINGEN.MPG.DE",
        "email": "TTIC.EDU;TUEBINGEN.MPG.DE;TUEBINGEN.MPG.DE;TUEBINGEN.MPG.DE",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Toyota Technological Institute;Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": ";Intelligent Systems",
        "aff_unique_url": "https://www.tti-chicago.org;https://www.mpi-is.mpg.de",
        "aff_unique_abbr": "TTI-Chicago;MPI-IS",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Chicago;",
        "aff_country_unique_index": "0;1;1;1",
        "aff_country_unique": "United States;Germany"
    },
    {
        "id": "5c31bf0058",
        "title": "Convergence rates for persistence diagram estimation in Topological Data Analysis",
        "site": "https://proceedings.mlr.press/v32/chazal14.html",
        "author": "Fr\u00e9d\u00e9ric Chazal; Marc Glisse; Catherine Labru\u00e8re; Bertrand Michel",
        "abstract": "Computational topology  has recently seen an important development toward data analysis, giving birth to Topological Data Analysis. Persistent homology appears as a fundamental tool in this field. We show that  the use of persistent homology can be naturally considered in general statistical frameworks. We establish convergence rates of persistence diagrams associated to data randomly sampled from any compact metric space to a well defined limit diagram encoding the topological features of the support of the measure from which the data have been sampled. Our approach relies on a recent and deep stability result for persistence that allows to relate our problem to support estimation problems (with respect to the Gromov-Hausdorff distance). Some numerical experiments are performed in various contexts to illustrate our results.",
        "bibtex": "@InProceedings{pmlr-v32-chazal14,\n  title = \t {Convergence rates for persistence diagram estimation in Topological Data Analysis},\n  author = \t {Chazal, Fr\u00e9d\u00e9ric and Glisse, Marc and Labru\u00e8re, Catherine and Michel, Bertrand},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {163--171},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/chazal14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/chazal14.html},\n  abstract = \t {Computational topology  has recently seen an important development toward data analysis, giving birth to Topological Data Analysis. Persistent homology appears as a fundamental tool in this field. We show that  the use of persistent homology can be naturally considered in general statistical frameworks. We establish convergence rates of persistence diagrams associated to data randomly sampled from any compact metric space to a well defined limit diagram encoding the topological features of the support of the measure from which the data have been sampled. Our approach relies on a recent and deep stability result for persistence that allows to relate our problem to support estimation problems (with respect to the Gromov-Hausdorff distance). Some numerical experiments are performed in various contexts to illustrate our results.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/chazal14.pdf",
        "supp": "",
        "pdf_size": 557415,
        "gs_citation": 129,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6798638585811361027&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 22,
        "aff": "INRIA Saclay \u00cele-de-France, Palaiseau, France; INRIA Saclay \u00cele-de-France, Palaiseau, France; Institut de Math\u00e9matiques de Bourgogne, France; LSTA, Universit\u00e9 Pierre et Marie Curie, Paris",
        "aff_domain": "INRIA.FR;INRIA.FR;U-BOURGOGNE.FR;UPMC.FR",
        "email": "INRIA.FR;INRIA.FR;U-BOURGOGNE.FR;UPMC.FR",
        "github": "",
        "project": "https://sites.google.com/site/nips2012topology/",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "INRIA;Institut de Math\u00e9matiques de Bourgogne;Universit\u00e9 Pierre et Marie Curie",
        "aff_unique_dep": ";Math\u00e9matiques;LSTA",
        "aff_unique_url": "https://www.inria.fr;;https://www.upmc.fr",
        "aff_unique_abbr": "INRIA;;UPMC",
        "aff_campus_unique_index": "0;0;2",
        "aff_campus_unique": "Saclay \u00cele-de-France;;Paris",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "49e6342338",
        "title": "Convex Total Least Squares",
        "site": "https://proceedings.mlr.press/v32/malioutov14.html",
        "author": "Dmitry Malioutov; Nikolai Slavov",
        "abstract": "We study the total least squares (TLS) problem that generalizes least squares regression by allowing measurement errors in both dependent and independent variables. TLS is widely used in applied fields including computer vision, system identification and econometrics.  The special case when all dependent and independent variables have the same  level of uncorrelated Gaussian noise, known  as ordinary TLS, can be solved by singular  value decomposition (SVD). However, SVD cannot solve many important practical TLS  problems with realistic noise structure, such  as having varying measurement noise, known  structure on the errors, or large outliers requiring robust error-norms. To solve such  problems, we develop convex relaxation approaches for a general class of structured  TLS (STLS). We show both theoretically  and experimentally, that while the plain nuclear  norm relaxation incurs large approximation errors for STLS, the re-weighted nuclear  norm approach is very effective, and achieves better accuracy on challenging STLS  problems than popular non-convex solvers.  We describe a fast solution based on augmented  Lagrangian formulation, and apply our approach to an important class of biological  problems that use population average measurements to infer cell-type and  physiological-state specific expression levels that are very hard to measure directly.",
        "bibtex": "@InProceedings{pmlr-v32-malioutov14,\n  title = \t {Convex Total Least Squares},\n  author = \t {Malioutov, Dmitry and Slavov, Nikolai},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {109--117},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/malioutov14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/malioutov14.html},\n  abstract = \t {We study the total least squares (TLS) problem that generalizes least squares regression by allowing measurement errors in both dependent and independent variables. TLS is widely used in applied fields including computer vision, system identification and econometrics.  The special case when all dependent and independent variables have the same  level of uncorrelated Gaussian noise, known  as ordinary TLS, can be solved by singular  value decomposition (SVD). However, SVD cannot solve many important practical TLS  problems with realistic noise structure, such  as having varying measurement noise, known  structure on the errors, or large outliers requiring robust error-norms. To solve such  problems, we develop convex relaxation approaches for a general class of structured  TLS (STLS). We show both theoretically  and experimentally, that while the plain nuclear  norm relaxation incurs large approximation errors for STLS, the re-weighted nuclear  norm approach is very effective, and achieves better accuracy on challenging STLS  problems than popular non-convex solvers.  We describe a fast solution based on augmented  Lagrangian formulation, and apply our approach to an important class of biological  problems that use population average measurements to infer cell-type and  physiological-state specific expression levels that are very hard to measure directly.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/malioutov14.pdf",
        "supp": "",
        "pdf_size": 635639,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5612747825485202110&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "IBM Research, 1101 Kitchawan Road, Yorktown Heights, NY 10598 USA; Departments of Physics and Biology, MIT, 77 Massachusetts Avenue, Cambridge, MA 02139, USA",
        "aff_domain": "US.IBM.COM;ALUM.MIT.EDU",
        "email": "US.IBM.COM;ALUM.MIT.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "IBM;Massachusetts Institute of Technology",
        "aff_unique_dep": "IBM Research;Departments of Physics and Biology",
        "aff_unique_url": "https://www.ibm.com/research;https://www.mit.edu",
        "aff_unique_abbr": "IBM;MIT",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Yorktown Heights;Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4e5e0490d6",
        "title": "Coordinate-descent for learning orthogonal matrices through Givens rotations",
        "site": "https://proceedings.mlr.press/v32/shalit14.html",
        "author": "Uri Shalit; Gal Chechik",
        "abstract": "Optimizing over the set of orthogonal matrices is a central component in problems like sparse-PCA or tensor decomposition. Unfortunately, such optimization is hard since simple operations on orthogonal matrices easily break orthogonality, and correcting orthogonality usually costs a large amount of computation.  Here we propose a framework for optimizing orthogonal matrices, that is the parallel of coordinate-descent in Euclidean spaces. It is based on \\em Givens-rotations, a fast-to-compute operation that affects a small number of entries in the learned matrix, and preserves orthogonality.  We show two applications of this approach: an algorithm for tensor decompositions used in learning mixture models, and an algorithm for sparse-PCA. We study the parameter regime where a  Givens rotation approach converges faster and achieves a superior model on a genome-wide brain-wide mRNA expression dataset.",
        "bibtex": "@InProceedings{pmlr-v32-shalit14,\n  title = \t {Coordinate-descent for learning orthogonal matrices through Givens rotations},\n  author = \t {Shalit, Uri and Chechik, Gal},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {548--556},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/shalit14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/shalit14.html},\n  abstract = \t {Optimizing over the set of orthogonal matrices is a central component in problems like sparse-PCA or tensor decomposition. Unfortunately, such optimization is hard since simple operations on orthogonal matrices easily break orthogonality, and correcting orthogonality usually costs a large amount of computation.  Here we propose a framework for optimizing orthogonal matrices, that is the parallel of coordinate-descent in Euclidean spaces. It is based on \\em Givens-rotations, a fast-to-compute operation that affects a small number of entries in the learned matrix, and preserves orthogonality.  We show two applications of this approach: an algorithm for tensor decompositions used in learning mixture models, and an algorithm for sparse-PCA. We study the parameter regime where a  Givens rotation approach converges faster and achieves a superior model on a genome-wide brain-wide mRNA expression dataset.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/shalit14.pdf",
        "supp": "",
        "pdf_size": 201651,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=469319123347613941&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "ICNC-ELSC & Computer Science Department, The Hebrew University of Jerusalem, 91904 Jerusalem Israel + The Gonda Brain Research Center, Bar Ilan University, 52900 Ramat-Gan, Israel; The Gonda Brain Research Center, Bar Ilan University, 52900 Ramat-Gan, Israel",
        "aff_domain": "MAIL.HUJI.AC.IL;BIU.AC.IL",
        "email": "MAIL.HUJI.AC.IL;BIU.AC.IL",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1",
        "aff_unique_norm": "Hebrew University of Jerusalem;Bar-Ilan University",
        "aff_unique_dep": "Computer Science Department;Gonda Brain Research Center",
        "aff_unique_url": "http://www.huji.ac.il;https://www.biu.ac.il",
        "aff_unique_abbr": "HUJI;BIU",
        "aff_campus_unique_index": "0+1;1",
        "aff_campus_unique": "Jerusalem;Ramat-Gan",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "ffa1f63756",
        "title": "Coupled Group Lasso for Web-Scale CTR Prediction in Display Advertising",
        "site": "https://proceedings.mlr.press/v32/yan14.html",
        "author": "Ling Yan; Wu-Jun Li; Gui-Rong Xue; Dingyi Han",
        "abstract": "In display advertising, click through rate(CTR) prediction is the problem of estimating the probability  that an advertisement (ad) is clicked when displayed to a user in a specific context. Due to its easy implementation and promising performance, logistic regression(LR) model has been widely used for CTR prediction, especially in industrial systems. However, it is not easy for LR to capture the nonlinear information, such as the conjunction information, from user features and ad features. In this paper, we propose a novel model, called coupled group lasso(CGL), for CTR prediction in display advertising. CGL can seamlessly integrate the conjunction information from user features and ad features for modeling. Furthermore, CGL can automatically eliminate useless features for both users and ads, which may facilitate fast online prediction. Scalability of CGL is ensured through feature hashing and distributed implementation. Experimental results on real-world data sets show that our CGL model can achieve state-of-the-art performance on web-scale CTR prediction tasks.",
        "bibtex": "@InProceedings{pmlr-v32-yan14,\n  title = \t {Coupled Group Lasso for Web-Scale CTR Prediction in Display Advertising},\n  author = \t {Yan, Ling and Li, Wu-Jun and Xue, Gui-Rong and Han, Dingyi},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {802--810},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/yan14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/yan14.html},\n  abstract = \t {In display advertising, click through rate(CTR) prediction is the problem of estimating the probability  that an advertisement (ad) is clicked when displayed to a user in a specific context. Due to its easy implementation and promising performance, logistic regression(LR) model has been widely used for CTR prediction, especially in industrial systems. However, it is not easy for LR to capture the nonlinear information, such as the conjunction information, from user features and ad features. In this paper, we propose a novel model, called coupled group lasso(CGL), for CTR prediction in display advertising. CGL can seamlessly integrate the conjunction information from user features and ad features for modeling. Furthermore, CGL can automatically eliminate useless features for both users and ads, which may facilitate fast online prediction. Scalability of CGL is ensured through feature hashing and distributed implementation. Experimental results on real-world data sets show that our CGL model can achieve state-of-the-art performance on web-scale CTR prediction tasks.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/yan14.pdf",
        "supp": "",
        "pdf_size": 566474,
        "gs_citation": 153,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18002918009877973364&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Shanghai Key Laboratory of Scalable Computing and Systems, Department of Computer Science and Engineering, Shanghai Jiao Tong University, China; National Key Laboratory for Novel Software Technology, Department of Computer Science and Technology, Nanjing University, China; Alibaba Group, China; Alibaba Group, China",
        "aff_domain": "sjtu.edu.cn;nju.edu.cn;alibaba-inc.com;alibaba-inc.com",
        "email": "sjtu.edu.cn;nju.edu.cn;alibaba-inc.com;alibaba-inc.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;2",
        "aff_unique_norm": "Shanghai Jiao Tong University;Nanjing University;Alibaba Group",
        "aff_unique_dep": "Department of Computer Science and Engineering;Department of Computer Science and Technology;",
        "aff_unique_url": "https://www.sjtu.edu.cn;http://www.nju.edu.cn;https://www.alibaba.com",
        "aff_unique_abbr": "SJTU;Nanjing U;Alibaba",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "8da91e70bb",
        "title": "Covering Number for Efficient Heuristic-based POMDP Planning",
        "site": "https://proceedings.mlr.press/v32/zhanga14.html",
        "author": "Zongzhang Zhang; David Hsu; Wee Sun Lee",
        "abstract": "The difficulty of POMDP planning depends on the size of the search space involved. Heuristics are often used to reduce the search space size and improve computational efficiency; however, there are few theoretical bounds on their effectiveness.  In this paper, we use the covering number to characterize the size of the search space reachable under heuristics and  connect the complexity of POMDP planning to the effectiveness of heuristics. With insights from the theoretical analysis, we have developed  a practical POMDP algorithm, Packing-Guided Value Iteration (PGVI). Empirically, PGVI is competitive with the state-of-the-art point-based POMDP algorithms on 65 small benchmark problems and outperforms them on 4 larger problems.",
        "bibtex": "@InProceedings{pmlr-v32-zhanga14,\n  title = \t {Covering Number for Efficient Heuristic-based POMDP Planning},\n  author = \t {Zhang, Zongzhang and Hsu, David and Lee, Wee Sun},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {28--36},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/zhanga14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/zhanga14.html},\n  abstract = \t {The difficulty of POMDP planning depends on the size of the search space involved. Heuristics are often used to reduce the search space size and improve computational efficiency; however, there are few theoretical bounds on their effectiveness.  In this paper, we use the covering number to characterize the size of the search space reachable under heuristics and  connect the complexity of POMDP planning to the effectiveness of heuristics. With insights from the theoretical analysis, we have developed  a practical POMDP algorithm, Packing-Guided Value Iteration (PGVI). Empirically, PGVI is competitive with the state-of-the-art point-based POMDP algorithms on 65 small benchmark problems and outperforms them on 4 larger problems.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/zhanga14.pdf",
        "supp": "",
        "pdf_size": 349364,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15885065531960493643&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, National University of Singapore, Singapore 117417, Singapore; Department of Computer Science, National University of Singapore, Singapore 117417, Singapore; Department of Computer Science, National University of Singapore, Singapore 117417, Singapore",
        "aff_domain": "comp.nus.edu.sg;comp.nus.edu.sg;comp.nus.edu.sg",
        "email": "comp.nus.edu.sg;comp.nus.edu.sg;comp.nus.edu.sg",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "National University of Singapore",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.nus.edu.sg",
        "aff_unique_abbr": "NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "6faf22bdf3",
        "title": "DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition",
        "site": "https://proceedings.mlr.press/v32/donahue14.html",
        "author": "Jeff Donahue; Yangqing Jia; Oriol Vinyals; Judy Hoffman; Ning Zhang; Eric Tzeng; Trevor Darrell",
        "abstract": "We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks.  Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks.  We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges.  We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges.  We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.",
        "bibtex": "@InProceedings{pmlr-v32-donahue14,\n  title = \t {DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition},\n  author = \t {Donahue, Jeff and Jia, Yangqing and Vinyals, Oriol and Hoffman, Judy and Zhang, Ning and Tzeng, Eric and Darrell, Trevor},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {647--655},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/donahue14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/donahue14.html},\n  abstract = \t {We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks.  Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks.  We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges.  We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges.  We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/donahue14.pdf",
        "supp": "",
        "pdf_size": 3819584,
        "gs_citation": 6288,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2385750463715067520&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "UC Berkeley & ICSI, Berkeley, CA, USA; UC Berkeley & ICSI, Berkeley, CA, USA; UC Berkeley & ICSI, Berkeley, CA, USA; UC Berkeley & ICSI, Berkeley, CA, USA; UC Berkeley & ICSI, Berkeley, CA, USA; UC Berkeley & ICSI, Berkeley, CA, USA; UC Berkeley & ICSI, Berkeley, CA, USA",
        "aff_domain": "EECS.BERKELEY.EDU;EECS.BERKELEY.EDU;EECS.BERKELEY.EDU;EECS.BERKELEY.EDU;EECS.BERKELEY.EDU;EECS.BERKELEY.EDU;EECS.BERKELEY.EDU",
        "email": "EECS.BERKELEY.EDU;EECS.BERKELEY.EDU;EECS.BERKELEY.EDU;EECS.BERKELEY.EDU;EECS.BERKELEY.EDU;EECS.BERKELEY.EDU;EECS.BERKELEY.EDU",
        "github": "",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8ba6a953bd",
        "title": "Deep AutoRegressive Networks",
        "site": "https://proceedings.mlr.press/v32/gregor14.html",
        "author": "Karol Gregor; Ivo Danihelka; Andriy Mnih; Charles Blundell; Daan Wierstra",
        "abstract": "We introduce a deep, generative autoencoder capable of learning hierarchies of distributed representations from data.  Successive deep stochastic hidden layers are equipped with autoregressive connections, which enable the model to be sampled from quickly and exactly via ancestral sampling.  We derive an efficient approximate parameter estimation method based on the minimum  description length (MDL) principle,  which can be seen as maximising a variational lower bound on the log-likelihood, with a feedforward neural network implementing approximate inference.   We demonstrate state-of-the-art generative performance on a number of classic data sets: several UCI data sets, MNIST and Atari 2600 games.",
        "bibtex": "@InProceedings{pmlr-v32-gregor14,\n  title = \t {Deep AutoRegressive Networks},\n  author = \t {Gregor, Karol and Danihelka, Ivo and Mnih, Andriy and Blundell, Charles and Wierstra, Daan},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1242--1250},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/gregor14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/gregor14.html},\n  abstract = \t {We introduce a deep, generative autoencoder capable of learning hierarchies of distributed representations from data.  Successive deep stochastic hidden layers are equipped with autoregressive connections, which enable the model to be sampled from quickly and exactly via ancestral sampling.  We derive an efficient approximate parameter estimation method based on the minimum  description length (MDL) principle,  which can be seen as maximising a variational lower bound on the log-likelihood, with a feedforward neural network implementing approximate inference.   We demonstrate state-of-the-art generative performance on a number of classic data sets: several UCI data sets, MNIST and Atari 2600 games.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/gregor14.pdf",
        "supp": "",
        "pdf_size": 405646,
        "gs_citation": 386,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7200899607570189084&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Google DeepMind; Google DeepMind; Google DeepMind; Google DeepMind; Google DeepMind",
        "aff_domain": "GOOGLE.COM;GOOGLE.COM;GOOGLE.COM;GOOGLE.COM;GOOGLE.COM",
        "email": "GOOGLE.COM;GOOGLE.COM;GOOGLE.COM;GOOGLE.COM;GOOGLE.COM",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google DeepMind",
        "aff_unique_url": "https://deepmind.com",
        "aff_unique_abbr": "DeepMind",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "24ab44cc90",
        "title": "Deep Boosting",
        "site": "https://proceedings.mlr.press/v32/cortesb14.html",
        "author": "Corinna Cortes; Mehryar Mohri; Umar Syed",
        "abstract": "We present a new ensemble learning algorithm, DeepBoost, which can use as base classifiers a hypothesis set containing deep decision trees, or members of other rich or complex families, and succeed in achieving high accuracy without overfitting the data. The key to the success of the algorithm is a \u2018capacity-conscious\u2019 criterion for the selection of the hypotheses.  We give new data-dependent learning bounds for convex ensembles expressed in terms of the Rademacher complexities of the sub-families composing the base classifier set, and the mixture weight assigned to each sub-family. Our algorithm directly benefits from these guarantees since it seeks to minimize the corresponding learning bound. We give a full description of our algorithm, including the details of its derivation, and report the results of several experiments showing that its performance compares favorably to that of AdaBoost and Logistic Regression and their L_1-regularized variants.",
        "bibtex": "@InProceedings{pmlr-v32-cortesb14,\n  title = \t {Deep Boosting},\n  author = \t {Cortes, Corinna and Mohri, Mehryar and Syed, Umar},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1179--1187},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/cortesb14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/cortesb14.html},\n  abstract = \t {We present a new ensemble learning algorithm, DeepBoost, which can use as base classifiers a hypothesis set containing deep decision trees, or members of other rich or complex families, and succeed in achieving high accuracy without overfitting the data. The key to the success of the algorithm is a \u2018capacity-conscious\u2019 criterion for the selection of the hypotheses.  We give new data-dependent learning bounds for convex ensembles expressed in terms of the Rademacher complexities of the sub-families composing the base classifier set, and the mixture weight assigned to each sub-family. Our algorithm directly benefits from these guarantees since it seeks to minimize the corresponding learning bound. We give a full description of our algorithm, including the details of its derivation, and report the results of several experiments showing that its performance compares favorably to that of AdaBoost and Logistic Regression and their L_1-regularized variants.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/cortesb14.pdf",
        "supp": "",
        "pdf_size": 1377929,
        "gs_citation": 123,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7547438241943735763&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Google Research, 111 8th Avenue, New York, NY 10011; Courant Institute and Google Research, 251 Mercer Street, New York, NY 10012; Google Research, 111 8th Avenue, New York, NY 10011",
        "aff_domain": "GOOGLE.COM;CIMS.NYU.EDU;GOOGLE.COM",
        "email": "GOOGLE.COM;CIMS.NYU.EDU;GOOGLE.COM",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Google;Courant Institute of Mathematical Sciences",
        "aff_unique_dep": "Google Research;Mathematical Sciences",
        "aff_unique_url": "https://research.google;https://courant.nyu.edu",
        "aff_unique_abbr": "Google;Courant",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "New York",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "46033159f2",
        "title": "Deep Generative Stochastic Networks Trainable by Backprop",
        "site": "https://proceedings.mlr.press/v32/bengio14.html",
        "author": "Yoshua Bengio; Eric Laufer; Guillaume Alain; Jason Yosinski",
        "abstract": "We introduce a novel training principle for probabilistic models that is an alternative to maximum likelihood. The proposed Generative Stochastic Networks (GSN) framework is based on learning the transition operator of a Markov chain whose stationary distribution estimates the data distribution.  Because the transition distribution is a conditional distribution generally involving a small move, it has fewer dominant modes, being unimodal in the limit of small moves. Thus, it is easier to learn, more like learning to perform supervised function approximation, with gradients that can be obtained by backprop. The theorems provided here generalize recent work on the probabilistic interpretation of denoising autoencoders and provide an interesting justification for dependency networks and generalized pseudolikelihood (along with defining an appropriate joint distribution and sampling mechanism, even when the conditionals are not consistent). GSNs can be used with missing inputs and can be used to sample subsets of variables given the rest.  Successful experiments are conducted, validating these theoretical results, on two image datasets and with a particular architecture that mimics the Deep Boltzmann Machine Gibbs sampler but allows training to proceed with backprop, without the need for layerwise pretraining.",
        "bibtex": "@InProceedings{pmlr-v32-bengio14,\n  title = \t {Deep Generative Stochastic Networks Trainable by Backprop},\n  author = \t {Bengio, Yoshua and Laufer, Eric and Alain, Guillaume and Yosinski, Jason},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {226--234},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/bengio14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/bengio14.html},\n  abstract = \t {We introduce a novel training principle for probabilistic models that is an alternative to maximum likelihood. The proposed Generative Stochastic Networks (GSN) framework is based on learning the transition operator of a Markov chain whose stationary distribution estimates the data distribution.  Because the transition distribution is a conditional distribution generally involving a small move, it has fewer dominant modes, being unimodal in the limit of small moves. Thus, it is easier to learn, more like learning to perform supervised function approximation, with gradients that can be obtained by backprop. The theorems provided here generalize recent work on the probabilistic interpretation of denoising autoencoders and provide an interesting justification for dependency networks and generalized pseudolikelihood (along with defining an appropriate joint distribution and sampling mechanism, even when the conditionals are not consistent). GSNs can be used with missing inputs and can be used to sample subsets of variables given the rest.  Successful experiments are conducted, validating these theoretical results, on two image datasets and with a particular architecture that mimics the Deep Boltzmann Machine Gibbs sampler but allows training to proceed with backprop, without the need for layerwise pretraining.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/bengio14.pdf",
        "supp": "",
        "pdf_size": 837337,
        "gs_citation": 514,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3877899611232964202&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "D\u00b4epartement d\u2019informatique et recherche op\u00b4erationnelle, Universit\u00b4e de Montr\u00b4eal,\u2217& Canadian Inst. for Advanced Research; D\u00b4epartement d\u2019informatique et recherche op\u00b4erationnelle, Universit\u00b4e de Montr\u00b4eal,\u2217& Canadian Inst. for Advanced Research; D\u00b4epartement d\u2019informatique et recherche op\u00b4erationnelle, Universit\u00b4e de Montr\u00b4eal,\u2217& Canadian Inst. for Advanced Research; Department of Computer Science, Cornell University",
        "aff_domain": "ON.THE.WEB; ; ; ",
        "email": "ON.THE.WEB; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Universit\u00b4e de Montr\u00b4eal;Cornell University",
        "aff_unique_dep": "D\u00b4epartement d\u2019informatique et recherche op\u00b4erationnelle;Department of Computer Science",
        "aff_unique_url": "https://www.umontreal.ca;https://www.cornell.edu",
        "aff_unique_abbr": "UdeM;Cornell",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Montr\u00b4eal;",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "bdc56b7fbb",
        "title": "Deep Supervised and Convolutional Generative Stochastic Network for Protein Secondary Structure Prediction",
        "site": "https://proceedings.mlr.press/v32/zhou14.html",
        "author": "Jian Zhou; Olga Troyanskaya",
        "abstract": "Predicting protein secondary structure is a fundamental problem in protein structure prediction. Here we present a new supervised generative stochastic network (GSN) based method to predict local secondary structure with deep hierarchical representations. GSN is a recently proposed deep learning technique (Bengio & Thibodeau-Laufer, 2013) to globally train deep generative model. We present the supervised extension of GSN, which learns a Markov chain to sample from a conditional distribution, and applied it to protein structure prediction. To scale the model to full-sized, high-dimensional data, like protein sequences with hundreds of amino-acids, we introduce a convolutional architecture, which allows efficient learning across multiple layers of hierarchical representations. Our architecture uniquely focuses on predicting structured low-level labels informed with both low and high-level representations learned by the model. In our application this corresponds to labeling the secondary structure state of each amino-acid residue. We trained and tested the model on separate sets of non-homologous proteins sharing less than 30% sequence identity. Our model achieves 66.4% Q8 accuracy on the CB513 dataset, better than the previously reported best performance 64.9% (Wang et al., 2011) for this challenging secondary structure prediction problem.",
        "bibtex": "@InProceedings{pmlr-v32-zhou14,\n  title = \t {Deep Supervised and Convolutional Generative Stochastic Network for Protein Secondary Structure Prediction},\n  author = \t {Zhou, Jian and Troyanskaya, Olga},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {745--753},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/zhou14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/zhou14.html},\n  abstract = \t {Predicting protein secondary structure is a fundamental problem in protein structure prediction. Here we present a new supervised generative stochastic network (GSN) based method to predict local secondary structure with deep hierarchical representations. GSN is a recently proposed deep learning technique (Bengio & Thibodeau-Laufer, 2013) to globally train deep generative model. We present the supervised extension of GSN, which learns a Markov chain to sample from a conditional distribution, and applied it to protein structure prediction. To scale the model to full-sized, high-dimensional data, like protein sequences with hundreds of amino-acids, we introduce a convolutional architecture, which allows efficient learning across multiple layers of hierarchical representations. Our architecture uniquely focuses on predicting structured low-level labels informed with both low and high-level representations learned by the model. In our application this corresponds to labeling the secondary structure state of each amino-acid residue. We trained and tested the model on separate sets of non-homologous proteins sharing less than 30% sequence identity. Our model achieves 66.4% Q8 accuracy on the CB513 dataset, better than the previously reported best performance 64.9% (Wang et al., 2011) for this challenging secondary structure prediction problem.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/zhou14.pdf",
        "supp": "",
        "pdf_size": 467449,
        "gs_citation": 205,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14121325348324995982&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Princeton University; Princeton University",
        "aff_domain": "PRINCETON.EDU;CS.PRINCETON.EDU",
        "email": "PRINCETON.EDU;CS.PRINCETON.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Princeton University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.princeton.edu",
        "aff_unique_abbr": "Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3bdb213899",
        "title": "Demystifying Information-Theoretic Clustering",
        "site": "https://proceedings.mlr.press/v32/steeg14.html",
        "author": "Greg Ver Steeg; Aram Galstyan; Fei Sha; Simon DeDeo",
        "abstract": "We propose a novel method for clustering data which is grounded in information-theoretic principles and requires no parametric assumptions.  Previous attempts to use information theory to define clusters in an assumption-free way are based on maximizing mutual information between data and cluster labels. We demonstrate that this intuition suffers from a fundamental conceptual flaw that causes clustering performance to deteriorate as the amount of data increases. Instead, we return to the axiomatic foundations of information theory to define a meaningful clustering measure based on the notion of consistency under coarse-graining for finite data.",
        "bibtex": "@InProceedings{pmlr-v32-steeg14,\n  title = \t {Demystifying Information-Theoretic Clustering},\n  author = \t {Ver Steeg, Greg and Galstyan, Aram and Sha, Fei and DeDeo, Simon},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {19--27},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/steeg14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/steeg14.html},\n  abstract = \t {We propose a novel method for clustering data which is grounded in information-theoretic principles and requires no parametric assumptions.  Previous attempts to use information theory to define clusters in an assumption-free way are based on maximizing mutual information between data and cluster labels. We demonstrate that this intuition suffers from a fundamental conceptual flaw that causes clustering performance to deteriorate as the amount of data increases. Instead, we return to the axiomatic foundations of information theory to define a meaningful clustering measure based on the notion of consistency under coarse-graining for finite data.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/steeg14.pdf",
        "supp": "",
        "pdf_size": 1477603,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3144066018875974657&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292, USA+University of Southern California, Los Angeles, CA 90089, USA; Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292, USA+University of Southern California, Los Angeles, CA 90089, USA; University of Southern California, Los Angeles, CA 90089, USA; Santa Fe Institute, 1399 Hyde Park Rd., Santa Fe, NM 87501, USA+School of Informatics and Computing, Indiana University, 901 E 10th St., Bloomington, IN 47408, USA",
        "aff_domain": "isi.edu;isi.edu;usc.edu;gmail.com",
        "email": "isi.edu;isi.edu;usc.edu;gmail.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+0;0+0;0;1+2",
        "aff_unique_norm": "University of Southern California;Santa Fe Institute;Indiana University",
        "aff_unique_dep": "Information Sciences Institute;;School of Informatics and Computing",
        "aff_unique_url": "https://isi.usc.edu;https://www.santafe.edu;https://www.indiana.edu",
        "aff_unique_abbr": "USC ISI;SFI;IU",
        "aff_campus_unique_index": "0+1;0+1;1;2+3",
        "aff_campus_unique": "Marina del Rey;Los Angeles;Santa Fe;Bloomington",
        "aff_country_unique_index": "0+0;0+0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "57c9aaff90",
        "title": "Densifying One Permutation Hashing via Rotation for Fast Near Neighbor Search",
        "site": "https://proceedings.mlr.press/v32/shrivastava14.html",
        "author": "Anshumali Shrivastava; Ping Li",
        "abstract": "The query complexity of \\em locality sensitive hashing (LSH) based similarity search is dominated by the number of hash evaluations, and this number grows with the data size\u00a0\\citeProc:Indyk_STOC98. In industrial applications such as search where the data are often high-dimensional and binary (e.g., text n-grams),  \\em minwise hashing is widely adopted, which requires applying a large number  of permutations on the data. This is  costly in computation and energy-consumption.    In this paper, we propose a  hashing technique which generates all the necessary hash evaluations needed for similarity search, using  one single permutation.  The heart of the proposed hash function is a  \u201crotation\u201d scheme which densifies the sparse sketches of \\em one permutation hashing\u00a0\\citeProc:Li_Owen_Zhang_NIPS12 in an unbiased fashion thereby maintaining the LSH property. This makes the obtained sketches suitable for hash table construction. This idea of rotation presented in this paper could be of independent  interest  for densifying  other types of sparse sketches.     Using our proposed hashing method, the  query time of a (K,L)-parameterized LSH is reduced from  the typical O(dKL) complexity to merely O(KL+dL), where d is the  number of nonzeros of the data vector, K is the number of hashes in each hash table, and L is the number of hash tables.  Our experimental evaluation on real data confirms that the proposed scheme significantly reduces the query processing time over minwise hashing without loss in retrieval accuracies.",
        "bibtex": "@InProceedings{pmlr-v32-shrivastava14,\n  title = \t {Densifying One Permutation Hashing via Rotation for Fast Near Neighbor Search},\n  author = \t {Shrivastava, Anshumali and Li, Ping},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {557--565},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/shrivastava14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/shrivastava14.html},\n  abstract = \t {The query complexity of \\em locality sensitive hashing (LSH) based similarity search is dominated by the number of hash evaluations, and this number grows with the data size\u00a0\\citeProc:Indyk_STOC98. In industrial applications such as search where the data are often high-dimensional and binary (e.g., text n-grams),  \\em minwise hashing is widely adopted, which requires applying a large number  of permutations on the data. This is  costly in computation and energy-consumption.    In this paper, we propose a  hashing technique which generates all the necessary hash evaluations needed for similarity search, using  one single permutation.  The heart of the proposed hash function is a  \u201crotation\u201d scheme which densifies the sparse sketches of \\em one permutation hashing\u00a0\\citeProc:Li_Owen_Zhang_NIPS12 in an unbiased fashion thereby maintaining the LSH property. This makes the obtained sketches suitable for hash table construction. This idea of rotation presented in this paper could be of independent  interest  for densifying  other types of sparse sketches.     Using our proposed hashing method, the  query time of a (K,L)-parameterized LSH is reduced from  the typical O(dKL) complexity to merely O(KL+dL), where d is the  number of nonzeros of the data vector, K is the number of hashes in each hash table, and L is the number of hash tables.  Our experimental evaluation on real data confirms that the proposed scheme significantly reduces the query processing time over minwise hashing without loss in retrieval accuracies.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/shrivastava14.pdf",
        "supp": "",
        "pdf_size": 203935,
        "gs_citation": 132,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18298477748140035613&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Dept. of Computer Science, Computing and Information Science (CIS), Cornell University, Ithaca, NY 14853, USA; Dept. of Statistics & Biostatistics, Dept. of Computer Science, Rutgers University, Piscataway, NJ 08854, USA",
        "aff_domain": "cs.cornell.edu;stat.rutgers.edu",
        "email": "cs.cornell.edu;stat.rutgers.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Cornell University;Rutgers University",
        "aff_unique_dep": "Dept. of Computer Science;Dept. of Statistics & Biostatistics",
        "aff_unique_url": "https://www.cornell.edu;https://www.rutgers.edu",
        "aff_unique_abbr": "Cornell;Rutgers",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Ithaca;Piscataway",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "524ac8e976",
        "title": "Deterministic Anytime Inference for Stochastic Continuous-Time Markov Processes",
        "site": "https://proceedings.mlr.press/v32/celikkaya14.html",
        "author": "E. Busra Celikkaya; Christian Shelton",
        "abstract": "We describe a deterministic anytime method for calculating  filtered and smoothed distributions in large variable-based continuous time  Markov processes.  Prior non-random algorithms do not converge to the true  distribution in the limit of infinite computation time.  Sampling  algorithms give different results each time run, which can lead to  instability when used inside expectation-maximization or other algorithms.  Our method combines the anytime convergent properties of sampling with the  non-random nature of variational approaches.  It is built upon a sum of  time-ordered products, an expansion of the matrix exponential.  We  demonstrate that our method performs as well as or better than the current  best sampling approaches on benchmark problems.",
        "bibtex": "@InProceedings{pmlr-v32-celikkaya14,\n  title = \t {Deterministic Anytime Inference for Stochastic Continuous-Time Markov Processes},\n  author = \t {Celikkaya, E. Busra and Shelton, Christian},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1962--1970},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/celikkaya14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/celikkaya14.html},\n  abstract = \t {We describe a deterministic anytime method for calculating  filtered and smoothed distributions in large variable-based continuous time  Markov processes.  Prior non-random algorithms do not converge to the true  distribution in the limit of infinite computation time.  Sampling  algorithms give different results each time run, which can lead to  instability when used inside expectation-maximization or other algorithms.  Our method combines the anytime convergent properties of sampling with the  non-random nature of variational approaches.  It is built upon a sum of  time-ordered products, an expansion of the matrix exponential.  We  demonstrate that our method performs as well as or better than the current  best sampling approaches on benchmark problems.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/celikkaya14.pdf",
        "supp": "",
        "pdf_size": 300837,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2301679813607209248&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 11,
        "aff": "University of California, Riverside; University of California, Riverside",
        "aff_domain": "cs.ucr.edu;cs.ucr.edu",
        "email": "cs.ucr.edu;cs.ucr.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Riverside",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucr.edu",
        "aff_unique_abbr": "UCR",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Riverside",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ba56df5929",
        "title": "Deterministic Policy Gradient Algorithms",
        "site": "https://proceedings.mlr.press/v32/silver14.html",
        "author": "David Silver; Guy Lever; Nicolas Heess; Thomas Degris; Daan Wierstra; Martin Riedmiller",
        "abstract": "In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. Deterministic policy gradient algorithms outperformed their stochastic counterparts in several benchmark problems, particularly in high-dimensional action spaces.",
        "bibtex": "@InProceedings{pmlr-v32-silver14,\n  title = \t {Deterministic Policy Gradient Algorithms},\n  author = \t {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {387--395},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/silver14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/silver14.html},\n  abstract = \t {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. Deterministic policy gradient algorithms outperformed their stochastic counterparts in several benchmark problems, particularly in high-dimensional action spaces.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/silver14.pdf",
        "supp": "",
        "pdf_size": 343663,
        "gs_citation": 5947,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2318393741435704115&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 29,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "c582ece8ec",
        "title": "Diagnosis determination: decision trees optimizing simultaneously worst and expected testing cost",
        "site": "https://proceedings.mlr.press/v32/cicalese14.html",
        "author": "Ferdinando Cicalese; Eduardo Laber; Aline Medeiros Saettler",
        "abstract": "In several applications of automatic diagnosis  and active learning a central problem is the evaluation of a discrete function by adaptively querying the values of its variables until the values read uniquely determine the value of the function.   In general reading the value of a variable is done at the expense of some cost (computational or possibly a fee to pay the corresponding experiment). The goal is to design a strategy for evaluating the function incurring little cost (in the worst case or in expectation according to a prior distribution on the possible variables\u2019 assignments).  We provide an algorithm that builds a strategy (decision tree) with both expected cost and worst cost which are at most an O(\\log n) factor away  from, respectively, the minimum possible expected cost and the minimum possible worst cost.  Our algorithm provides the best possible approximation simultaneously with respect to both criteria. In fact,  there is no algorithm that can guarantee o(\\log n) approximation, under the assumption  that \\cal P \u2260\\cal NP.",
        "bibtex": "@InProceedings{pmlr-v32-cicalese14,\n  title = \t {Diagnosis determination: decision trees optimizing simultaneously worst and expected testing cost},\n  author = \t {Cicalese, Ferdinando and Laber, Eduardo and Saettler, Aline Medeiros},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {414--422},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/cicalese14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/cicalese14.html},\n  abstract = \t {In several applications of automatic diagnosis  and active learning a central problem is the evaluation of a discrete function by adaptively querying the values of its variables until the values read uniquely determine the value of the function.   In general reading the value of a variable is done at the expense of some cost (computational or possibly a fee to pay the corresponding experiment). The goal is to design a strategy for evaluating the function incurring little cost (in the worst case or in expectation according to a prior distribution on the possible variables\u2019 assignments).  We provide an algorithm that builds a strategy (decision tree) with both expected cost and worst cost which are at most an O(\\log n) factor away  from, respectively, the minimum possible expected cost and the minimum possible worst cost.  Our algorithm provides the best possible approximation simultaneously with respect to both criteria. In fact,  there is no algorithm that can guarantee o(\\log n) approximation, under the assumption  that \\cal P \u2260\\cal NP.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/cicalese14.pdf",
        "supp": "",
        "pdf_size": 513593,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8632189768182987306&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Salerno, Italy; PUC-Rio, Brazil; PUC-Rio, Brazil",
        "aff_domain": "DIA.UNISA.IT;INF.PUC-RIO.BR;INF.PUC-RIO.BR",
        "email": "DIA.UNISA.IT;INF.PUC-RIO.BR;INF.PUC-RIO.BR",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Salerno;Pontifical Catholic University of Rio de Janeiro",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.unisalento.it;https://www.puc-rio.br",
        "aff_unique_abbr": ";PUC-Rio",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Rio de Janeiro",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Italy;Brazil"
    },
    {
        "id": "7fe67db59b",
        "title": "Dimension-free Concentration Bounds on Hankel Matrices for Spectral Learning",
        "site": "https://proceedings.mlr.press/v32/denis14.html",
        "author": "Fran\u00e7ois Denis; Mattias Gybels; Amaury Habrard",
        "abstract": "Learning probabilistic models over strings is an important issue for many applications. Spectral methods propose elegant solutions to the problem of inferring weighted automata from finite samples of variable-length strings drawn from an unknown target distribution. These methods rely on a singular value decomposition of a matrix H_S, called the Hankel matrix, that records the frequencies of (some of) the observed strings. The accuracy of the learned distribution depends both on the quantity of information embedded in H_S and on the distance between H_S and its mean H_r. Existing concentration bounds seem to indicate that the concentration over H_r gets looser with its size, suggesting to make a trade-off between the quantity of used information and the size of H_r. We propose new dimension-free concentration bounds for several variants of Hankel matrices. Experiments demonstrate that these bounds are tight and that they significantly improve existing bounds. These results suggest that the concentration rate of the Hankel matrix around its mean does not constitute an argument for limiting its size.",
        "bibtex": "@InProceedings{pmlr-v32-denis14,\n  title = \t {Dimension-free Concentration Bounds on Hankel Matrices for Spectral Learning},\n  author = \t {Denis, Fran\u00e7ois and Gybels, Mattias and Habrard, Amaury},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {449--457},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/denis14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/denis14.html},\n  abstract = \t {Learning probabilistic models over strings is an important issue for many applications. Spectral methods propose elegant solutions to the problem of inferring weighted automata from finite samples of variable-length strings drawn from an unknown target distribution. These methods rely on a singular value decomposition of a matrix H_S, called the Hankel matrix, that records the frequencies of (some of) the observed strings. The accuracy of the learned distribution depends both on the quantity of information embedded in H_S and on the distance between H_S and its mean H_r. Existing concentration bounds seem to indicate that the concentration over H_r gets looser with its size, suggesting to make a trade-off between the quantity of used information and the size of H_r. We propose new dimension-free concentration bounds for several variants of Hankel matrices. Experiments demonstrate that these bounds are tight and that they significantly improve existing bounds. These results suggest that the concentration rate of the Hankel matrix around its mean does not constitute an argument for limiting its size.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/denis14.pdf",
        "supp": "",
        "pdf_size": 361162,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5631114767040528558&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Aix Marseille Universit \u00b4e, CNRS, LIF, 13288 Marseille Cedex 9, FRANCE; Aix Marseille Universit \u00b4e, CNRS, LIF, 13288 Marseille Cedex 9, FRANCE; Universit \u00b4e Jean Monnet de Saint-Etienne, CNRS, LaHC, 42000 Saint-Etienne Cedex 2, FRANCE",
        "aff_domain": "LIF.UNIV-MRS.FR;LIF.UNIV-MRS.FR;UNIV-ST-ETIENNE.FR",
        "email": "LIF.UNIV-MRS.FR;LIF.UNIV-MRS.FR;UNIV-ST-ETIENNE.FR",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Aix Marseille University;Universit\u00e9 Jean Monnet de Saint-Etienne",
        "aff_unique_dep": "CNRS, LIF;",
        "aff_unique_url": "https://www.univ-amu.fr;https://www.univ-st-etienne.fr",
        "aff_unique_abbr": "AMU;UJM",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Saint-Etienne",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "12aa954408",
        "title": "Discovering Latent Network Structure in Point Process Data",
        "site": "https://proceedings.mlr.press/v32/linderman14.html",
        "author": "Scott Linderman; Ryan Adams",
        "abstract": "Networks play a central role in modern data analysis, enabling us to reason about systems by studying the relationships between their parts.  Most often in network analysis, the edges are given.  However, in many systems it is difficult or impossible to measure the network directly.  Examples of latent networks include economic interactions linking financial instruments and patterns of reciprocity in gang violence.  In these cases, we are limited to noisy observations of events associated with each node.  To enable analysis of these implicit networks, we develop a probabilistic model that combines mutually-exciting point processes with random graph models.  We show how the Poisson superposition principle enables an elegant auxiliary variable formulation and a fully-Bayesian, parallel inference algorithm.  We evaluate this new model empirically on several datasets.",
        "bibtex": "@InProceedings{pmlr-v32-linderman14,\n  title = \t {Discovering Latent Network Structure in Point Process Data},\n  author = \t {Linderman, Scott and Adams, Ryan},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1413--1421},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/linderman14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/linderman14.html},\n  abstract = \t {Networks play a central role in modern data analysis, enabling us to reason about systems by studying the relationships between their parts.  Most often in network analysis, the edges are given.  However, in many systems it is difficult or impossible to measure the network directly.  Examples of latent networks include economic interactions linking financial instruments and patterns of reciprocity in gang violence.  In these cases, we are limited to noisy observations of events associated with each node.  To enable analysis of these implicit networks, we develop a probabilistic model that combines mutually-exciting point processes with random graph models.  We show how the Poisson superposition principle enables an elegant auxiliary variable formulation and a fully-Bayesian, parallel inference algorithm.  We evaluate this new model empirically on several datasets.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/linderman14.pdf",
        "supp": "",
        "pdf_size": 801268,
        "gs_citation": 358,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16793949854640700460&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "Harvard University, Cambridge, MA 02138 USA; Harvard University, Cambridge, MA 02138 USA",
        "aff_domain": "SEAS.HARVARD.EDU;SEAS.HARVARD.EDU",
        "email": "SEAS.HARVARD.EDU;SEAS.HARVARD.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Harvard University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.harvard.edu",
        "aff_unique_abbr": "Harvard",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8dc9b0d251",
        "title": "Discrete Chebyshev Classifiers",
        "site": "https://proceedings.mlr.press/v32/eban14.html",
        "author": "Elad Eban; Elad Mezuman; Amir Globerson",
        "abstract": "In large scale learning problems it is often easy to collect simple statistics of the data, but hard or impractical to store all the original data. A key question in this setting is how to construct classifiers based on such partial information. One traditional approach to the problem has been to use maximum entropy arguments to induce a complete distribution on variables from statistics. However, this approach essentially makes conditional independence assumptions about the distribution, and furthermore does not optimize prediction loss. Here we present a framework for discriminative learning given a set of statistics. Specifically, we address the case where all variables are discrete and we have access to various marginals.  Our approach minimizes the worst case hinge loss in this case, which upper bounds the generalization error. We show that for certain sets of statistics the problem is tractable, and in the general case can be approximated using MAP LP relaxations. Empirical results show that the method is competitive with other approaches that use the same input.",
        "bibtex": "@InProceedings{pmlr-v32-eban14,\n  title = \t {Discrete Chebyshev Classifiers},\n  author = \t {Eban, Elad and Mezuman, Elad and Globerson, Amir},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1233--1241},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/eban14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/eban14.html},\n  abstract = \t {In large scale learning problems it is often easy to collect simple statistics of the data, but hard or impractical to store all the original data. A key question in this setting is how to construct classifiers based on such partial information. One traditional approach to the problem has been to use maximum entropy arguments to induce a complete distribution on variables from statistics. However, this approach essentially makes conditional independence assumptions about the distribution, and furthermore does not optimize prediction loss. Here we present a framework for discriminative learning given a set of statistics. Specifically, we address the case where all variables are discrete and we have access to various marginals.  Our approach minimizes the worst case hinge loss in this case, which upper bounds the generalization error. We show that for certain sets of statistics the problem is tractable, and in the general case can be approximated using MAP LP relaxations. Empirical results show that the method is competitive with other approaches that use the same input.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/eban14.pdf",
        "supp": "",
        "pdf_size": 382970,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13307584000107694821&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "The Selim and Rachel Benin School of Computer Science and Engineering. The Hebrew University of Jerusalem + Edmond and Lily Safra Center for Brain Sciences. The Hebrew University of Jerusalem; Edmond and Lily Safra Center for Brain Sciences. The Hebrew University of Jerusalem; The Selim and Rachel Benin School of Computer Science and Engineering. The Hebrew University of Jerusalem",
        "aff_domain": "cs.huji.ac.il;mail.huji.ac.il;cs.huji.ac.il",
        "email": "cs.huji.ac.il;mail.huji.ac.il;cs.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+0;0;0",
        "aff_unique_norm": "Hebrew University of Jerusalem",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "https://www.huji.ac.il",
        "aff_unique_abbr": "HUJI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Jerusalem;",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "b5788732c4",
        "title": "Discriminative Features via Generalized Eigenvectors",
        "site": "https://proceedings.mlr.press/v32/karampatziakis14.html",
        "author": "Nikos Karampatziakis; Paul Mineiro",
        "abstract": "Representing examples in a way that is compatible with the underlying classifier can greatly enhance the performance of a learning system. In this paper we investigate scalable techniques for inducing discriminative features by taking advantage of simple second order structure in the data. We focus on multiclass classification and show that features extracted from the generalized eigenvectors of the class conditional second moments lead to classifiers with excellent empirical performance. Moreover, these features have attractive theoretical properties, such as inducing representations that are invariant to linear transformations of the input. We evaluate classifiers built from these features on three different tasks, obtaining state of the art results.",
        "bibtex": "@InProceedings{pmlr-v32-karampatziakis14,\n  title = \t {Discriminative Features via Generalized Eigenvectors},\n  author = \t {Karampatziakis, Nikos and Mineiro, Paul},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {494--502},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/karampatziakis14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/karampatziakis14.html},\n  abstract = \t {Representing examples in a way that is compatible with the underlying classifier can greatly enhance the performance of a learning system. In this paper we investigate scalable techniques for inducing discriminative features by taking advantage of simple second order structure in the data. We focus on multiclass classification and show that features extracted from the generalized eigenvectors of the class conditional second moments lead to classifiers with excellent empirical performance. Moreover, these features have attractive theoretical properties, such as inducing representations that are invariant to linear transformations of the input. We evaluate classifiers built from these features on three different tasks, obtaining state of the art results.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/karampatziakis14.pdf",
        "supp": "",
        "pdf_size": 335343,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16896203294622680036&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Microsoft CISL, 1 Microsoft Way, Redmond, WA 98052 USA; Microsoft CISL, 1 Microsoft Way, Redmond, WA 98052 USA",
        "aff_domain": "microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Microsoft CISL",
        "aff_unique_url": "https://www.microsoft.com",
        "aff_unique_abbr": "Microsoft",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Redmond",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8608ce62c1",
        "title": "Distributed Representations of Sentences and Documents",
        "site": "https://proceedings.mlr.press/v32/le14.html",
        "author": "Quoc Le; Tomas Mikolov",
        "abstract": "Many machine learning algorithms require the  input to be represented as a fixed length feature  vector. When it comes to texts, one of the most  common representations is bag-of-words. Despite their popularity, bag-of-words models have  two major weaknesses: they lose the ordering  of the words and they also ignore semantics of  the words. For example, \"powerful,\" \"strong\"  and \"Paris\" are equally distant. In this paper,  we propose an unsupervised algorithm that learns  vector representations of sentences and text documents. This algorithm represents each document by a dense vector which is trained to predict  words in the document. Its construction gives our  algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that our technique outperforms bag-of-words models as well as other techniques for  text representations. Finally, we achieve new  state-of-the-art results on several text classification and sentiment analysis tasks.",
        "bibtex": "@InProceedings{pmlr-v32-le14,\n  title = \t {Distributed Representations of Sentences and Documents},\n  author = \t {Le, Quoc and Mikolov, Tomas},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1188--1196},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/le14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/le14.html},\n  abstract = \t {Many machine learning algorithms require the  input to be represented as a fixed length feature  vector. When it comes to texts, one of the most  common representations is bag-of-words. Despite their popularity, bag-of-words models have  two major weaknesses: they lose the ordering  of the words and they also ignore semantics of  the words. For example, \"powerful,\" \"strong\"  and \"Paris\" are equally distant. In this paper,  we propose an unsupervised algorithm that learns  vector representations of sentences and text documents. This algorithm represents each document by a dense vector which is trained to predict  words in the document. Its construction gives our  algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that our technique outperforms bag-of-words models as well as other techniques for  text representations. Finally, we achieve new  state-of-the-art results on several text classification and sentiment analysis tasks.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/le14.pdf",
        "supp": "",
        "pdf_size": 146112,
        "gs_citation": 13722,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5370888768768083562&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "Google Inc, 1600 Amphitheatre Parkway, Mountain View, CA 94043; Google Inc, 1600 Amphitheatre Parkway, Mountain View, CA 94043",
        "aff_domain": "GOOGLE.COM;GOOGLE.COM",
        "email": "GOOGLE.COM;GOOGLE.COM",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google",
        "aff_unique_url": "https://www.google.com",
        "aff_unique_abbr": "Google",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ea9b154c96",
        "title": "Distributed Stochastic Gradient MCMC",
        "site": "https://proceedings.mlr.press/v32/ahn14.html",
        "author": "Sungjin Ahn; Babak Shahbaba; Max Welling",
        "abstract": "Probabilistic inference on a big data scale is becoming increasingly relevant to both the machine learning and statistics communities. Here we introduce the first fully distributed MCMC algorithm based on stochastic gradients. We argue that stochastic gradient MCMC algorithms are particularly suited for distributed inference because individual chains can draw minibatches from their local pool of data for a flexible amount of time before jumping to or syncing with other chains. This greatly reduces communication overhead and allows adaptive load balancing. Our experiments for LDA on Wikipedia and Pubmed show that relative to the state of the art in distributed MCMC we reduce compute time from 27 hours to half an hour in order to reach the same perplexity level.",
        "bibtex": "@InProceedings{pmlr-v32-ahn14,\n  title = \t {Distributed Stochastic Gradient MCMC},\n  author = \t {Ahn, Sungjin and Shahbaba, Babak and Welling, Max},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1044--1052},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/ahn14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/ahn14.html},\n  abstract = \t {Probabilistic inference on a big data scale is becoming increasingly relevant to both the machine learning and statistics communities. Here we introduce the first fully distributed MCMC algorithm based on stochastic gradients. We argue that stochastic gradient MCMC algorithms are particularly suited for distributed inference because individual chains can draw minibatches from their local pool of data for a flexible amount of time before jumping to or syncing with other chains. This greatly reduces communication overhead and allows adaptive load balancing. Our experiments for LDA on Wikipedia and Pubmed show that relative to the state of the art in distributed MCMC we reduce compute time from 27 hours to half an hour in order to reach the same perplexity level.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/ahn14.pdf",
        "supp": "",
        "pdf_size": 834653,
        "gs_citation": 132,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13665140117049535430&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, University of California, Irvine; Department of Statistics, University of California, Irvine; Machine Learning Group, University of Amsterdam",
        "aff_domain": "ICS.UCI.EDU;UCI.EDU;UVA.NL",
        "email": "ICS.UCI.EDU;UCI.EDU;UVA.NL",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of California, Irvine;University of Amsterdam",
        "aff_unique_dep": "Department of Computer Science;Machine Learning Group",
        "aff_unique_url": "https://www.uci.edu;https://www.uva.nl",
        "aff_unique_abbr": "UCI;UvA",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Irvine;",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United States;Netherlands"
    },
    {
        "id": "9e461ca84f",
        "title": "Doubly Stochastic Variational Bayes for non-Conjugate Inference",
        "site": "https://proceedings.mlr.press/v32/titsias14.html",
        "author": "Michalis Titsias; Miguel L\u00e1zaro-Gredilla",
        "abstract": "We propose a simple and effective variational inference algorithm based on stochastic optimisation   that can be widely applied for Bayesian non-conjugate inference in continuous parameter spaces. This algorithm is based on stochastic approximation and allows for efficient use of gradient information from the model joint density. We demonstrate these properties using illustrative examples as well as in challenging and diverse Bayesian inference   problems such as variable selection in logistic regression and fully   Bayesian inference over kernel hyperparameters in Gaussian process regression.",
        "bibtex": "@InProceedings{pmlr-v32-titsias14,\n  title = \t {Doubly Stochastic Variational Bayes for non-Conjugate Inference},\n  author = \t {Titsias, Michalis and L\u00e1zaro-Gredilla, Miguel},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1971--1979},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/titsias14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/titsias14.html},\n  abstract = \t {We propose a simple and effective variational inference algorithm based on stochastic optimisation   that can be widely applied for Bayesian non-conjugate inference in continuous parameter spaces. This algorithm is based on stochastic approximation and allows for efficient use of gradient information from the model joint density. We demonstrate these properties using illustrative examples as well as in challenging and diverse Bayesian inference   problems such as variable selection in logistic regression and fully   Bayesian inference over kernel hyperparameters in Gaussian process regression.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/titsias14.pdf",
        "supp": "",
        "pdf_size": 567240,
        "gs_citation": 476,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11570687137289124942&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Informatics, Athens University of Economics and Business, Greece; Dpt. Signal Processing & Communications, Universidad Carlos III de Madrid, Spain",
        "aff_domain": "AUEB.GR;TSC.UC3M.ES",
        "email": "AUEB.GR;TSC.UC3M.ES",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Athens University of Economics and Business;Universidad Carlos III de Madrid",
        "aff_unique_dep": "Department of Informatics;Dpt. Signal Processing & Communications",
        "aff_unique_url": "https://www.aueb.gr;https://www.uc3m.es",
        "aff_unique_abbr": "AUEB;UC3M",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Athens;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Greece;Spain"
    },
    {
        "id": "5285cddc05",
        "title": "Dual Query: Practical Private Query Release for High Dimensional Data",
        "site": "https://proceedings.mlr.press/v32/gaboardi14.html",
        "author": "Marco Gaboardi; Emilio Jesus Gallego Arias; Justin Hsu; Aaron Roth; Zhiwei Steven Wu",
        "abstract": "We present a practical, differentially private algorithm for answering a large number of queries on high dimensional datasets. Like all algorithms for this task, ours necessarily has worst-case complexity exponential in the dimension of the data. However, our algorithm packages the computationally hard step into a concisely defined integer program, which can be solved non-privately using standard solvers. We prove accuracy and privacy theorems for our algorithm, and then demonstrate experimentally that our algorithm performs well in practice. For example,  our algorithm can efficiently and accurately answer millions of queries on the Netflix dataset, which has over 17,000 attributes; this is an improvement on the state of the art by multiple orders of magnitude.",
        "bibtex": "@InProceedings{pmlr-v32-gaboardi14,\n  title = \t {Dual Query: Practical Private Query Release for High Dimensional Data},\n  author = \t {Gaboardi, Marco and Arias, Emilio Jesus Gallego and Hsu, Justin and Roth, Aaron and Wu, Zhiwei Steven},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1170--1178},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/gaboardi14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/gaboardi14.html},\n  abstract = \t {We present a practical, differentially private algorithm for answering a large number of queries on high dimensional datasets. Like all algorithms for this task, ours necessarily has worst-case complexity exponential in the dimension of the data. However, our algorithm packages the computationally hard step into a concisely defined integer program, which can be solved non-privately using standard solvers. We prove accuracy and privacy theorems for our algorithm, and then demonstrate experimentally that our algorithm performs well in practice. For example,  our algorithm can efficiently and accurately answer millions of queries on the Netflix dataset, which has over 17,000 attributes; this is an improvement on the state of the art by multiple orders of magnitude.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/gaboardi14.pdf",
        "supp": "",
        "pdf_size": 489370,
        "gs_citation": 122,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5024755416235832056&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 28,
        "aff": "University of Dundee, Dundee, Scotland, UK; University of Pennsylvania, Philadelphia, USA; University of Pennsylvania, Philadelphia, USA; University of Pennsylvania, Philadelphia, USA; University of Pennsylvania, Philadelphia, USA",
        "aff_domain": "DUNDEE.AC.UK;CIS.UPENN.EDU;CIS.UPENN.EDU;CIS.UPENN.EDU;CIS.UPENN.EDU",
        "email": "DUNDEE.AC.UK;CIS.UPENN.EDU;CIS.UPENN.EDU;CIS.UPENN.EDU;CIS.UPENN.EDU",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1;1",
        "aff_unique_norm": "University of Dundee;University of Pennsylvania",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.dundee.ac.uk;https://www.upenn.edu",
        "aff_unique_abbr": "Dundee;UPenn",
        "aff_campus_unique_index": "0;1;1;1;1",
        "aff_campus_unique": "Dundee;Philadelphia",
        "aff_country_unique_index": "0;1;1;1;1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "3576e101d8",
        "title": "Dynamic Programming Boosting for Discriminative Macro-Action Discovery",
        "site": "https://proceedings.mlr.press/v32/lefakis14.html",
        "author": "Leonidas Lefakis; Francois Fleuret",
        "abstract": "We consider the problem of automatic macro-action discovery in imitation learning, which we cast as one of change-point detection. Unlike prior work in change-point detection, the present work leverages discriminative learning algorithms. Our main contribution is a novel supervised learning algorithm which extends the classical Boosting framework by combining it with dynamic programming. The resulting process alternatively improves the performance of individual strong predictors and the estimated change-points in the training sequence. Empirical evaluation is presented for the proposed method on tasks where change-points arise naturally as part of a classification problem. Finally we show the applicability of the algorithm to macro-action discovery in imitation learning and demonstrate it allows us to solve complex image-based goal-planning problems with thousands of features.",
        "bibtex": "@InProceedings{pmlr-v32-lefakis14,\n  title = \t {Dynamic Programming Boosting for Discriminative Macro-Action Discovery},\n  author = \t {Lefakis, Leonidas and Fleuret, Francois},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1548--1556},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/lefakis14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/lefakis14.html},\n  abstract = \t {We consider the problem of automatic macro-action discovery in imitation learning, which we cast as one of change-point detection. Unlike prior work in change-point detection, the present work leverages discriminative learning algorithms. Our main contribution is a novel supervised learning algorithm which extends the classical Boosting framework by combining it with dynamic programming. The resulting process alternatively improves the performance of individual strong predictors and the estimated change-points in the training sequence. Empirical evaluation is presented for the proposed method on tasks where change-points arise naturally as part of a classification problem. Finally we show the applicability of the algorithm to macro-action discovery in imitation learning and demonstrate it allows us to solve complex image-based goal-planning problems with thousands of features.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/lefakis14.pdf",
        "supp": "",
        "pdf_size": 1825168,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18384039878917719201&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Idiap Research Institute, Martigny, Switzerland+\u00b4Ecole Polytechnique F\u00b4ed\u00b4erale de Lausanne, Lausanne, Switzerland; Idiap Research Institute, Martigny, Switzerland+\u00b4Ecole Polytechnique F\u00b4ed\u00b4erale de Lausanne, Lausanne, Switzerland",
        "aff_domain": "IDIAP.CH;IDIAP.CH",
        "email": "IDIAP.CH;IDIAP.CH",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Idiap Research Institute;EPFL",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.idiap.ch;https://www.epfl.ch",
        "aff_unique_abbr": "Idiap;EPFL",
        "aff_campus_unique_index": "0+1;0+1",
        "aff_campus_unique": "Martigny;Lausanne",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "486c68ff64",
        "title": "Effective Bayesian Modeling of Groups of Related Count Time Series",
        "site": "https://proceedings.mlr.press/v32/chapados14.html",
        "author": "Nicolas Chapados",
        "abstract": "Time series of counts arise in a variety of forecasting applications, for which traditional models are generally inappropriate. This paper introduces a hierarchical Bayesian formulation applicable to count time series that can easily account for explanatory variables and share statistical strength across groups of related time series. We derive an efficient approximate inference technique, and illustrate its performance on a number of datasets from supply chain planning.",
        "bibtex": "@InProceedings{pmlr-v32-chapados14,\n  title = \t {Effective Bayesian Modeling of Groups of Related Count Time Series},\n  author = \t {Chapados, Nicolas},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1395--1403},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/chapados14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/chapados14.html},\n  abstract = \t {Time series of counts arise in a variety of forecasting applications, for which traditional models are generally inappropriate. This paper introduces a hierarchical Bayesian formulation applicable to count time series that can easily account for explanatory variables and share statistical strength across groups of related time series. We derive an efficient approximate inference technique, and illustrate its performance on a number of datasets from supply chain planning.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/chapados14.pdf",
        "supp": "",
        "pdf_size": 417309,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9712336816012988015&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "ApSTAT Technologies Inc., 408-4200 Boul. St-Laurent, Montral, QC, H2W 2R2, CANADA",
        "aff_domain": "APSTAT.COM",
        "email": "APSTAT.COM",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "ApSTAT Technologies Inc.",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "f06f558020",
        "title": "Efficient Algorithms for Robust One-bit Compressive Sensing",
        "site": "https://proceedings.mlr.press/v32/zhangc14.html",
        "author": "Lijun Zhang; Jinfeng Yi; Rong Jin",
        "abstract": "While the conventional compressive sensing assumes measurements of infinite precision, one-bit compressive sensing considers an extreme setting where each measurement is quantized to just a single bit. In this paper, we study the vector recovery problem from noisy one-bit measurements, and develop two novel algorithms with formal theoretical guarantees. First, we propose a passive algorithm, which is very efficient in the sense it only needs to solve a convex optimization problem that has a closed-form solution. Despite the apparent simplicity, our theoretical analysis reveals that the proposed algorithm can recover both the exactly sparse and the approximately sparse vectors. In particular, for a sparse vector with s nonzero elements, the sample complexity is O(s \\log n/\u03b5^2), where n is the dimensionality and \u03b5is the recovery error. This result improves significantly over the previously best known sample complexity in the noisy setting, which is O(s \\log n/\u03b5^4). Second, in the case that the noise model is known, we develop an adaptive algorithm based on the principle of active learning. The key idea is to solicit the sign information only when it cannot be inferred from the current estimator. Compared with the passive algorithm, the adaptive one has a lower sample complexity if a high-precision solution is desired.",
        "bibtex": "@InProceedings{pmlr-v32-zhangc14,\n  title = \t {Efficient Algorithms for Robust One-bit Compressive Sensing},\n  author = \t {Zhang, Lijun and Yi, Jinfeng and Jin, Rong},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {820--828},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/zhangc14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/zhangc14.html},\n  abstract = \t {While the conventional compressive sensing assumes measurements of infinite precision, one-bit compressive sensing considers an extreme setting where each measurement is quantized to just a single bit. In this paper, we study the vector recovery problem from noisy one-bit measurements, and develop two novel algorithms with formal theoretical guarantees. First, we propose a passive algorithm, which is very efficient in the sense it only needs to solve a convex optimization problem that has a closed-form solution. Despite the apparent simplicity, our theoretical analysis reveals that the proposed algorithm can recover both the exactly sparse and the approximately sparse vectors. In particular, for a sparse vector with s nonzero elements, the sample complexity is O(s \\log n/\u03b5^2), where n is the dimensionality and \u03b5is the recovery error. This result improves significantly over the previously best known sample complexity in the noisy setting, which is O(s \\log n/\u03b5^4). Second, in the case that the noise model is known, we develop an adaptive algorithm based on the principle of active learning. The key idea is to solicit the sign information only when it cannot be inferred from the current estimator. Compared with the passive algorithm, the adaptive one has a lower sample complexity if a high-precision solution is desired.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/zhangc14.pdf",
        "supp": "",
        "pdf_size": 225894,
        "gs_citation": 95,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8300637196927277435&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China; IBM Thomas J. Watson Research Center, Yorktown Heights, NY 10598, USA; Department of Computer Science and Engineering, Michigan State University, East Lansing, MI 48824, USA",
        "aff_domain": "LAMDA.NJU.EDU.CN;US.IBM.COM;CSE.MSU.EDU",
        "email": "LAMDA.NJU.EDU.CN;US.IBM.COM;CSE.MSU.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Nanjing University;IBM;Michigan State University",
        "aff_unique_dep": "National Key Laboratory for Novel Software Technology;IBM Thomas J. Watson Research Center;Department of Computer Science and Engineering",
        "aff_unique_url": "http://www.nju.edu.cn;https://www.ibm.com/research/watson;https://www.msu.edu",
        "aff_unique_abbr": "Nanjing U;IBM Watson;MSU",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "Nanjing;Yorktown Heights;East Lansing",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "e9ed624fed",
        "title": "Efficient Approximation of Cross-Validation for Kernel Methods using Bouligand Influence Function",
        "site": "https://proceedings.mlr.press/v32/liua14.html",
        "author": "Yong Liu; Shali Jiang; Shizhong Liao",
        "abstract": "Model selection is one of the key issues both in recent research and application of kernel methods. Cross-validation is a commonly employed and widely accepted model selection criterion. However, it requires multiple times of training the algorithm under consideration, which is computationally intensive. In this paper, we present a novel strategy for approximating the cross-validation based on the Bouligand influence function (BIF), which only requires the solution of the algorithm once. The BIF measures the impact of an infinitesimal small amount of contamination of the original distribution. We first establish the link between the concept of BIF and the concept of cross-validation. The BIF is related to the first order term of a Taylor expansion. Then, we calculate the BIF and higher order BIFs, and apply these theoretical results to approximate the cross-validation error in practice. Experimental results demonstrate that our approximate cross-validation criterion is sound and efficient.",
        "bibtex": "@InProceedings{pmlr-v32-liua14,\n  title = \t {Efficient Approximation of Cross-Validation for Kernel Methods using Bouligand Influence Function},\n  author = \t {Liu, Yong and Jiang, Shali and Liao, Shizhong},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {324--332},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/liua14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/liua14.html},\n  abstract = \t {Model selection is one of the key issues both in recent research and application of kernel methods. Cross-validation is a commonly employed and widely accepted model selection criterion. However, it requires multiple times of training the algorithm under consideration, which is computationally intensive. In this paper, we present a novel strategy for approximating the cross-validation based on the Bouligand influence function (BIF), which only requires the solution of the algorithm once. The BIF measures the impact of an infinitesimal small amount of contamination of the original distribution. We first establish the link between the concept of BIF and the concept of cross-validation. The BIF is related to the first order term of a Taylor expansion. Then, we calculate the BIF and higher order BIFs, and apply these theoretical results to approximate the cross-validation error in practice. Experimental results demonstrate that our approximate cross-validation criterion is sound and efficient.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/liua14.pdf",
        "supp": "",
        "pdf_size": 179311,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14054644670663861249&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "School of Computer Science and Technology, Tianjin University, Tianjin 300072, P. R. China; School of Computer Science and Technology, Tianjin University, Tianjin 300072, P. R. China; School of Computer Science and Technology, Tianjin University, Tianjin 300072, P. R. China",
        "aff_domain": "TJU.EDU.CN;TJU.EDU.CN;TJU.EDU.CN",
        "email": "TJU.EDU.CN;TJU.EDU.CN;TJU.EDU.CN",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Tianjin University",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "http://www.tju.edu.cn",
        "aff_unique_abbr": "Tianjin University",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Tianjin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "1089ce9698",
        "title": "Efficient Continuous-Time Markov Chain Estimation",
        "site": "https://proceedings.mlr.press/v32/hajiaghayi14.html",
        "author": "Monir Hajiaghayi; Bonnie Kirkpatrick; Liangliang Wang; Alexandre Bouchard-C\u00f4t\u00e9",
        "abstract": "Many problems of practical interest rely on Continuous-time Markov chains\u00a0(CTMCs) defined over combinatorial state spaces, rendering the computation of transition probabilities, and hence probabilistic inference, difficult or impossible with existing methods.  For problems with countably infinite states, where classical methods such as matrix exponentiation are not applicable,  the main alternative has been particle Markov chain Monte Carlo methods imputing both the holding times and sequences of visited states.    We propose a particle-based Monte Carlo approach where the holding times are marginalized analytically.  We demonstrate that in a range of realistic inferential setups, our scheme dramatically reduces the variance of the Monte Carlo approximation and yields more accurate parameter posterior approximations given a fixed computational budget. These experiments are performed on both synthetic and real datasets, drawing from two important examples of CTMCs having combinatorial state spaces: string-valued mutation models in phylogenetics and nucleic acid folding pathways.",
        "bibtex": "@InProceedings{pmlr-v32-hajiaghayi14,\n  title = \t {Efficient Continuous-Time Markov Chain Estimation},\n  author = \t {Hajiaghayi, Monir and Kirkpatrick, Bonnie and Wang, Liangliang and Bouchard-C\u00f4t\u00e9, Alexandre},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {638--646},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/hajiaghayi14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/hajiaghayi14.html},\n  abstract = \t {Many problems of practical interest rely on Continuous-time Markov chains\u00a0(CTMCs) defined over combinatorial state spaces, rendering the computation of transition probabilities, and hence probabilistic inference, difficult or impossible with existing methods.  For problems with countably infinite states, where classical methods such as matrix exponentiation are not applicable,  the main alternative has been particle Markov chain Monte Carlo methods imputing both the holding times and sequences of visited states.    We propose a particle-based Monte Carlo approach where the holding times are marginalized analytically.  We demonstrate that in a range of realistic inferential setups, our scheme dramatically reduces the variance of the Monte Carlo approximation and yields more accurate parameter posterior approximations given a fixed computational budget. These experiments are performed on both synthetic and real datasets, drawing from two important examples of CTMCs having combinatorial state spaces: string-valued mutation models in phylogenetics and nucleic acid folding pathways.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/hajiaghayi14.pdf",
        "supp": "",
        "pdf_size": 914269,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14541105071555780136&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computer Science, University of British Columbia, Vancouver, BC V6T 1Z4, Canada; Department of Computer Science, University of Miami, Coral Gables, FL 33124, United States; Department of Statistical and Actuarial Sciences, Simon Fraser University, Burnaby, BC V5A 1S6, Canada; Statistics Department, University of British Columbia, Vancouver, BC V6T 1Z4, Canada",
        "aff_domain": "CS.UBC.CA;CS.MIAMI.EDU;SFU.CA;STAT.UBC.CA",
        "email": "CS.UBC.CA;CS.MIAMI.EDU;SFU.CA;STAT.UBC.CA",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "University of British Columbia;University of Miami;Simon Fraser University",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science;Department of Statistical and Actuarial Sciences",
        "aff_unique_url": "https://www.ubc.ca;https://www.miami.edu;https://www.sfu.ca",
        "aff_unique_abbr": "UBC;UM;SFU",
        "aff_campus_unique_index": "0;1;2;0",
        "aff_campus_unique": "Vancouver;Coral Gables;Burnaby",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "a9323ebcba",
        "title": "Efficient Dimensionality Reduction for High-Dimensional Network Estimation",
        "site": "https://proceedings.mlr.press/v32/celik14.html",
        "author": "Safiye Celik; Benjamin Logsdon; Su-In Lee",
        "abstract": "We propose module graphical lasso (MGL), an aggressive dimensionality reduction and network estimation technique for a high-dimensional Gaussian graphical model (GGM). MGL achieves scalability, interpretability and robustness by exploiting the modularity property of many real-world networks. Variables are organized into tightly coupled modules and a graph structure is estimated to determine the conditional independencies among modules. MGL iteratively learns the module assignment of variables, the latent variables, each corresponding to a module, and the parameters of the GGM of the latent variables. In synthetic data experiments, MGL outperforms the standard graphical lasso and three other methods that incorporate latent variables into GGMs. When applied to gene expression data from ovarian cancer, MGL outperforms standard clustering algorithms in identifying functionally coherent gene sets and predicting survival time of patients. The learned modules and their dependencies provide novel insights into cancer biology as well as identifying possible novel drug targets.",
        "bibtex": "@InProceedings{pmlr-v32-celik14,\n  title = \t {Efficient Dimensionality Reduction for High-Dimensional Network Estimation},\n  author = \t {Celik, Safiye and Logsdon, Benjamin and Lee, Su-In},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1953--1961},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/celik14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/celik14.html},\n  abstract = \t {We propose module graphical lasso (MGL), an aggressive dimensionality reduction and network estimation technique for a high-dimensional Gaussian graphical model (GGM). MGL achieves scalability, interpretability and robustness by exploiting the modularity property of many real-world networks. Variables are organized into tightly coupled modules and a graph structure is estimated to determine the conditional independencies among modules. MGL iteratively learns the module assignment of variables, the latent variables, each corresponding to a module, and the parameters of the GGM of the latent variables. In synthetic data experiments, MGL outperforms the standard graphical lasso and three other methods that incorporate latent variables into GGMs. When applied to gene expression data from ovarian cancer, MGL outperforms standard clustering algorithms in identifying functionally coherent gene sets and predicting survival time of patients. The learned modules and their dependencies provide novel insights into cancer biology as well as identifying possible novel drug targets.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/celik14.pdf",
        "supp": "",
        "pdf_size": 3505786,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15096820365199275536&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science and Engineering, University of Washington, Seattle, WA 98195; Department of Genome Sciences, University of Washington, Seattle, WA 98195; Departments of Computer Science and Engineering, Genome Sciences, University of Washington, Seattle, WA 98195",
        "aff_domain": "CS.WASHINGTON.EDU;CS.WASHINGTON.EDU;CS.WASHINGTON.EDU",
        "email": "CS.WASHINGTON.EDU;CS.WASHINGTON.EDU;CS.WASHINGTON.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a94e4e0801",
        "title": "Efficient Gradient-Based Inference through Transformations between Bayes Nets and Neural Nets",
        "site": "https://proceedings.mlr.press/v32/kingma14.html",
        "author": "Diederik Kingma; Max Welling",
        "abstract": "Hierarchical Bayesian networks and neural networks with stochastic hidden units are commonly perceived as two separate types of models. We show that either of these types of models can often be transformed into an instance of the other, by switching between centered and differentiable non-centered parameterizations of the latent variables. The choice of parameterization greatly influences the efficiency of gradient-based posterior inference; we show that they are often complementary to eachother, we clarify when each parameterization is preferred and show how inference can be made robust. In the non-centered form, a simple Monte Carlo estimator of the marginal likelihood can be used for learning the parameters. Theoretical results are supported by experiments.",
        "bibtex": "@InProceedings{pmlr-v32-kingma14,\n  title = \t {Efficient Gradient-Based Inference through Transformations between Bayes Nets and Neural Nets},\n  author = \t {Kingma, Diederik and Welling, Max},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1782--1790},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/kingma14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/kingma14.html},\n  abstract = \t {Hierarchical Bayesian networks and neural networks with stochastic hidden units are commonly perceived as two separate types of models. We show that either of these types of models can often be transformed into an instance of the other, by switching between centered and differentiable non-centered parameterizations of the latent variables. The choice of parameterization greatly influences the efficiency of gradient-based posterior inference; we show that they are often complementary to eachother, we clarify when each parameterization is preferred and show how inference can be made robust. In the non-centered form, a simple Monte Carlo estimator of the marginal likelihood can be used for learning the parameters. Theoretical results are supported by experiments.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/kingma14.pdf",
        "supp": "",
        "pdf_size": 2460339,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10247073116756532709&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Machine Learning Group, University of Amsterdam; Machine Learning Group, University of Amsterdam",
        "aff_domain": "UVA.NL;UVA.NL",
        "email": "UVA.NL;UVA.NL",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Amsterdam",
        "aff_unique_dep": "Machine Learning Group",
        "aff_unique_url": "https://www.uva.nl",
        "aff_unique_abbr": "UvA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "ae2aaa0736",
        "title": "Efficient Label Propagation",
        "site": "https://proceedings.mlr.press/v32/fujiwara14.html",
        "author": "Yasuhiro Fujiwara; Go Irie",
        "abstract": "Label propagation is a popular graph-based semi-supervised learning framework.   So as to obtain the optimal labeling scores, the label propagation algorithm requires an inverse matrix which incurs the high computational cost of O(n^3+cn^2), where n and c are the numbers of data points and labels, respectively.   This paper proposes an efficient label propagation algorithm that guarantees exactly the same labeling results as those yielded by optimal labeling scores.   The key to our approach is to iteratively compute lower and upper bounds of labeling scores to prune unnecessary score computations.   This idea significantly reduces the computational cost to O(cnt) where t is the average number of iterations for each label and t << n in practice.   Experiments demonstrate the significant superiority of our algorithm over existing label propagation methods.",
        "bibtex": "@InProceedings{pmlr-v32-fujiwara14,\n  title = \t {Efficient Label Propagation},\n  author = \t {Fujiwara, Yasuhiro and Irie, Go},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {784--792},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/fujiwara14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/fujiwara14.html},\n  abstract = \t {Label propagation is a popular graph-based semi-supervised learning framework.   So as to obtain the optimal labeling scores, the label propagation algorithm requires an inverse matrix which incurs the high computational cost of O(n^3+cn^2), where n and c are the numbers of data points and labels, respectively.   This paper proposes an efficient label propagation algorithm that guarantees exactly the same labeling results as those yielded by optimal labeling scores.   The key to our approach is to iteratively compute lower and upper bounds of labeling scores to prune unnecessary score computations.   This idea significantly reduces the computational cost to O(cnt) where t is the average number of iterations for each label and t << n in practice.   Experiments demonstrate the significant superiority of our algorithm over existing label propagation methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/fujiwara14.pdf",
        "supp": "",
        "pdf_size": 120036,
        "gs_citation": 84,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3886368261411570141&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "NTT Software Innovation Center, 3-9-11 Midori-cho Musashino-shi, Tokyo, Japan; NTT Media Intelligence Laboratories, 1-1 Hikarinooka Yokosuka-shi, Kanagawa, Japan",
        "aff_domain": "LAB.NTT.CO.JP;LAB.NTT.CO.JP",
        "email": "LAB.NTT.CO.JP;LAB.NTT.CO.JP",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "NTT Software Innovation Center;NTT Media Intelligence Laboratories",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.ntt.co.jp",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "85de0db1f4",
        "title": "Efficient Learning of Mahalanobis Metrics for Ranking",
        "site": "https://proceedings.mlr.press/v32/lim14.html",
        "author": "Daryl Lim; Gert Lanckriet",
        "abstract": "We develop an efficient algorithm to learn a Mahalanobis distance metric by directly optimizing a ranking loss.  Our approach focuses on optimizing the top of the induced rankings, which is desirable in tasks such as visualization and nearest-neighbor retrieval.  We further develop and justify a simple technique to reduce training time significantly with minimal impact on performance.   Our proposed method significantly outperforms alternative methods on several real-world tasks, and can scale to large and high-dimensional data.",
        "bibtex": "@InProceedings{pmlr-v32-lim14,\n  title = \t {Efficient Learning of Mahalanobis Metrics for Ranking},\n  author = \t {Lim, Daryl and Lanckriet, Gert},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1980--1988},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/lim14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/lim14.html},\n  abstract = \t {We develop an efficient algorithm to learn a Mahalanobis distance metric by directly optimizing a ranking loss.  Our approach focuses on optimizing the top of the induced rankings, which is desirable in tasks such as visualization and nearest-neighbor retrieval.  We further develop and justify a simple technique to reduce training time significantly with minimal impact on performance.   Our proposed method significantly outperforms alternative methods on several real-world tasks, and can scale to large and high-dimensional data.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/lim14.pdf",
        "supp": "",
        "pdf_size": 454183,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7624492838366910933&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Electrical and Computer Engineering, University of California, San Diego, CA 92093 USA; Department of Electrical and Computer Engineering, University of California, San Diego, CA 92093 USA",
        "aff_domain": "UCSD.EDU;ECE.UCSD.EDU",
        "email": "UCSD.EDU;ECE.UCSD.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "800aff44f4",
        "title": "Elementary Estimators for High-Dimensional Linear Regression",
        "site": "https://proceedings.mlr.press/v32/yangc14.html",
        "author": "Eunho Yang; Aurelie Lozano; Pradeep Ravikumar",
        "abstract": "We consider the problem of structurally constrained high-dimensional linear regression. This has attracted considerable attention over the last decade, with state of the art statistical estimators based on solving regularized convex programs. While these typically non-smooth convex programs can be solved in polynomial time, scaling the state of the art optimization methods to very large-scale problems is an ongoing and rich area of research. In this paper, we attempt to address this scaling issue at the source, by asking whether one can build \\emphsimpler possibly closed-form estimators, that yet come with statistical guarantees that are nonetheless comparable to regularized likelihood estimators! We answer this question in the affirmative, with variants of the classical ridge and OLS (ordinary least squares estimators) for linear regression. We analyze our estimators in the high-dimensional setting, and moreover provide empirical corroboration of its performance on simulated as well as real world microarray data.",
        "bibtex": "@InProceedings{pmlr-v32-yangc14,\n  title = \t {Elementary Estimators for High-Dimensional Linear Regression},\n  author = \t {Yang, Eunho and Lozano, Aurelie and Ravikumar, Pradeep},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {388--396},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/yangc14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/yangc14.html},\n  abstract = \t {We consider the problem of structurally constrained high-dimensional linear regression. This has attracted considerable attention over the last decade, with state of the art statistical estimators based on solving regularized convex programs. While these typically non-smooth convex programs can be solved in polynomial time, scaling the state of the art optimization methods to very large-scale problems is an ongoing and rich area of research. In this paper, we attempt to address this scaling issue at the source, by asking whether one can build \\emphsimpler possibly closed-form estimators, that yet come with statistical guarantees that are nonetheless comparable to regularized likelihood estimators! We answer this question in the affirmative, with variants of the classical ridge and OLS (ordinary least squares estimators) for linear regression. We analyze our estimators in the high-dimensional setting, and moreover provide empirical corroboration of its performance on simulated as well as real world microarray data.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/yangc14.pdf",
        "supp": "",
        "pdf_size": 419241,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10599417333445545696&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Department of Computer Science, The University of Texas, Austin, TX 78712, USA; IBM T.J. Watson Research Center, Yorktown Heights, NY 10598, USA; Department of Computer Science, The University of Texas, Austin, TX 78712, USA",
        "aff_domain": "cs.utexas.edu;us.ibm.com;cs.utexas.edu",
        "email": "cs.utexas.edu;us.ibm.com;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Texas at Austin;IBM",
        "aff_unique_dep": "Department of Computer Science;IBM T.J. Watson Research Center",
        "aff_unique_url": "https://www.utexas.edu;https://www.ibm.com/research/watson",
        "aff_unique_abbr": "UT Austin;IBM Watson",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Austin;Yorktown Heights",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "16b8b138f3",
        "title": "Elementary Estimators for Sparse Covariance Matrices and other Structured Moments",
        "site": "https://proceedings.mlr.press/v32/yangd14.html",
        "author": "Eunho Yang; Aurelie Lozano; Pradeep Ravikumar",
        "abstract": "We consider the problem of estimating distributional parameters that are expected values of given feature functions. We are interested in recovery under high-dimensional regimes, where the number of variables p is potentially larger than the number of samples n, and where we need to impose structural constraints upon the parameters. In a natural distributional setting for this problem, the feature functions comprise the sufficient statistics of an exponential family, so that the problem would entail estimating structured moments of exponential family distributions. A special case of the above involves estimating the covariance matrix of a random vector, and where the natural distributional setting would correspond to the multivariate Gaussian distribution. Unlike the inverse covariance estimation case, we show that the regularized MLEs for covariance estimation, as well as natural Dantzig variants, are \\emphnon-convex, even when the regularization functions themselves are convex; with the same holding for the general structured moment case. We propose a class of elementary convex estimators, that in many cases are available in \\emphclosed-form, for estimating general structured moments. We then provide a unified statistical analysis of our class of estimators. Finally, we demonstrate the applicability of our class of estimators on real-world climatology and biology datasets.",
        "bibtex": "@InProceedings{pmlr-v32-yangd14,\n  title = \t {Elementary Estimators for Sparse Covariance Matrices and other Structured Moments},\n  author = \t {Yang, Eunho and Lozano, Aurelie and Ravikumar, Pradeep},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {397--405},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/yangd14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/yangd14.html},\n  abstract = \t {We consider the problem of estimating distributional parameters that are expected values of given feature functions. We are interested in recovery under high-dimensional regimes, where the number of variables p is potentially larger than the number of samples n, and where we need to impose structural constraints upon the parameters. In a natural distributional setting for this problem, the feature functions comprise the sufficient statistics of an exponential family, so that the problem would entail estimating structured moments of exponential family distributions. A special case of the above involves estimating the covariance matrix of a random vector, and where the natural distributional setting would correspond to the multivariate Gaussian distribution. Unlike the inverse covariance estimation case, we show that the regularized MLEs for covariance estimation, as well as natural Dantzig variants, are \\emphnon-convex, even when the regularization functions themselves are convex; with the same holding for the general structured moment case. We propose a class of elementary convex estimators, that in many cases are available in \\emphclosed-form, for estimating general structured moments. We then provide a unified statistical analysis of our class of estimators. Finally, we demonstrate the applicability of our class of estimators on real-world climatology and biology datasets.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/yangd14.pdf",
        "supp": "",
        "pdf_size": 1355964,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9945105973278584344&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computer Science, The University of Texas, Austin, TX 78712, USA; IBM T.J. Watson Research Center, Yorktown Heights, NY 10598, USA; Department of Computer Science, The University of Texas, Austin, TX 78712, USA",
        "aff_domain": "cs.utexas.edu;us.ibm.com;cs.utexas.edu",
        "email": "cs.utexas.edu;us.ibm.com;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Texas at Austin;IBM",
        "aff_unique_dep": "Department of Computer Science;IBM T.J. Watson Research Center",
        "aff_unique_url": "https://www.utexas.edu;https://www.ibm.com/research/watson",
        "aff_unique_abbr": "UT Austin;IBM Watson",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Austin;Yorktown Heights",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d89fbe2c40",
        "title": "Ensemble Methods for Structured Prediction",
        "site": "https://proceedings.mlr.press/v32/cortesa14.html",
        "author": "Corinna Cortes; Vitaly Kuznetsov; Mehryar Mohri",
        "abstract": "We present a series of learning algorithms and theoretical guarantees for designing accurate ensembles of structured prediction tasks. This includes several randomized and deterministic algorithms devised by converting on-line learning algorithms to batch ones, and a boosting-style algorithm applicable in the context of structured prediction with a large number of labels. We give a detailed study of all these algorithms, including the description of new on-line-to-batch conversions and learning guarantees. We also report the results of extensive experiments with these algorithms in several structured prediction tasks.",
        "bibtex": "@InProceedings{pmlr-v32-cortesa14,\n  title = \t {Ensemble Methods for Structured Prediction},\n  author = \t {Cortes, Corinna and Kuznetsov, Vitaly and Mohri, Mehryar},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1134--1142},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/cortesa14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/cortesa14.html},\n  abstract = \t {We present a series of learning algorithms and theoretical guarantees for designing accurate ensembles of structured prediction tasks. This includes several randomized and deterministic algorithms devised by converting on-line learning algorithms to batch ones, and a boosting-style algorithm applicable in the context of structured prediction with a large number of labels. We give a detailed study of all these algorithms, including the description of new on-line-to-batch conversions and learning guarantees. We also report the results of extensive experiments with these algorithms in several structured prediction tasks.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/cortesa14.pdf",
        "supp": "",
        "pdf_size": 334904,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9972489707494707371&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Google Research; Courant Institute of Mathematical Sciences; Courant Institute and Google Research",
        "aff_domain": "GOOGLE.COM;CIMS.NYU.EDU;CIMS.NYU.EDU",
        "email": "GOOGLE.COM;CIMS.NYU.EDU;CIMS.NYU.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Google;Courant Institute of Mathematical Sciences;Courant Institute",
        "aff_unique_dep": "Google Research;Mathematical Sciences;Courant Institute",
        "aff_unique_url": "https://research.google;https://cims.nyu.edu;https://courant.nyu.edu",
        "aff_unique_abbr": "Google Research;CIMS;Courant",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "05968c4f41",
        "title": "Ensemble-Based Tracking: Aggregating Crowdsourced Structured Time Series Data",
        "site": "https://proceedings.mlr.press/v32/wangg14.html",
        "author": "Naiyan Wang; Dit-Yan Yeung",
        "abstract": "We study the problem of aggregating the contributions of multiple contributors in a crowdsourcing setting.  The data involved is in a form not typically considered in most crowdsourcing tasks, in that the data is structured and has a temporal dimension.  In particular, we study the visual tracking problem in which the unknown data to  be estimated is in the form of a sequence of bounding boxes representing the trajectory of the target object being tracked.  We propose a factorial hidden Markov model (FHMM) for ensemble-based tracking by learning jointly the unknown trajectory of the target and the reliability of each tracker in the ensemble.  For efficient online inference of the FHMM, we devise a conditional particle filter algorithm by exploiting the structure of the joint posterior distribution of the hidden variables.  Using the largest open benchmark for visual tracking, we empirically compare two ensemble methods constructed from five state-of-the-art trackers with the individual trackers.  The promising experimental results provide empirical evidence for our ensemble approach to \"get the best of all worlds\".",
        "bibtex": "@InProceedings{pmlr-v32-wangg14,\n  title = \t {Ensemble-Based Tracking: Aggregating Crowdsourced Structured Time Series Data},\n  author = \t {Wang, Naiyan and Yeung, Dit-Yan},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1107--1115},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/wangg14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/wangg14.html},\n  abstract = \t {We study the problem of aggregating the contributions of multiple contributors in a crowdsourcing setting.  The data involved is in a form not typically considered in most crowdsourcing tasks, in that the data is structured and has a temporal dimension.  In particular, we study the visual tracking problem in which the unknown data to  be estimated is in the form of a sequence of bounding boxes representing the trajectory of the target object being tracked.  We propose a factorial hidden Markov model (FHMM) for ensemble-based tracking by learning jointly the unknown trajectory of the target and the reliability of each tracker in the ensemble.  For efficient online inference of the FHMM, we devise a conditional particle filter algorithm by exploiting the structure of the joint posterior distribution of the hidden variables.  Using the largest open benchmark for visual tracking, we empirically compare two ensemble methods constructed from five state-of-the-art trackers with the individual trackers.  The promising experimental results provide empirical evidence for our ensemble approach to \"get the best of all worlds\".}\n}",
        "pdf": "http://proceedings.mlr.press/v32/wangg14.pdf",
        "supp": "",
        "pdf_size": 1142029,
        "gs_citation": 122,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3341312947522730465&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Science and Engineering, Hong Kong Univeristy of Science and Technology; Department of Computer Science and Engineering, Hong Kong Univeristy of Science and Technology",
        "aff_domain": "gmail.com;cse.ust.hk",
        "email": "gmail.com;cse.ust.hk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.ust.hk",
        "aff_unique_abbr": "HKUST",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "4da3209d28",
        "title": "Estimating Diffusion Network Structures: Recovery Conditions, Sample Complexity & Soft-thresholding Algorithm",
        "site": "https://proceedings.mlr.press/v32/daneshmand14.html",
        "author": "Hadi Daneshmand; Manuel Gomez-Rodriguez; Le Song; Bernhard Schoelkopf",
        "abstract": "Information spreads across social and technological networks, but often the network structures are hidden from us and we only observe the traces left by the diffusion processes, called cascades. Can we recover the hidden network structures from these observed cascades? What kind of cascades and how many cascades do we need? Are there some network structures which are more difficult than others to recover? Can we design efficient inference algorithms with provable guarantees?    Despite the increasing availability of cascade data and methods for inferring networks from these data, a thorough theoretical understanding of the above questions remains largely unexplored in the literature. In this paper, we investigate the network structure inference problem for a general family of continuous-time diffusion models using an l1-regularized likelihood maximization framework. We show that, as long as the cascade sampling process satisfies a natural incoherence condition, our framework can recover the correct network structure with high probability if we observe O(d^3 log N) cascades, where d is the maximum number of parents of a node and N is the total number of nodes. Moreover, we develop a simple and efficient soft-thresholding inference algorithm, which we use to illustrate the consequences of our theoretical results, and show that our framework outperforms other alternatives in practice.",
        "bibtex": "@InProceedings{pmlr-v32-daneshmand14,\n  title = \t {Estimating Diffusion Network Structures: Recovery Conditions, Sample Complexity & Soft-thresholding Algorithm},\n  author = \t {Daneshmand, Hadi and Gomez-Rodriguez, Manuel and Song, Le and Schoelkopf, Bernhard},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {793--801},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/daneshmand14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/daneshmand14.html},\n  abstract = \t {Information spreads across social and technological networks, but often the network structures are hidden from us and we only observe the traces left by the diffusion processes, called cascades. Can we recover the hidden network structures from these observed cascades? What kind of cascades and how many cascades do we need? Are there some network structures which are more difficult than others to recover? Can we design efficient inference algorithms with provable guarantees?    Despite the increasing availability of cascade data and methods for inferring networks from these data, a thorough theoretical understanding of the above questions remains largely unexplored in the literature. In this paper, we investigate the network structure inference problem for a general family of continuous-time diffusion models using an l1-regularized likelihood maximization framework. We show that, as long as the cascade sampling process satisfies a natural incoherence condition, our framework can recover the correct network structure with high probability if we observe O(d^3 log N) cascades, where d is the maximum number of parents of a node and N is the total number of nodes. Moreover, we develop a simple and efficient soft-thresholding inference algorithm, which we use to illustrate the consequences of our theoretical results, and show that our framework outperforms other alternatives in practice.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/daneshmand14.pdf",
        "supp": "",
        "pdf_size": 582058,
        "gs_citation": 120,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15324553191112131119&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 20,
        "aff": "MPI for Intelligent Systems; MPI for Intelligent Systems; Georgia Institute of Technology; MPI for Intelligent Systems",
        "aff_domain": "TUE.MPG.DE;TUE.MPG.DE;CC.GATECH.EDU;TUE.MPG.DE",
        "email": "TUE.MPG.DE;TUE.MPG.DE;CC.GATECH.EDU;TUE.MPG.DE",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems;Georgia Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.mpi-is.mpg.de;https://www.gatech.edu",
        "aff_unique_abbr": "MPI-IS;Georgia Tech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "Germany;United States"
    },
    {
        "id": "96bcd71712",
        "title": "Estimating Latent-Variable Graphical Models using Moments and Likelihoods",
        "site": "https://proceedings.mlr.press/v32/chaganty14.html",
        "author": "Arun Tejasvi Chaganty; Percy Liang",
        "abstract": "Recent work in method of moments provide consistent estimates for  latent-variable models, avoiding local optima issues, but these methods can  only be applied to certain types of graphical models. In this work, we show  that the method of moments in conjunction with a composite marginal likelihood  objective yields consistent parameter estimates for a much broader class of  directed and undirected graphical models, including loopy graphs with high  treewidth. Specifically, we use tensor factorization to reveal partial  information about the hidden variables, rendering the otherwise non-convex  negative log-likelihood convex. Our approach gracefully extends to models  outside our class by incorporating the partial information via posterior  regulraization.",
        "bibtex": "@InProceedings{pmlr-v32-chaganty14,\n  title = \t {Estimating Latent-Variable Graphical Models using Moments and Likelihoods},\n  author = \t {Chaganty, Arun Tejasvi and Liang, Percy},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1872--1880},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/chaganty14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/chaganty14.html},\n  abstract = \t {Recent work in method of moments provide consistent estimates for  latent-variable models, avoiding local optima issues, but these methods can  only be applied to certain types of graphical models. In this work, we show  that the method of moments in conjunction with a composite marginal likelihood  objective yields consistent parameter estimates for a much broader class of  directed and undirected graphical models, including loopy graphs with high  treewidth. Specifically, we use tensor factorization to reveal partial  information about the hidden variables, rendering the otherwise non-convex  negative log-likelihood convex. Our approach gracefully extends to models  outside our class by incorporating the partial information via posterior  regulraization.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/chaganty14.pdf",
        "supp": "",
        "pdf_size": 512917,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14961265935611827193&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Stanford University, Stanford, CA, USA; Stanford University, Stanford, CA, USA",
        "aff_domain": "CS.STANFORD.EDU;CS.STANFORD.EDU",
        "email": "CS.STANFORD.EDU;CS.STANFORD.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b4ec585ac2",
        "title": "Exchangeable Variable Models",
        "site": "https://proceedings.mlr.press/v32/niepert14.html",
        "author": "Mathias Niepert; Pedro Domingos",
        "abstract": "A sequence of random variables is exchangeable if its joint distribution is invariant under variable permutations. We introduce exchangeable variable models (EVMs) as a novel class of probabilistic models whose basic building blocks are partially exchangeable sequences, a generalization of exchangeable sequences. We prove that a family of tractable EVMs is optimal under zero-one loss for a large class of functions, including parity and threshold functions, and strictly subsumes existing tractable independence-based model families. Extensive experiments show that EVMs outperform state of the art classifiers such as SVMs and  probabilistic models which are solely based on independence assumptions.",
        "bibtex": "@InProceedings{pmlr-v32-niepert14,\n  title = \t {Exchangeable Variable Models},\n  author = \t {Niepert, Mathias and Domingos, Pedro},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {271--279},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/niepert14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/niepert14.html},\n  abstract = \t {A sequence of random variables is exchangeable if its joint distribution is invariant under variable permutations. We introduce exchangeable variable models (EVMs) as a novel class of probabilistic models whose basic building blocks are partially exchangeable sequences, a generalization of exchangeable sequences. We prove that a family of tractable EVMs is optimal under zero-one loss for a large class of functions, including parity and threshold functions, and strictly subsumes existing tractable independence-based model families. Extensive experiments show that EVMs outperform state of the art classifiers such as SVMs and  probabilistic models which are solely based on independence assumptions.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/niepert14.pdf",
        "supp": "",
        "pdf_size": 417512,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14759868358765399626&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science & Engineering, University of Washington, Seattle, WA 98195, USA; Department of Computer Science & Engineering, University of Washington, Seattle, WA 98195, USA",
        "aff_domain": "CS.WASHINGTON.EDU;CS.WASHINGTON.EDU",
        "email": "CS.WASHINGTON.EDU;CS.WASHINGTON.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "Department of Computer Science & Engineering",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "063f6d08cb",
        "title": "Exponential Family Matrix Completion under Structural Constraints",
        "site": "https://proceedings.mlr.press/v32/gunasekar14.html",
        "author": "Suriya Gunasekar; Pradeep Ravikumar; Joydeep Ghosh",
        "abstract": "We consider the matrix completion problem of recovering a structured matrix from noisy and partial measurements. Recent works have proposed tractable estimators with strong statistical guarantees for the case where the underlying matrix is low\u2013rank, and the measurements consist of a subset, either of the exact individual entries,  or of the entries perturbed by additive Gaussian noise, which is thus implicitly suited for thin\u2013tailed continuous data. Arguably, common applications of matrix completion require estimators for (a) heterogeneous data\u2013types, such as skewed\u2013continuous, count, binary, etc., (b) for heterogeneous noise models (beyond Gaussian), which capture varied uncertainty in the measurements, and (c) heterogeneous structural constraints beyond low\u2013rank, such as block\u2013sparsity, or a superposition structure of low\u2013rank plus elementwise sparseness, among others. In this paper, we provide a vastly unified framework for generalized matrix completion by considering a  matrix completion setting wherein the matrix entries are sampled from any member of the rich family of \\textitexponential family distributions; and impose general structural constraints on the underlying matrix, as captured by a general regularizer \\mathcalR(.). We propose a simple convex regularized M\u2013estimator for the generalized framework, and provide a unified and novel statistical analysis for this general class of estimators. We finally corroborate our theoretical results on simulated datasets.",
        "bibtex": "@InProceedings{pmlr-v32-gunasekar14,\n  title = \t {Exponential Family Matrix Completion under Structural Constraints},\n  author = \t {Gunasekar, Suriya and Ravikumar, Pradeep and Ghosh, Joydeep},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1917--1925},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/gunasekar14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/gunasekar14.html},\n  abstract = \t {We consider the matrix completion problem of recovering a structured matrix from noisy and partial measurements. Recent works have proposed tractable estimators with strong statistical guarantees for the case where the underlying matrix is low\u2013rank, and the measurements consist of a subset, either of the exact individual entries,  or of the entries perturbed by additive Gaussian noise, which is thus implicitly suited for thin\u2013tailed continuous data. Arguably, common applications of matrix completion require estimators for (a) heterogeneous data\u2013types, such as skewed\u2013continuous, count, binary, etc., (b) for heterogeneous noise models (beyond Gaussian), which capture varied uncertainty in the measurements, and (c) heterogeneous structural constraints beyond low\u2013rank, such as block\u2013sparsity, or a superposition structure of low\u2013rank plus elementwise sparseness, among others. In this paper, we provide a vastly unified framework for generalized matrix completion by considering a  matrix completion setting wherein the matrix entries are sampled from any member of the rich family of \\textitexponential family distributions; and impose general structural constraints on the underlying matrix, as captured by a general regularizer \\mathcalR(.). We propose a simple convex regularized M\u2013estimator for the generalized framework, and provide a unified and novel statistical analysis for this general class of estimators. We finally corroborate our theoretical results on simulated datasets.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/gunasekar14.pdf",
        "supp": "",
        "pdf_size": 392180,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15174674687711936158&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "The University of Texas at Austin, Texas, USA; The University of Texas at Austin, Texas, USA; The University of Texas at Austin, Texas, USA",
        "aff_domain": "UTEXAS.EDU;CS.UTEXAS.EDU;ECE.UTEXAS.EDU",
        "email": "UTEXAS.EDU;CS.UTEXAS.EDU;ECE.UTEXAS.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "cef9566edd",
        "title": "Factorized Point Process Intensities: A Spatial Analysis of Professional Basketball",
        "site": "https://proceedings.mlr.press/v32/miller14.html",
        "author": "Andrew Miller; Luke Bornn; Ryan Adams; Kirk Goldsberry",
        "abstract": "We develop a machine learning approach to represent and analyze the underlying spatial structure that governs shot selection among professional basketball players in the NBA.  Typically, NBA players are discussed and compared in an heuristic, imprecise manner that relies on unmeasured intuitions about player behavior.  This makes it difficult to draw comparisons between players and make accurate player specific predictions.  Modeling shot attempt data as a point process, we create a low dimensional representation of offensive player types in the NBA.  Using non-negative matrix factorization (NMF), an unsupervised dimensionality reduction technique, we show that a low-rank spatial decomposition summarizes the shooting habits of NBA players.  The spatial representations discovered by the algorithm correspond to intuitive descriptions of NBA player types, and can be used to model other spatial effects, such as shooting accuracy.",
        "bibtex": "@InProceedings{pmlr-v32-miller14,\n  title = \t {Factorized Point Process Intensities: A Spatial Analysis of Professional Basketball},\n  author = \t {Miller, Andrew and Bornn, Luke and Adams, Ryan and Goldsberry, Kirk},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {235--243},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/miller14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/miller14.html},\n  abstract = \t {We develop a machine learning approach to represent and analyze the underlying spatial structure that governs shot selection among professional basketball players in the NBA.  Typically, NBA players are discussed and compared in an heuristic, imprecise manner that relies on unmeasured intuitions about player behavior.  This makes it difficult to draw comparisons between players and make accurate player specific predictions.  Modeling shot attempt data as a point process, we create a low dimensional representation of offensive player types in the NBA.  Using non-negative matrix factorization (NMF), an unsupervised dimensionality reduction technique, we show that a low-rank spatial decomposition summarizes the shooting habits of NBA players.  The spatial representations discovered by the algorithm correspond to intuitive descriptions of NBA player types, and can be used to model other spatial effects, such as shooting accuracy.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/miller14.pdf",
        "supp": "",
        "pdf_size": 1007949,
        "gs_citation": 174,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17190439962071633403&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "School of Engineering and Applied Sciences, Harvard University, Cambridge, USA; Department of Statistics, Harvard University, Cambridge, USA; School of Engineering and Applied Sciences, Harvard University, Cambridge, USA; Center for Geographic Analysis, Harvard University, Cambridge, USA",
        "aff_domain": "SEAS.HARVARD.EDU;STAT.HARVARD.EDU;SEAS.HARVARD.EDU;FAS.HARVARD.EDU",
        "email": "SEAS.HARVARD.EDU;STAT.HARVARD.EDU;SEAS.HARVARD.EDU;FAS.HARVARD.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Harvard University",
        "aff_unique_dep": "School of Engineering and Applied Sciences",
        "aff_unique_url": "https://www.harvard.edu",
        "aff_unique_abbr": "Harvard",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0f25b9eb12",
        "title": "Fast Allocation of Gaussian Process Experts",
        "site": "https://proceedings.mlr.press/v32/nguyena14.html",
        "author": "Trung Nguyen; Edwin Bonilla",
        "abstract": "We propose a scalable nonparametric Bayesian regression model based on a mixture of Gaussian process (GP) experts  and the inducing points formalism underpinning sparse GP approximations. Each expert is augmented with a set of inducing points, and the allocation of data points to experts is defined probabilistically based on their proximity to the experts. This allocation mechanism enables a fast variational inference procedure for learning of the inducing inputs and hyperparameters of the experts. When using K experts, our method can  run K^2 times faster and use K^2 times less memory than popular sparse methods such as the FITC approximation. Furthermore, it is easy to parallelize and handles non-stationarity  straightforwardly. Our experiments show that on medium-sized datasets (of around 10^4 training points) it  trains up to 5 times faster than FITC while achieving comparable accuracy. On a large dataset  of 10^5 training points, our method significantly outperforms six  competitive baselines while requiring only a few hours of training.",
        "bibtex": "@InProceedings{pmlr-v32-nguyena14,\n  title = \t {Fast Allocation of Gaussian Process Experts},\n  author = \t {Nguyen, Trung and Bonilla, Edwin},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {145--153},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/nguyena14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/nguyena14.html},\n  abstract = \t {We propose a scalable nonparametric Bayesian regression model based on a mixture of Gaussian process (GP) experts  and the inducing points formalism underpinning sparse GP approximations. Each expert is augmented with a set of inducing points, and the allocation of data points to experts is defined probabilistically based on their proximity to the experts. This allocation mechanism enables a fast variational inference procedure for learning of the inducing inputs and hyperparameters of the experts. When using K experts, our method can  run K^2 times faster and use K^2 times less memory than popular sparse methods such as the FITC approximation. Furthermore, it is easy to parallelize and handles non-stationarity  straightforwardly. Our experiments show that on medium-sized datasets (of around 10^4 training points) it  trains up to 5 times faster than FITC while achieving comparable accuracy. On a large dataset  of 10^5 training points, our method significantly outperforms six  competitive baselines while requiring only a few hours of training.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/nguyena14.pdf",
        "supp": "",
        "pdf_size": 732044,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3078661606605216009&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "ANU & NICTA; NICTA & ANU",
        "aff_domain": "NICTA.COM.AU;NICTA.COM.AU",
        "email": "NICTA.COM.AU;NICTA.COM.AU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Australian National University;National Information and Communications Technology Australia",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.anu.edu.au;https://www.nicta.com.au",
        "aff_unique_abbr": "ANU;NICTA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "335bc84a7d",
        "title": "Fast Computation of Wasserstein Barycenters",
        "site": "https://proceedings.mlr.press/v32/cuturi14.html",
        "author": "Marco Cuturi; Arnaud Doucet",
        "abstract": "We present new algorithms to compute the mean of a set of $N$ empirical probability measures under the optimal transport metric. This mean, known as the Wasserstein barycenter\u00a0(Agueh and Carlier, 2011; Rabin et al, 2012), is the measure that minimizes the sum of its Wasserstein distances to each element in that set. We argue through a simple example that Wasserstein barycenters have appealing properties that differentiate them from other barycenters proposed recently, which all build on kernel smoothing and/or Bregman divergences. Two original algorithms are proposed that require the repeated computation of primal and dual optimal solutions of transport problems. However direct implementation of these algorithms is too costly as optimal transports are notoriously computationally expensive. Extending the work of Cuturi (2013), we smooth both the primal and dual of the optimal transport problem to recover fast approximations of the primal and dual optimal solutions. We apply these algorithms to the visualization of perturbed images and to a clustering problem.",
        "bibtex": "@InProceedings{pmlr-v32-cuturi14,\n  title = \t {Fast Computation of Wasserstein Barycenters},\n  author = \t {Cuturi, Marco and Doucet, Arnaud},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {685--693},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/cuturi14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/cuturi14.html},\n  abstract = \t {We present new algorithms to compute the mean of a set of $N$ empirical probability measures under the optimal transport metric. This mean, known as the Wasserstein barycenter\u00a0(Agueh and Carlier, 2011; Rabin et al, 2012), is the measure that minimizes the sum of its Wasserstein distances to each element in that set. We argue through a simple example that Wasserstein barycenters have appealing properties that differentiate them from other barycenters proposed recently, which all build on kernel smoothing and/or Bregman divergences. Two original algorithms are proposed that require the repeated computation of primal and dual optimal solutions of transport problems. However direct implementation of these algorithms is too costly as optimal transports are notoriously computationally expensive. Extending the work of Cuturi (2013), we smooth both the primal and dual of the optimal transport problem to recover fast approximations of the primal and dual optimal solutions. We apply these algorithms to the visualization of perturbed images and to a clustering problem.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/cuturi14.pdf",
        "supp": "",
        "pdf_size": 681907,
        "gs_citation": 944,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5548431079525395171&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Graduate School of Informatics, Kyoto University; Department of Statistics, University of Oxford",
        "aff_domain": "I.KYOTO-U.AC.JP;STAT.OXFORD.AC.UK",
        "email": "I.KYOTO-U.AC.JP;STAT.OXFORD.AC.UK",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Kyoto University;University of Oxford",
        "aff_unique_dep": "Graduate School of Informatics;Department of Statistics",
        "aff_unique_url": "https://www.kyoto-u.ac.jp;https://www.ox.ac.uk",
        "aff_unique_abbr": "Kyoto U;Oxford",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Kyoto;Oxford",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Japan;United Kingdom"
    },
    {
        "id": "fc7bbb8f61",
        "title": "Fast Multi-stage Submodular Maximization",
        "site": "https://proceedings.mlr.press/v32/wei14.html",
        "author": "Kai Wei; Rishabh Iyer; Jeff Bilmes",
        "abstract": "We introduce a new multi-stage algorithmic framework for submodular maximization. We are motivated by extremely large scale machine learning problems, where both storing the whole data for function evaluation and running the standard accelerated greedy algorithm are prohibitive. We propose a multi-stage framework (called MultGreed), where at each stage we apply an approximate greedy procedure to maximize surrogate submodular functions. The surrogates serve as proxies for a target submodular function but require less memory and are easy to evaluate. We theoretically analyze the performance guarantee of the multi-stage framework, and give examples on how to design instances of MultGreed for a broad range of natural submodular functions. We show that MultGreed  performs very close to the standard greedy algorithm, given appropriate surrogate functions, and argue how our framework can easily be integrated with distributive algorithms for optimization. We complement our theory by empirically evaluating on several real world problems, including data subset selection on millions of speech samples, where MultGreed yields at least a thousand times speedup and superior results over the state-of-the-art selection methods.",
        "bibtex": "@InProceedings{pmlr-v32-wei14,\n  title = \t {Fast Multi-stage Submodular Maximization},\n  author = \t {Wei, Kai and Iyer, Rishabh and Bilmes, Jeff},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1494--1502},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/wei14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/wei14.html},\n  abstract = \t {We introduce a new multi-stage algorithmic framework for submodular maximization. We are motivated by extremely large scale machine learning problems, where both storing the whole data for function evaluation and running the standard accelerated greedy algorithm are prohibitive. We propose a multi-stage framework (called MultGreed), where at each stage we apply an approximate greedy procedure to maximize surrogate submodular functions. The surrogates serve as proxies for a target submodular function but require less memory and are easy to evaluate. We theoretically analyze the performance guarantee of the multi-stage framework, and give examples on how to design instances of MultGreed for a broad range of natural submodular functions. We show that MultGreed  performs very close to the standard greedy algorithm, given appropriate surrogate functions, and argue how our framework can easily be integrated with distributive algorithms for optimization. We complement our theory by empirically evaluating on several real world problems, including data subset selection on millions of speech samples, where MultGreed yields at least a thousand times speedup and superior results over the state-of-the-art selection methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/wei14.pdf",
        "supp": "",
        "pdf_size": 530909,
        "gs_citation": 109,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9092923829458319157&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "University of Washington, Seattle, WA 98195, USA; University of Washington, Seattle, WA 98195, USA; University of Washington, Seattle, WA 98195, USA",
        "aff_domain": "U.WASHINGTON.EDU;U.WASHINGTON.EDU;U.WASHINGTON.EDU",
        "email": "U.WASHINGTON.EDU;U.WASHINGTON.EDU;U.WASHINGTON.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d315a694e8",
        "title": "Fast Stochastic Alternating Direction Method of Multipliers",
        "site": "https://proceedings.mlr.press/v32/zhong14.html",
        "author": "Wenliang Zhong; James Kwok",
        "abstract": "We propose a new stochastic alternating direction method of multipliers (ADMM) algorithm, which incrementally approximates the full gradient in the linearized ADMM formulation. Besides having a low per-iteration complexity as existing stochastic ADMM algorithms,  it improves the convergence rate on convex problems from \\mO(1/\\sqrtT) to \\mO(1/T), where T is the number of iterations. This matches the  convergence rate of the batch ADMM algorithm, but without the need to visit all the samples in each iteration. Experiments on the graph-guided fused lasso demonstrate that the new algorithm is significantly faster than state-of-the-art stochastic and batch ADMM algorithms.",
        "bibtex": "@InProceedings{pmlr-v32-zhong14,\n  title = \t {Fast Stochastic Alternating Direction Method of Multipliers},\n  author = \t {Zhong, Wenliang and Kwok, James},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {46--54},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/zhong14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/zhong14.html},\n  abstract = \t {We propose a new stochastic alternating direction method of multipliers (ADMM) algorithm, which incrementally approximates the full gradient in the linearized ADMM formulation. Besides having a low per-iteration complexity as existing stochastic ADMM algorithms,  it improves the convergence rate on convex problems from \\mO(1/\\sqrtT) to \\mO(1/T), where T is the number of iterations. This matches the  convergence rate of the batch ADMM algorithm, but without the need to visit all the samples in each iteration. Experiments on the graph-guided fused lasso demonstrate that the new algorithm is significantly faster than state-of-the-art stochastic and batch ADMM algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/zhong14.pdf",
        "supp": "",
        "pdf_size": 2615219,
        "gs_citation": 145,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5492911920877348213&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong; Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong",
        "aff_domain": "CSE.UST.HK;CSE.UST.HK",
        "email": "CSE.UST.HK;CSE.UST.HK",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.ust.hk",
        "aff_unique_abbr": "HKUST",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "9f1dd43cb9",
        "title": "Fast large-scale optimization by unifying stochastic gradient and quasi-Newton methods",
        "site": "https://proceedings.mlr.press/v32/sohl-dicksteinb14.html",
        "author": "Jascha Sohl-Dickstein; Ben Poole; Surya Ganguli",
        "abstract": "We present an algorithm for minimizing a sum of functions that combines the computational efficiency of stochastic gradient descent (SGD) with the second order curvature information leveraged by quasi-Newton methods. We unify these disparate approaches by maintaining an independent Hessian approximation for each contributing function in the sum. We maintain computational tractability and limit memory requirements even for high dimensional optimization problems by storing and manipulating these quadratic approximations in a shared, time evolving, low dimensional subspace. This algorithm contrasts with earlier stochastic second order techniques that treat the Hessian of each contributing function as a noisy approximation to the full Hessian, rather than as a target for direct estimation. Each update step requires only a single contributing function or minibatch evaluation (as in SGD), and each step is scaled using an approximate inverse Hessian and little to no adjustment of hyperparameters is required (as is typical for quasi-Newton methods). We experimentally demonstrate improved convergence on seven diverse optimization problems. The algorithm is released as open source Python and MATLAB packages.",
        "bibtex": "@InProceedings{pmlr-v32-sohl-dicksteinb14,\n  title = \t {Fast large-scale optimization by unifying stochastic gradient and quasi-Newton methods},\n  author = \t {Sohl-Dickstein, Jascha and Poole, Ben and Ganguli, Surya},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {604--612},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/sohl-dicksteinb14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/sohl-dicksteinb14.html},\n  abstract = \t {We present an algorithm for minimizing a sum of functions that combines the computational efficiency of stochastic gradient descent (SGD) with the second order curvature information leveraged by quasi-Newton methods. We unify these disparate approaches by maintaining an independent Hessian approximation for each contributing function in the sum. We maintain computational tractability and limit memory requirements even for high dimensional optimization problems by storing and manipulating these quadratic approximations in a shared, time evolving, low dimensional subspace. This algorithm contrasts with earlier stochastic second order techniques that treat the Hessian of each contributing function as a noisy approximation to the full Hessian, rather than as a target for direct estimation. Each update step requires only a single contributing function or minibatch evaluation (as in SGD), and each step is scaled using an approximate inverse Hessian and little to no adjustment of hyperparameters is required (as is typical for quasi-Newton methods). We experimentally demonstrate improved convergence on seven diverse optimization problems. The algorithm is released as open source Python and MATLAB packages.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/sohl-dicksteinb14.pdf",
        "supp": "",
        "pdf_size": 1189799,
        "gs_citation": 151,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11342652753582861769&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 11,
        "aff": "Stanford University; Stanford University; Stanford University",
        "aff_domain": "stanford.edu;cs.stanford.edu;stanford.edu",
        "email": "stanford.edu;cs.stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f8ffb18c5c",
        "title": "Filtering with Abstract Particles",
        "site": "https://proceedings.mlr.press/v32/steinhardt14.html",
        "author": "Jacob Steinhardt; Percy Liang",
        "abstract": "Using particles, beam search and sequential Monte Carlo can approximate distributions in an extremely flexible manner. However, they can suffer from sparsity and inadequate coverage on large state spaces. We present a new filtering method that addresses this issue  by using \u201cabstract particles\u201d that each represent an entire region of the state space. These abstract particles are combined into a hierarchical decomposition, yielding a  representation that is both compact and flexible. Empirically, our method outperforms beam search and sequential Monte Carlo on both a text reconstruction task and a multiple object tracking task.",
        "bibtex": "@InProceedings{pmlr-v32-steinhardt14,\n  title = \t {Filtering with Abstract Particles},\n  author = \t {Steinhardt, Jacob and Liang, Percy},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {727--735},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/steinhardt14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/steinhardt14.html},\n  abstract = \t {Using particles, beam search and sequential Monte Carlo can approximate distributions in an extremely flexible manner. However, they can suffer from sparsity and inadequate coverage on large state spaces. We present a new filtering method that addresses this issue  by using \u201cabstract particles\u201d that each represent an entire region of the state space. These abstract particles are combined into a hierarchical decomposition, yielding a  representation that is both compact and flexible. Empirically, our method outperforms beam search and sequential Monte Carlo on both a text reconstruction task and a multiple object tracking task.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/steinhardt14.pdf",
        "supp": "",
        "pdf_size": 588177,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13246494970868782764&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Stanford University; Stanford University",
        "aff_domain": "CS.STANFORD.EDU;CS.STANFORD.EDU",
        "email": "CS.STANFORD.EDU;CS.STANFORD.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a7eb1a6372",
        "title": "Finding Dense Subgraphs via Low-Rank Bilinear Optimization",
        "site": "https://proceedings.mlr.press/v32/papailiopoulos14.html",
        "author": "Dimitris Papailiopoulos; Ioannis Mitliagkas; Alexandros Dimakis; Constantine Caramanis",
        "abstract": "Given a graph, the Densest k-Subgraph (\\DkS) problem asks for the subgraph on k vertices that contains the largest number of edges. In this work, we develop a novel algorithm for \\DkS that searches a low-dimensional space for provably good solutions.  We obtain provable performance bounds that depend on the graph spectrum.  One of our results is that if there exists a k-subgraph that contains a constant fraction of all the edges, we can approximate \\DkS within a factor arbitrarily close to two in polynomial time.     Our algorithm runs in nearly linear time, under spectral assumptions satisfied by   most graphs found in applications. Moreover, it is highly scalable and parallelizable.  We demonstrate this by implementing it in MapReduce and executing numerous experiments on  massive real-world graphs that have up to billions of edges.  We empirically show that our algorithm can find subgraphs of significantly higher density compared to the previous state of the art.",
        "bibtex": "@InProceedings{pmlr-v32-papailiopoulos14,\n  title = \t {Finding Dense Subgraphs via Low-Rank Bilinear Optimization},\n  author = \t {Papailiopoulos, Dimitris and Mitliagkas, Ioannis and Dimakis, Alexandros and Caramanis, Constantine},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1890--1898},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/papailiopoulos14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/papailiopoulos14.html},\n  abstract = \t {Given a graph, the Densest k-Subgraph (\\DkS) problem asks for the subgraph on k vertices that contains the largest number of edges. In this work, we develop a novel algorithm for \\DkS that searches a low-dimensional space for provably good solutions.  We obtain provable performance bounds that depend on the graph spectrum.  One of our results is that if there exists a k-subgraph that contains a constant fraction of all the edges, we can approximate \\DkS within a factor arbitrarily close to two in polynomial time.     Our algorithm runs in nearly linear time, under spectral assumptions satisfied by   most graphs found in applications. Moreover, it is highly scalable and parallelizable.  We demonstrate this by implementing it in MapReduce and executing numerous experiments on  massive real-world graphs that have up to billions of edges.  We empirically show that our algorithm can find subgraphs of significantly higher density compared to the previous state of the art.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/papailiopoulos14.pdf",
        "supp": "",
        "pdf_size": 606157,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7514496456103173947&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "The University of Texas at Austin; The University of Texas at Austin; The University of Texas at Austin; The University of Texas at Austin",
        "aff_domain": "UTEXAS.EDU;UTEXAS.EDU;AUSTIN.UTEXAS.EDU;UTEXAS.EDU",
        "email": "UTEXAS.EDU;UTEXAS.EDU;AUSTIN.UTEXAS.EDU;UTEXAS.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a5577330a8",
        "title": "Finito: A faster, permutable incremental gradient method for big data problems",
        "site": "https://proceedings.mlr.press/v32/defazio14.html",
        "author": "Aaron Defazio; Justin Domke;  Caetano",
        "abstract": "Recent advances in optimization theory have shown that smooth strongly convex finite sums can be minimized faster than by treating them as a black box \"batch\" problem. In this work we introduce a new method in this class with a theoretical convergence rate four times faster than existing methods, for sums with sufficiently many terms. This method is also amendable to a sampling without replacement scheme that in practice gives further speed-ups. We give empirical results showing state of the art performance.",
        "bibtex": "@InProceedings{pmlr-v32-defazio14,\n  title = \t {Finito: A faster, permutable incremental gradient method for big data problems},\n  author = \t {Defazio, Aaron and Domke, Justin and Caetano, },\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1125--1133},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/defazio14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/defazio14.html},\n  abstract = \t {Recent advances in optimization theory have shown that smooth strongly convex finite sums can be minimized faster than by treating them as a black box \"batch\" problem. In this work we introduce a new method in this class with a theoretical convergence rate four times faster than existing methods, for sums with sufficiently many terms. This method is also amendable to a sampling without replacement scheme that in practice gives further speed-ups. We give empirical results showing state of the art performance.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/defazio14.pdf",
        "supp": "",
        "pdf_size": 314435,
        "gs_citation": 213,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11804523194056331415&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "NICTA and Australian National University; NICTA and Australian National University; NICTA and Australian National University",
        "aff_domain": "ANU.EDU.AU;NICTA.COM.AU;NICTA.COM.AU",
        "email": "ANU.EDU.AU;NICTA.COM.AU;NICTA.COM.AU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Australian National University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.anu.edu.au",
        "aff_unique_abbr": "ANU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "c309cd484d",
        "title": "Forward-Backward Greedy Algorithms for General Convex Smooth Functions over A Cardinality Constraint",
        "site": "https://proceedings.mlr.press/v32/liub14.html",
        "author": "Ji Liu; Jieping Ye; Ryohei Fujimaki",
        "abstract": "We consider forward-backward greedy algorithms for solving sparse feature selection problems with general convex smooth functions. A state-of-the-art greedy method, the Forward-Backward greedy algorithm (FoBa-obj) requires to solve a large number of optimization problems, thus it is not scalable for large-size problems. The FoBa-gdt algorithm, which uses the gradient information for feature selection at each forward iteration, significantly improves the efficiency of FoBa-obj. In this paper, we systematically analyze the theoretical properties of both algorithms. Our main contributions are: 1) We derive better theoretical bounds than existing analyses regarding FoBa-obj for general smooth convex functions; 2) We show that FoBa-gdt achieves the same theoretical performance as FoBa-obj under the same condition: restricted strong convexity condition. Our new bounds are consistent with the bounds of a special case (least squares) and fills a previously existing theoretical gap for general convex smooth functions; 3) We show that the restricted strong convexity condition is satisfied if the number of independent samples is more than \\bark\\log d where \\bark is the sparsity number and d is the dimension of the variable; 4) We apply FoBa-gdt (with the conditional random field objective) to the sensor selection problem for human indoor activity recognition and our results show that FoBa-gdt outperforms other methods based on forward greedy selection and L1-regularization.",
        "bibtex": "@InProceedings{pmlr-v32-liub14,\n  title = \t {Forward-Backward Greedy Algorithms for General Convex Smooth Functions over A Cardinality Constraint},\n  author = \t {Liu, Ji and Ye, Jieping and Fujimaki, Ryohei},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {503--511},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/liub14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/liub14.html},\n  abstract = \t {We consider forward-backward greedy algorithms for solving sparse feature selection problems with general convex smooth functions. A state-of-the-art greedy method, the Forward-Backward greedy algorithm (FoBa-obj) requires to solve a large number of optimization problems, thus it is not scalable for large-size problems. The FoBa-gdt algorithm, which uses the gradient information for feature selection at each forward iteration, significantly improves the efficiency of FoBa-obj. In this paper, we systematically analyze the theoretical properties of both algorithms. Our main contributions are: 1) We derive better theoretical bounds than existing analyses regarding FoBa-obj for general smooth convex functions; 2) We show that FoBa-gdt achieves the same theoretical performance as FoBa-obj under the same condition: restricted strong convexity condition. Our new bounds are consistent with the bounds of a special case (least squares) and fills a previously existing theoretical gap for general convex smooth functions; 3) We show that the restricted strong convexity condition is satisfied if the number of independent samples is more than \\bark\\log d where \\bark is the sparsity number and d is the dimension of the variable; 4) We apply FoBa-gdt (with the conditional random field objective) to the sensor selection problem for human indoor activity recognition and our results show that FoBa-gdt outperforms other methods based on forward greedy selection and L1-regularization.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/liub14.pdf",
        "supp": "",
        "pdf_size": 414248,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12542824018833057957&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Computer Sciences, University of Wisconsin-Madison; Department of Media Analytics, NEC Lab America, Inc.; Department of Computer Science and Engineering, Arizona State University",
        "aff_domain": "CS.WISC.EDU;NEC-LABS.COM;ASU.EDU",
        "email": "CS.WISC.EDU;NEC-LABS.COM;ASU.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Wisconsin-Madison;NEC Lab America, Inc.;Arizona State University",
        "aff_unique_dep": "Department of Computer Sciences;Department of Media Analytics;Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.wisc.edu;https://www.nec.com/en/global/;https://www.asu.edu",
        "aff_unique_abbr": "UW-Madison;NEC Lab;ASU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Madison;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4a42d28760",
        "title": "GEV-Canonical Regression for Accurate Binary Class Probability Estimation when One Class is Rare",
        "site": "https://proceedings.mlr.press/v32/agarwalc14.html",
        "author": "Arpit Agarwal; Harikrishna Narasimhan; Shivaram Kalyanakrishnan; Shivani Agarwal",
        "abstract": "We consider the problem of binary class probability estimation (CPE) when one class is rare compared to the other. It is well known that standard algorithms such as logistic regression do not perform well on this task as they tend to under-estimate the probability of the rare class. Common fixes include under-sampling and weighting, together with various correction schemes. Recently, Wang & Dey (2010) suggested the use of a parametrized family of asymmetric link functions based on the generalized extreme value (GEV) distribution, which has been used for modeling rare events in statistics. The approach showed promising initial results, but combined with the logarithmic CPE loss implicitly used in their work, it results in a non-convex composite loss that is difficult to optimize. In this paper, we use tools from the theory of proper composite losses (Buja et al, 2005; Reid & Williamson, 2010) to construct a canonical underlying CPE loss corresponding to the GEV link, which yields a convex proper composite loss that we call the GEV-canonical loss; this loss is tailored for the task of CPE when one class is rare, and is easy to minimize using an IRLS-type algorithm similar to that used for logistic regression. Our experiments on both synthetic and real data demonstrate that the resulting algorithm \u2013 which we term GEV-canonical regression \u2013 outperforms common approaches such as under-sampling and weights correction for this problem.",
        "bibtex": "@InProceedings{pmlr-v32-agarwalc14,\n  title = \t {GEV-Canonical Regression for Accurate Binary Class Probability Estimation when One Class is Rare},\n  author = \t {Agarwal, Arpit and Narasimhan, Harikrishna and Kalyanakrishnan, Shivaram and Agarwal, Shivani},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1989--1997},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/agarwalc14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/agarwalc14.html},\n  abstract = \t {We consider the problem of binary class probability estimation (CPE) when one class is rare compared to the other. It is well known that standard algorithms such as logistic regression do not perform well on this task as they tend to under-estimate the probability of the rare class. Common fixes include under-sampling and weighting, together with various correction schemes. Recently, Wang & Dey (2010) suggested the use of a parametrized family of asymmetric link functions based on the generalized extreme value (GEV) distribution, which has been used for modeling rare events in statistics. The approach showed promising initial results, but combined with the logarithmic CPE loss implicitly used in their work, it results in a non-convex composite loss that is difficult to optimize. In this paper, we use tools from the theory of proper composite losses (Buja et al, 2005; Reid & Williamson, 2010) to construct a canonical underlying CPE loss corresponding to the GEV link, which yields a convex proper composite loss that we call the GEV-canonical loss; this loss is tailored for the task of CPE when one class is rare, and is easy to minimize using an IRLS-type algorithm similar to that used for logistic regression. Our experiments on both synthetic and real data demonstrate that the resulting algorithm \u2013 which we term GEV-canonical regression \u2013 outperforms common approaches such as under-sampling and weights correction for this problem.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/agarwalc14.pdf",
        "supp": "",
        "pdf_size": 464267,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5535309874791416203&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Indian Institute of Science, Bangalore 560012, India; Indian Institute of Science, Bangalore 560012, India; Yahoo Labs Bangalore, Bangalore 560071, India; Indian Institute of Science, Bangalore 560012, India",
        "aff_domain": "CSA.IISC.ERNET.IN;CSA.IISC.ERNET.IN;YAHOO-INC.COM;CSA.IISC.ERNET.IN",
        "email": "CSA.IISC.ERNET.IN;CSA.IISC.ERNET.IN;YAHOO-INC.COM;CSA.IISC.ERNET.IN",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Indian Institute of Science;Yahoo Labs",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iisc.ac.in;https://labs.yahoo.com",
        "aff_unique_abbr": "IISc;Yahoo Labs",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Bangalore",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "f6c9be1b9d",
        "title": "Gaussian Approximation of Collective Graphical Models",
        "site": "https://proceedings.mlr.press/v32/liuf14.html",
        "author": "Liping Liu; Daniel Sheldon; Thomas Dietterich",
        "abstract": "The Collective Graphical Model (CGM) models a population of  independent and identically distributed individuals when only  collective statistics (i.e., counts of individuals) are   observed. Exact inference in CGMs is intractable, and previous work  has explored Markov Chain Monte Carlo (MCMC) and MAP approximations  for learning and inference. This paper studies Gaussian approximations  to the CGM. As the population grows large, we show that the CGM   distribution converges to a multivariate Gaussian distribution (GCGM)  that maintains the conditional independence properties of the original  CGM.  If the observations are exact marginals of the CGM or marginals  that are corrupted by Gaussian noise, inference in the GCGM  approximation can be computed efficiently in closed form. If the   observations follow a different noise model (e.g., Poisson), then  expectation propagation provides efficient and accurate approximate  inference. The accuracy and speed of GCGM inference is compared to the   MCMC and MAP methods on a simulated bird migration problem. The GCGM  matches or exceeds the accuracy of the MAP method while being significantly  faster.",
        "bibtex": "@InProceedings{pmlr-v32-liuf14,\n  title = \t {Gaussian Approximation of Collective Graphical Models},\n  author = \t {Liu, Liping and Sheldon, Daniel and Dietterich, Thomas},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1602--1610},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/liuf14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/liuf14.html},\n  abstract = \t {The Collective Graphical Model (CGM) models a population of  independent and identically distributed individuals when only  collective statistics (i.e., counts of individuals) are   observed. Exact inference in CGMs is intractable, and previous work  has explored Markov Chain Monte Carlo (MCMC) and MAP approximations  for learning and inference. This paper studies Gaussian approximations  to the CGM. As the population grows large, we show that the CGM   distribution converges to a multivariate Gaussian distribution (GCGM)  that maintains the conditional independence properties of the original  CGM.  If the observations are exact marginals of the CGM or marginals  that are corrupted by Gaussian noise, inference in the GCGM  approximation can be computed efficiently in closed form. If the   observations follow a different noise model (e.g., Poisson), then  expectation propagation provides efficient and accurate approximate  inference. The accuracy and speed of GCGM inference is compared to the   MCMC and MAP methods on a simulated bird migration problem. The GCGM  matches or exceeds the accuracy of the MAP method while being significantly  faster.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/liuf14.pdf",
        "supp": "",
        "pdf_size": 310414,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1229663334788722967&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "School of EECS, Oregon State University, Corvallis, OR 97331 USA; University of Massachusetts, Amherst, MA 01002 and Mount Holyoke College, South Hadley, MA 01075; School of EECS, Oregon State University, Corvallis, OR 97331 USA",
        "aff_domain": "EECS.OREGONSTATE.EDU;CS.UMASS.EDU;EECS.OREGONSTATE.EDU",
        "email": "EECS.OREGONSTATE.EDU;CS.UMASS.EDU;EECS.OREGONSTATE.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Oregon State University;University of Massachusetts Amherst",
        "aff_unique_dep": "School of EECS;",
        "aff_unique_url": "https://osu.edu;https://www.umass.edu",
        "aff_unique_abbr": "OSU;UMass Amherst",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Corvallis;Amherst",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3c7700f8d5",
        "title": "Gaussian Process Classification and Active Learning with Multiple Annotators",
        "site": "https://proceedings.mlr.press/v32/rodrigues14.html",
        "author": "Filipe Rodrigues; Francisco Pereira; Bernardete Ribeiro",
        "abstract": "Learning from multiple annotators took a valuable step towards modelling data that does not fit the usual single annotator setting. However, multiple annotators sometimes offer varying degrees of expertise. When disagreements arise, the establishment of the correct label through trivial solutions such as majority voting may not be adequate, since without considering heterogeneity in the annotators, we risk generating a flawed model.   In this paper, we extend GP classification in order to account for multiple annotators with different levels expertise. By explicitly handling uncertainty, Gaussian processes (GPs) provide a natural framework to build proper multiple-annotator models. We empirically show that our model significantly outperforms other commonly used approaches, such as majority voting, without a significant increase in the computational cost of approximate Bayesian inference. Furthermore, an active learning methodology is proposed, which is able to reduce annotation cost even further.",
        "bibtex": "@InProceedings{pmlr-v32-rodrigues14,\n  title = \t {Gaussian Process Classification and Active Learning with Multiple Annotators},\n  author = \t {Rodrigues, Filipe and Pereira, Francisco and Ribeiro, Bernardete},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {433--441},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/rodrigues14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/rodrigues14.html},\n  abstract = \t {Learning from multiple annotators took a valuable step towards modelling data that does not fit the usual single annotator setting. However, multiple annotators sometimes offer varying degrees of expertise. When disagreements arise, the establishment of the correct label through trivial solutions such as majority voting may not be adequate, since without considering heterogeneity in the annotators, we risk generating a flawed model.   In this paper, we extend GP classification in order to account for multiple annotators with different levels expertise. By explicitly handling uncertainty, Gaussian processes (GPs) provide a natural framework to build proper multiple-annotator models. We empirically show that our model significantly outperforms other commonly used approaches, such as majority voting, without a significant increase in the computational cost of approximate Bayesian inference. Furthermore, an active learning methodology is proposed, which is able to reduce annotation cost even further.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/rodrigues14.pdf",
        "supp": "",
        "pdf_size": 283971,
        "gs_citation": 164,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16157834658585240918&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Centre for Informatics and Systems of the University of Coimbra (CISUC), 3030-290 Coimbra, PORTUGAL; Singapore-MIT Alliance for Research and Technology (SMART) 47 1 CREATE Way, SINGAPORE; Centre for Informatics and Systems of the University of Coimbra (CISUC), 3030-290 Coimbra, PORTUGAL",
        "aff_domain": "DEI.UC.PT;SMART.MIT.EDU;DEI.UC.PT",
        "email": "DEI.UC.PT;SMART.MIT.EDU;DEI.UC.PT",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Coimbra;Singapore-MIT Alliance for Research and Technology",
        "aff_unique_dep": "Centre for Informatics and Systems;",
        "aff_unique_url": "https://www.uc.pt;",
        "aff_unique_abbr": "UC;SMART",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Coimbra;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Portugal;Singapore"
    },
    {
        "id": "dbe8494cfa",
        "title": "Gaussian Process Optimization with Mutual Information",
        "site": "https://proceedings.mlr.press/v32/contal14.html",
        "author": "Emile Contal; Vianney Perchet; Nicolas Vayatis",
        "abstract": "In this paper, we analyze a generic algorithm scheme for sequential global optimization using Gaussian processes. The upper bounds we derive on the cumulative regret for this generic algorithm improve by an exponential factor the previously known bounds for algorithms like GP-UCB. We also introduce the novel Gaussian Process Mutual Information algorithm (GP-MI), which significantly improves further these upper bounds for the cumulative regret. We confirm the efficiency of this algorithm on synthetic and real tasks against the natural competitor, GP-UCB, and also the Expected Improvement heuristic.",
        "bibtex": "@InProceedings{pmlr-v32-contal14,\n  title = \t {Gaussian Process Optimization with Mutual Information},\n  author = \t {Contal, Emile and Perchet, Vianney and Vayatis, Nicolas},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {253--261},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/contal14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/contal14.html},\n  abstract = \t {In this paper, we analyze a generic algorithm scheme for sequential global optimization using Gaussian processes. The upper bounds we derive on the cumulative regret for this generic algorithm improve by an exponential factor the previously known bounds for algorithms like GP-UCB. We also introduce the novel Gaussian Process Mutual Information algorithm (GP-MI), which significantly improves further these upper bounds for the cumulative regret. We confirm the efficiency of this algorithm on synthetic and real tasks against the natural competitor, GP-UCB, and also the Expected Improvement heuristic.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/contal14.pdf",
        "supp": "",
        "pdf_size": 929300,
        "gs_citation": 131,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11164425412519900569&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "CMLA, UMR CNRS 8536, ENS Cachan, France; LPMA, Universit \u00b4e Paris Diderot, France; CMLA, UMR CNRS 8536, ENS Cachan, France",
        "aff_domain": "CMLA.ENS-CACHAN.FR;NORMALESUP.ORG;CMLA.ENS-CACHAN.FR",
        "email": "CMLA.ENS-CACHAN.FR;NORMALESUP.ORG;CMLA.ENS-CACHAN.FR",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "ENS Cachan;Universit\u00e9 Paris Diderot",
        "aff_unique_dep": "CMLA, UMR CNRS 8536;LPMA",
        "aff_unique_url": ";https://www.univ-paris-diderot.fr",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "008a260e15",
        "title": "Gaussian Processes for Bayesian Estimation in Ordinary Differential Equations",
        "site": "https://proceedings.mlr.press/v32/barber14.html",
        "author": "David Barber; Yali Wang",
        "abstract": "Bayesian parameter estimation in coupled ordinary differential equations (ODEs) is challenging due to the high computational cost of numerical integration. In gradient matching a separate data model is introduced with the property that its gradient can be calculated easily. Parameter estimation is achieved by requiring consistency between the gradients computed from the data model and those specified by the ODE. We propose a Gaussian process model that directly links state derivative information with system observations, simplifying previous approaches and providing a natural generative model.",
        "bibtex": "@InProceedings{pmlr-v32-barber14,\n  title = \t {Gaussian Processes for Bayesian Estimation in Ordinary Differential Equations},\n  author = \t {Barber, David and Wang, Yali},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1485--1493},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/barber14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/barber14.html},\n  abstract = \t {Bayesian parameter estimation in coupled ordinary differential equations (ODEs) is challenging due to the high computational cost of numerical integration. In gradient matching a separate data model is introduced with the property that its gradient can be calculated easily. Parameter estimation is achieved by requiring consistency between the gradients computed from the data model and those specified by the ODE. We propose a Gaussian process model that directly links state derivative information with system observations, simplifying previous approaches and providing a natural generative model.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/barber14.pdf",
        "supp": "",
        "pdf_size": 653295,
        "gs_citation": 96,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11651737010796637428&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, Laval University, Canada; Department of Computer Science, University College London, U.K.",
        "aff_domain": "ulaval.ca;ucl.ac.uk",
        "email": "ulaval.ca;ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Laval University;University College London",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.laval.ca;https://www.ucl.ac.uk",
        "aff_unique_abbr": "Laval;UCL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Canada;United Kingdom"
    },
    {
        "id": "8076437a44",
        "title": "GeNGA: A Generalization of Natural Gradient Ascent with Positive and Negative Convergence Results",
        "site": "https://proceedings.mlr.press/v32/thomasb14.html",
        "author": "Philip Thomas",
        "abstract": "Natural gradient ascent (NGA) is a popular optimization method that uses a positive definite metric tensor. In many applications the metric tensor is only guaranteed to be positive semidefinite (e.g., when using the Fisher information matrix as the metric tensor), in which case NGA is not applicable. In our first contribution, we derive generalized natural gradient ascent (GeNGA), a generalization of NGA which allows for positive semidefinite non-smooth metric tensors. In our second contribution we show that, in standard settings, GeNGA and NGA can both be divergent. We then establish sufficient conditions to ensure that both achieve various forms of convergence. In our third contribution we show how several reinforcement learning methods that use NGA without positive definite metric tensors can be adapted to properly use GeNGA.",
        "bibtex": "@InProceedings{pmlr-v32-thomasb14,\n  title = \t {GeNGA: A Generalization of Natural Gradient Ascent with Positive and Negative Convergence Results},\n  author = \t {Thomas, Philip},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1575--1583},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/thomasb14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/thomasb14.html},\n  abstract = \t {Natural gradient ascent (NGA) is a popular optimization method that uses a positive definite metric tensor. In many applications the metric tensor is only guaranteed to be positive semidefinite (e.g., when using the Fisher information matrix as the metric tensor), in which case NGA is not applicable. In our first contribution, we derive generalized natural gradient ascent (GeNGA), a generalization of NGA which allows for positive semidefinite non-smooth metric tensors. In our second contribution we show that, in standard settings, GeNGA and NGA can both be divergent. We then establish sufficient conditions to ensure that both achieve various forms of convergence. In our third contribution we show how several reinforcement learning methods that use NGA without positive definite metric tensors can be adapted to properly use GeNGA.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/thomasb14.pdf",
        "supp": "",
        "pdf_size": 353409,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18028327082363303168&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "School of Computer Science, University of Massachusetts, Amherst, MA 01003 USA",
        "aff_domain": "CS.UMASS.EDU",
        "email": "CS.UMASS.EDU",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Massachusetts Amherst",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.umass.edu",
        "aff_unique_abbr": "UMass Amherst",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Amherst",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a02040f344",
        "title": "Generalized Exponential Concentration Inequality for Renyi Divergence Estimation",
        "site": "https://proceedings.mlr.press/v32/singh14.html",
        "author": "Shashank Singh; Barnabas Poczos",
        "abstract": "Estimating divergences between probability distributions in a consistent way is of great importance in many machine learning tasks. Although this is a fundamental problem in nonparametric statistics, to the best of our knowledge there has been no finite sample exponential inequality convergence bound derived for any divergence estimators. The main contribution of our work is to provide such a bound for an estimator of Renyi divergence for a smooth Holder class of densities on the d-dimensional unit cube. We also illustrate our theoretical results with a numerical experiment.",
        "bibtex": "@InProceedings{pmlr-v32-singh14,\n  title = \t {Generalized Exponential Concentration Inequality for Renyi Divergence Estimation},\n  author = \t {Singh, Shashank and Poczos, Barnabas},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {333--341},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/singh14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/singh14.html},\n  abstract = \t {Estimating divergences between probability distributions in a consistent way is of great importance in many machine learning tasks. Although this is a fundamental problem in nonparametric statistics, to the best of our knowledge there has been no finite sample exponential inequality convergence bound derived for any divergence estimators. The main contribution of our work is to provide such a bound for an estimator of Renyi divergence for a smooth Holder class of densities on the d-dimensional unit cube. We also illustrate our theoretical results with a numerical experiment.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/singh14.pdf",
        "supp": "",
        "pdf_size": 357029,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12154665925054298223&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Carnegie Mellon University, 5000 Forbes Ave., Pittsburgh, PA 15213 USA; Carnegie Mellon University, 5000 Forbes Ave., Pittsburgh, PA 15213 USA",
        "aff_domain": "ANDREW.CMU.EDU;CS.CMU.EDU",
        "email": "ANDREW.CMU.EDU;CS.CMU.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6b53c45ef1",
        "title": "Geodesic Distance Function Learning via Heat Flow on Vector Fields",
        "site": "https://proceedings.mlr.press/v32/linb14.html",
        "author": "Binbin Lin; Ji Yang; Xiaofei He; Jieping Ye",
        "abstract": "Learning a distance function or metric on a given data manifold is of great importance in machine learning and pattern recognition. Many of the previous works first embed the manifold to Euclidean space and then learn the distance function. However, such a scheme might not faithfully preserve the distance function if the original manifold is not Euclidean. In this paper, we propose to learn the distance function directly on the manifold without embedding. We first provide a theoretical characterization of the distance function by its gradient field. Based on our theoretical analysis, we propose to first learn the gradient field of the distance function and then learn the distance function itself. Specifically, we set the gradient field of a local distance function as an initial vector field. Then we transport it to the whole manifold via heat flow on vector fields. Finally, the geodesic distance function can be obtained by requiring its gradient field to be close to the normalized vector field. Experimental results on both synthetic and real data demonstrate the effectiveness of our proposed algorithm.",
        "bibtex": "@InProceedings{pmlr-v32-linb14,\n  title = \t {Geodesic Distance Function Learning via Heat Flow on Vector Fields},\n  author = \t {Lin, Binbin and Yang, Ji and He, Xiaofei and Ye, Jieping},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {145--153},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/linb14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/linb14.html},\n  abstract = \t {Learning a distance function or metric on a given data manifold is of great importance in machine learning and pattern recognition. Many of the previous works first embed the manifold to Euclidean space and then learn the distance function. However, such a scheme might not faithfully preserve the distance function if the original manifold is not Euclidean. In this paper, we propose to learn the distance function directly on the manifold without embedding. We first provide a theoretical characterization of the distance function by its gradient field. Based on our theoretical analysis, we propose to first learn the gradient field of the distance function and then learn the distance function itself. Specifically, we set the gradient field of a local distance function as an initial vector field. Then we transport it to the whole manifold via heat flow on vector fields. Finally, the geodesic distance function can be obtained by requiring its gradient field to be close to the normalized vector field. Experimental results on both synthetic and real data demonstrate the effectiveness of our proposed algorithm.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/linb14.pdf",
        "supp": "",
        "pdf_size": 5022672,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13242433057912831593&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Center for Evolutionary Medicine and Informatics, Arizona State University, Tempe, AZ 85287, USA; State Key Lab of CAD&CG, College of Computer Science, Zhejiang University Hangzhou 310058, China; State Key Lab of CAD&CG, College of Computer Science, Zhejiang University Hangzhou 310058, China; Center for Evolutionary Medicine and Informatics, Arizona State University, Tempe, AZ 85287, USA",
        "aff_domain": "ASU.EDU;GMAIL.COM;CAD.ZJU.EDU.CN;ASU.EDU",
        "email": "ASU.EDU;GMAIL.COM;CAD.ZJU.EDU.CN;ASU.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Arizona State University;Zhejiang University",
        "aff_unique_dep": "Center for Evolutionary Medicine and Informatics;College of Computer Science",
        "aff_unique_url": "https://asu.edu;http://www.zju.edu.cn",
        "aff_unique_abbr": "ASU;ZJU",
        "aff_campus_unique_index": "0;1;1;0",
        "aff_campus_unique": "Tempe;Hangzhou",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "31d3aa8120",
        "title": "Global graph kernels using geometric embeddings",
        "site": "https://proceedings.mlr.press/v32/johansson14.html",
        "author": "Fredrik Johansson; Vinay Jethava; Devdatt Dubhashi; Chiranjib Bhattacharyya",
        "abstract": "Applications of machine learning methods increasingly deal with graph structured data through kernels. Most existing graph kernels compare graphs in terms of features defined on small subgraphs such as walks, paths or graphlets, adopting an inherently local perspective. However, several interesting properties such as girth or chromatic number are global properties of the graph, and are not captured in local substructures. This paper presents two graph kernels defined on unlabeled graphs which capture global properties of graphs using the celebrated Lov\u00e1sz number and its associated orthonormal representation. We make progress towards theoretical results aiding kernel choice, proving a result about the separation margin of our kernel for classes of graphs. We give empirical results on classification of synthesized graphs with important global properties as well as established benchmark graph datasets, showing that the accuracy of our kernels is better than or competitive to existing graph kernels.",
        "bibtex": "@InProceedings{pmlr-v32-johansson14,\n  title = \t {Global graph kernels using geometric embeddings},\n  author = \t {Johansson, Fredrik and Jethava, Vinay and Dubhashi, Devdatt and Bhattacharyya, Chiranjib},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {694--702},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/johansson14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/johansson14.html},\n  abstract = \t {Applications of machine learning methods increasingly deal with graph structured data through kernels. Most existing graph kernels compare graphs in terms of features defined on small subgraphs such as walks, paths or graphlets, adopting an inherently local perspective. However, several interesting properties such as girth or chromatic number are global properties of the graph, and are not captured in local substructures. This paper presents two graph kernels defined on unlabeled graphs which capture global properties of graphs using the celebrated Lov\u00e1sz number and its associated orthonormal representation. We make progress towards theoretical results aiding kernel choice, proving a result about the separation margin of our kernel for classes of graphs. We give empirical results on classification of synthesized graphs with important global properties as well as established benchmark graph datasets, showing that the accuracy of our kernels is better than or competitive to existing graph kernels.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/johansson14.pdf",
        "supp": "",
        "pdf_size": 190711,
        "gs_citation": 81,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7537691340541667529&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Chalmers University of Technology, SE-412 96 Gothenburg, Sweden; Chalmers University of Technology, SE-412 96 Gothenburg, Sweden; Chalmers University of Technology, SE-412 96 Gothenburg, Sweden; Indian Institute of Science, Bangalore 560012 Karnataka, India",
        "aff_domain": "CHALMERS.SE;CHALMERS.SE;CHALMERS.SE;CSA.IISC.ERNET.IN",
        "email": "CHALMERS.SE;CHALMERS.SE;CHALMERS.SE;CSA.IISC.ERNET.IN",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Chalmers University of Technology;Indian Institute of Science",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.chalmers.se;https://www.iisc.ac.in",
        "aff_unique_abbr": "Chalmers;IISc",
        "aff_campus_unique_index": "0;0;0;1",
        "aff_campus_unique": "Gothenburg;Bangalore",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "Sweden;India"
    },
    {
        "id": "7717c3dfda",
        "title": "Globally Convergent Parallel MAP LP Relaxation Solver using the Frank-Wolfe Algorithm",
        "site": "https://proceedings.mlr.press/v32/schwing14.html",
        "author": "Alexander Schwing; Tamir Hazan; Marc Pollefeys; Raquel Urtasun",
        "abstract": "While MAP inference is typically intractable for many real-world applications, linear programming relaxations have been proven very effective. Dual block-coordinate descent methods are among the most efficient solvers, however, they are prone to get stuck in sub-optimal  points. Although subgradient approaches achieve global convergence, they are typically slower in practice. To improve convergence speed, algorithms which compute the steepest \u03b5-descent direction by solving a quadratic program have been proposed. In this paper we suggest to decouple the quadratic program based on the Frank-Wolfe approach. This allows us to obtain an efficient and easy to parallelize algorithm while retaining the global convergence properties. Our method proves superior when compared to existing algorithms on a set of spin-glass models and protein design tasks.",
        "bibtex": "@InProceedings{pmlr-v32-schwing14,\n  title = \t {Globally Convergent Parallel MAP LP Relaxation Solver using the Frank-Wolfe Algorithm},\n  author = \t {Schwing, Alexander and Hazan, Tamir and Pollefeys, Marc and Urtasun, Raquel},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {487--495},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/schwing14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/schwing14.html},\n  abstract = \t {While MAP inference is typically intractable for many real-world applications, linear programming relaxations have been proven very effective. Dual block-coordinate descent methods are among the most efficient solvers, however, they are prone to get stuck in sub-optimal  points. Although subgradient approaches achieve global convergence, they are typically slower in practice. To improve convergence speed, algorithms which compute the steepest \u03b5-descent direction by solving a quadratic program have been proposed. In this paper we suggest to decouple the quadratic program based on the Frank-Wolfe approach. This allows us to obtain an efficient and easy to parallelize algorithm while retaining the global convergence properties. Our method proves superior when compared to existing algorithms on a set of spin-glass models and protein design tasks.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/schwing14.pdf",
        "supp": "",
        "pdf_size": 634933,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7920880603435159302&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "University of Toronto; University of Haifa; ETH Zurich; University of Toronto",
        "aff_domain": "CS.TORONTO.EDU;CS.HAIFA.AC.IL;INF.ETHZ.CH;CS.TORONTO.EDU",
        "email": "CS.TORONTO.EDU;CS.HAIFA.AC.IL;INF.ETHZ.CH;CS.TORONTO.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "University of Toronto;University of Haifa;ETH Zurich",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.utoronto.ca;https://www.haifa.ac.il;https://www.ethz.ch",
        "aff_unique_abbr": "U of T;UoH;ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;2;0",
        "aff_country_unique": "Canada;Israel;Switzerland"
    },
    {
        "id": "f1315cf1b1",
        "title": "Gradient Hard Thresholding Pursuit for Sparsity-Constrained Optimization",
        "site": "https://proceedings.mlr.press/v32/yuan14.html",
        "author": "Xiaotong Yuan; Ping Li; Tong Zhang",
        "abstract": "Hard Thresholding Pursuit (HTP) is an iterative greedy selection procedure for finding sparse solutions of underdetermined linear systems. This method has been shown to have strong theoretical guarantees and impressive numerical performance. In this paper, we generalize HTP from compressed sensing to a generic problem setup of sparsity-constrained convex optimization. The proposed algorithm iterates between a standard gradient descent step and a hard truncation step with or without debiasing. We prove that our method enjoys the strong guarantees analogous to HTP in terms of rate of convergence and parameter estimation accuracy. Numerical evidences show that our method is superior to the state-of-the-art greedy selection methods when applied to learning tasks of sparse logistic regression and sparse support vector machines.",
        "bibtex": "@InProceedings{pmlr-v32-yuan14,\n  title = \t {Gradient Hard Thresholding Pursuit for Sparsity-Constrained Optimization},\n  author = \t {Yuan, Xiaotong and Li, Ping and Zhang, Tong},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {127--135},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/yuan14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/yuan14.html},\n  abstract = \t {Hard Thresholding Pursuit (HTP) is an iterative greedy selection procedure for finding sparse solutions of underdetermined linear systems. This method has been shown to have strong theoretical guarantees and impressive numerical performance. In this paper, we generalize HTP from compressed sensing to a generic problem setup of sparsity-constrained convex optimization. The proposed algorithm iterates between a standard gradient descent step and a hard truncation step with or without debiasing. We prove that our method enjoys the strong guarantees analogous to HTP in terms of rate of convergence and parameter estimation accuracy. Numerical evidences show that our method is superior to the state-of-the-art greedy selection methods when applied to learning tasks of sparse logistic regression and sparse support vector machines.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/yuan14.pdf",
        "supp": "",
        "pdf_size": 148148,
        "gs_citation": 148,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15433441727978082070&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Statistical Science, Cornell University, Ithaca, NY 14853, USA + Dept. of Statistics & Biostatistics, Dept. of Computer Science, Rutgers University, Piscataway, NJ 08854, USA; Dept. of Statistics & Biostatistics, Dept. of Computer Science, Rutgers University, Piscataway, NJ 08854, USA; Dept. of Statistics & Biostatistics, Rutgers University, Piscataway, NJ 08854, USA",
        "aff_domain": "gmail.com;stat.rutgers.edu;stat.rutgers.edu",
        "email": "gmail.com;stat.rutgers.edu;stat.rutgers.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;1",
        "aff_unique_norm": "Cornell University;Rutgers University",
        "aff_unique_dep": "Department of Statistical Science;Dept. of Statistics & Biostatistics",
        "aff_unique_url": "https://www.cornell.edu;https://www.rutgers.edu",
        "aff_unique_abbr": "Cornell;Rutgers",
        "aff_campus_unique_index": "0+1;1;1",
        "aff_campus_unique": "Ithaca;Piscataway",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "cd2fcf369f",
        "title": "Graph-based Semi-supervised Learning: Realizing Pointwise Smoothness Probabilistically",
        "site": "https://proceedings.mlr.press/v32/fang14.html",
        "author": "Yuan Fang; Kevin Chang; Hady Lauw",
        "abstract": "As the central notion in semi-supervised learning, smoothness is often realized on a graph representation of the data. In this paper, we study two complementary dimensions of smoothness: its pointwise nature and probabilistic modeling. While no existing graph-based work exploits them in conjunction, we encompass both in a novel framework of Probabilistic Graph-based Pointwise Smoothness (PGP), building upon two foundational models of data closeness and label coupling. This new form of smoothness axiomatizes a set of probability constraints, which ultimately enables class prediction. Theoretically, we provide an error and robustness analysis of PGP. Empirically, we conduct extensive experiments to show the advantages of PGP.",
        "bibtex": "@InProceedings{pmlr-v32-fang14,\n  title = \t {Graph-based Semi-supervised Learning: Realizing Pointwise Smoothness Probabilistically},\n  author = \t {Fang, Yuan and Chang, Kevin and Lauw, Hady},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {406--414},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/fang14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/fang14.html},\n  abstract = \t {As the central notion in semi-supervised learning, smoothness is often realized on a graph representation of the data. In this paper, we study two complementary dimensions of smoothness: its pointwise nature and probabilistic modeling. While no existing graph-based work exploits them in conjunction, we encompass both in a novel framework of Probabilistic Graph-based Pointwise Smoothness (PGP), building upon two foundational models of data closeness and label coupling. This new form of smoothness axiomatizes a set of probability constraints, which ultimately enables class prediction. Theoretically, we provide an error and robustness analysis of PGP. Empirically, we conduct extensive experiments to show the advantages of PGP.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/fang14.pdf",
        "supp": "",
        "pdf_size": 407640,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2522547994095312188&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "University of Illinois at Urbana-Champaign, USA + Advanced Digital Sciences Center, Singapore; University of Illinois at Urbana-Champaign, USA + Advanced Digital Sciences Center, Singapore; Singapore Management University, Singapore",
        "aff_domain": "illinois.edu;illinois.edu;smu.edu.sg",
        "email": "illinois.edu;illinois.edu;smu.edu.sg",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;2",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;Advanced Digital Sciences Center;Singapore Management University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://illinois.edu;;https://www.smu.edu.sg",
        "aff_unique_abbr": "UIUC;;SMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0+1;0+1;1",
        "aff_country_unique": "United States;Singapore"
    },
    {
        "id": "b623173c6e",
        "title": "Guess-Averse Loss Functions For Cost-Sensitive Multiclass Boosting",
        "site": "https://proceedings.mlr.press/v32/beijbom14.html",
        "author": "Oscar Beijbom; Mohammad Saberian; David Kriegman; Nuno Vasconcelos",
        "abstract": "Cost-sensitive multiclass classification has recently  acquired significance in several applications, through the introduction  of multiclass datasets with well-defined misclassification  costs. The design of classification algorithms for this  setting is considered. It is argued that the unreliable performance  of current algorithms is due to the inability of the underlying  loss functions to enforce a certain fundamental underlying property.   This property, denoted guess-aversion, is that  the loss should encourage correct classifications over the arbitrary guessing  that ensues when all classes are equally scored by the classifier.  While guess-aversion holds trivially for binary classification, this is not true in  the multiclass setting. A new family of cost-sensitive guess-averse   loss functions is derived, and used to design new cost-sensitive multiclass   boosting algorithms, denoted GEL- and GLL-MCBoost.  Extensive experiments demonstrate (1) the general importance of   guess-aversion and (2) that the GLL loss function outperforms other loss functions for multiclass boosting.",
        "bibtex": "@InProceedings{pmlr-v32-beijbom14,\n  title = \t {Guess-Averse Loss Functions For Cost-Sensitive Multiclass Boosting},\n  author = \t {Beijbom, Oscar and Saberian, Mohammad and Kriegman, David and Vasconcelos, Nuno},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {586--594},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/beijbom14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/beijbom14.html},\n  abstract = \t {Cost-sensitive multiclass classification has recently  acquired significance in several applications, through the introduction  of multiclass datasets with well-defined misclassification  costs. The design of classification algorithms for this  setting is considered. It is argued that the unreliable performance  of current algorithms is due to the inability of the underlying  loss functions to enforce a certain fundamental underlying property.   This property, denoted guess-aversion, is that  the loss should encourage correct classifications over the arbitrary guessing  that ensues when all classes are equally scored by the classifier.  While guess-aversion holds trivially for binary classification, this is not true in  the multiclass setting. A new family of cost-sensitive guess-averse   loss functions is derived, and used to design new cost-sensitive multiclass   boosting algorithms, denoted GEL- and GLL-MCBoost.  Extensive experiments demonstrate (1) the general importance of   guess-aversion and (2) that the GLL loss function outperforms other loss functions for multiclass boosting.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/beijbom14.pdf",
        "supp": "",
        "pdf_size": 639530,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9200662365017532552&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "University of California, San Diego; University of California, San Diego; University of California, San Diego; University of California, San Diego",
        "aff_domain": "UCSD.EDU;UCSD.EDU;UCSD.EDU;UCSD.EDU",
        "email": "UCSD.EDU;UCSD.EDU;UCSD.EDU;UCSD.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f2b3e4d4f9",
        "title": "Hamiltonian Monte Carlo Without Detailed Balance",
        "site": "https://proceedings.mlr.press/v32/sohl-dickstein14.html",
        "author": "Jascha Sohl-Dickstein; Mayur Mudigonda; Michael DeWeese",
        "abstract": "We present a method for performing Hamiltonian Monte Carlo that largely eliminates sample rejection.  In situations that would normally lead to rejection, instead a longer trajectory is computed until a new state is reached that can be accepted.  This is achieved using Markov chain transitions that satisfy the fixed point equation, but do not satisfy detailed balance.  The resulting algorithm significantly suppresses the random walk behavior and wasted function evaluations that are typically the consequence of update rejection.  We demonstrate a greater than factor of two improvement in mixing time on three test problems.  We release the source code as Python and MATLAB packages.",
        "bibtex": "@InProceedings{pmlr-v32-sohl-dickstein14,\n  title = \t {Hamiltonian Monte Carlo Without Detailed Balance},\n  author = \t {Sohl-Dickstein, Jascha and Mudigonda, Mayur and DeWeese, Michael},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {719--726},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/sohl-dickstein14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/sohl-dickstein14.html},\n  abstract = \t {We present a method for performing Hamiltonian Monte Carlo that largely eliminates sample rejection.  In situations that would normally lead to rejection, instead a longer trajectory is computed until a new state is reached that can be accepted.  This is achieved using Markov chain transitions that satisfy the fixed point equation, but do not satisfy detailed balance.  The resulting algorithm significantly suppresses the random walk behavior and wasted function evaluations that are typically the consequence of update rejection.  We demonstrate a greater than factor of two improvement in mixing time on three test problems.  We release the source code as Python and MATLAB packages.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/sohl-dickstein14.pdf",
        "supp": "",
        "pdf_size": 458891,
        "gs_citation": 89,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10023223626354294801&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Stanford University, Palo Alto + Khan Academy, Mountain View; Redwood Institute for Theoretical Neuroscience, University of California at Berkeley; Redwood Institute for Theoretical Neuroscience, University of California at Berkeley",
        "aff_domain": "STANFORD.EDU;BERKELEY.EDU;BERKELEY.EDU",
        "email": "STANFORD.EDU;BERKELEY.EDU;BERKELEY.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;2",
        "aff_unique_norm": "Stanford University;Khan Academy;University of California, Berkeley",
        "aff_unique_dep": ";;Redwood Institute for Theoretical Neuroscience",
        "aff_unique_url": "https://www.stanford.edu;https://www.khanacademy.org;https://www.berkeley.edu",
        "aff_unique_abbr": "Stanford;Khan Academy;UC Berkeley",
        "aff_campus_unique_index": "0+1;2;2",
        "aff_campus_unique": "Palo Alto;Mountain View;Berkeley",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1f0a9a7ebb",
        "title": "Hard-Margin Active Linear Regression",
        "site": "https://proceedings.mlr.press/v32/hazan14.html",
        "author": "Elad Hazan; Zohar Karnin",
        "abstract": "We consider the fundamental problem of linear regression in which the designer can actively choose observations.   This model naturally captures various experiment design settings in medical experiments, ad placement problems and more. Whereas previous literature addresses the soft-margin or mean-square-error variants of the problem, we consider a natural machine learning hard-margin criterion. In this setting, we show that active learning admits significantly better sample complexity bounds than the passive learning counterpart, and give  efficient algorithms that attain near-optimal  bounds.",
        "bibtex": "@InProceedings{pmlr-v32-hazan14,\n  title = \t {Hard-Margin Active Linear Regression},\n  author = \t {Hazan, Elad and Karnin, Zohar},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {883--891},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/hazan14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/hazan14.html},\n  abstract = \t {We consider the fundamental problem of linear regression in which the designer can actively choose observations.   This model naturally captures various experiment design settings in medical experiments, ad placement problems and more. Whereas previous literature addresses the soft-margin or mean-square-error variants of the problem, we consider a natural machine learning hard-margin criterion. In this setting, we show that active learning admits significantly better sample complexity bounds than the passive learning counterpart, and give  efficient algorithms that attain near-optimal  bounds.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/hazan14.pdf",
        "supp": "",
        "pdf_size": 320729,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4199931225352957350&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Technion, Haifa, Israel; Yahoo Labs, Haifa, Israel",
        "aff_domain": "ie.technion.ac.il;yahoo.com",
        "email": "ie.technion.ac.il;yahoo.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Technion - Israel Institute of Technology;Yahoo Labs",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.technion.ac.il/en/;https://labs.yahoo.com",
        "aff_unique_abbr": "Technion;Yahoo Labs",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Haifa",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "4a9ad7718a",
        "title": "Heavy-tailed regression with a generalized median-of-means",
        "site": "https://proceedings.mlr.press/v32/hsu14.html",
        "author": "Daniel Hsu; Sivan Sabato",
        "abstract": "This work proposes a simple and computationally efficient estimator for  linear regression, and other smooth and strongly convex loss minimization  problems.  We prove loss approximation guarantees that hold for general distributions,  including those with heavy tails. All prior results only hold for estimators which  either assume bounded or subgaussian distributions,  require prior knowledge of distributional properties, or are not known to be computationally tractable.  In the special case of linear regression with possibly heavy-tailed responses and with bounded and well-conditioned covariates in d-dimensions, we show that a random sample of size  \\tildeO(d\\log(1/\u03b4)) suffices to obtain a constant factor  approximation to the optimal loss with probability 1-\u03b4, a minimax optimal sample complexity up to log factors.  The core technique used in the proposed estimator is a new generalization of  the median-of-means estimator to arbitrary metric spaces.",
        "bibtex": "@InProceedings{pmlr-v32-hsu14,\n  title = \t {Heavy-tailed regression with a generalized median-of-means},\n  author = \t {Hsu, Daniel and Sabato, Sivan},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {37--45},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/hsu14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/hsu14.html},\n  abstract = \t {This work proposes a simple and computationally efficient estimator for  linear regression, and other smooth and strongly convex loss minimization  problems.  We prove loss approximation guarantees that hold for general distributions,  including those with heavy tails. All prior results only hold for estimators which  either assume bounded or subgaussian distributions,  require prior knowledge of distributional properties, or are not known to be computationally tractable.  In the special case of linear regression with possibly heavy-tailed responses and with bounded and well-conditioned covariates in d-dimensions, we show that a random sample of size  \\tildeO(d\\log(1/\u03b4)) suffices to obtain a constant factor  approximation to the optimal loss with probability 1-\u03b4, a minimax optimal sample complexity up to log factors.  The core technique used in the proposed estimator is a new generalization of  the median-of-means estimator to arbitrary metric spaces.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/hsu14.pdf",
        "supp": "",
        "pdf_size": 336630,
        "gs_citation": 66,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3711555013175005226&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, Columbia University; Microsoft Research New England, 1 Memorial Drive, Cambridge, MA 02446",
        "aff_domain": "CS.COLUMBIA.EDU;MICROSOFT.COM",
        "email": "CS.COLUMBIA.EDU;MICROSOFT.COM",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Columbia University;Microsoft",
        "aff_unique_dep": "Department of Computer Science;Microsoft Research New England",
        "aff_unique_url": "https://www.columbia.edu;https://www.microsoft.com/en-us/research/group/new-england",
        "aff_unique_abbr": "Columbia;MSR NE",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "65c20eebd2",
        "title": "Hierarchical Conditional Random Fields for Outlier Detection: An Application to Detecting Epileptogenic Cortical Malformations",
        "site": "https://proceedings.mlr.press/v32/ahmed14.html",
        "author": "Bilal Ahmed; Thomas Thesen; Karen Blackmon; Yijun Zhao; Orrin Devinsky; Ruben Kuzniecky; Carla Brodley",
        "abstract": "We cast the problem of detecting and isolating regions of abnormal cortical tissue in the MRIs of epilepsy patients in an image segmentation framework. Employing a multiscale approach we divide the surface images into segments of different sizes and then classify each segment as being an outlier, by comparing it to the same region across controls. The final classification is obtained by fusing the outlier probabilities obtained at multiple scales using a tree-structured hierarchical conditional random field (HCRF). The proposed method correctly detects abnormal regions in 90% of patients whose abnormality was detected via routine visual inspection of their clinical MRI. More importantly, it detects abnormalities in 80% of patients whose abnormality escaped visual inspection by expert radiologists.",
        "bibtex": "@InProceedings{pmlr-v32-ahmed14,\n  title = \t {Hierarchical Conditional Random Fields for Outlier Detection: An Application to Detecting Epileptogenic Cortical Malformations},\n  author = \t {Ahmed, Bilal and Thesen, Thomas and Blackmon, Karen and Zhao, Yijun and Devinsky, Orrin and Kuzniecky, Ruben and Brodley, Carla},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1080--1088},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/ahmed14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/ahmed14.html},\n  abstract = \t {We cast the problem of detecting and isolating regions of abnormal cortical tissue in the MRIs of epilepsy patients in an image segmentation framework. Employing a multiscale approach we divide the surface images into segments of different sizes and then classify each segment as being an outlier, by comparing it to the same region across controls. The final classification is obtained by fusing the outlier probabilities obtained at multiple scales using a tree-structured hierarchical conditional random field (HCRF). The proposed method correctly detects abnormal regions in 90% of patients whose abnormality was detected via routine visual inspection of their clinical MRI. More importantly, it detects abnormalities in 80% of patients whose abnormality escaped visual inspection by expert radiologists.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/ahmed14.pdf",
        "supp": "",
        "pdf_size": 4881143,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=735246921011575886&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, Tufts University, Medford, Massachusetts, USA; Comprehensive Epilepsy Center, Department of Neurology, School of Medicine, New York University, New York, USA; Comprehensive Epilepsy Center, Department of Neurology, School of Medicine, New York University, New York, USA; Department of Computer Science, Tufts University, Medford, Massachusetts, USA; Comprehensive Epilepsy Center, Department of Neurology, School of Medicine, New York University, New York, USA; Comprehensive Epilepsy Center, Department of Neurology, School of Medicine, New York University, New York, USA; Department of Computer Science, Tufts University, Medford, Massachusetts, USA",
        "aff_domain": "CS.TUFTS.EDU;MED.NYU.EDU;NYUMC.EDU;CS.TUFTS.EDU;MED.NYU.EDU;NYUMC.EDU;CS.TUFTS.EDU",
        "email": "CS.TUFTS.EDU;MED.NYU.EDU;NYUMC.EDU;CS.TUFTS.EDU;MED.NYU.EDU;NYUMC.EDU;CS.TUFTS.EDU",
        "github": "",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0;1;1;0",
        "aff_unique_norm": "Tufts University;New York University",
        "aff_unique_dep": "Department of Computer Science;Department of Neurology",
        "aff_unique_url": "https://www.tufts.edu;https://www.nyu.edu",
        "aff_unique_abbr": "Tufts;NYU",
        "aff_campus_unique_index": "0;1;1;0;1;1;0",
        "aff_campus_unique": "Medford;New York",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "864f64da5b",
        "title": "Hierarchical Dirichlet Scaling Process",
        "site": "https://proceedings.mlr.press/v32/kim14.html",
        "author": "Dongwoo Kim; Alice Oh",
        "abstract": "We present the hierarchical Dirichlet scaling process (HDSP), a Bayesian nonparametric mixed membership model for multi-labeled data. We construct the HDSP based on the gamma representation of the hierarchical Dirichlet process (HDP) which allows scaling the mixture components. With such construction, HDSP allocates a latent location to each label and mixture component in a space, and uses the distance between them to guide membership probabilities. We develop a variational Bayes algorithm for the approximate posterior inference of the HDSP. Through experiments on synthetic datasets as well as datasets of newswire, medical journal articles, and Wikipedia, we show that the HDSP results in better predictive performance than HDP, labeled LDA and partially labeled LDA.",
        "bibtex": "@InProceedings{pmlr-v32-kim14,\n  title = \t {Hierarchical Dirichlet Scaling Process},\n  author = \t {Kim, Dongwoo and Oh, Alice},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {973--981},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/kim14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/kim14.html},\n  abstract = \t {We present the hierarchical Dirichlet scaling process (HDSP), a Bayesian nonparametric mixed membership model for multi-labeled data. We construct the HDSP based on the gamma representation of the hierarchical Dirichlet process (HDP) which allows scaling the mixture components. With such construction, HDSP allocates a latent location to each label and mixture component in a space, and uses the distance between them to guide membership probabilities. We develop a variational Bayes algorithm for the approximate posterior inference of the HDSP. Through experiments on synthetic datasets as well as datasets of newswire, medical journal articles, and Wikipedia, we show that the HDSP results in better predictive performance than HDP, labeled LDA and partially labeled LDA.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/kim14.pdf",
        "supp": "",
        "pdf_size": 543287,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11359023474643477848&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 25,
        "aff": "KAIST, Daejeon, Korea; KAIST, Daejeon, Korea",
        "aff_domain": "KAIST.AC.KR;KAIST.EDU",
        "email": "KAIST.AC.KR;KAIST.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.kaist.ac.kr",
        "aff_unique_abbr": "KAIST",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Daejeon",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "1433423739",
        "title": "Hierarchical Quasi-Clustering Methods for Asymmetric Networks",
        "site": "https://proceedings.mlr.press/v32/carlsson14.html",
        "author": "Gunnar Carlsson; Facundo M\u00e9moli; Alejandro Ribeiro; Santiago Segarra",
        "abstract": "This paper introduces hierarchical quasi-clustering methods, a generalization of hierarchical clustering for asymmetric networks where the output structure preserves the asymmetry of the input data. We show that this output structure is equivalent to a finite quasi-ultrametric space and study admissibility with respect to two desirable properties. We prove that a modified version of single linkage is the only admissible quasi-clustering method. Moreover, we show stability of the proposed method and we establish invariance properties fulfilled by it. Algorithms are further developed and the value of quasi-clustering analysis is illustrated with a study of internal migration within United States.",
        "bibtex": "@InProceedings{pmlr-v32-carlsson14,\n  title = \t {Hierarchical Quasi-Clustering Methods for Asymmetric Networks},\n  author = \t {Carlsson, Gunnar and M\u00e9moli, Facundo and Ribeiro, Alejandro and Segarra, Santiago},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {352--360},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/carlsson14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/carlsson14.html},\n  abstract = \t {This paper introduces hierarchical quasi-clustering methods, a generalization of hierarchical clustering for asymmetric networks where the output structure preserves the asymmetry of the input data. We show that this output structure is equivalent to a finite quasi-ultrametric space and study admissibility with respect to two desirable properties. We prove that a modified version of single linkage is the only admissible quasi-clustering method. Moreover, we show stability of the proposed method and we establish invariance properties fulfilled by it. Algorithms are further developed and the value of quasi-clustering analysis is illustrated with a study of internal migration within United States.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/carlsson14.pdf",
        "supp": "",
        "pdf_size": 1949216,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7368465563140210932&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Mathematics, Stanford University; Department of Mathematics and Department of Computer Science and Engineering, Ohio State University; Department of Electrical and Systems Engineering, University of Pennsylvania; Department of Electrical and Systems Engineering, University of Pennsylvania",
        "aff_domain": "MATH.STANFORD.EDU;MATH.OSU.EDU;SEAS.UPENN.EDU;SEAS.UPENN.EDU",
        "email": "MATH.STANFORD.EDU;MATH.OSU.EDU;SEAS.UPENN.EDU;SEAS.UPENN.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;2",
        "aff_unique_norm": "Stanford University;Ohio State University;University of Pennsylvania",
        "aff_unique_dep": "Department of Mathematics;Department of Mathematics;Department of Electrical and Systems Engineering",
        "aff_unique_url": "https://www.stanford.edu;https://www.osu.edu;https://www.upenn.edu",
        "aff_unique_abbr": "Stanford;OSU;UPenn",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "945347bad9",
        "title": "High Order Regularization for Semi-Supervised Learning of Structured Output Problems",
        "site": "https://proceedings.mlr.press/v32/lif14.html",
        "author": "Yujia Li; Rich Zemel",
        "abstract": "Semi-supervised learning, which uses unlabeled data to help learn a discriminative model, is especially important for structured output problems, as considerably more effort is needed to label its multidimensional outputs versus standard single output problems. We propose a new max-margin framework for semi-supervised structured output learning, that allows the use of powerful discrete optimization algorithms and high order regularizers defined directly on model predictions for the unlabeled examples. We show that our framework is closely related to Posterior Regularization, and the two frameworks optimize special cases of the same objective. The new framework is instantiated on two image segmentation tasks, using both a graph regularizer and a cardinality regularizer. Experiments also demonstrate that this framework can utilize unlabeled data from a different source than the labeled data to significantly improve performance while saving labeling effort.",
        "bibtex": "@InProceedings{pmlr-v32-lif14,\n  title = \t {High Order Regularization for Semi-Supervised Learning of Structured Output Problems},\n  author = \t {Li, Yujia and Zemel, Rich},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1368--1376},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/lif14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/lif14.html},\n  abstract = \t {Semi-supervised learning, which uses unlabeled data to help learn a discriminative model, is especially important for structured output problems, as considerably more effort is needed to label its multidimensional outputs versus standard single output problems. We propose a new max-margin framework for semi-supervised structured output learning, that allows the use of powerful discrete optimization algorithms and high order regularizers defined directly on model predictions for the unlabeled examples. We show that our framework is closely related to Posterior Regularization, and the two frameworks optimize special cases of the same objective. The new framework is instantiated on two image segmentation tasks, using both a graph regularizer and a cardinality regularizer. Experiments also demonstrate that this framework can utilize unlabeled data from a different source than the labeled data to significantly improve performance while saving labeling effort.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/lif14.pdf",
        "supp": "",
        "pdf_size": 759615,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Department of Computer Science, University of Toronto, Toronto, ON, Canada + Canadian Institute for Advanced Research, Toronto, ON, Canada; Department of Computer Science, University of Toronto, Toronto, ON, Canada + Canadian Institute for Advanced Research, Toronto, ON, Canada",
        "aff_domain": "CS.TORONTO.EDU;CS.TORONTO.EDU",
        "email": "CS.TORONTO.EDU;CS.TORONTO.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "University of Toronto;Canadian Institute for Advanced Research",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.utoronto.ca;https://www.cifar.ca",
        "aff_unique_abbr": "U of T;CIFAR",
        "aff_campus_unique_index": "0+0;0+0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "ff33b0602b",
        "title": "Improving offline evaluation of contextual bandit algorithms via bootstrapping techniques",
        "site": "https://proceedings.mlr.press/v32/mary14.html",
        "author": "J\u00e9r\u00e9mie Mary; Philippe Preux; Olivier Nicol",
        "abstract": "In many recommendation applications such as news recommendation, the  items that can be recommended come and go at a very fast pace.  This  is a challenge for recommender systems (RS) to face this setting.  Online learning algorithms seem to be the most straight forward  solution. The contextual bandit framework was introduced for that very  purpose. In general the evaluation of a RS is a critical issue. Live  evaluation is often avoided due to the potential loss of revenue,  hence the need for offline evaluation methods. Two options are  available. Model based methods are biased by nature and are thus  difficult to trust when used alone. Data driven methods are therefore  what we consider here. Evaluating online learning algorithms with past  data is not simple but some methods exist in the  literature. Nonetheless their accuracy is not satisfactory mainly due  to their mechanism of data rejection that only allow the exploitation  of a small fraction of the data. We precisely address this issue in  this paper. After highlighting the limitations of the previous  methods, we present a new method, based on bootstrapping  techniques. This new method comes with two important improvements: it  is much more accurate and it provides a measure of quality of its  estimation. The latter is a highly desirable property in order to  minimize the risks entailed by putting online a RS for the first  time. We provide both theoretical and experimental proofs of its  superiority compared to state-of-the-art methods, as well as an  analysis of the convergence of the measure of quality.",
        "bibtex": "@InProceedings{pmlr-v32-mary14,\n  title = \t {Improving offline evaluation of contextual bandit algorithms via bootstrapping techniques},\n  author = \t {Mary, J\u00e9r\u00e9mie and Preux, Philippe and Nicol, Olivier},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {172--180},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/mary14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/mary14.html},\n  abstract = \t {In many recommendation applications such as news recommendation, the  items that can be recommended come and go at a very fast pace.  This  is a challenge for recommender systems (RS) to face this setting.  Online learning algorithms seem to be the most straight forward  solution. The contextual bandit framework was introduced for that very  purpose. In general the evaluation of a RS is a critical issue. Live  evaluation is often avoided due to the potential loss of revenue,  hence the need for offline evaluation methods. Two options are  available. Model based methods are biased by nature and are thus  difficult to trust when used alone. Data driven methods are therefore  what we consider here. Evaluating online learning algorithms with past  data is not simple but some methods exist in the  literature. Nonetheless their accuracy is not satisfactory mainly due  to their mechanism of data rejection that only allow the exploitation  of a small fraction of the data. We precisely address this issue in  this paper. After highlighting the limitations of the previous  methods, we present a new method, based on bootstrapping  techniques. This new method comes with two important improvements: it  is much more accurate and it provides a measure of quality of its  estimation. The latter is a highly desirable property in order to  minimize the risks entailed by putting online a RS for the first  time. We provide both theoretical and experimental proofs of its  superiority compared to state-of-the-art methods, as well as an  analysis of the convergence of the measure of quality.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/mary14.pdf",
        "supp": "",
        "pdf_size": 423974,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13858178339668928227&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "University of Lille / LIFL (CNRS) & INRIA Lille Nord Europe; University of Lille / LIFL (CNRS) & INRIA Lille Nord Europe; University of Lille / LIFL (CNRS) & INRIA Lille Nord Europe",
        "aff_domain": "GMAIL.COM;INRIA.FR;UNIV-LILLE3.FR",
        "email": "GMAIL.COM;INRIA.FR;UNIV-LILLE3.FR",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Lille",
        "aff_unique_dep": "Lille Nord Europe",
        "aff_unique_url": "https://www.univ-lille.fr",
        "aff_unique_abbr": "ULille",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "d6998a4571",
        "title": "Inferning with High Girth Graphical Models",
        "site": "https://proceedings.mlr.press/v32/heinemann14.html",
        "author": "Uri Heinemann; Amir Globerson",
        "abstract": "Unsupervised learning of graphical models is an important task in many domains. Although maximum likelihood learning is computationally hard, there do exist consistent learning algorithms (e.g., psuedo-likelihood and its variants). However, inference in the learned models is still hard, and thus they are not directly usable. In other words, given a probabilistic query they are not guaranteed to provide an answer that is close to the true one.   In the current paper, we provide a learning algorithm that is guaranteed to provide approximately correct probabilistic inference. We focus on a particular class of models, namely high girth graphs in the correlation decay regime. It is well known that approximate inference (e.g, using loopy BP) in such models yields marginals that are close to the true ones. Motivated by this, we propose an algorithm that always returns models of this type, and hence in the models it returns inference is approximately correct. We derive finite sample results guaranteeing that beyond a certain sample size, the resulting models will answer probabilistic queries with a high level of accuracy.   Results on synthetic data show that the models we learn indeed outperform those obtained by other algorithms, which do not return high girth graphs.",
        "bibtex": "@InProceedings{pmlr-v32-heinemann14,\n  title = \t {Inferning with High Girth Graphical Models},\n  author = \t {Heinemann, Uri and Globerson, Amir},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1260--1268},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/heinemann14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/heinemann14.html},\n  abstract = \t {Unsupervised learning of graphical models is an important task in many domains. Although maximum likelihood learning is computationally hard, there do exist consistent learning algorithms (e.g., psuedo-likelihood and its variants). However, inference in the learned models is still hard, and thus they are not directly usable. In other words, given a probabilistic query they are not guaranteed to provide an answer that is close to the true one.   In the current paper, we provide a learning algorithm that is guaranteed to provide approximately correct probabilistic inference. We focus on a particular class of models, namely high girth graphs in the correlation decay regime. It is well known that approximate inference (e.g, using loopy BP) in such models yields marginals that are close to the true ones. Motivated by this, we propose an algorithm that always returns models of this type, and hence in the models it returns inference is approximately correct. We derive finite sample results guaranteeing that beyond a certain sample size, the resulting models will answer probabilistic queries with a high level of accuracy.   Results on synthetic data show that the models we learn indeed outperform those obtained by other algorithms, which do not return high girth graphs.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/heinemann14.pdf",
        "supp": "",
        "pdf_size": 335390,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=727017765101600635&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "The Hebrew University of Jerusalem, Jerusalem, Israel; The Hebrew University of Jerusalem, Jerusalem, Israel",
        "aff_domain": "cs.huji.ac.il;cs.huji.ac.il",
        "email": "cs.huji.ac.il;cs.huji.ac.il",
        "github": "",
        "project": "http://inferning.cs.umass.edu/2012",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hebrew University of Jerusalem",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.huji.ac.il",
        "aff_unique_abbr": "HUJI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Jerusalem",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "909ca0f2ef",
        "title": "Influence Function Learning in Information Diffusion Networks",
        "site": "https://proceedings.mlr.press/v32/du14.html",
        "author": "Nan Du; Yingyu Liang; Maria Balcan; Le Song",
        "abstract": "Can we learn the influence of a set of people in a social network from cascades of information diffusion? This question is often addressed by a two-stage approach: first learn a diffusion model, and then calculate the influence based on the learned model. Thus, the success of this approach relies heavily on the correctness of the diffusion model which is hard to verify for real world data. In this paper, we exploit the insight that the influence functions in many diffusion models are coverage functions, and propose a novel parameterization of such functions using a convex combination of random basis functions. Moreover, we propose an efficient maximum likelihood based algorithm to learn such functions directly from cascade data, and hence bypass the need to specify a particular diffusion model in advance. We provide both theoretical and empirical analysis for our approach, showing that the proposed approach can provably learn the influence function with low sample complexity, be robust to the unknown diffusion models, and significantly outperform existing approaches in both synthetic and real world data.",
        "bibtex": "@InProceedings{pmlr-v32-du14,\n  title = \t {Influence Function Learning in Information Diffusion Networks},\n  author = \t {Du, Nan and Liang, Yingyu and Balcan, Maria and Song, Le},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {2016--2024},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/du14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/du14.html},\n  abstract = \t {Can we learn the influence of a set of people in a social network from cascades of information diffusion? This question is often addressed by a two-stage approach: first learn a diffusion model, and then calculate the influence based on the learned model. Thus, the success of this approach relies heavily on the correctness of the diffusion model which is hard to verify for real world data. In this paper, we exploit the insight that the influence functions in many diffusion models are coverage functions, and propose a novel parameterization of such functions using a convex combination of random basis functions. Moreover, we propose an efficient maximum likelihood based algorithm to learn such functions directly from cascade data, and hence bypass the need to specify a particular diffusion model in advance. We provide both theoretical and empirical analysis for our approach, showing that the proposed approach can provably learn the influence function with low sample complexity, be robust to the unknown diffusion models, and significantly outperform existing approaches in both synthetic and real world data.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/du14.pdf",
        "supp": "",
        "pdf_size": 403925,
        "gs_citation": 117,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1843549875651765184&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 21,
        "aff": "College of Computing, Georgia Institute of Technology; College of Computing, Georgia Institute of Technology; College of Computing, Georgia Institute of Technology; College of Computing, Georgia Institute of Technology",
        "aff_domain": "cc.gatech.edu;cc.gatech.edu;cc.gatech.edu;cc.gatech.edu",
        "email": "cc.gatech.edu;cc.gatech.edu;cc.gatech.edu;cc.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Georgia Institute of Technology",
        "aff_unique_dep": "College of Computing",
        "aff_unique_url": "https://www.gatech.edu",
        "aff_unique_abbr": "Georgia Tech",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Atlanta",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9eefd0dc1d",
        "title": "Input Warping for Bayesian Optimization of Non-Stationary Functions",
        "site": "https://proceedings.mlr.press/v32/snoek14.html",
        "author": "Jasper Snoek; Kevin Swersky; Rich Zemel; Ryan Adams",
        "abstract": "Bayesian optimization has proven to be a highly effective methodology for the global optimization of unknown, expensive and multimodal functions.  The ability to accurately model distributions over functions is critical to the effectiveness of Bayesian optimization.  Although Gaussian processes provide a flexible prior over functions, there are various classes of functions that remain difficult to model.  One of the most frequently occurring of these is the class of non-stationary functions.  The optimization of the hyperparameters of machine learning algorithms is a problem domain in which parameters are often manually transformed a priori, for example by optimizing in \"log-space\", to mitigate the effects of spatially-varying length scale.  We develop a methodology for automatically learning a wide family of bijective transformations or warpings of the input space using the Beta cumulative distribution function.  We further extend the warping framework to multi-task Bayesian optimization so that multiple tasks can be warped into a jointly stationary space. On a set of challenging benchmark optimization tasks, we observe that the inclusion of warping greatly improves on the state-of-the-art, producing better results faster and more reliably.",
        "bibtex": "@InProceedings{pmlr-v32-snoek14,\n  title = \t {Input Warping for Bayesian Optimization of Non-Stationary Functions},\n  author = \t {Snoek, Jasper and Swersky, Kevin and Zemel, Rich and Adams, Ryan},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1674--1682},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/snoek14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/snoek14.html},\n  abstract = \t {Bayesian optimization has proven to be a highly effective methodology for the global optimization of unknown, expensive and multimodal functions.  The ability to accurately model distributions over functions is critical to the effectiveness of Bayesian optimization.  Although Gaussian processes provide a flexible prior over functions, there are various classes of functions that remain difficult to model.  One of the most frequently occurring of these is the class of non-stationary functions.  The optimization of the hyperparameters of machine learning algorithms is a problem domain in which parameters are often manually transformed a priori, for example by optimizing in \"log-space\", to mitigate the effects of spatially-varying length scale.  We develop a methodology for automatically learning a wide family of bijective transformations or warpings of the input space using the Beta cumulative distribution function.  We further extend the warping framework to multi-task Bayesian optimization so that multiple tasks can be warped into a jointly stationary space. On a set of challenging benchmark optimization tasks, we observe that the inclusion of warping greatly improves on the state-of-the-art, producing better results faster and more reliably.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/snoek14.pdf",
        "supp": "",
        "pdf_size": 1283114,
        "gs_citation": 315,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1474318809555405205&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff": "School of Engineering and Applied Sciences, Harvard University; Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto + Canadian Institute for Advanced Research; School of Engineering and Applied Sciences, Harvard University",
        "aff_domain": "SEAS.HARVARD.EDU;CS.TORONTO.EDU;CS.TORONTO.EDU;SEAS.HARVARD.EDU",
        "email": "SEAS.HARVARD.EDU;CS.TORONTO.EDU;CS.TORONTO.EDU;SEAS.HARVARD.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1+2;0",
        "aff_unique_norm": "Harvard University;University of Toronto;Canadian Institute for Advanced Research",
        "aff_unique_dep": "School of Engineering and Applied Sciences;Department of Computer Science;",
        "aff_unique_url": "https://www.harvard.edu;https://www.utoronto.ca;https://www.cifar.ca",
        "aff_unique_abbr": "Harvard;U of T;CIFAR",
        "aff_campus_unique_index": "0;1;1;0",
        "aff_campus_unique": "Cambridge;Toronto;",
        "aff_country_unique_index": "0;1;1+1;0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "141be0ff60",
        "title": "Joint Inference of Multiple Label Types in Large Networks",
        "site": "https://proceedings.mlr.press/v32/chakrabarti14.html",
        "author": "Deepayan Chakrabarti; Stanislav Funiak; Jonathan Chang; Sofus Macskassy",
        "abstract": "We tackle the problem of inferring node labels in a partially labeled  graph where each node in the graph has multiple label types and  each label type has a large number of possible labels.  Our primary  example, and the focus of this paper, is the joint inference of label  types such as hometown, current city, and employers, for users  connected by a social network.  Standard label propagation fails to  consider the properties of the label types and the interactions  between them.  Our proposed method, called EdgeExplain, explicitly  models these, while still enabling scalable inference under a  distributed message-passing architecture.  On a billion-node subset of the Facebook social network,  EdgeExplain significantly outperforms label propagation for several  label types, with lifts of up to 120% for recall@1 and 60% for  recall@3.",
        "bibtex": "@InProceedings{pmlr-v32-chakrabarti14,\n  title = \t {Joint Inference of Multiple Label Types in Large Networks},\n  author = \t {Chakrabarti, Deepayan and Funiak, Stanislav and Chang, Jonathan and Macskassy, Sofus},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {874--882},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/chakrabarti14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/chakrabarti14.html},\n  abstract = \t {We tackle the problem of inferring node labels in a partially labeled  graph where each node in the graph has multiple label types and  each label type has a large number of possible labels.  Our primary  example, and the focus of this paper, is the joint inference of label  types such as hometown, current city, and employers, for users  connected by a social network.  Standard label propagation fails to  consider the properties of the label types and the interactions  between them.  Our proposed method, called EdgeExplain, explicitly  models these, while still enabling scalable inference under a  distributed message-passing architecture.  On a billion-node subset of the Facebook social network,  EdgeExplain significantly outperforms label propagation for several  label types, with lifts of up to 120% for recall@1 and 60% for  recall@3.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/chakrabarti14.pdf",
        "supp": "",
        "pdf_size": 321611,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12838978985001817875&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Facebook Inc.; Facebook Inc.; Facebook Inc.; Facebook Inc.",
        "aff_domain": "fb.com;fb.com;fb.com;fb.com",
        "email": "fb.com;fb.com;fb.com;fb.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Meta",
        "aff_unique_dep": "Facebook",
        "aff_unique_url": "https://www.facebook.com",
        "aff_unique_abbr": "FB",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3107e16277",
        "title": "K-means recovers ICA filters when independent components are sparse",
        "site": "https://proceedings.mlr.press/v32/vinnikov14.html",
        "author": "Alon Vinnikov; Shai Shalev-Shwartz",
        "abstract": "Unsupervised feature learning is the task of using unlabeled examples  for building a representation of objects as vectors. This task has  been extensively studied in recent years, mainly in the context of  unsupervised pre-training of neural networks. Recently, (Coates et al., 2011)  conducted extensive experiments, comparing the accuracy of a linear  classifier that has been trained using features learnt by several  unsupervised feature learning methods.  Surprisingly, the best  performing method was the simplest feature learning approach that was  based on applying the K-means clustering algorithm after a whitening  of the data. The goal of this work is to shed light on the success of  K-means with whitening for the task of unsupervised feature learning.  Our main result is a close connection between K-means and ICA  (Independent Component Analysis).  Specifically, we show that K-means  and similar clustering algorithms can be used to recover the ICA  mixing matrix or its inverse, the ICA filters. It is well known that  the independent components found by ICA form useful features for  classification (Le et al., 2012; 2011; 2010), hence the connection between K-mean and ICA explains  the empirical success of K-means as a feature learner. Moreover, our  analysis underscores the significance of the whitening operation, as was also  observed in the experiments reported in (Coates et al., 2011).  Finally, our  analysis leads to a better initialization of K-means for the task of feature learning.",
        "bibtex": "@InProceedings{pmlr-v32-vinnikov14,\n  title = \t {K-means recovers ICA filters when independent components are sparse},\n  author = \t {Vinnikov, Alon and Shalev-Shwartz, Shai},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {712--720},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/vinnikov14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/vinnikov14.html},\n  abstract = \t {Unsupervised feature learning is the task of using unlabeled examples  for building a representation of objects as vectors. This task has  been extensively studied in recent years, mainly in the context of  unsupervised pre-training of neural networks. Recently, (Coates et al., 2011)  conducted extensive experiments, comparing the accuracy of a linear  classifier that has been trained using features learnt by several  unsupervised feature learning methods.  Surprisingly, the best  performing method was the simplest feature learning approach that was  based on applying the K-means clustering algorithm after a whitening  of the data. The goal of this work is to shed light on the success of  K-means with whitening for the task of unsupervised feature learning.  Our main result is a close connection between K-means and ICA  (Independent Component Analysis).  Specifically, we show that K-means  and similar clustering algorithms can be used to recover the ICA  mixing matrix or its inverse, the ICA filters. It is well known that  the independent components found by ICA form useful features for  classification (Le et al., 2012; 2011; 2010), hence the connection between K-mean and ICA explains  the empirical success of K-means as a feature learner. Moreover, our  analysis underscores the significance of the whitening operation, as was also  observed in the experiments reported in (Coates et al., 2011).  Finally, our  analysis leads to a better initialization of K-means for the task of feature learning.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/vinnikov14.pdf",
        "supp": "",
        "pdf_size": 613285,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17186635709872880568&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "School of Computer Science and Engineering, The Hebrew University of Jerusalem, ISRAEL; School of Computer Science and Engineering, The Hebrew University of Jerusalem, ISRAEL",
        "aff_domain": "MAIL.HUJI.AC.IL;CS.HUJI.AC.IL",
        "email": "MAIL.HUJI.AC.IL;CS.HUJI.AC.IL",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hebrew University of Jerusalem",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "http://www.huji.ac.il",
        "aff_unique_abbr": "HUJI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Jerusalem",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "9d5352f969",
        "title": "Kernel Adaptive Metropolis-Hastings",
        "site": "https://proceedings.mlr.press/v32/sejdinovic14.html",
        "author": "Dino Sejdinovic; Heiko Strathmann; Maria Lomeli Garcia; Christophe Andrieu; Arthur Gretton",
        "abstract": "A Kernel Adaptive Metropolis-Hastings algorithm is introduced, for the purpose of sampling from a target distribution with strongly nonlinear support. The algorithm embeds the trajectory of the Markov chain into a reproducing kernel Hilbert space (RKHS), such that the feature space covariance of the samples informs the choice of proposal. The procedure is computationally efficient and straightforward to implement, since the RKHS moves can be integrated out analytically: our proposal distribution in the original space is a normal distribution whose mean and covariance depend on where the current sample lies in the support of the target distribution, and adapts to its local covariance structure. Furthermore, the procedure requires neither gradients nor any other higher order information about the target, making it particularly attractive for contexts such as Pseudo-Marginal MCMC. Kernel Adaptive Metropolis-Hastings outperforms competing fixed and adaptive samplers on multivariate, highly nonlinear target distributions, arising in both real-world and synthetic examples.",
        "bibtex": "@InProceedings{pmlr-v32-sejdinovic14,\n  title = \t {Kernel Adaptive Metropolis-Hastings},\n  author = \t {Sejdinovic, Dino and Strathmann, Heiko and Garcia, Maria Lomeli and Andrieu, Christophe and Gretton, Arthur},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1665--1673},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/sejdinovic14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/sejdinovic14.html},\n  abstract = \t {A Kernel Adaptive Metropolis-Hastings algorithm is introduced, for the purpose of sampling from a target distribution with strongly nonlinear support. The algorithm embeds the trajectory of the Markov chain into a reproducing kernel Hilbert space (RKHS), such that the feature space covariance of the samples informs the choice of proposal. The procedure is computationally efficient and straightforward to implement, since the RKHS moves can be integrated out analytically: our proposal distribution in the original space is a normal distribution whose mean and covariance depend on where the current sample lies in the support of the target distribution, and adapts to its local covariance structure. Furthermore, the procedure requires neither gradients nor any other higher order information about the target, making it particularly attractive for contexts such as Pseudo-Marginal MCMC. Kernel Adaptive Metropolis-Hastings outperforms competing fixed and adaptive samplers on multivariate, highly nonlinear target distributions, arising in both real-world and synthetic examples.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/sejdinovic14.pdf",
        "supp": "",
        "pdf_size": 876599,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=368924657308341042&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "Gatsby Unit, CSML, University College London, UK; Gatsby Unit, CSML, University College London, UK; Gatsby Unit, CSML, University College London, UK; School of Mathematics, University of Bristol, UK; Gatsby Unit, CSML, University College London, UK",
        "aff_domain": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk;gatsby.ucl.ac.uk;bristol.ac.uk;gmail.com",
        "email": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk;gatsby.ucl.ac.uk;bristol.ac.uk;gmail.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "University College London;University of Bristol",
        "aff_unique_dep": "Gatsby Unit, CSML;School of Mathematics",
        "aff_unique_url": "https://www.ucl.ac.uk;https://www.bristol.ac.uk",
        "aff_unique_abbr": "UCL;UoB",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "f577050f34",
        "title": "Kernel Mean Estimation and Stein Effect",
        "site": "https://proceedings.mlr.press/v32/muandet14.html",
        "author": "Krikamol Muandet; Kenji Fukumizu; Bharath Sriperumbudur; Arthur Gretton; Bernhard Schoelkopf",
        "abstract": "A mean function in reproducing kernel Hilbert space (RKHS), or a kernel mean, is an important part of many algorithms ranging from kernel principal component analysis to Hilbert-space embedding of distributions. Given a finite sample, an empirical average is the standard estimate for the true kernel mean. We show that this estimator can be improved due to a well-known phenomenon in statistics called Stein phenomenon. After consideration, our theoretical analysis reveals the existence of a wide class of estimators that are better than the standard one. Focusing on a subset of this class, we propose efficient shrinkage estimators for the kernel mean. Empirical evaluations on several applications clearly demonstrate that the proposed estimators outperform the standard kernel mean estimator.",
        "bibtex": "@InProceedings{pmlr-v32-muandet14,\n  title = \t {Kernel Mean Estimation and Stein Effect},\n  author = \t {Muandet, Krikamol and Fukumizu, Kenji and Sriperumbudur, Bharath and Gretton, Arthur and Schoelkopf, Bernhard},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {10--18},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/muandet14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/muandet14.html},\n  abstract = \t {A mean function in reproducing kernel Hilbert space (RKHS), or a kernel mean, is an important part of many algorithms ranging from kernel principal component analysis to Hilbert-space embedding of distributions. Given a finite sample, an empirical average is the standard estimate for the true kernel mean. We show that this estimator can be improved due to a well-known phenomenon in statistics called Stein phenomenon. After consideration, our theoretical analysis reveals the existence of a wide class of estimators that are better than the standard one. Focusing on a subset of this class, we propose efficient shrinkage estimators for the kernel mean. Empirical evaluations on several applications clearly demonstrate that the proposed estimators outperform the standard kernel mean estimator.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/muandet14.pdf",
        "supp": "",
        "pdf_size": 212927,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6292961417655864146&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 17,
        "aff": "Empirical Inference Department, Max Planck Institute for Intelligent Systems, T\u00fcbingen, Germany; The Institute of Statistical Mathematics, Tokyo, Japan; Statistical Laboratory, University of Cambridge, Cambridge, United Kingdom; Gatsby Computational Neuroscience Unit, University College London, London, United Kingdom; Empirical Inference Department, Max Planck Institute for Intelligent Systems, T\u00fcbingen, Germany",
        "aff_domain": "TUEBINGEN.MPG.DE;ISM.AC.JP;STATSLAB.CAM.AC.UK;GMAIL.COM;TUEBINGEN.MPG.DE",
        "email": "TUEBINGEN.MPG.DE;ISM.AC.JP;STATSLAB.CAM.AC.UK;GMAIL.COM;TUEBINGEN.MPG.DE",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3;0",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems;Institute of Statistical Mathematics;University of Cambridge;University College London",
        "aff_unique_dep": "Empirical Inference Department;;Statistical Laboratory;Gatsby Computational Neuroscience Unit",
        "aff_unique_url": "https://www.mpituebingen.mpg.de;https://www.ism.ac.jp;https://www.cam.ac.uk;https://www.ucl.ac.uk",
        "aff_unique_abbr": "MPI-IS;ISM;Cambridge;UCL",
        "aff_campus_unique_index": "0;1;2;3;0",
        "aff_campus_unique": "T\u00fcbingen;Tokyo;Cambridge;London",
        "aff_country_unique_index": "0;1;2;2;0",
        "aff_country_unique": "Germany;Japan;United Kingdom"
    },
    {
        "id": "60bad2a2ff",
        "title": "Large-Margin Metric Learning for Constrained Partitioning Problems",
        "site": "https://proceedings.mlr.press/v32/lajugie14.html",
        "author": "R\u00e9mi Lajugie; Francis Bach; Sylvain Arlot",
        "abstract": "We consider unsupervised partitioning problems based explicitly or implicitly on the minimization of Euclidean distortions, such as clustering, image or video segmentation, and other change-point detection problems. We emphasize on cases with specific structure, which include many practical situations ranging from mean-based change-point detection to image segmentation problems. We aim at learning a Mahalanobis metric for these unsupervised problems, leading to feature weighting and/or selection. This is done in a supervised way by assuming the availability of several (partially) labeled datasets that share the same metric. We cast the metric learning problem as a large-margin structured prediction problem, with proper definition of regularizers and losses, leading to a convex optimization problem which can be solved efficiently. Our experiments show how learning the metric can significantly improve performance on bioinformatics, video  or image segmentation problems.",
        "bibtex": "@InProceedings{pmlr-v32-lajugie14,\n  title = \t {Large-Margin Metric Learning for Constrained Partitioning Problems},\n  author = \t {Lajugie, R\u00e9mi and Bach, Francis and Arlot, Sylvain},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {297--305},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/lajugie14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/lajugie14.html},\n  abstract = \t {We consider unsupervised partitioning problems based explicitly or implicitly on the minimization of Euclidean distortions, such as clustering, image or video segmentation, and other change-point detection problems. We emphasize on cases with specific structure, which include many practical situations ranging from mean-based change-point detection to image segmentation problems. We aim at learning a Mahalanobis metric for these unsupervised problems, leading to feature weighting and/or selection. This is done in a supervised way by assuming the availability of several (partially) labeled datasets that share the same metric. We cast the metric learning problem as a large-margin structured prediction problem, with proper definition of regularizers and losses, leading to a convex optimization problem which can be solved efficiently. Our experiments show how learning the metric can significantly improve performance on bioinformatics, video  or image segmentation problems.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/lajugie14.pdf",
        "supp": "",
        "pdf_size": 609783,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9718545682671101706&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "D\u00b4epartement d\u2019Informatique de l\u2019Ecole Normale Sup \u00b4erieure, (CNRS/INRIA/ENS), Paris, France; D\u00b4epartement d\u2019Informatique de l\u2019Ecole Normale Sup \u00b4erieure, (CNRS/INRIA/ENS), Paris, France; D\u00b4epartement d\u2019Informatique de l\u2019Ecole Normale Sup \u00b4erieure, (CNRS/INRIA/ENS), Paris, France",
        "aff_domain": "ENS.FR;ENS.FR;INRIA.FR",
        "email": "ENS.FR;ENS.FR;INRIA.FR",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Ecole Normale Sup\u00e9rieure",
        "aff_unique_dep": "D\u00e9partement d\u2019Informatique",
        "aff_unique_url": "https://www.ens.fr",
        "aff_unique_abbr": "ENS",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Paris",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "44713cb056",
        "title": "Large-margin  Weakly Supervised Dimensionality Reduction",
        "site": "https://proceedings.mlr.press/v32/xu14.html",
        "author": "Chang Xu; Dacheng Tao; Chao Xu; Yong Rui",
        "abstract": "This paper  studies dimensionality reduction in a weakly supervised setting, in which the preference relationship between examples is indicated by weak cues. A novel framework is proposed that integrates two aspects of the large margin principle (angle and distance), which simultaneously encourage angle consistency between preference pairs and maximize the distance between examples in preference pairs. Two specific algorithms are developed: an alternating direction method to learn a linear transformation matrix and a gradient boosting technique to optimize a non-linear transformation directly in the function space. Theoretical analysis demonstrates that the proposed large margin optimization criteria can strengthen and improve the robustness and generalization performance of preference learning algorithms on the obtained low-dimensional subspace. Experimental results on real-world datasets demonstrate the significance of studying dimensionality reduction in the weakly supervised setting and the effectiveness of the proposed framework.",
        "bibtex": "@InProceedings{pmlr-v32-xu14,\n  title = \t {Large-margin  Weakly Supervised Dimensionality Reduction},\n  author = \t {Xu, Chang and Tao, Dacheng and Xu, Chao and Rui, Yong},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {865--873},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/xu14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/xu14.html},\n  abstract = \t {This paper  studies dimensionality reduction in a weakly supervised setting, in which the preference relationship between examples is indicated by weak cues. A novel framework is proposed that integrates two aspects of the large margin principle (angle and distance), which simultaneously encourage angle consistency between preference pairs and maximize the distance between examples in preference pairs. Two specific algorithms are developed: an alternating direction method to learn a linear transformation matrix and a gradient boosting technique to optimize a non-linear transformation directly in the function space. Theoretical analysis demonstrates that the proposed large margin optimization criteria can strengthen and improve the robustness and generalization performance of preference learning algorithms on the obtained low-dimensional subspace. Experimental results on real-world datasets demonstrate the significance of studying dimensionality reduction in the weakly supervised setting and the effectiveness of the proposed framework.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/xu14.pdf",
        "supp": "",
        "pdf_size": 1471901,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17838766567415101971&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Key Lab. of Machine Perception (Ministry of Education), Peking University, Beijing 100871, China+Centre for Quantum Computation and Intelligent Systems, University of Technology, Sydney 2007, Australia; Centre for Quantum Computation and Intelligent Systems, University of Technology, Sydney 2007, Australia; Key Lab. of Machine Perception (Ministry of Education), Peking University, Beijing 100871, China; Microsoft Research, No. 5, Dan Ling Street, Haidian District, Beijing 10080, China",
        "aff_domain": "GMAIL.COM;UTS.EDU.CN;CIS.PKU.EDU.CN;MICROSOFT.COM",
        "email": "GMAIL.COM;UTS.EDU.CN;CIS.PKU.EDU.CN;MICROSOFT.COM",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;0;2",
        "aff_unique_norm": "Peking University;University of Technology Sydney;Microsoft",
        "aff_unique_dep": "Key Lab. of Machine Perception;Centre for Quantum Computation and Intelligent Systems;Microsoft Research",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.uts.edu.au;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "PKU;UTS;MSR",
        "aff_campus_unique_index": "0+1;1;0;0",
        "aff_campus_unique": "Beijing;Sydney",
        "aff_country_unique_index": "0+1;1;0;0",
        "aff_country_unique": "China;Australia"
    },
    {
        "id": "bf957266e2",
        "title": "Large-scale Multi-label Learning with Missing Labels",
        "site": "https://proceedings.mlr.press/v32/yu14.html",
        "author": "Hsiang-Fu Yu; Prateek Jain; Purushottam Kar; Inderjit Dhillon",
        "abstract": "The multi-label classification problem has generated significant interest in recent years. However, existing approaches do not adequately address two key challenges: (a) scaling up to problems with a large number (say millions) of labels, and (b) handling data with missing labels. In this paper, we directly address both these problems by studying the multi-label problem in a generic empirical risk minimization (ERM) framework. Our framework, despite being simple, is surprisingly able to encompass several recent label-compression based methods which can be derived as special cases of our method. To optimize the ERM problem, we develop techniques that exploit the structure of specific loss functions - such as the squared loss function - to obtain efficient algorithms. We further show that our learning framework admits excess risk bounds even in the presence of missing labels. Our bounds are tight and demonstrate better generalization performance for low-rank promoting trace-norm regularization when compared to (rank insensitive) Frobenius norm regularization. Finally, we present extensive empirical results on a variety of benchmark datasets and show that our methods perform significantly better than existing label compression based methods and can scale up to very large datasets such as a Wikipedia dataset that has more than 200,000 labels.",
        "bibtex": "@InProceedings{pmlr-v32-yu14,\n  title = \t {Large-scale Multi-label Learning with Missing Labels},\n  author = \t {Yu, Hsiang-Fu and Jain, Prateek and Kar, Purushottam and Dhillon, Inderjit},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {593--601},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/yu14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/yu14.html},\n  abstract = \t {The multi-label classification problem has generated significant interest in recent years. However, existing approaches do not adequately address two key challenges: (a) scaling up to problems with a large number (say millions) of labels, and (b) handling data with missing labels. In this paper, we directly address both these problems by studying the multi-label problem in a generic empirical risk minimization (ERM) framework. Our framework, despite being simple, is surprisingly able to encompass several recent label-compression based methods which can be derived as special cases of our method. To optimize the ERM problem, we develop techniques that exploit the structure of specific loss functions - such as the squared loss function - to obtain efficient algorithms. We further show that our learning framework admits excess risk bounds even in the presence of missing labels. Our bounds are tight and demonstrate better generalization performance for low-rank promoting trace-norm regularization when compared to (rank insensitive) Frobenius norm regularization. Finally, we present extensive empirical results on a variety of benchmark datasets and show that our methods perform significantly better than existing label compression based methods and can scale up to very large datasets such as a Wikipedia dataset that has more than 200,000 labels.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/yu14.pdf",
        "supp": "",
        "pdf_size": 490049,
        "gs_citation": 593,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12083805013376929245&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "Department of Computer Science, University of Texas at Austin; Microsoft Research India, Bangalore; Microsoft Research India, Bangalore; Department of Computer Science, University of Texas at Austin",
        "aff_domain": "CS.UTEXAS.EDU;MICROSOFT.COM;MICROSOFT.COM;CS.UTEXAS.EDU",
        "email": "CS.UTEXAS.EDU;MICROSOFT.COM;MICROSOFT.COM;CS.UTEXAS.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "University of Texas at Austin;Microsoft",
        "aff_unique_dep": "Department of Computer Science;Microsoft Research India",
        "aff_unique_url": "https://www.utexas.edu;https://www.microsoft.com/en-us/research/group/microsoft-research-india",
        "aff_unique_abbr": "UT Austin;MSRI",
        "aff_campus_unique_index": "0;1;1;0",
        "aff_campus_unique": "Austin;Bangalore",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "United States;India"
    },
    {
        "id": "2255838602",
        "title": "Latent Bandits.",
        "site": "https://proceedings.mlr.press/v32/maillard14.html",
        "author": "Odalric-Ambrym Maillard; Shie Mannor",
        "abstract": "We consider a multi-armed bandit problem where the reward distributions are indexed by two sets \u2013one for arms, one for type\u2013 and can be partitioned into a small number of clusters according to the type. First, we consider the setting where all reward distributions are known and all types have the same underlying cluster, the type\u2019s identity is, however, unknown. Second, we study the case  where types may come from different classes, which is significantly more challenging. Finally, we tackle the case where the reward distributions are completely unknown. In each setting, we introduce specific algorithms and derive non-trivial regret performance. Numerical experiments show that,  in the most challenging agnostic case, the proposed algorithm  achieves excellent performance in several difficult scenarios.",
        "bibtex": "@InProceedings{pmlr-v32-maillard14,\n  title = \t {Latent Bandits.},\n  author = \t {Maillard, Odalric-Ambrym and Mannor, Shie},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {136--144},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/maillard14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/maillard14.html},\n  abstract = \t {We consider a multi-armed bandit problem where the reward distributions are indexed by two sets \u2013one for arms, one for type\u2013 and can be partitioned into a small number of clusters according to the type. First, we consider the setting where all reward distributions are known and all types have the same underlying cluster, the type\u2019s identity is, however, unknown. Second, we study the case  where types may come from different classes, which is significantly more challenging. Finally, we tackle the case where the reward distributions are completely unknown. In each setting, we introduce specific algorithms and derive non-trivial regret performance. Numerical experiments show that,  in the most challenging agnostic case, the proposed algorithm  achieves excellent performance in several difficult scenarios.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/maillard14.pdf",
        "supp": "",
        "pdf_size": 483485,
        "gs_citation": 113,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1449149011905783844&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "The Technion, Faculty of Electrical Engineering 32000 Haifa, ISRAEL; The Technion, Faculty of Electrical Engineering 32000 Haifa, ISRAEL",
        "aff_domain": "ens-cachan.org;ee.technion.ac.il",
        "email": "ens-cachan.org;ee.technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Technion - Israel Institute of Technology",
        "aff_unique_dep": "Faculty of Electrical Engineering",
        "aff_unique_url": "https://www.technion.ac.il",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Haifa",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "551f125ba9",
        "title": "Latent Confusion Analysis by Normalized Gamma Construction",
        "site": "https://proceedings.mlr.press/v32/satob14.html",
        "author": "Issei Sato; Hisashi Kashima; Hiroshi Nakagawa",
        "abstract": "We developed a flexible framework for modeling the annotation and judgment processes of humans, which we called \u201cnormalized gamma construction of a confusion matrix.\u201d  This framework enabled us to model three properties: (1) the abilities of humans, (2) a confusion matrix with labeling, and (3) the difficulty with which items are correctly annotated.  We also provided the concept of \u201clatent confusion analysis (LCA),\u201d whose main purpose was to analyze the principal confusions behind human annotations and judgments.  It is assumed in LCA that confusion matrices are shared between persons, which we called \u201clatent confusions\u201d, in tribute to the \u201clatent topics\u201d of topic modeling.  We aim at summarizing the workers\u2019 confusion matrices with the small number of latent principal confusion matrices because many personal confusion matrices is difficult to analyze.  We used LCA to analyze latent confusions regarding the effects of radioactivity on  fish and shellfish following the Fukushima Daiichi nuclear disaster in 2011.",
        "bibtex": "@InProceedings{pmlr-v32-satob14,\n  title = \t {Latent Confusion Analysis by Normalized Gamma Construction},\n  author = \t {Sato, Issei and Kashima, Hisashi and Nakagawa, Hiroshi},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1116--1124},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/satob14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/satob14.html},\n  abstract = \t {We developed a flexible framework for modeling the annotation and judgment processes of humans, which we called \u201cnormalized gamma construction of a confusion matrix.\u201d  This framework enabled us to model three properties: (1) the abilities of humans, (2) a confusion matrix with labeling, and (3) the difficulty with which items are correctly annotated.  We also provided the concept of \u201clatent confusion analysis (LCA),\u201d whose main purpose was to analyze the principal confusions behind human annotations and judgments.  It is assumed in LCA that confusion matrices are shared between persons, which we called \u201clatent confusions\u201d, in tribute to the \u201clatent topics\u201d of topic modeling.  We aim at summarizing the workers\u2019 confusion matrices with the small number of latent principal confusion matrices because many personal confusion matrices is difficult to analyze.  We used LCA to analyze latent confusions regarding the effects of radioactivity on  fish and shellfish following the Fukushima Daiichi nuclear disaster in 2011.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/satob14.pdf",
        "supp": "",
        "pdf_size": 801504,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=228167291736837444&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "The University of Tokyo; Kyoto University; The University of Tokyo",
        "aff_domain": "R.DL.ITC.U-TOKYO.AC.JP;I.KYOTO-U.AC.JP;DL.ITC.U-TOKYO.AC.JP",
        "email": "R.DL.ITC.U-TOKYO.AC.JP;I.KYOTO-U.AC.JP;DL.ITC.U-TOKYO.AC.JP",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Tokyo;Kyoto University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.u-tokyo.ac.jp;https://www.kyoto-u.ac.jp",
        "aff_unique_abbr": "UTokyo;Kyoto U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "89897eee85",
        "title": "Latent Semantic Representation Learning for Scene Classification",
        "site": "https://proceedings.mlr.press/v32/lid14.html",
        "author": "Xin Li; Yuhong Guo",
        "abstract": "The performance of machine learning methods is heavily dependent on the choice of data representation. In real world applications such as scene recognition problems, the widely used low-level input features can fail to explain the high-level semantic label concepts. In this work, we address this problem by proposing a novel patch-based latent variable model to integrate latent contextual representation learning and classification model training in one joint optimization framework. Within this framework, the latent layer of variables bridge the gap between inputs and outputs by providing discriminative explanations for the semantic output labels, while being predictable from the low-level input features. Experiments conducted on standard scene recognition tasks demonstrate the efficacy of the proposed approach, comparing to the state-of-the-art scene recognition methods.",
        "bibtex": "@InProceedings{pmlr-v32-lid14,\n  title = \t {Latent Semantic Representation Learning for Scene Classification},\n  author = \t {Li, Xin and Guo, Yuhong},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {532--540},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/lid14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/lid14.html},\n  abstract = \t {The performance of machine learning methods is heavily dependent on the choice of data representation. In real world applications such as scene recognition problems, the widely used low-level input features can fail to explain the high-level semantic label concepts. In this work, we address this problem by proposing a novel patch-based latent variable model to integrate latent contextual representation learning and classification model training in one joint optimization framework. Within this framework, the latent layer of variables bridge the gap between inputs and outputs by providing discriminative explanations for the semantic output labels, while being predictable from the low-level input features. Experiments conducted on standard scene recognition tasks demonstrate the efficacy of the proposed approach, comparing to the state-of-the-art scene recognition methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/lid14.pdf",
        "supp": "",
        "pdf_size": 401616,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13766215993651910499&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer and Information Sciences, Temple University, Philadelphia, PA 19122, USA; Department of Computer and Information Sciences, Temple University, Philadelphia, PA 19122, USA",
        "aff_domain": "TEMPLE.EDU;TEMPLE.EDU",
        "email": "TEMPLE.EDU;TEMPLE.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Temple University",
        "aff_unique_dep": "Department of Computer and Information Sciences",
        "aff_unique_url": "https://www.temple.edu",
        "aff_unique_abbr": "Temple",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Philadelphia",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "94967c1986",
        "title": "Latent Variable Copula Inference for Bundle Pricing from Retail Transaction Data",
        "site": "https://proceedings.mlr.press/v32/letham14.html",
        "author": "Benjamin Letham; Wei Sun; Anshul Sheopuri",
        "abstract": "Bundle discounts are used by retailers in many industries. Optimal bundle pricing requires learning the joint distribution of consumer valuations for the items in the bundle, that is, how much they are willing to pay for each of the items. We suppose that a retailer has sales transaction data, and the corresponding consumer valuations are latent variables. We develop a statistically consistent and computationally tractable inference procedure for fitting a copula model over correlated valuations, using only sales transaction data for the individual items. Simulations and data experiments demonstrate consistency, scalability, and the importance of incorporating correlations in the joint distribution.",
        "bibtex": "@InProceedings{pmlr-v32-letham14,\n  title = \t {Latent Variable Copula Inference for Bundle Pricing from Retail Transaction Data},\n  author = \t {Letham, Benjamin and Sun, Wei and Sheopuri, Anshul},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {217--225},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/letham14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/letham14.html},\n  abstract = \t {Bundle discounts are used by retailers in many industries. Optimal bundle pricing requires learning the joint distribution of consumer valuations for the items in the bundle, that is, how much they are willing to pay for each of the items. We suppose that a retailer has sales transaction data, and the corresponding consumer valuations are latent variables. We develop a statistically consistent and computationally tractable inference procedure for fitting a copula model over correlated valuations, using only sales transaction data for the individual items. Simulations and data experiments demonstrate consistency, scalability, and the importance of incorporating correlations in the joint distribution.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/letham14.pdf",
        "supp": "",
        "pdf_size": 400733,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1644623951990725580&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Operations Research Center, Massachusetts Institute of Technology, 77 Mass. Ave., Cambridge, MA 02139 USA; IBM Thomas J. Watson Research Center, 1101 Kitchawan Road, Yorktown Heights, NY 10598 USA; IBM Thomas J. Watson Research Center, 1101 Kitchawan Road, Yorktown Heights, NY 10598 USA",
        "aff_domain": "MIT.EDU;US.IBM.COM;US.IBM.COM",
        "email": "MIT.EDU;US.IBM.COM;US.IBM.COM",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;IBM",
        "aff_unique_dep": "Operations Research Center;IBM Thomas J. Watson Research Center",
        "aff_unique_url": "https://web.mit.edu;https://www.ibm.com/research/watson",
        "aff_unique_abbr": "MIT;IBM Watson",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Cambridge;Yorktown Heights",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "75b64d7988",
        "title": "Learnability of the Superset Label Learning Problem",
        "site": "https://proceedings.mlr.press/v32/liug14.html",
        "author": "Liping Liu; Thomas Dietterich",
        "abstract": "In the Superset Label Learning (SLL) problem, weak supervision is  provided in the form of a \\it superset of labels that contains the  true label.  If the classifier predicts a label outside of the  superset, it commits a \\it superset error.  Most existing SLL  algorithms learn a multiclass classifier by minimizing the superset  error. However, only limited theoretical analysis has been dedicated  to this approach. In this paper, we analyze Empirical Risk Minimizing  learners that use the superset error as the empirical risk measure.  SLL data can arise either in the form of independent instances or as  multiple-instance bags. For both scenarios, we give the conditions for  ERM learnability and sample complexity for the realizable case.",
        "bibtex": "@InProceedings{pmlr-v32-liug14,\n  title = \t {Learnability of the Superset Label Learning Problem},\n  author = \t {Liu, Liping and Dietterich, Thomas},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1629--1637},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/liug14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/liug14.html},\n  abstract = \t {In the Superset Label Learning (SLL) problem, weak supervision is  provided in the form of a \\it superset of labels that contains the  true label.  If the classifier predicts a label outside of the  superset, it commits a \\it superset error.  Most existing SLL  algorithms learn a multiclass classifier by minimizing the superset  error. However, only limited theoretical analysis has been dedicated  to this approach. In this paper, we analyze Empirical Risk Minimizing  learners that use the superset error as the empirical risk measure.  SLL data can arise either in the form of independent instances or as  multiple-instance bags. For both scenarios, we give the conditions for  ERM learnability and sample complexity for the realizable case.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/liug14.pdf",
        "supp": "",
        "pdf_size": 291259,
        "gs_citation": 126,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9046258219084961609&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, Oregon 97331, USA; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, Oregon 97331, USA",
        "aff_domain": "EECS.OREGONSTATE.EDU;EECS.OREGONSTATE.EDU",
        "email": "EECS.OREGONSTATE.EDU;EECS.OREGONSTATE.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Oregon State University",
        "aff_unique_dep": "School of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://osu.edu",
        "aff_unique_abbr": "OSU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Corvallis",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e7dd122881",
        "title": "Learning Character-level Representations for Part-of-Speech Tagging",
        "site": "https://proceedings.mlr.press/v32/santos14.html",
        "author": "Cicero Dos Santos; Bianca Zadrozny",
        "abstract": "Distributed word representations have recently been proven to be an invaluable resource for NLP. These representations are normally learned using neural networks and capture syntactic and semantic information about words. Information about word morphology and shape is normally ignored when learning word representations. However, for tasks like part-of-speech tagging, intra-word information is extremely useful, specially when dealing with morphologically rich languages. In this paper, we propose a deep neural network that learns character-level representation of words and associate them with usual word representations to perform POS tagging. Using the proposed approach, while avoiding the use of any handcrafted feature, we produce state-of-the-art POS taggers for two languages: English, with 97.32% accuracy on the Penn Treebank WSJ corpus; and Portuguese, with 97.47% accuracy on the Mac-Morpho corpus, where the latter represents an error reduction of 12.2% on the best previous known result.",
        "bibtex": "@InProceedings{pmlr-v32-santos14,\n  title = \t {Learning Character-level Representations for Part-of-Speech Tagging},\n  author = \t {Santos, Cicero Dos and Zadrozny, Bianca},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1818--1826},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/santos14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/santos14.html},\n  abstract = \t {Distributed word representations have recently been proven to be an invaluable resource for NLP. These representations are normally learned using neural networks and capture syntactic and semantic information about words. Information about word morphology and shape is normally ignored when learning word representations. However, for tasks like part-of-speech tagging, intra-word information is extremely useful, specially when dealing with morphologically rich languages. In this paper, we propose a deep neural network that learns character-level representation of words and associate them with usual word representations to perform POS tagging. Using the proposed approach, while avoiding the use of any handcrafted feature, we produce state-of-the-art POS taggers for two languages: English, with 97.32% accuracy on the Penn Treebank WSJ corpus; and Portuguese, with 97.47% accuracy on the Mac-Morpho corpus, where the latter represents an error reduction of 12.2% on the best previous known result.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/santos14.pdf",
        "supp": "",
        "pdf_size": 165827,
        "gs_citation": 839,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11556422495082988183&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "IBM Research - Brazil, Av. Pasteur, 138/146 \u2013 Rio de Janeiro, 22296-903, Brazil; IBM Research - Brazil, Av. Pasteur, 138/146 \u2013 Rio de Janeiro, 22296-903, Brazil",
        "aff_domain": "BR.IBM.COM;BR.IBM.COM",
        "email": "BR.IBM.COM;BR.IBM.COM",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "IBM",
        "aff_unique_dep": "Research",
        "aff_unique_url": "https://www.ibm.com/research",
        "aff_unique_abbr": "IBM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Rio de Janeiro",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Brazil"
    },
    {
        "id": "b90e211468",
        "title": "Learning Complex Neural Network Policies with Trajectory Optimization",
        "site": "https://proceedings.mlr.press/v32/levine14.html",
        "author": "Sergey Levine; Vladlen Koltun",
        "abstract": "Direct policy search methods offer the promise of automatically learning controllers for complex, high-dimensional tasks. However, prior applications of policy search often required specialized, low-dimensional policy classes, limiting their generality. In this work, we introduce a policy search algorithm that can directly learn high-dimensional, general-purpose policies, represented by neural networks. We formulate the policy search problem as an optimization over trajectory distributions, alternating between optimizing the policy to match the trajectories, and optimizing the trajectories to match the policy and minimize expected cost. Our method can learn policies for complex tasks such as bipedal push recovery and walking on uneven terrain, while outperforming prior methods.",
        "bibtex": "@InProceedings{pmlr-v32-levine14,\n  title = \t {Learning Complex Neural Network Policies with Trajectory Optimization},\n  author = \t {Levine, Sergey and Koltun, Vladlen},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {829--837},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/levine14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/levine14.html},\n  abstract = \t {Direct policy search methods offer the promise of automatically learning controllers for complex, high-dimensional tasks. However, prior applications of policy search often required specialized, low-dimensional policy classes, limiting their generality. In this work, we introduce a policy search algorithm that can directly learn high-dimensional, general-purpose policies, represented by neural networks. We formulate the policy search problem as an optimization over trajectory distributions, alternating between optimizing the policy to match the trajectories, and optimizing the trajectories to match the policy and minimize expected cost. Our method can learn policies for complex tasks such as bipedal push recovery and walking on uneven terrain, while outperforming prior methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/levine14.pdf",
        "supp": "",
        "pdf_size": 1364527,
        "gs_citation": 191,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7748915071871275288&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Computer Science Department, Stanford University, Stanford, CA 94305 USA; Adobe Research, San Francisco, CA 94103 USA",
        "aff_domain": "CS.STANFORD.EDU;ADOBE.COM",
        "email": "CS.STANFORD.EDU;ADOBE.COM",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Stanford University;Adobe",
        "aff_unique_dep": "Computer Science Department;Adobe Research",
        "aff_unique_url": "https://www.stanford.edu;https://research.adobe.com",
        "aff_unique_abbr": "Stanford;Adobe",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Stanford;San Francisco",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ba3b57b11d",
        "title": "Learning Graphs with a Few Hubs",
        "site": "https://proceedings.mlr.press/v32/tandon14.html",
        "author": "Rashish Tandon; Pradeep Ravikumar",
        "abstract": "We consider the problem of recovering the graph structure of a \u201chub-networked\u201d Ising model given iid samples, under high-dimensional settings, where number of nodes p could be potentially larger than the number of samples n. By a \u201chub-networked\u201d graph, we mean a graph with a few \u201chub nodes\u201d with very large degrees. State of the art estimators for Ising models have a sample complexity that scales polynomially with the maximum node-degree, and are thus ill-suited to recovering such graphs with a few hub nodes. Some recent proposals for specifically recovering hub graphical models do not come with theoretical guarantees, and even empirically provide limited improvements over vanilla Ising model estimators. Here, we show that under such low sample settings, instead of estimating \u201cdifficult\u201d components such as hub-neighborhoods, we can use quantitative indicators of our inability to do so, and thereby identify hub-nodes. This simple procedure allows us to recover hub-networked graphs with very strong statistical guarantees even under very low sample settings.",
        "bibtex": "@InProceedings{pmlr-v32-tandon14,\n  title = \t {Learning Graphs with a Few Hubs},\n  author = \t {Tandon, Rashish and Ravikumar, Pradeep},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {602--610},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/tandon14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/tandon14.html},\n  abstract = \t {We consider the problem of recovering the graph structure of a \u201chub-networked\u201d Ising model given iid samples, under high-dimensional settings, where number of nodes p could be potentially larger than the number of samples n. By a \u201chub-networked\u201d graph, we mean a graph with a few \u201chub nodes\u201d with very large degrees. State of the art estimators for Ising models have a sample complexity that scales polynomially with the maximum node-degree, and are thus ill-suited to recovering such graphs with a few hub nodes. Some recent proposals for specifically recovering hub graphical models do not come with theoretical guarantees, and even empirically provide limited improvements over vanilla Ising model estimators. Here, we show that under such low sample settings, instead of estimating \u201cdifficult\u201d components such as hub-neighborhoods, we can use quantitative indicators of our inability to do so, and thereby identify hub-nodes. This simple procedure allows us to recover hub-networked graphs with very strong statistical guarantees even under very low sample settings.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/tandon14.pdf",
        "supp": "",
        "pdf_size": 405531,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5068020063094498460&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, The University of Texas at Austin, USA; Department of Computer Science, The University of Texas at Austin, USA",
        "aff_domain": "CS.UTEXAS.EDU;CS.UTEXAS.EDU",
        "email": "CS.UTEXAS.EDU;CS.UTEXAS.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8cda4ba959",
        "title": "Learning Latent Variable Gaussian Graphical Models",
        "site": "https://proceedings.mlr.press/v32/meng14.html",
        "author": "Zhaoshi Meng; Brian Eriksson; Al Hero",
        "abstract": "Gaussian graphical models (GGM) have been widely used in many high-dimensional applications ranging from biological and financial data to recommender systems. Sparsity in GGM plays a central role both statistically and computationally. Unfortunately, real-world data often does not fit well to sparse graphical models.  In this paper, we focus on a family of latent variable Gaussian graphical models (LVGGM), where the model is conditionally sparse given latent variables, but marginally non-sparse. In LVGGM, the inverse covariance matrix has a low-rank plus sparse structure, and can be learned in a regularized maximum likelihood framework. We derive novel parameter estimation error bounds for LVGGM under mild conditions in the high-dimensional setting. These results complement the existing theory on the structural learning, and open up new possibilities of using LVGGM for statistical inference.",
        "bibtex": "@InProceedings{pmlr-v32-meng14,\n  title = \t {Learning Latent Variable Gaussian Graphical Models},\n  author = \t {Meng, Zhaoshi and Eriksson, Brian and Hero, Al},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1269--1277},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/meng14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/meng14.html},\n  abstract = \t {Gaussian graphical models (GGM) have been widely used in many high-dimensional applications ranging from biological and financial data to recommender systems. Sparsity in GGM plays a central role both statistically and computationally. Unfortunately, real-world data often does not fit well to sparse graphical models.  In this paper, we focus on a family of latent variable Gaussian graphical models (LVGGM), where the model is conditionally sparse given latent variables, but marginally non-sparse. In LVGGM, the inverse covariance matrix has a low-rank plus sparse structure, and can be learned in a regularized maximum likelihood framework. We derive novel parameter estimation error bounds for LVGGM under mild conditions in the high-dimensional setting. These results complement the existing theory on the structural learning, and open up new possibilities of using LVGGM for statistical inference.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/meng14.pdf",
        "supp": "",
        "pdf_size": 305945,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7147146445681943245&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI 48109, USA; Technicolor Research Center, 735 Emerson Street, Palo Alto, CA 94301, USA; Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI 48109, USA",
        "aff_domain": "UMICH.EDU;TECHNICOLOR.COM;EECS.UMICH.EDU",
        "email": "UMICH.EDU;TECHNICOLOR.COM;EECS.UMICH.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Michigan;Technicolor Research Center",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Science;",
        "aff_unique_url": "https://www.umich.edu;https://www.technicolor.com/en",
        "aff_unique_abbr": "UM;",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Ann Arbor;Palo Alto",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "722ba55baf",
        "title": "Learning Mixtures of Linear Classifiers",
        "site": "https://proceedings.mlr.press/v32/sunb14.html",
        "author": "Yuekai Sun; Stratis Ioannidis; Andrea Montanari",
        "abstract": "We consider a discriminative learning (regression) problem, whereby the regression function is a convex combination of k linear classifiers. Existing approaches are based on the EM algorithm, or similar techniques, without provable guarantees. We develop a simple method based on spectral techniques and a \u2018mirroring\u2019 trick, that discovers the subspace spanned by the classifiers\u2019 parameter vectors. Under a probabilistic assumption on the  feature vector distribution, we prove that this approach has nearly optimal statistical efficiency.",
        "bibtex": "@InProceedings{pmlr-v32-sunb14,\n  title = \t {Learning Mixtures of Linear Classifiers},\n  author = \t {Sun, Yuekai and Ioannidis, Stratis and Montanari, Andrea},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {721--729},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/sunb14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/sunb14.html},\n  abstract = \t {We consider a discriminative learning (regression) problem, whereby the regression function is a convex combination of k linear classifiers. Existing approaches are based on the EM algorithm, or similar techniques, without provable guarantees. We develop a simple method based on spectral techniques and a \u2018mirroring\u2019 trick, that discovers the subspace spanned by the classifiers\u2019 parameter vectors. Under a probabilistic assumption on the  feature vector distribution, we prove that this approach has nearly optimal statistical efficiency.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/sunb14.pdf",
        "supp": "",
        "pdf_size": 561518,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2114865125904396148&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Institute for Computational and Mathematical Engineering, Stanford University; Technicolor; Dept. of Electrical Engineering + Dept. of Statistics, Stanford University",
        "aff_domain": "stanford.edu;technicolor.com;stanford.edu",
        "email": "stanford.edu;technicolor.com;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2+0",
        "aff_unique_norm": "Stanford University;Technicolor;University Affiliation Not Specified",
        "aff_unique_dep": "Institute for Computational and Mathematical Engineering;;Department of Electrical Engineering",
        "aff_unique_url": "https://www.stanford.edu;https://www.technicolor.com;",
        "aff_unique_abbr": "Stanford;Tec;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;France;"
    },
    {
        "id": "33386b3f94",
        "title": "Learning Modular Structures from Network Data and Node Variables",
        "site": "https://proceedings.mlr.press/v32/azizi14.html",
        "author": "Elham Azizi; Edoardo Airoldi; James Galagan",
        "abstract": "A standard technique for understanding underlying dependency structures among a set of variables posits a shared conditional probability distribution for the variables measured on individuals within a group. This approach is often referred to as module networks, where individuals are represented by nodes in a network, groups are termed modules, and the focus is on estimating the network structure among modules. However, estimation solely from node-specific variables can lead to spurious dependencies, and unverifiable structural assumptions are often used for regularization.  Here, we propose an extended model that leverages direct observations about the network in addition to node-specific variables. By integrating complementary data types, we avoid the need for structural assumptions. We illustrate theoretical and practical significance of the model and develop a reversible-jump MCMC learning procedure for learning modules and model parameters. We demonstrate the method accuracy in predicting modular structures from synthetic data and capability to learn regulatory modules in the  Mycobacterium tuberculosis gene regulatory network.",
        "bibtex": "@InProceedings{pmlr-v32-azizi14,\n  title = \t {Learning Modular Structures from Network Data and Node Variables},\n  author = \t {Azizi, Elham and Airoldi, Edoardo and Galagan, James},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1440--1448},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/azizi14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/azizi14.html},\n  abstract = \t {A standard technique for understanding underlying dependency structures among a set of variables posits a shared conditional probability distribution for the variables measured on individuals within a group. This approach is often referred to as module networks, where individuals are represented by nodes in a network, groups are termed modules, and the focus is on estimating the network structure among modules. However, estimation solely from node-specific variables can lead to spurious dependencies, and unverifiable structural assumptions are often used for regularization.  Here, we propose an extended model that leverages direct observations about the network in addition to node-specific variables. By integrating complementary data types, we avoid the need for structural assumptions. We illustrate theoretical and practical significance of the model and develop a reversible-jump MCMC learning procedure for learning modules and model parameters. We demonstrate the method accuracy in predicting modular structures from synthetic data and capability to learn regulatory modules in the  Mycobacterium tuberculosis gene regulatory network.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/azizi14.pdf",
        "supp": "",
        "pdf_size": 2573525,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14400926028724261978&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Bioinformatics Program, Boston University, Boston, MA 02215 USA; Department of Statistics, Harvard University, Camrbdige, MA 02138 USA; Departments of Biomedical Engineering and Microbiology, Boston University, Boston, MA 02215 USA",
        "aff_domain": "bu.edu;fas.harvard.edu;bu.edu",
        "email": "bu.edu;fas.harvard.edu;bu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Boston University;Harvard University",
        "aff_unique_dep": "Bioinformatics Program;Department of Statistics",
        "aff_unique_url": "https://www.bu.edu;https://www.harvard.edu",
        "aff_unique_abbr": "BU;Harvard",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Boston;Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c732c26cf8",
        "title": "Learning Ordered Representations with Nested Dropout",
        "site": "https://proceedings.mlr.press/v32/rippel14.html",
        "author": "Oren Rippel; Michael Gelbart; Ryan Adams",
        "abstract": "In this paper, we present results on ordered representations of data in which different dimensions have different degrees of importance. To learn these representations we introduce nested dropout, a procedure for stochastically removing coherent nested sets of hidden units in a neural network. We first present a sequence of theoretical results in the simple case of a semi-linear autoencoder.  We rigorously show that the application of nested dropout enforces identifiability of the units, which leads to an exact equivalence with PCA.  We then extend the algorithm to deep models and demonstrate the relevance of ordered representations to a number of applications.  Specifically, we use the ordered property of the learned codes to construct hash-based data structures that permit very fast retrieval, achieving retrieval in time logarithmic in the database size and independent of the dimensionality of the representation. This allows the use of codes that are hundreds of times longer than currently feasible for retrieval.  We therefore avoid the diminished quality associated with short codes, while still performing retrieval that is competitive in speed with existing methods.  We also show that ordered representations are a promising way to learn adaptive compression for efficient online data reconstruction.",
        "bibtex": "@InProceedings{pmlr-v32-rippel14,\n  title = \t {Learning Ordered Representations with Nested Dropout},\n  author = \t {Rippel, Oren and Gelbart, Michael and Adams, Ryan},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1746--1754},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/rippel14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/rippel14.html},\n  abstract = \t {In this paper, we present results on ordered representations of data in which different dimensions have different degrees of importance. To learn these representations we introduce nested dropout, a procedure for stochastically removing coherent nested sets of hidden units in a neural network. We first present a sequence of theoretical results in the simple case of a semi-linear autoencoder.  We rigorously show that the application of nested dropout enforces identifiability of the units, which leads to an exact equivalence with PCA.  We then extend the algorithm to deep models and demonstrate the relevance of ordered representations to a number of applications.  Specifically, we use the ordered property of the learned codes to construct hash-based data structures that permit very fast retrieval, achieving retrieval in time logarithmic in the database size and independent of the dimensionality of the representation. This allows the use of codes that are hundreds of times longer than currently feasible for retrieval.  We therefore avoid the diminished quality associated with short codes, while still performing retrieval that is competitive in speed with existing methods.  We also show that ordered representations are a promising way to learn adaptive compression for efficient online data reconstruction.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/rippel14.pdf",
        "supp": "",
        "pdf_size": 1204896,
        "gs_citation": 118,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10455691519245574741&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Mathematics, MIT + School of Engineering and Applied Sciences, Harvard University; Program in Biophysics and School of Engineering and Applied Sciences, Harvard University; School of Engineering and Applied Sciences, Harvard University",
        "aff_domain": "MATH.MIT.EDU;SEAS.HARVARD.EDU;SEAS.HARVARD.EDU",
        "email": "MATH.MIT.EDU;SEAS.HARVARD.EDU;SEAS.HARVARD.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;Harvard University",
        "aff_unique_dep": "Department of Mathematics;School of Engineering and Applied Sciences",
        "aff_unique_url": "https://web.mit.edu;https://www.harvard.edu",
        "aff_unique_abbr": "MIT;Harvard",
        "aff_campus_unique_index": "0+0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8269b920e4",
        "title": "Learning Polynomials with Neural Networks",
        "site": "https://proceedings.mlr.press/v32/andoni14.html",
        "author": "Alexandr Andoni; Rina Panigrahy; Gregory Valiant; Li Zhang",
        "abstract": "We study the effectiveness of learning low degree polynomials using   neural networks by the gradient descent method.  While neural   networks have been shown to have great expressive power, and gradient   descent has been widely used in practice for learning neural   networks, few theoretical guarantees are known for such methods.  In   particular, it is well known that gradient descent can get stuck at   local minima, even for simple classes of target functions.  In this   paper, we present several positive theoretical results to support the   effectiveness of neural networks.  We focus on two-layer neural   networks (i.e. one hidden layer) where the top layer node is a linear   function, similar to\u00a0\\citebarron93.  First we show that for a   randomly initialized neural network with sufficiently many hidden   units, the gradient descent method can learn any low degree   polynomial.  Secondly, we show that if we use complex-valued weights   (the target function can still be real), then under suitable   conditions, there are no \u201crobust local minima\u201d: the neural network   can always escape a local minimum by performing a random   perturbation. This property does not hold for real-valued weights.   Thirdly, we discuss whether sparse polynomials can be learned   with \\emphsmall neural networks, where the size is dependent on the   sparsity of the target function.",
        "bibtex": "@InProceedings{pmlr-v32-andoni14,\n  title = \t {Learning Polynomials with Neural Networks},\n  author = \t {Andoni, Alexandr and Panigrahy, Rina and Valiant, Gregory and Zhang, Li},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1908--1916},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/andoni14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/andoni14.html},\n  abstract = \t {We study the effectiveness of learning low degree polynomials using   neural networks by the gradient descent method.  While neural   networks have been shown to have great expressive power, and gradient   descent has been widely used in practice for learning neural   networks, few theoretical guarantees are known for such methods.  In   particular, it is well known that gradient descent can get stuck at   local minima, even for simple classes of target functions.  In this   paper, we present several positive theoretical results to support the   effectiveness of neural networks.  We focus on two-layer neural   networks (i.e. one hidden layer) where the top layer node is a linear   function, similar to\u00a0\\citebarron93.  First we show that for a   randomly initialized neural network with sufficiently many hidden   units, the gradient descent method can learn any low degree   polynomial.  Secondly, we show that if we use complex-valued weights   (the target function can still be real), then under suitable   conditions, there are no \u201crobust local minima\u201d: the neural network   can always escape a local minimum by performing a random   perturbation. This property does not hold for real-valued weights.   Thirdly, we discuss whether sparse polynomials can be learned   with \\emphsmall neural networks, where the size is dependent on the   sparsity of the target function.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/andoni14.pdf",
        "supp": "",
        "pdf_size": 315289,
        "gs_citation": 203,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10614318723724968087&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Microsoft Research; Microsoft Research; Stanford University; Microsoft Research",
        "aff_domain": "MICROSOFT.COM;MICROSOFT.COM;GMAIL.COM;MICROSOFT.COM",
        "email": "MICROSOFT.COM;MICROSOFT.COM;GMAIL.COM;MICROSOFT.COM",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Microsoft;Stanford University",
        "aff_unique_dep": "Microsoft Research;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://www.stanford.edu",
        "aff_unique_abbr": "MSR;Stanford",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2faf9d2025",
        "title": "Learning Sum-Product Networks with Direct and Indirect Variable Interactions",
        "site": "https://proceedings.mlr.press/v32/rooshenas14.html",
        "author": "Amirmohammad Rooshenas; Daniel Lowd",
        "abstract": "Sum-product networks (SPNs) are a deep probabilistic representation that allows for efficient, exact inference.  SPNs generalize many other tractable models, including thin junction trees, latent tree models, and many types of mixtures.  Previous work on learning SPN structure has mainly focused on using top-down or bottom-up clustering to find mixtures, which capture variable interactions indirectly through implicit latent variables.  In contrast, most work on learning graphical models, thin junction trees, and arithmetic circuits has focused on finding direct interactions among variables.  In this paper, we present ID-SPN, a new algorithm for learning SPN structure that unifies the two approaches. In experiments on 20 benchmark datasets, we find that the combination of direct and indirect interactions leads to significantly better accuracy than several state-of-the-art algorithms for learning SPNs and other tractable models.",
        "bibtex": "@InProceedings{pmlr-v32-rooshenas14,\n  title = \t {Learning Sum-Product Networks with Direct and Indirect Variable Interactions},\n  author = \t {Rooshenas, Amirmohammad and Lowd, Daniel},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {710--718},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/rooshenas14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/rooshenas14.html},\n  abstract = \t {Sum-product networks (SPNs) are a deep probabilistic representation that allows for efficient, exact inference.  SPNs generalize many other tractable models, including thin junction trees, latent tree models, and many types of mixtures.  Previous work on learning SPN structure has mainly focused on using top-down or bottom-up clustering to find mixtures, which capture variable interactions indirectly through implicit latent variables.  In contrast, most work on learning graphical models, thin junction trees, and arithmetic circuits has focused on finding direct interactions among variables.  In this paper, we present ID-SPN, a new algorithm for learning SPN structure that unifies the two approaches. In experiments on 20 benchmark datasets, we find that the combination of direct and indirect interactions leads to significantly better accuracy than several state-of-the-art algorithms for learning SPNs and other tractable models.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/rooshenas14.pdf",
        "supp": "",
        "pdf_size": 429847,
        "gs_citation": 209,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10274734015547783331&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer and Information Science, University of Oregon; Department of Computer and Information Science, University of Oregon",
        "aff_domain": "CS.UOREGON.EDU;CS.UOREGON.EDU",
        "email": "CS.UOREGON.EDU;CS.UOREGON.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Oregon",
        "aff_unique_dep": "Department of Computer and Information Science",
        "aff_unique_url": "https://www.uoregon.edu",
        "aff_unique_abbr": "UO",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b371f96fee",
        "title": "Learning Theory and Algorithms for revenue optimization in second price auctions with reserve",
        "site": "https://proceedings.mlr.press/v32/mohri14.html",
        "author": "Mehryar Mohri; Andres Munoz Medina",
        "abstract": "Second-price auctions with reserve play a critical role for    modern search engine and popular online sites since the revenue of   these companies often directly depends on the outcome of such   auctions. The choice of the reserve price is the main mechanism   through which the auction revenue can be influenced in these   electronic markets. We cast the problem of selecting the reserve   price to optimize revenue as a learning problem and present a full   theoretical analysis dealing with the complex properties of the   corresponding loss function (it is non-convex and discontinuous). We further give novel algorithms for solving this problem and report the results of encouraging experiments   demonstrating their effectiveness.",
        "bibtex": "@InProceedings{pmlr-v32-mohri14,\n  title = \t {Learning Theory and Algorithms for revenue optimization in second price auctions with reserve},\n  author = \t {Mohri, Mehryar and Medina, Andres Munoz},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {262--270},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/mohri14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/mohri14.html},\n  abstract = \t {Second-price auctions with reserve play a critical role for    modern search engine and popular online sites since the revenue of   these companies often directly depends on the outcome of such   auctions. The choice of the reserve price is the main mechanism   through which the auction revenue can be influenced in these   electronic markets. We cast the problem of selecting the reserve   price to optimize revenue as a learning problem and present a full   theoretical analysis dealing with the complex properties of the   corresponding loss function (it is non-convex and discontinuous). We further give novel algorithms for solving this problem and report the results of encouraging experiments   demonstrating their effectiveness.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/mohri14.pdf",
        "supp": "",
        "pdf_size": 482747,
        "gs_citation": 172,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13851864417330326819&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Courant Institute and Google Research; Courant Institute",
        "aff_domain": "CIMS.NYU.EDU;CIMS.NYU.EDU",
        "email": "CIMS.NYU.EDU;CIMS.NYU.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Courant Institute;Courant Institute of Mathematical Sciences",
        "aff_unique_dep": "Courant Institute;Mathematical Sciences",
        "aff_unique_url": "https://courant.nyu.edu;https://courant.nyu.edu",
        "aff_unique_abbr": "Courant;Courant",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0d128652c3",
        "title": "Learning by Stretching Deep Networks",
        "site": "https://proceedings.mlr.press/v32/pandey14.html",
        "author": "Gaurav Pandey; Ambedkar Dukkipati",
        "abstract": "In recent years, deep architectures have gained a lot of prominence for learning complex AI tasks  because of their capability to incorporate complex variations in data within the model. However, these models often need to be trained for a long time in order to obtain good results. In this paper, we propose a technique, called \u2018stretching\u2019, that allows the same models to perform considerably better with very little training.  We show that learning can be done tractably, even when the weight matrix is stretched to infinity, for some specific models. We also study tractable algorithms for implementing stretching in deep convolutional architectures in an iterative manner and derive bounds for its convergence. Our experimental results suggest that the proposed stretched deep convolutional networks are capable of achieving good performance for many object recognition tasks. More importantly, for a fixed network architecture, one can achieve much better accuracy using stretching rather than learning the weights using backpropagation.",
        "bibtex": "@InProceedings{pmlr-v32-pandey14,\n  title = \t {Learning by Stretching Deep Networks},\n  author = \t {Pandey, Gaurav and Dukkipati, Ambedkar},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1719--1727},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/pandey14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/pandey14.html},\n  abstract = \t {In recent years, deep architectures have gained a lot of prominence for learning complex AI tasks  because of their capability to incorporate complex variations in data within the model. However, these models often need to be trained for a long time in order to obtain good results. In this paper, we propose a technique, called \u2018stretching\u2019, that allows the same models to perform considerably better with very little training.  We show that learning can be done tractably, even when the weight matrix is stretched to infinity, for some specific models. We also study tractable algorithms for implementing stretching in deep convolutional architectures in an iterative manner and derive bounds for its convergence. Our experimental results suggest that the proposed stretched deep convolutional networks are capable of achieving good performance for many object recognition tasks. More importantly, for a fixed network architecture, one can achieve much better accuracy using stretching rather than learning the weights using backpropagation.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/pandey14.pdf",
        "supp": "",
        "pdf_size": 480091,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13602018701726060574&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science and Automation, Indian Institute of Science, Bangalore-560012, India; Department of Computer Science and Automation, Indian Institute of Science, Bangalore-560012, India",
        "aff_domain": "CSA.IISC.ERNET.IN;CSA.IISC.ERNET.IN",
        "email": "CSA.IISC.ERNET.IN;CSA.IISC.ERNET.IN",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indian Institute of Science",
        "aff_unique_dep": "Department of Computer Science and Automation",
        "aff_unique_url": "https://www.iisc.ac.in",
        "aff_unique_abbr": "IISc",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Bangalore",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "d824aad9fa",
        "title": "Learning from Contagion (Without Timestamps)",
        "site": "https://proceedings.mlr.press/v32/amin14.html",
        "author": "Kareem Amin; Hoda Heidari; Michael Kearns",
        "abstract": "We introduce and study new models for learning from contagion processes in a network. A learning algorithm is allowed to either choose or passively observe an initial set of seed infections. This seed set then induces a final set of infections resulting from the underlying stochastic contagion dynamics. Our models differ from prior work in that detailed vertex-by-vertex timestamps for the spread of the contagion are not observed. The goal of learning is to infer the unknown network structure. Our main theoretical results are efficient and provably correct algorithms for exactly learning trees. We provide empirical evidence that our algorithm performs well more generally on realistic sparse graphs.",
        "bibtex": "@InProceedings{pmlr-v32-amin14,\n  title = \t {Learning from Contagion (Without Timestamps)},\n  author = \t {Amin, Kareem and Heidari, Hoda and Kearns, Michael},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1845--1853},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/amin14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/amin14.html},\n  abstract = \t {We introduce and study new models for learning from contagion processes in a network. A learning algorithm is allowed to either choose or passively observe an initial set of seed infections. This seed set then induces a final set of infections resulting from the underlying stochastic contagion dynamics. Our models differ from prior work in that detailed vertex-by-vertex timestamps for the spread of the contagion are not observed. The goal of learning is to infer the unknown network structure. Our main theoretical results are efficient and provably correct algorithms for exactly learning trees. We provide empirical evidence that our algorithm performs well more generally on realistic sparse graphs.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/amin14.pdf",
        "supp": "",
        "pdf_size": 301678,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13392185540966432319&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Computer and Information Science, University of Pennsylvania; Computer and Information Science, University of Pennsylvania; Computer and Information Science, University of Pennsylvania",
        "aff_domain": "SEAS.UPENN.EDU;SEAS.UPENN.EDU;CIS.UPENN.EDU",
        "email": "SEAS.UPENN.EDU;SEAS.UPENN.EDU;CIS.UPENN.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "Computer and Information Science",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "428d027f2f",
        "title": "Learning the Consistent Behavior of Common Users for Target Node Prediction across Social Networks",
        "site": "https://proceedings.mlr.press/v32/wu14.html",
        "author": "Shan-Hung Wu; Hao-Heng Chien; Kuan-Hua Lin; Philip Yu",
        "abstract": "We study the target node prediction problem: given two social networks, identify those nodes/users from one network (called the source network) who are likely to join another (called the target network, with nodes called target nodes). Although this problem can be solved using existing techniques in the field of cross domain classification, we observe that in many real-world situations the cross-domain classifiers perform sub-optimally due to the heterogeneity between source and target networks that prevents the knowledge from being transferred. In this paper, we propose learning the consistent behavior of common users to help the knowledge transfer. We first present the Consistent Incidence Co-Factorization (CICF) for identifying the consistent users, i.e., common users that behave consistently across networks. Then we introduce the Domain-UnBiased (DUB) classifiers that transfer knowledge only through those consistent users. Extensive experiments are conducted and the results show that our proposal copes with heterogeneity and improves prediction accuracy.",
        "bibtex": "@InProceedings{pmlr-v32-wu14,\n  title = \t {Learning the Consistent Behavior of Common Users for Target Node Prediction across Social Networks},\n  author = \t {Wu, Shan-Hung and Chien, Hao-Heng and Lin, Kuan-Hua and Yu, Philip},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {298--306},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/wu14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/wu14.html},\n  abstract = \t {We study the target node prediction problem: given two social networks, identify those nodes/users from one network (called the source network) who are likely to join another (called the target network, with nodes called target nodes). Although this problem can be solved using existing techniques in the field of cross domain classification, we observe that in many real-world situations the cross-domain classifiers perform sub-optimally due to the heterogeneity between source and target networks that prevents the knowledge from being transferred. In this paper, we propose learning the consistent behavior of common users to help the knowledge transfer. We first present the Consistent Incidence Co-Factorization (CICF) for identifying the consistent users, i.e., common users that behave consistently across networks. Then we introduce the Domain-UnBiased (DUB) classifiers that transfer knowledge only through those consistent users. Extensive experiments are conducted and the results show that our proposal copes with heterogeneity and improves prediction accuracy.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/wu14.pdf",
        "supp": "",
        "pdf_size": 364949,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7788929422746309178&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "National Tsing Hua University, ROC; National Tsing Hua University, ROC; National Tsing Hua University, ROC; University of Illinois at Chicago, USA",
        "aff_domain": "CS.NTHU.EDU.TW;NETDB.CS.NTHU.EDU.TW;NETDB.CS.NTHU.EDU.TW;CS.UIC.EDU",
        "email": "CS.NTHU.EDU.TW;NETDB.CS.NTHU.EDU.TW;NETDB.CS.NTHU.EDU.TW;CS.UIC.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "National Tsing Hua University;University of Illinois at Chicago",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.nthu.edu.tw;https://www.uic.edu",
        "aff_unique_abbr": "NTHU;UIC",
        "aff_campus_unique_index": "0;0;0;1",
        "aff_campus_unique": "Taiwan;Chicago",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "73bd9cf616",
        "title": "Learning the Irreducible Representations of Commutative Lie Groups",
        "site": "https://proceedings.mlr.press/v32/cohen14.html",
        "author": "Taco Cohen; Max Welling",
        "abstract": "We present a new probabilistic model of compact commutative Lie groups that produces invariant-equivariant and disentangled representations of data. To define the notion of disentangling, we borrow a fundamental principle from physics that is used to derive the elementary particles of a system from its symmetries. Our model employs a newfound Bayesian conjugacy relation that enables fully tractable probabilistic inference over compact commutative Lie groups \u2013 a class that includes the groups that describe the rotation and cyclic translation of images. We train the model on pairs of transformed image patches, and show that the learned invariant representation is highly effective for classification.",
        "bibtex": "@InProceedings{pmlr-v32-cohen14,\n  title = \t {Learning the Irreducible Representations of Commutative Lie Groups},\n  author = \t {Cohen, Taco and Welling, Max},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1755--1763},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/cohen14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/cohen14.html},\n  abstract = \t {We present a new probabilistic model of compact commutative Lie groups that produces invariant-equivariant and disentangled representations of data. To define the notion of disentangling, we borrow a fundamental principle from physics that is used to derive the elementary particles of a system from its symmetries. Our model employs a newfound Bayesian conjugacy relation that enables fully tractable probabilistic inference over compact commutative Lie groups \u2013 a class that includes the groups that describe the rotation and cyclic translation of images. We train the model on pairs of transformed image patches, and show that the learned invariant representation is highly effective for classification.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/cohen14.pdf",
        "supp": "",
        "pdf_size": 406007,
        "gs_citation": 136,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3184378842105092803&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Machine Learning Group, University of Amsterdam; Machine Learning Group, University of Amsterdam",
        "aff_domain": "UVA.NL;UVA.NL",
        "email": "UVA.NL;UVA.NL",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Amsterdam",
        "aff_unique_dep": "Machine Learning Group",
        "aff_unique_url": "https://www.uva.nl",
        "aff_unique_abbr": "UvA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "6bd973e00a",
        "title": "Learning the Parameters of Determinantal Point Process Kernels",
        "site": "https://proceedings.mlr.press/v32/affandi14.html",
        "author": "Raja Hafiz Affandi; Emily Fox; Ryan Adams; Ben Taskar",
        "abstract": "Determinantal point processes (DPPs) are  well-suited for modeling repulsion and have  proven useful in applications where diversity  is desired. While DPPs have many appealing  properties, learning the parameters of a DPP  is difficult, as the likelihood is non-convex  and is infeasible to compute in many scenarios. Here we propose Bayesian methods for  learning the DPP kernel parameters. These methods are applicable in large-scale discrete  and continuous DPP settings, even when the  likelihood can only be bounded. We demonstrate  the utility of our DPP learning methods  in studying the progression of diabetic neuropathy  based on the spatial distribution of  nerve fibers, and in studying human perception  of diversity in images.",
        "bibtex": "@InProceedings{pmlr-v32-affandi14,\n  title = \t {Learning the Parameters of Determinantal Point Process Kernels},\n  author = \t {Affandi, Raja Hafiz and Fox, Emily and Adams, Ryan and Taskar, Ben},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1224--1232},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/affandi14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/affandi14.html},\n  abstract = \t {Determinantal point processes (DPPs) are  well-suited for modeling repulsion and have  proven useful in applications where diversity  is desired. While DPPs have many appealing  properties, learning the parameters of a DPP  is difficult, as the likelihood is non-convex  and is infeasible to compute in many scenarios. Here we propose Bayesian methods for  learning the DPP kernel parameters. These methods are applicable in large-scale discrete  and continuous DPP settings, even when the  likelihood can only be bounded. We demonstrate  the utility of our DPP learning methods  in studying the progression of diabetic neuropathy  based on the spatial distribution of  nerve fibers, and in studying human perception  of diversity in images.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/affandi14.pdf",
        "supp": "",
        "pdf_size": 1000949,
        "gs_citation": 130,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5296145768497922108&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Statistics, University of Pennsylvania; Department of Statistics, University of Washington; Department of Statistics, Harvard Univerity; Department of Computer Science & Engineering, University of Washington",
        "aff_domain": "WHARTON.UPENN.EDU;STAT.WASHINGTON.EDU;SEAS.HARVARD.EDU;CS.WASHINGTON.EDU",
        "email": "WHARTON.UPENN.EDU;STAT.WASHINGTON.EDU;SEAS.HARVARD.EDU;CS.WASHINGTON.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;1",
        "aff_unique_norm": "University of Pennsylvania;University of Washington;Harvard University",
        "aff_unique_dep": "Department of Statistics;Department of Statistics;Department of Statistics",
        "aff_unique_url": "https://www.upenn.edu;https://www.washington.edu;https://www.harvard.edu",
        "aff_unique_abbr": "UPenn;UW;Harvard",
        "aff_campus_unique_index": "1;2;1",
        "aff_campus_unique": ";Seattle;Cambridge",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5b8c80abf4",
        "title": "Learning to Disentangle Factors of Variation with Manifold Interaction",
        "site": "https://proceedings.mlr.press/v32/reed14.html",
        "author": "Scott Reed; Kihyuk Sohn; Yuting Zhang; Honglak Lee",
        "abstract": "Many latent factors of variation interact to generate sensory data; for example pose, morphology and expression in face images. We propose to learn manifold coordinates for the relevant factors of variation and to model their joint interaction. Most existing feature learning algorithms focus on a single task and extract features that are sensitive to the task-relevant factors and invariant to all others. However, models that just extract a single set of invariant features do not exploit the relationships among the latent factors. To address this we propose a higher-order Boltzmann machine that incorporates multiplicative interactions among groups of hidden units that each learn to encode a factor of variation. Furthermore, we propose a manifold-based training strategy that allows effective disentangling, meaning that units in each group encode a distinct type of variation. Our model achieves state-of-the-art emotion recognition and face verification performance on the Toronto Face Database, and we also demonstrate disentangled features learned on the CMU Multi-PIE dataset.",
        "bibtex": "@InProceedings{pmlr-v32-reed14,\n  title = \t {Learning to Disentangle Factors of Variation with Manifold Interaction},\n  author = \t {Reed, Scott and Sohn, Kihyuk and Zhang, Yuting and Lee, Honglak},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1431--1439},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/reed14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/reed14.html},\n  abstract = \t {Many latent factors of variation interact to generate sensory data; for example pose, morphology and expression in face images. We propose to learn manifold coordinates for the relevant factors of variation and to model their joint interaction. Most existing feature learning algorithms focus on a single task and extract features that are sensitive to the task-relevant factors and invariant to all others. However, models that just extract a single set of invariant features do not exploit the relationships among the latent factors. To address this we propose a higher-order Boltzmann machine that incorporates multiplicative interactions among groups of hidden units that each learn to encode a factor of variation. Furthermore, we propose a manifold-based training strategy that allows effective disentangling, meaning that units in each group encode a distinct type of variation. Our model achieves state-of-the-art emotion recognition and face verification performance on the Toronto Face Database, and we also demonstrate disentangled features learned on the CMU Multi-PIE dataset.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/reed14.pdf",
        "supp": "",
        "pdf_size": 2038598,
        "gs_citation": 293,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15396194608488950557&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Dept. of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI 48109, USA; Dept. of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI 48109, USA; Dept. of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI 48109, USA; Dept. of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI 48109, USA",
        "aff_domain": "UMICH.EDU;UMICH.EDU;UMICH.EDU;UMICH.EDU",
        "email": "UMICH.EDU;UMICH.EDU;UMICH.EDU;UMICH.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Dept. of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "59ddcc5ddf",
        "title": "Least Squares Revisited: Scalable Approaches for Multi-class Prediction",
        "site": "https://proceedings.mlr.press/v32/agarwala14.html",
        "author": "Alekh Agarwal; Sham Kakade; Nikos Karampatziakis; Le Song; Gregory Valiant",
        "abstract": "This work provides simple algorithms for multi-class (and multi-label) prediction in settings where both the number of examples n and the data dimension d are relatively    large. These robust and parameter free algorithms are essentially    iterative least-squares updates and very versatile both in theory and in practice. On the theoretical front, we present several variants with convergence guarantees. Owing to their effective use of second-order structure, these algorithms are substantially better than first-order methods in many practical scenarios. On the empirical side, we show how to scale our approach to high dimensional datasets, achieving dramatic computational speedups over popular optimization packages such as Liblinear and Vowpal Wabbit on standard datasets (MNIST and CIFAR-10), while attaining state-of-the-art accuracies.",
        "bibtex": "@InProceedings{pmlr-v32-agarwala14,\n  title = \t {Least Squares Revisited: Scalable Approaches for Multi-class Prediction},\n  author = \t {Agarwal, Alekh and Kakade, Sham and Karampatziakis, Nikos and Song, Le and Valiant, Gregory},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {541--549},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/agarwala14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/agarwala14.html},\n  abstract = \t {This work provides simple algorithms for multi-class (and multi-label) prediction in settings where both the number of examples n and the data dimension d are relatively    large. These robust and parameter free algorithms are essentially    iterative least-squares updates and very versatile both in theory and in practice. On the theoretical front, we present several variants with convergence guarantees. Owing to their effective use of second-order structure, these algorithms are substantially better than first-order methods in many practical scenarios. On the empirical side, we show how to scale our approach to high dimensional datasets, achieving dramatic computational speedups over popular optimization packages such as Liblinear and Vowpal Wabbit on standard datasets (MNIST and CIFAR-10), while attaining state-of-the-art accuracies.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/agarwala14.pdf",
        "supp": "",
        "pdf_size": 366708,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1859091430070782368&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Microsoft Research New York, NY; Microsoft Research Cambridge, MA; Microsoft Cloud and Information Services Lab, Redmond, WA; College of Computing, Georgia Tech, Atlanta, Georgia; Computer Science Department, Stanford University, CA",
        "aff_domain": "microsoft.com;microsoft.com;microsoft.com;cc.gatech.edu;stanford.edu",
        "email": "microsoft.com;microsoft.com;microsoft.com;cc.gatech.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;2",
        "aff_unique_norm": "Microsoft;Georgia Institute of Technology;Stanford University",
        "aff_unique_dep": "Microsoft Research;College of Computing;Computer Science Department",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/group/microsoft-research-new-york;https://www.gatech.edu;https://www.stanford.edu",
        "aff_unique_abbr": "MSR NY;Georgia Tech;Stanford",
        "aff_campus_unique_index": "0;1;2;3;4",
        "aff_campus_unique": "New York;Cambridge;Redmond;Atlanta;Stanford",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3ac101f0b8",
        "title": "Linear Programming for Large-Scale Markov Decision Problems",
        "site": "https://proceedings.mlr.press/v32/malek14.html",
        "author": "Alan Malek; Yasin Abbasi-Yadkori; Peter Bartlett",
        "abstract": "We consider the problem of controlling a Markov decision  process (MDP) with a large state space, so as to minimize average cost.  Since it is intractable to compete with the optimal policy for large  scale problems, we pursue the more modest goal of competing with a  low-dimensional family of policies. We use the dual linear programming  formulation of the MDP average cost problem, in which the variable is  a stationary distribution over state-action pairs, and we consider a  neighborhood of a low-dimensional subset of the set of stationary  distributions (defined in terms of state-action features) as  the comparison class.  We propose two techniques, one based on stochastic convex optimization,  and one based on constraint sampling. In both cases, we give bounds  that show that the performance of our algorithms approaches the best  achievable by any policy in the comparison class. Most importantly,  these results depend on the size of the comparison class, but not  on the size of the state space.  Preliminary experiments  show the effectiveness of the proposed algorithms in a queuing  application.",
        "bibtex": "@InProceedings{pmlr-v32-malek14,\n  title = \t {Linear Programming for Large-Scale Markov Decision Problems},\n  author = \t {Malek, Alan and Abbasi-Yadkori, Yasin and Bartlett, Peter},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {496--504},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/malek14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/malek14.html},\n  abstract = \t {We consider the problem of controlling a Markov decision  process (MDP) with a large state space, so as to minimize average cost.  Since it is intractable to compete with the optimal policy for large  scale problems, we pursue the more modest goal of competing with a  low-dimensional family of policies. We use the dual linear programming  formulation of the MDP average cost problem, in which the variable is  a stationary distribution over state-action pairs, and we consider a  neighborhood of a low-dimensional subset of the set of stationary  distributions (defined in terms of state-action features) as  the comparison class.  We propose two techniques, one based on stochastic convex optimization,  and one based on constraint sampling. In both cases, we give bounds  that show that the performance of our algorithms approaches the best  achievable by any policy in the comparison class. Most importantly,  these results depend on the size of the comparison class, but not  on the size of the state space.  Preliminary experiments  show the effectiveness of the proposed algorithms in a queuing  application.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/malek14.pdf",
        "supp": "",
        "pdf_size": 407283,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=644870930972619904&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Queensland University of Technology, Brisbane, QLD, Australia 4000; University of California, Berkeley, CA 94720 + Queensland University of Technology, Brisbane, QLD, Australia 4000; University of California, Berkeley, CA 94720",
        "aff_domain": "QUT.EDU.AU;EECS.BERKELEY.EDU;EECS.BERKELEY.EDU",
        "email": "QUT.EDU.AU;EECS.BERKELEY.EDU;EECS.BERKELEY.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;1",
        "aff_unique_norm": "Queensland University of Technology;University of California, Berkeley",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.qut.edu.au;https://www.berkeley.edu",
        "aff_unique_abbr": "QUT;UC Berkeley",
        "aff_campus_unique_index": "0;1+0;1",
        "aff_campus_unique": "Brisbane;Berkeley",
        "aff_country_unique_index": "0;1+0;1",
        "aff_country_unique": "Australia;United States"
    },
    {
        "id": "9fc59d2e43",
        "title": "Linear Time Solver for Primal SVM",
        "site": "https://proceedings.mlr.press/v32/niea14.html",
        "author": "Feiping Nie; Yizhen Huang; Heng Huang",
        "abstract": "Support Vector Machines (SVM) is among the most popular classification techniques in machine learning, hence designing fast primal SVM algorithms for large-scale datasets is a hot topic in recent years. This paper presents a new L2-norm regularized primal SVM solver using Augmented Lagrange Multipliers, with linear-time computational cost for Lp-norm loss functions. The most computationally intensive steps (that determine the algorithmic complexity) of the proposed algorithm is purely and simply matrix-by-vector multiplication, which can be easily parallelized on a multi-core server for parallel computing. We implement and integrate our algorithm into the interfaces and framework of the well-known LibLinear software toolbox. Experiments show that our algorithm is with stable performance and on average faster than the state-of-the-art solvers such as SVMperf , Pegasos and the LibLinear that integrates the TRON, PCD and DCD algorithms.",
        "bibtex": "@InProceedings{pmlr-v32-niea14,\n  title = \t {Linear Time Solver for Primal SVM},\n  author = \t {Nie, Feiping and Huang, Yizhen and Huang, Heng},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {505--513},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/niea14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/niea14.html},\n  abstract = \t {Support Vector Machines (SVM) is among the most popular classification techniques in machine learning, hence designing fast primal SVM algorithms for large-scale datasets is a hot topic in recent years. This paper presents a new L2-norm regularized primal SVM solver using Augmented Lagrange Multipliers, with linear-time computational cost for Lp-norm loss functions. The most computationally intensive steps (that determine the algorithmic complexity) of the proposed algorithm is purely and simply matrix-by-vector multiplication, which can be easily parallelized on a multi-core server for parallel computing. We implement and integrate our algorithm into the interfaces and framework of the well-known LibLinear software toolbox. Experiments show that our algorithm is with stable performance and on average faster than the state-of-the-art solvers such as SVMperf , Pegasos and the LibLinear that integrates the TRON, PCD and DCD algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/niea14.pdf",
        "supp": "",
        "pdf_size": 188342,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3556634551519068026&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "db7b75277a",
        "title": "Linear and Parallel Learning of Markov Random Fields",
        "site": "https://proceedings.mlr.press/v32/mizrahi14.html",
        "author": "Yariv Mizrahi; Misha Denil; Nando De Freitas",
        "abstract": "We introduce a new embarrassingly parallel parameter learning algorithm for Markov random fields which is efficient for a large class of practical models.  Our algorithm parallelizes naturally over cliques and, for graphs of bounded degree, its complexity is linear in the number of cliques. Unlike its competitors, our algorithm is fully parallel and for log-linear models it is also data efficient, requiring only the local sufficient statistics of the data to estimate parameters.",
        "bibtex": "@InProceedings{pmlr-v32-mizrahi14,\n  title = \t {Linear and Parallel Learning of Markov Random Fields},\n  author = \t {Mizrahi, Yariv and Denil, Misha and Freitas, Nando De},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {199--207},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/mizrahi14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/mizrahi14.html},\n  abstract = \t {We introduce a new embarrassingly parallel parameter learning algorithm for Markov random fields which is efficient for a large class of practical models.  Our algorithm parallelizes naturally over cliques and, for graphs of bounded degree, its complexity is linear in the number of cliques. Unlike its competitors, our algorithm is fully parallel and for log-linear models it is also data efficient, requiring only the local sufficient statistics of the data to estimate parameters.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/mizrahi14.pdf",
        "supp": "",
        "pdf_size": 609360,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:HR430xjnCN4J:scholar.google.com/&scioq=Linear+and+Parallel+Learning+of+Markov+Random+Fields&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "University of British Columbia, Canada; University of Oxford, United Kingdom; University of British Columbia, Canada + University of Oxford, United Kingdom + Canadian Institute for Advanced Research, CIFAR NCAP Program",
        "aff_domain": "MATH.UBC.CA;CS.OX.AC.UK;CS.OX.AC.UK",
        "email": "MATH.UBC.CA;CS.OX.AC.UK;CS.OX.AC.UK",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0+1+2",
        "aff_unique_norm": "University of British Columbia;University of Oxford;Canadian Institute for Advanced Research",
        "aff_unique_dep": ";;CIFAR NCAP Program",
        "aff_unique_url": "https://www.ubc.ca;https://www.ox.ac.uk;https://www.cifar.ca",
        "aff_unique_abbr": "UBC;Oxford;CIFAR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0+1+0",
        "aff_country_unique": "Canada;United Kingdom"
    },
    {
        "id": "36a0c1aa03",
        "title": "Local Ordinal Embedding",
        "site": "https://proceedings.mlr.press/v32/terada14.html",
        "author": "Yoshikazu Terada; Ulrike Luxburg",
        "abstract": "We study the problem of ordinal embedding: given a set of ordinal constraints of the form distance(i,j) < distance(k,l) for some_quadruples (i,j,k,l) of indices, the goal is to construct a point configuration \\hat\\bmx_1, ..., \\hat\\bmx_n in \\R^p that preserves these constraints as well as possible. Our first contribution is to suggest a simple new algorithm for this problem, Soft Ordinal Embedding. The key feature of the algorithm is that it recovers not only the ordinal constraints, but even the density structure of the underlying data set. As our second contribution we prove that in the large sample limit it is enough to know \u201clocal ordinal information\u201d in order to perfectly reconstruct a given point configuration. This leads to our Local Ordinal Embedding algorithm, which can also be used for graph drawing.",
        "bibtex": "@InProceedings{pmlr-v32-terada14,\n  title = \t {Local Ordinal Embedding},\n  author = \t {Terada, Yoshikazu and Luxburg, Ulrike},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {847--855},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/terada14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/terada14.html},\n  abstract = \t {We study the problem of ordinal embedding: given a set of ordinal constraints of the form distance(i,j) < distance(k,l) for some_quadruples (i,j,k,l) of indices, the goal is to construct a point configuration \\hat\\bmx_1, ..., \\hat\\bmx_n in \\R^p that preserves these constraints as well as possible. Our first contribution is to suggest a simple new algorithm for this problem, Soft Ordinal Embedding. The key feature of the algorithm is that it recovers not only the ordinal constraints, but even the density structure of the underlying data set. As our second contribution we prove that in the large sample limit it is enough to know \u201clocal ordinal information\u201d in order to perfectly reconstruct a given point configuration. This leads to our Local Ordinal Embedding algorithm, which can also be used for graph drawing.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/terada14.pdf",
        "supp": "",
        "pdf_size": 1984827,
        "gs_citation": 104,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7476338504840907348&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Graduate School of Engineering Science, Osaka University, Japan + CiNet, National Institute of Information and Communications Technology, Japan; Department of Computer Science, University of Hamburg, Germany",
        "aff_domain": "sigmath.es.osaka-u.ac.jp;informatik.uni-hamburg.de",
        "email": "sigmath.es.osaka-u.ac.jp;informatik.uni-hamburg.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "Osaka University;National Institute of Information and Communications Technology;University of Hamburg",
        "aff_unique_dep": "Graduate School of Engineering Science;CiNet;Department of Computer Science",
        "aff_unique_url": "https://www.osaka-u.ac.jp;https://www.nict.go.jp/en/;https://www.uni-hamburg.de",
        "aff_unique_abbr": "Osaka U;NICT;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Osaka;",
        "aff_country_unique_index": "0+0;1",
        "aff_country_unique": "Japan;Germany"
    },
    {
        "id": "7b2410e50e",
        "title": "Local algorithms for interactive clustering",
        "site": "https://proceedings.mlr.press/v32/awasthi14.html",
        "author": "Pranjal Awasthi; Maria Balcan; Konstantin Voevodski",
        "abstract": "We study the design of interactive clustering algorithms  for data sets satisfying natural stability  assumptions. Our algorithms start with any initial clustering  and only make local changes in each step; both are desirable features in many applications.  We show that in this constrained setting one can still design provably efficient algorithms that produce  accurate clusterings.  We also show that our algorithms perform well on real-world data.",
        "bibtex": "@InProceedings{pmlr-v32-awasthi14,\n  title = \t {Local algorithms for interactive clustering},\n  author = \t {Awasthi, Pranjal and Balcan, Maria and Voevodski, Konstantin},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {550--558},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/awasthi14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/awasthi14.html},\n  abstract = \t {We study the design of interactive clustering algorithms  for data sets satisfying natural stability  assumptions. Our algorithms start with any initial clustering  and only make local changes in each step; both are desirable features in many applications.  We show that in this constrained setting one can still design provably efficient algorithms that produce  accurate clusterings.  We also show that our algorithms perform well on real-world data.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/awasthi14.pdf",
        "supp": "",
        "pdf_size": 991316,
        "gs_citation": 112,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9572362603055131345&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Carnegie Mellon University, Pittsburgh, USA; Georgia Institute of Technology, Atlanta, USA; Google, NY, USA",
        "aff_domain": "CS.CMU.EDU;CC.GATECH.EDU;GOOGLE.COM",
        "email": "CS.CMU.EDU;CC.GATECH.EDU;GOOGLE.COM",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Carnegie Mellon University;Georgia Institute of Technology;Google",
        "aff_unique_dep": ";;Google",
        "aff_unique_url": "https://www.cmu.edu;https://www.gatech.edu;https://www.google.com",
        "aff_unique_abbr": "CMU;Georgia Tech;Google",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "Pittsburgh;Atlanta;New York",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e74697c2e1",
        "title": "Low-density Parity Constraints for Hashing-Based Discrete Integration",
        "site": "https://proceedings.mlr.press/v32/ermon14.html",
        "author": "Stefano Ermon; Carla Gomes; Ashish Sabharwal; Bart Selman",
        "abstract": "In recent years, a number of probabilistic inference and counting techniques have been proposed that exploit pairwise independent hash functions to infer properties of succinctly defined high-dimensional sets. While providing desirable statistical guarantees, typical constructions of such hash functions are themselves not amenable to efficient inference. Inspired by the success of LDPC codes, we propose the use of low-density parity constraints to make inference more tractable in practice. While not strongly universal, we show that such sparse constraints belong to a new class of hash functions that we call Average Universal. These weaker hash functions retain the desirable statistical guarantees needed by most such probabilistic inference methods. Thus, they continue to provide provable accuracy guarantees while at the same time making a number of algorithms significantly more scalable in practice. Using this technique, we provide new, tighter bounds for challenging discrete integration and model counting problems.",
        "bibtex": "@InProceedings{pmlr-v32-ermon14,\n  title = \t {Low-density Parity Constraints for Hashing-Based Discrete Integration},\n  author = \t {Ermon, Stefano and Gomes, Carla and Sabharwal, Ashish and Selman, Bart},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {271--279},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/ermon14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/ermon14.html},\n  abstract = \t {In recent years, a number of probabilistic inference and counting techniques have been proposed that exploit pairwise independent hash functions to infer properties of succinctly defined high-dimensional sets. While providing desirable statistical guarantees, typical constructions of such hash functions are themselves not amenable to efficient inference. Inspired by the success of LDPC codes, we propose the use of low-density parity constraints to make inference more tractable in practice. While not strongly universal, we show that such sparse constraints belong to a new class of hash functions that we call Average Universal. These weaker hash functions retain the desirable statistical guarantees needed by most such probabilistic inference methods. Thus, they continue to provide provable accuracy guarantees while at the same time making a number of algorithms significantly more scalable in practice. Using this technique, we provide new, tighter bounds for challenging discrete integration and model counting problems.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/ermon14.pdf",
        "supp": "",
        "pdf_size": 393054,
        "gs_citation": 76,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5776527988679793481&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Dept. of Computer Science, Cornell University, Ithaca NY 14853, U.S.A.; Dept. of Computer Science, Cornell University, Ithaca NY 14853, U.S.A.; IBM Watson Research Center, Yorktown Heights, NY 10598, U.S.A.; Dept. of Computer Science, Cornell University, Ithaca NY 14853, U.S.A.",
        "aff_domain": "CS.CORNELL.EDU;CS.CORNELL.EDU;US.IBM.COM;CS.CORNELL.EDU",
        "email": "CS.CORNELL.EDU;CS.CORNELL.EDU;US.IBM.COM;CS.CORNELL.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Cornell University;IBM",
        "aff_unique_dep": "Department of Computer Science;IBM Watson Research Center",
        "aff_unique_url": "https://www.cornell.edu;https://www.ibm.com/watson",
        "aff_unique_abbr": "Cornell;IBM Watson",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Ithaca;Yorktown Heights",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c7bd9488a9",
        "title": "Lower Bounds for the Gibbs Sampler over Mixtures of Gaussians",
        "site": "https://proceedings.mlr.press/v32/tosh14.html",
        "author": "Christopher Tosh; Sanjoy Dasgupta",
        "abstract": "The mixing time of a Markov chain is the minimum time t necessary for the total variation distance between the distribution of the Markov chain\u2019s current state X_t and its stationary distribution to fall below some \u03b5> 0. In this paper, we present lower bounds for the mixing time of the Gibbs sampler over Gaussian mixture models with Dirichlet priors.",
        "bibtex": "@InProceedings{pmlr-v32-tosh14,\n  title = \t {Lower Bounds for the Gibbs Sampler over Mixtures of Gaussians},\n  author = \t {Tosh, Christopher and Dasgupta, Sanjoy},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1467--1475},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/tosh14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/tosh14.html},\n  abstract = \t {The mixing time of a Markov chain is the minimum time t necessary for the total variation distance between the distribution of the Markov chain\u2019s current state X_t and its stationary distribution to fall below some \u03b5> 0. In this paper, we present lower bounds for the mixing time of the Gibbs sampler over Gaussian mixture models with Dirichlet priors.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/tosh14.pdf",
        "supp": "",
        "pdf_size": 861776,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2937521776385599631&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science and Engineering, UCSD; Department of Computer Science and Engineering, UCSD",
        "aff_domain": "CS.UCSD.EDU;CS.UCSD.EDU",
        "email": "CS.UCSD.EDU;CS.UCSD.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "La Jolla",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "38a2366c2c",
        "title": "Making Fisher Discriminant Analysis Scalable",
        "site": "https://proceedings.mlr.press/v32/tu14.html",
        "author": "Bojun Tu; Zhihua Zhang; Shusen Wang; Hui Qian",
        "abstract": "The Fisher linear discriminant analysis (LDA) is a classical method for classification and dimension reduction jointly. A major limitation of the conventional LDA is a so-called singularity issue. Many LDA variants, especially two-stage methods such as PCA+LDA and LDA/QR,  were proposed to solve this issue. In the two-stage methods, an intermediate stage for dimension reduction is developed before  the actual LDA method works. These two-stage methods are scalable because they are an approximate alternative of the LDA method. However, there is no theoretical analysis on how well they approximate the conventional LDA problem. In this paper we present theoretical analysis on the approximation error of a two-stage algorithm. Accordingly, we develop a new two-stage algorithm. Furthermore, we resort to a random projection approach, making our algorithm scalable. We also provide an implemention on distributed system to handle large scale problems. Our algorithm takes LDA/QR as its special case, and outperforms PCA+LDA while having a similar scalability. We also generalize our algorithm to kernel discriminant analysis, a nonlinear version of the classical LDA. Extensive experiments show that our algorithms outperform PCA+LDA and have a similar scalability with it.",
        "bibtex": "@InProceedings{pmlr-v32-tu14,\n  title = \t {Making Fisher Discriminant Analysis Scalable},\n  author = \t {Tu, Bojun and Zhang, Zhihua and Wang, Shusen and Qian, Hui},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {964--972},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/tu14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/tu14.html},\n  abstract = \t {The Fisher linear discriminant analysis (LDA) is a classical method for classification and dimension reduction jointly. A major limitation of the conventional LDA is a so-called singularity issue. Many LDA variants, especially two-stage methods such as PCA+LDA and LDA/QR,  were proposed to solve this issue. In the two-stage methods, an intermediate stage for dimension reduction is developed before  the actual LDA method works. These two-stage methods are scalable because they are an approximate alternative of the LDA method. However, there is no theoretical analysis on how well they approximate the conventional LDA problem. In this paper we present theoretical analysis on the approximation error of a two-stage algorithm. Accordingly, we develop a new two-stage algorithm. Furthermore, we resort to a random projection approach, making our algorithm scalable. We also provide an implemention on distributed system to handle large scale problems. Our algorithm takes LDA/QR as its special case, and outperforms PCA+LDA while having a similar scalability. We also generalize our algorithm to kernel discriminant analysis, a nonlinear version of the classical LDA. Extensive experiments show that our algorithms outperform PCA+LDA and have a similar scalability with it.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/tu14.pdf",
        "supp": "",
        "pdf_size": 434527,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12175731186506905816&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science & Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science & Engineering, Shanghai Jiao Tong University, Shanghai, China; College of Computer Science & Technology, Zhejiang University, Hangzhou, China; College of Computer Science & Technology, Zhejiang University, Hangzhou, China",
        "aff_domain": "gmail.com;sjtu.edu.cn;zju.edu.cn;zju.edu.cn",
        "email": "gmail.com;sjtu.edu.cn;zju.edu.cn;zju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;1",
        "aff_unique_norm": "Shanghai Jiao Tong University;Zhejiang University",
        "aff_unique_dep": "Department of Computer Science & Engineering;College of Computer Science & Technology",
        "aff_unique_url": "https://www.sjtu.edu.cn;http://www.zju.edu.cn",
        "aff_unique_abbr": "SJTU;ZJU",
        "aff_campus_unique_index": "0;0;1;1",
        "aff_campus_unique": "Shanghai;Hangzhou",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "b668f17afa",
        "title": "Making the Most of Bag of Words: Sentence Regularization with Alternating Direction Method of Multipliers",
        "site": "https://proceedings.mlr.press/v32/yogatama14.html",
        "author": "Dani Yogatama; Noah Smith",
        "abstract": "In many high-dimensional learning problems, only some parts of an observation are important to the prediction task; for example, the cues to correctly categorizing a document may lie in a handful of its sentences. We introduce a learning algorithm that exploits this intuition by encoding it in a regularizer.  Specifically, we apply the sparse overlapping group lasso with one group for every bundle of features occurring together in a training-data sentence, leading to thousands to millions of overlapping groups. We show how to efficiently solve the resulting optimization challenge using the alternating directions method of multipliers.  We find that the resulting method significantly outperforms competitive baselines (standard ridge, lasso, and elastic net regularizers) on a suite of real-world text categorization problems.",
        "bibtex": "@InProceedings{pmlr-v32-yogatama14,\n  title = \t {Making the Most of Bag of Words: Sentence Regularization with Alternating Direction Method of Multipliers},\n  author = \t {Yogatama, Dani and Smith, Noah},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {656--664},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/yogatama14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/yogatama14.html},\n  abstract = \t {In many high-dimensional learning problems, only some parts of an observation are important to the prediction task; for example, the cues to correctly categorizing a document may lie in a handful of its sentences. We introduce a learning algorithm that exploits this intuition by encoding it in a regularizer.  Specifically, we apply the sparse overlapping group lasso with one group for every bundle of features occurring together in a training-data sentence, leading to thousands to millions of overlapping groups. We show how to efficiently solve the resulting optimization challenge using the alternating directions method of multipliers.  We find that the resulting method significantly outperforms competitive baselines (standard ridge, lasso, and elastic net regularizers) on a suite of real-world text categorization problems.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/yogatama14.pdf",
        "supp": "",
        "pdf_size": 250387,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18260692760065701708&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Language Technologies Institute, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213 USA; Language Technologies Institute, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213 USA",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Language Technologies Institute, School of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6f7d646c26",
        "title": "Marginal Structured SVM with Hidden Variables",
        "site": "https://proceedings.mlr.press/v32/ping14.html",
        "author": "Wei Ping; Qiang Liu; Alex Ihler",
        "abstract": "In this work, we propose the marginal structured SVM (MSSVM) for structured prediction with hidden variables. MSSVM properly accounts for the uncertainty of hidden variables, and can significantly outperform the previously proposed latent structured SVM (LSSVM; Yu & Joachims (2009)) and other state-of-art methods, especially when that uncertainty is large. Our method also results in a smoother objective function, making gradient-based optimization of MSSVMs converge significantly faster than for LSSVMs. We also show that our method consistently outperforms hidden conditional random fields (HCRFs; Quattoni et al. (2007)) on both simulated and real-world datasets. Furthermore, we propose a unified framework that includes both our and several other existing methods as special cases, and provides insights into the comparison of different models in practice.",
        "bibtex": "@InProceedings{pmlr-v32-ping14,\n  title = \t {Marginal Structured SVM with Hidden Variables},\n  author = \t {Ping, Wei and Liu, Qiang and Ihler, Alex},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {190--198},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/ping14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/ping14.html},\n  abstract = \t {In this work, we propose the marginal structured SVM (MSSVM) for structured prediction with hidden variables. MSSVM properly accounts for the uncertainty of hidden variables, and can significantly outperform the previously proposed latent structured SVM (LSSVM; Yu & Joachims (2009)) and other state-of-art methods, especially when that uncertainty is large. Our method also results in a smoother objective function, making gradient-based optimization of MSSVMs converge significantly faster than for LSSVMs. We also show that our method consistently outperforms hidden conditional random fields (HCRFs; Quattoni et al. (2007)) on both simulated and real-world datasets. Furthermore, we propose a unified framework that includes both our and several other existing methods as special cases, and provides insights into the comparison of different models in practice.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/ping14.pdf",
        "supp": "",
        "pdf_size": 361121,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9724451546216652441&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Computer Science, University of California, Irvine; Department of Computer Science, University of California, Irvine; Department of Computer Science, University of California, Irvine",
        "aff_domain": "ICS.UCI.EDU;ICS.UCI.EDU;ICS.UCI.EDU",
        "email": "ICS.UCI.EDU;ICS.UCI.EDU;ICS.UCI.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Irvine",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uci.edu",
        "aff_unique_abbr": "UCI",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Irvine",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b0b270a4b1",
        "title": "Marginalized Denoising Auto-encoders for Nonlinear Representations",
        "site": "https://proceedings.mlr.press/v32/cheng14.html",
        "author": "Minmin Chen; Kilian Weinberger; Fei Sha; Yoshua Bengio",
        "abstract": "Denoising auto-encoders (DAEs) have been successfully  used to learn new representations for a  wide range of machine learning tasks. During  training, DAEs make many passes over the training  dataset and reconstruct it from partial corruption  generated from a pre-specified corrupting  distribution. This process learns robust representation,  though at the expense of requiring many  training epochs, in which the data is explicitly  corrupted. In this paper we present the marginalized  Denoising Auto-encoder (mDAE), which  (approximately) marginalizes out the corruption  during training. Effectively, the mDAE takes  into account infinitely many corrupted copies of  the training data in every epoch, and therefore is  able to match or outperform the DAE with much  fewer training epochs. We analyze our proposed  algorithm and show that it can be understood as  a classic auto-encoder with a special form of regularization.  In empirical evaluations we show  that it attains 1-2 order-of-magnitude speedup in  training time over other competing approaches.",
        "bibtex": "@InProceedings{pmlr-v32-cheng14,\n  title = \t {Marginalized Denoising Auto-encoders for Nonlinear Representations},\n  author = \t {Chen, Minmin and Weinberger, Kilian and Sha, Fei and Bengio, Yoshua},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1476--1484},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/cheng14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/cheng14.html},\n  abstract = \t {Denoising auto-encoders (DAEs) have been successfully  used to learn new representations for a  wide range of machine learning tasks. During  training, DAEs make many passes over the training  dataset and reconstruct it from partial corruption  generated from a pre-specified corrupting  distribution. This process learns robust representation,  though at the expense of requiring many  training epochs, in which the data is explicitly  corrupted. In this paper we present the marginalized  Denoising Auto-encoder (mDAE), which  (approximately) marginalizes out the corruption  during training. Effectively, the mDAE takes  into account infinitely many corrupted copies of  the training data in every epoch, and therefore is  able to match or outperform the DAE with much  fewer training epochs. We analyze our proposed  algorithm and show that it can be understood as  a classic auto-encoder with a special form of regularization.  In empirical evaluations we show  that it attains 1-2 order-of-magnitude speedup in  training time over other competing approaches.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/cheng14.pdf",
        "supp": "",
        "pdf_size": 1096110,
        "gs_citation": 1130,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1549955917453958823&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "Criteo; Washington University in St. Louis; University of Southern California; Universit \u00b4e de Montr \u00b4eal, Canadian Institute for Advanced Research",
        "aff_domain": "CRITEO.COM;WUSTL.EDU;USC.EDU; ",
        "email": "CRITEO.COM;WUSTL.EDU;USC.EDU; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "Criteo;Washington University in St. Louis;University of Southern California;Universit\u00e9 de Montr\u00e9al",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.criteo.com;https://wustl.edu;https://www.usc.edu;https://www.umontreal.ca",
        "aff_unique_abbr": "Criteo;WashU;USC;UdeM",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";St. Louis;Los Angeles",
        "aff_country_unique_index": "0;1;1;2",
        "aff_country_unique": "France;United States;Canada"
    },
    {
        "id": "94147dd581",
        "title": "Margins, Kernels and Non-linear Smoothed Perceptrons",
        "site": "https://proceedings.mlr.press/v32/ramdas14.html",
        "author": "Aaditya Ramdas; Javier Pe\u00f1a",
        "abstract": "We focus on the problem of finding a non-linear classification function that lies in a Reproducing Kernel Hilbert Space (RKHS) both from the primal point of view (finding a perfect separator when one exists) and the dual point of view (giving a certificate of non-existence), with special focus on generalizations of two classical schemes - the Perceptron (primal) and Von-Neumann (dual) algorithms.   We cast our problem as one of maximizing the regularized normalized hard-margin (\u03c1) in an RKHS and use the Representer Theorem to  rephrase it in terms of a Mahalanobis dot-product/semi-norm associated with the kernel\u2019s (normalized and signed) Gram matrix. We derive an accelerated smoothed algorithm with a convergence rate of \\tfrac\\sqrt \\log n\u03c1 given n separable points, which is strikingly similar to the classical kernelized Perceptron algorithm whose rate is \\tfrac1\u03c1^2. When no such classifier exists, we prove a version of Gordan\u2019s separation theorem for RKHSs, and give a reinterpretation of negative margins. This allows us to give guarantees for a primal-dual algorithm that halts in \\min{\\tfrac\\sqrt n|\u03c1|, \\tfrac\\sqrt n\u03b5} iterations with a perfect separator in the RKHS if the primal is feasible or a dual \u03b5-certificate of near-infeasibility.",
        "bibtex": "@InProceedings{pmlr-v32-ramdas14,\n  title = \t {Margins, Kernels and Non-linear Smoothed Perceptrons},\n  author = \t {Ramdas, Aaditya and Pe\u00f1a, Javier},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {244--252},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/ramdas14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/ramdas14.html},\n  abstract = \t {We focus on the problem of finding a non-linear classification function that lies in a Reproducing Kernel Hilbert Space (RKHS) both from the primal point of view (finding a perfect separator when one exists) and the dual point of view (giving a certificate of non-existence), with special focus on generalizations of two classical schemes - the Perceptron (primal) and Von-Neumann (dual) algorithms.   We cast our problem as one of maximizing the regularized normalized hard-margin (\u03c1) in an RKHS and use the Representer Theorem to  rephrase it in terms of a Mahalanobis dot-product/semi-norm associated with the kernel\u2019s (normalized and signed) Gram matrix. We derive an accelerated smoothed algorithm with a convergence rate of \\tfrac\\sqrt \\log n\u03c1 given n separable points, which is strikingly similar to the classical kernelized Perceptron algorithm whose rate is \\tfrac1\u03c1^2. When no such classifier exists, we prove a version of Gordan\u2019s separation theorem for RKHSs, and give a reinterpretation of negative margins. This allows us to give guarantees for a primal-dual algorithm that halts in \\min{\\tfrac\\sqrt n|\u03c1|, \\tfrac\\sqrt n\u03b5} iterations with a perfect separator in the RKHS if the primal is feasible or a dual \u03b5-certificate of near-infeasibility.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/ramdas14.pdf",
        "supp": "",
        "pdf_size": 218397,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18078422867677096338&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, PA 15213 USA; Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, PA 15213 USA",
        "aff_domain": "CS.CMU.EDU;ANDREW.CMU.EDU",
        "email": "CS.CMU.EDU;ANDREW.CMU.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3247065064",
        "title": "Max-Margin Infinite Hidden Markov Models",
        "site": "https://proceedings.mlr.press/v32/zhangb14.html",
        "author": "Aonan Zhang; Jun Zhu; Bo Zhang",
        "abstract": "Infinite hidden Markov models (iHMMs) are nonparametric Bayesian extensions of hidden Markov models (HMMs) with an infinite number of states. Though flexible in describing sequential data, the generative formulation of iHMMs could limit their discriminative ability in sequential prediction tasks. Our paper introduces max-margin infinite HMMs (M2iHMMs), new infinite HMMs that explore the max-margin principle for discriminative learning. By using the theory of Gibbs classifiers and data augmentation, we develop efficient beam sampling algorithms without making restricting mean-field assumptions or truncated approximation. For single variate classification, M2iHMMs reduce to a new formulation of DP mixtures of max-margin machines. Empirical results on synthetic and real data sets show that our methods obtain superior performance than other competitors in both single variate classification and sequential prediction tasks.",
        "bibtex": "@InProceedings{pmlr-v32-zhangb14,\n  title = \t {Max-Margin Infinite Hidden Markov Models},\n  author = \t {Zhang, Aonan and Zhu, Jun and Zhang, Bo},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {315--323},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/zhangb14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/zhangb14.html},\n  abstract = \t {Infinite hidden Markov models (iHMMs) are nonparametric Bayesian extensions of hidden Markov models (HMMs) with an infinite number of states. Though flexible in describing sequential data, the generative formulation of iHMMs could limit their discriminative ability in sequential prediction tasks. Our paper introduces max-margin infinite HMMs (M2iHMMs), new infinite HMMs that explore the max-margin principle for discriminative learning. By using the theory of Gibbs classifiers and data augmentation, we develop efficient beam sampling algorithms without making restricting mean-field assumptions or truncated approximation. For single variate classification, M2iHMMs reduce to a new formulation of DP mixtures of max-margin machines. Empirical results on synthetic and real data sets show that our methods obtain superior performance than other competitors in both single variate classification and sequential prediction tasks.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/zhangb14.pdf",
        "supp": "",
        "pdf_size": 399222,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4910827705113277539&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Dept. of Comp. Sci. & Tech., TNList Lab, State Key Lab of Intell. Tech. & Sys., Tsinghua University, China; Dept. of Comp. Sci. & Tech., TNList Lab, State Key Lab of Intell. Tech. & Sys., Tsinghua University, China; Dept. of Comp. Sci. & Tech., TNList Lab, State Key Lab of Intell. Tech. & Sys., Tsinghua University, China",
        "aff_domain": "tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn",
        "email": "tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Tsinghua University",
        "aff_unique_dep": "Dept. of Comp. Sci. & Tech.",
        "aff_unique_url": "https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "THU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "56da9081c7",
        "title": "Maximum Margin Multiclass Nearest Neighbors",
        "site": "https://proceedings.mlr.press/v32/kontorovichb14.html",
        "author": "Aryeh Kontorovich; Roi Weiss",
        "abstract": "We develop a general framework for margin-based multicategory classification in metric spaces. The basic work-horse is a margin-regularized version of the nearest-neighbor classifier. We prove generalization bounds that match the state of the art in sample size n and significantly improve the dependence on the number of classes k. Our point of departure is a nearly Bayes-optimal finite-sample risk bound independent of k. Although k-free, this bound is unregularized and non-adaptive, which motivates our main result: Rademacher and scale-sensitive margin bounds with a logarithmic dependence on k. As the best previous risk estimates  in this setting were of order \\sqrt k, our bound is exponentially sharper. From the algorithmic standpoint, in doubling metric spaces our classifier may be trained on n examples in  O(n^2\\log n) time and evaluated on new points in O(\\log n) time.",
        "bibtex": "@InProceedings{pmlr-v32-kontorovichb14,\n  title = \t {Maximum Margin Multiclass Nearest Neighbors},\n  author = \t {Kontorovich, Aryeh and Weiss, Roi},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {892--900},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/kontorovichb14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/kontorovichb14.html},\n  abstract = \t {We develop a general framework for margin-based multicategory classification in metric spaces. The basic work-horse is a margin-regularized version of the nearest-neighbor classifier. We prove generalization bounds that match the state of the art in sample size n and significantly improve the dependence on the number of classes k. Our point of departure is a nearly Bayes-optimal finite-sample risk bound independent of k. Although k-free, this bound is unregularized and non-adaptive, which motivates our main result: Rademacher and scale-sensitive margin bounds with a logarithmic dependence on k. As the best previous risk estimates  in this setting were of order \\sqrt k, our bound is exponentially sharper. From the algorithmic standpoint, in doubling metric spaces our classifier may be trained on n examples in  O(n^2\\log n) time and evaluated on new points in O(\\log n) time.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/kontorovichb14.pdf",
        "supp": "",
        "pdf_size": 376093,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7931722319345056460&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, Ben-Gurion University, Beer Sheva 84105, ISRAEL; Department of Computer Science, Ben-Gurion University, Beer Sheva 84105, ISRAEL",
        "aff_domain": "CS.BGU.AC.IL;CS.BGU.AC.IL",
        "email": "CS.BGU.AC.IL;CS.BGU.AC.IL",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Ben-Gurion University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.bgu.ac.il",
        "aff_unique_abbr": "BGU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Beer Sheva",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "2bc8af0538",
        "title": "Maximum Mean Discrepancy for Class Ratio Estimation: Convergence Bounds and Kernel Selection",
        "site": "https://proceedings.mlr.press/v32/iyer14.html",
        "author": "Arun Iyer; Saketha Nath; Sunita Sarawagi",
        "abstract": "In recent times, many real world applications have emerged that require estimates of class ratios in an unlabeled instance collection as opposed to labels of individual instances in the collection.  In this paper we investigate the use of maximum mean discrepancy (MMD) in a reproducing kernel Hilbert space (RKHS) for estimating such ratios. First, we theoretically analyze the MMD-based estimates. Our analysis establishes that, under some mild conditions, the estimate is statistically consistent. More importantly, it provides an upper bound on the error in the estimate in terms of intuitive geometric quantities like class separation and data spread. Next, we use the insights obtained from the theoretical analysis, to propose a novel convex formulation that automatically learns the kernel to be employed in the MMD-based estimation. We design an efficient cutting plane algorithm for solving this formulation.  Finally, we empirically compare our estimator with several existing methods, and show significantly improved performance under varying datasets, class ratios, and training sizes.",
        "bibtex": "@InProceedings{pmlr-v32-iyer14,\n  title = \t {Maximum Mean Discrepancy for Class Ratio Estimation: Convergence Bounds and Kernel Selection},\n  author = \t {Iyer, Arun and Nath, Saketha and Sarawagi, Sunita},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {530--538},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/iyer14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/iyer14.html},\n  abstract = \t {In recent times, many real world applications have emerged that require estimates of class ratios in an unlabeled instance collection as opposed to labels of individual instances in the collection.  In this paper we investigate the use of maximum mean discrepancy (MMD) in a reproducing kernel Hilbert space (RKHS) for estimating such ratios. First, we theoretically analyze the MMD-based estimates. Our analysis establishes that, under some mild conditions, the estimate is statistically consistent. More importantly, it provides an upper bound on the error in the estimate in terms of intuitive geometric quantities like class separation and data spread. Next, we use the insights obtained from the theoretical analysis, to propose a novel convex formulation that automatically learns the kernel to be employed in the MMD-based estimation. We design an efficient cutting plane algorithm for solving this formulation.  Finally, we empirically compare our estimator with several existing methods, and show significantly improved performance under varying datasets, class ratios, and training sizes.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/iyer14.pdf",
        "supp": "",
        "pdf_size": 546689,
        "gs_citation": 99,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13978753331420177117&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Yahoo! Labs, Bangalore, Karnataka 560071 INDIA; IIT Bombay, Powai, Mumbai, Maharashtra 400076 INDIA; IIT Bombay, Powai, Mumbai, Maharashtra 400076 INDIA",
        "aff_domain": "YAHOO-INC.COM;CSE.IITB.AC.IN;CSE.IITB.AC.IN",
        "email": "YAHOO-INC.COM;CSE.IITB.AC.IN;CSE.IITB.AC.IN",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Yahoo! Labs;Indian Institute of Technology Bombay",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://labs.yahoo.com;https://www.iitb.ac.in",
        "aff_unique_abbr": "Yahoo! Labs;IIT Bombay",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Bangalore;Powai",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "e26dda6776",
        "title": "Memory (and Time) Efficient Sequential Monte Carlo",
        "site": "https://proceedings.mlr.press/v32/jun14.html",
        "author": "Seong-Hwan Jun; Alexandre Bouchard-C\u00f4t\u00e9",
        "abstract": "Memory efficiency is an important issue in Sequential Monte Carlo (SMC) algorithms, arising for example in inference of high-dimensional latent variables via Rao-Blackwellized SMC algorithms, where the size of individual particles combined with the required number of particles can stress the main memory. Standard SMC methods have a memory requirement that scales linearly in the number of particles present at all stage of the algorithm.   Our contribution is a simple scheme that makes the memory cost of SMC methods depends on the number of distinct particles that survive resampling.   We show that this difference has a large empirical impact on the quality of the approximation in realistic scenarios, and also\u2014since memory access is generally slow\u2014on the running time.    The method is based on a two pass generation of the particles, which are represented implicitly in the first pass.   We parameterize the accuracy of our algorithm with a memory budget rather than with a fixed number of particles. Our algorithm adaptively selects an optimal number of particle to exploit this fixed memory budget. We show that this adaptation does not interfere with the usual consistency guarantees that come with SMC algorithms.",
        "bibtex": "@InProceedings{pmlr-v32-jun14,\n  title = \t {Memory (and Time) Efficient Sequential Monte Carlo},\n  author = \t {Jun, Seong-Hwan and Bouchard-C\u00f4t\u00e9, Alexandre},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {514--522},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/jun14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/jun14.html},\n  abstract = \t {Memory efficiency is an important issue in Sequential Monte Carlo (SMC) algorithms, arising for example in inference of high-dimensional latent variables via Rao-Blackwellized SMC algorithms, where the size of individual particles combined with the required number of particles can stress the main memory. Standard SMC methods have a memory requirement that scales linearly in the number of particles present at all stage of the algorithm.   Our contribution is a simple scheme that makes the memory cost of SMC methods depends on the number of distinct particles that survive resampling.   We show that this difference has a large empirical impact on the quality of the approximation in realistic scenarios, and also\u2014since memory access is generally slow\u2014on the running time.    The method is based on a two pass generation of the particles, which are represented implicitly in the first pass.   We parameterize the accuracy of our algorithm with a memory budget rather than with a fixed number of particles. Our algorithm adaptively selects an optimal number of particle to exploit this fixed memory budget. We show that this adaptation does not interfere with the usual consistency guarantees that come with SMC algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/jun14.pdf",
        "supp": "",
        "pdf_size": 839423,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13272520167579991635&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "The University of British Columbia, Vancouver, Canada; The University of British Columbia, Vancouver, Canada",
        "aff_domain": "STAT.UBC.CA;STAT.UBC.CA",
        "email": "STAT.UBC.CA;STAT.UBC.CA",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of British Columbia",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ubc.ca",
        "aff_unique_abbr": "UBC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Vancouver",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "625d828535",
        "title": "Memory Efficient Kernel Approximation",
        "site": "https://proceedings.mlr.press/v32/si14.html",
        "author": "Si Si; Cho-Jui Hsieh; Inderjit Dhillon",
        "abstract": "The scalability of kernel machines is a big challenge when facing millions of samples due to storage and computation issues for large kernel matrices, that are usually dense. Recently, many papers have suggested tackling this problem by using a low rank approximation of the kernel matrix. In this paper, we first make the observation that the structure of shift-invariant kernels changes from low-rank to block-diagonal (without any low-rank structure) when varying the scale parameter. Based on this observation, we propose a new kernel approximation algorithm \u2013 Memory Efficient Kernel Approximation (MEKA), which considers both low-rank and clustering structure of the kernel matrix. We show that the resulting algorithm outperforms state-of-the-art low-rank kernel approximation methods in terms of speed, approximation error, and memory usage. As an example, on the MNIST2M dataset with two-million samples, our method takes 550 seconds on a single machine using less than 500 MBytes memory to achieve 0.2313 test RMSE for kernel ridge regression, while standard Nystr\u00f6m approximation takes more than 2700 seconds and uses more than 2 GBytes memory on the same problem to achieve 0.2318 test RMSE.",
        "bibtex": "@InProceedings{pmlr-v32-si14,\n  title = \t {Memory Efficient Kernel Approximation},\n  author = \t {Si, Si and Hsieh, Cho-Jui and Dhillon, Inderjit},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {701--709},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/si14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/si14.html},\n  abstract = \t {The scalability of kernel machines is a big challenge when facing millions of samples due to storage and computation issues for large kernel matrices, that are usually dense. Recently, many papers have suggested tackling this problem by using a low rank approximation of the kernel matrix. In this paper, we first make the observation that the structure of shift-invariant kernels changes from low-rank to block-diagonal (without any low-rank structure) when varying the scale parameter. Based on this observation, we propose a new kernel approximation algorithm \u2013 Memory Efficient Kernel Approximation (MEKA), which considers both low-rank and clustering structure of the kernel matrix. We show that the resulting algorithm outperforms state-of-the-art low-rank kernel approximation methods in terms of speed, approximation error, and memory usage. As an example, on the MNIST2M dataset with two-million samples, our method takes 550 seconds on a single machine using less than 500 MBytes memory to achieve 0.2313 test RMSE for kernel ridge regression, while standard Nystr\u00f6m approximation takes more than 2700 seconds and uses more than 2 GBytes memory on the same problem to achieve 0.2318 test RMSE.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/si14.pdf",
        "supp": "",
        "pdf_size": 436411,
        "gs_citation": 208,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17313539301487590615&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computer Science, The University of Texas, Austin, TX 78721, USA; Department of Computer Science, The University of Texas, Austin, TX 78721, USA; Department of Computer Science, The University of Texas, Austin, TX 78721, USA",
        "aff_domain": "CS.UTEXAS.EDU;CS.UTEXAS.EDU;CS.UTEXAS.EDU",
        "email": "CS.UTEXAS.EDU;CS.UTEXAS.EDU;CS.UTEXAS.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e26a9b8ec9",
        "title": "Memory and Computation Efficient PCA via Very Sparse Random Projections",
        "site": "https://proceedings.mlr.press/v32/anaraki14.html",
        "author": "Farhad Pourkamali Anaraki; Shannon Hughes",
        "abstract": "Algorithms that can efficiently recover principal components in very high-dimensional, streaming, and/or distributed data settings have become an important topic in the literature. In this paper, we propose an approach to principal component estimation that utilizes projections onto very sparse random vectors with Bernoulli-generated nonzero entries. Indeed, our approach is simultaneously efficient in memory/storage space, efficient in computation, and produces accurate PC estimates, while also allowing for rigorous theoretical performance analysis. Moreover, one can tune the sparsity of the random vectors deliberately to achieve a desired point on the tradeoffs between memory, computation, and accuracy. We rigorously characterize these tradeoffs and provide statistical performance guarantees. In addition to these very sparse random vectors, our analysis also applies to more general random projections. We present experimental results demonstrating that this approach allows for simultaneously achieving a substantial reduction of the computational complexity and memory/storage space, with little loss in accuracy, particularly for very high-dimensional data.",
        "bibtex": "@InProceedings{pmlr-v32-anaraki14,\n  title = \t {Memory and Computation Efficient PCA via Very Sparse Random Projections},\n  author = \t {Anaraki, Farhad Pourkamali and Hughes, Shannon},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1341--1349},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/anaraki14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/anaraki14.html},\n  abstract = \t {Algorithms that can efficiently recover principal components in very high-dimensional, streaming, and/or distributed data settings have become an important topic in the literature. In this paper, we propose an approach to principal component estimation that utilizes projections onto very sparse random vectors with Bernoulli-generated nonzero entries. Indeed, our approach is simultaneously efficient in memory/storage space, efficient in computation, and produces accurate PC estimates, while also allowing for rigorous theoretical performance analysis. Moreover, one can tune the sparsity of the random vectors deliberately to achieve a desired point on the tradeoffs between memory, computation, and accuracy. We rigorously characterize these tradeoffs and provide statistical performance guarantees. In addition to these very sparse random vectors, our analysis also applies to more general random projections. We present experimental results demonstrating that this approach allows for simultaneously achieving a substantial reduction of the computational complexity and memory/storage space, with little loss in accuracy, particularly for very high-dimensional data.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/anaraki14.pdf",
        "supp": "",
        "pdf_size": 2411502,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14418586428010714831&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Electrical, Computer, and Energy Engineering, University of Colorado at Boulder, CO, 80309, USA; Department of Electrical, Computer, and Energy Engineering, University of Colorado at Boulder, CO, 80309, USA",
        "aff_domain": "COLORADO.EDU;COLORADO.EDU",
        "email": "COLORADO.EDU;COLORADO.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Colorado at Boulder",
        "aff_unique_dep": "Department of Electrical, Computer, and Energy Engineering",
        "aff_unique_url": "https://www.colorado.edu",
        "aff_unique_abbr": "CU Boulder",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Boulder",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "eec6857669",
        "title": "Methods of Moments for Learning Stochastic Languages: Unified Presentation and Empirical Comparison",
        "site": "https://proceedings.mlr.press/v32/balle14.html",
        "author": "Borja Balle; William Hamilton; Joelle Pineau",
        "abstract": "Probabilistic latent-variable models are a powerful tool for modelling structured data.  However, traditional expectation-maximization methods of learning such models are both computationally expensive and prone to local-minima. In contrast to these traditional methods, recently developed learning algorithms based upon the method of moments are both computationally efficient and provide strong statistical guarantees.  In this work, we provide a unified presentation and empirical comparison of three general moment-based methods in the context of modelling stochastic languages. By rephrasing these methods upon a common theoretical ground, introducing novel theoretical results where necessary, we provide a clear comparison, making explicit the statistical assumptions upon which each method relies. With this theoretical grounding, we then provide an in-depth empirical analysis of the methods on both real and synthetic data with the goal of elucidating performance trends and highlighting important implementation details.",
        "bibtex": "@InProceedings{pmlr-v32-balle14,\n  title = \t {Methods of Moments for Learning Stochastic Languages: Unified Presentation and Empirical Comparison},\n  author = \t {Balle, Borja and Hamilton, William and Pineau, Joelle},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1386--1394},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/balle14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/balle14.html},\n  abstract = \t {Probabilistic latent-variable models are a powerful tool for modelling structured data.  However, traditional expectation-maximization methods of learning such models are both computationally expensive and prone to local-minima. In contrast to these traditional methods, recently developed learning algorithms based upon the method of moments are both computationally efficient and provide strong statistical guarantees.  In this work, we provide a unified presentation and empirical comparison of three general moment-based methods in the context of modelling stochastic languages. By rephrasing these methods upon a common theoretical ground, introducing novel theoretical results where necessary, we provide a clear comparison, making explicit the statistical assumptions upon which each method relies. With this theoretical grounding, we then provide an in-depth empirical analysis of the methods on both real and synthetic data with the goal of elucidating performance trends and highlighting important implementation details.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/balle14.pdf",
        "supp": "",
        "pdf_size": 387170,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=286049378763347407&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Reasoning and Learning Laboratory, School of Computer Science, McGill University, Montreal, QC, Canada; Reasoning and Learning Laboratory, School of Computer Science, McGill University, Montreal, QC, Canada; Reasoning and Learning Laboratory, School of Computer Science, McGill University, Montreal, QC, Canada",
        "aff_domain": "CS.MCGILL.CA;CS.MCGILL.CA;CS.MCGILL.CA",
        "email": "CS.MCGILL.CA;CS.MCGILL.CA;CS.MCGILL.CA",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "McGill University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.mcgill.ca",
        "aff_unique_abbr": "McGill",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Montreal",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "ff287b93d8",
        "title": "Min-Max Problems on Factor Graphs",
        "site": "https://proceedings.mlr.press/v32/ravanbakhsh14.html",
        "author": "Siamak Ravanbakhsh; Christopher Srinivasa; Brendan Frey; Russell Greiner",
        "abstract": "We study the min-max problem in factor graphs, which seeks the assignment that minimizes the maximum value over all factors. We reduce this problem to both min-sum and sum-product inference, and focus on the later. This approach reduces the min-max inference problem to a sequence of constraint satisfaction problems (CSPs) which allows us to sample from a uniform distribution over the set of solutions. We demonstrate how this scheme provides a message passing solution to several NP-hard combinatorial problems, such as min-max clustering (a.k.a. K-clustering), the asymmetric K-center problem, K-packing and the bottleneck traveling salesman problem. Furthermore we theoretically relate the min-max reductions to several NP hard decision problems, such as clique cover, set cover, maximum clique and Hamiltonian cycle, therefore also providing message passing solutions for these problems. Experimental results suggest that message passing often provides near optimal min-max solutions for moderate size instances.",
        "bibtex": "@InProceedings{pmlr-v32-ravanbakhsh14,\n  title = \t {Min-Max Problems on Factor Graphs},\n  author = \t {Ravanbakhsh, Siamak and Srinivasa, Christopher and Frey, Brendan and Greiner, Russell},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1035--1043},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/ravanbakhsh14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/ravanbakhsh14.html},\n  abstract = \t {We study the min-max problem in factor graphs, which seeks the assignment that minimizes the maximum value over all factors. We reduce this problem to both min-sum and sum-product inference, and focus on the later. This approach reduces the min-max inference problem to a sequence of constraint satisfaction problems (CSPs) which allows us to sample from a uniform distribution over the set of solutions. We demonstrate how this scheme provides a message passing solution to several NP-hard combinatorial problems, such as min-max clustering (a.k.a. K-clustering), the asymmetric K-center problem, K-packing and the bottleneck traveling salesman problem. Furthermore we theoretically relate the min-max reductions to several NP hard decision problems, such as clique cover, set cover, maximum clique and Hamiltonian cycle, therefore also providing message passing solutions for these problems. Experimental results suggest that message passing often provides near optimal min-max solutions for moderate size instances.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/ravanbakhsh14.pdf",
        "supp": "",
        "pdf_size": 1050919,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14817768526307609714&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Computing Science Dept., University of Alberta, Edmonton, AB T6G 2E8 Canada; PSI Group, University of Toronto, ON M5S 3G4 Canada; PSI Group, University of Toronto, ON M5S 3G4 Canada; Computing Science Dept., University of Alberta, Edmonton, AB T6G 2E8 Canada",
        "aff_domain": "UALBERTA.CA;PSI.UTORONTO.CA;PSI.UTORONTO.CA;UALBERTA.CA",
        "email": "UALBERTA.CA;PSI.UTORONTO.CA;PSI.UTORONTO.CA;UALBERTA.CA",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "University of Alberta;University of Toronto",
        "aff_unique_dep": "Computing Science Dept.;PSI Group",
        "aff_unique_url": "https://www.ualberta.ca;https://www.utoronto.ca",
        "aff_unique_abbr": "UAlberta;U of T",
        "aff_campus_unique_index": "0;1;1;0",
        "aff_campus_unique": "Edmonton;Toronto",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "c0090e4ae6",
        "title": "Model-Based Relational RL When Object Existence is Partially Observable",
        "site": "https://proceedings.mlr.press/v32/ngo14.html",
        "author": "Ngo Ahn Vien; Marc Toussaint",
        "abstract": "We consider learning and planning in relational MDPs when object existence is uncertain and new objects may appear or disappear depending on previous actions or properties of other objects. Optimal policies actively need to discover  objects to achieve a goal; planning in such domains in general amounts to a POMDP problem, where the belief is about the existence and properties of potential not-yet-discovered objects. We propose a computationally efficient extension of model-based relational RL methods that approximates these beliefs using discrete uncertainty predicates. In this formulation the belief update  is learned using probabilistic rules and planning in the approximated belief space can be achieved  using an extension of existing planners. We prove that the learned belief update rules encode an approximation of the exact belief updates of a POMDP formulation and demonstrate experimentally that the proposed approach successfully  learns a set of relational rules appropriate to solve  such problems.",
        "bibtex": "@InProceedings{pmlr-v32-ngo14,\n  title = \t {Model-Based Relational RL When Object Existence is Partially Observable},\n  author = \t {Vien, Ngo Ahn and Toussaint, Marc},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {559--567},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/ngo14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/ngo14.html},\n  abstract = \t {We consider learning and planning in relational MDPs when object existence is uncertain and new objects may appear or disappear depending on previous actions or properties of other objects. Optimal policies actively need to discover  objects to achieve a goal; planning in such domains in general amounts to a POMDP problem, where the belief is about the existence and properties of potential not-yet-discovered objects. We propose a computationally efficient extension of model-based relational RL methods that approximates these beliefs using discrete uncertainty predicates. In this formulation the belief update  is learned using probabilistic rules and planning in the approximated belief space can be achieved  using an extension of existing planners. We prove that the learned belief update rules encode an approximation of the exact belief updates of a POMDP formulation and demonstrate experimentally that the proposed approach successfully  learns a set of relational rules appropriate to solve  such problems.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/ngo14.pdf",
        "supp": "",
        "pdf_size": 365071,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16996681522857488929&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Machine Learning and Robotics Lab, University of Stuttgart, 70569 Germany; Machine Learning and Robotics Lab, University of Stuttgart, 70569 Germany",
        "aff_domain": "ipvs.uni-stuttgart.de;ipvs.uni-stuttgart.de",
        "email": "ipvs.uni-stuttgart.de;ipvs.uni-stuttgart.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Stuttgart",
        "aff_unique_dep": "Machine Learning and Robotics Lab",
        "aff_unique_url": "https://www.ira.uni-stuttgart.de",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "395f536d7a",
        "title": "Modeling Correlated Arrival Events with Latent Semi-Markov Processes",
        "site": "https://proceedings.mlr.press/v32/lian14.html",
        "author": "Wenzhao Lian; Vinayak Rao; Brian Eriksson; Lawrence Carin",
        "abstract": "The analysis and characterization of correlated point process data has wide applications, ranging from biomedical research to network analysis. In this work, we model such data as generated by a latent collection of continuous-time binary semi-Markov processes, corresponding to external events appearing and disappearing. A continuous-time modeling framework is more appropriate for multichannel point process data than a binning approach requiring time discretization, and we show connections between our model and recent ideas from the discrete-time literature. We describe an efficient MCMC algorithm for posterior inference, and apply our ideas to both synthetic data and a real-world biometrics application.",
        "bibtex": "@InProceedings{pmlr-v32-lian14,\n  title = \t {Modeling Correlated Arrival Events with Latent Semi-Markov Processes},\n  author = \t {Lian, Wenzhao and Rao, Vinayak and Eriksson, Brian and Carin, Lawrence},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {396--404},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/lian14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/lian14.html},\n  abstract = \t {The analysis and characterization of correlated point process data has wide applications, ranging from biomedical research to network analysis. In this work, we model such data as generated by a latent collection of continuous-time binary semi-Markov processes, corresponding to external events appearing and disappearing. A continuous-time modeling framework is more appropriate for multichannel point process data than a binning approach requiring time discretization, and we show connections between our model and recent ideas from the discrete-time literature. We describe an efficient MCMC algorithm for posterior inference, and apply our ideas to both synthetic data and a real-world biometrics application.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/lian14.pdf",
        "supp": "",
        "pdf_size": 378642,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5578731107444920621&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Electrical and Computer Engineering, Duke University; Department of Statistical Science, Duke University; Technicolor Research Center; Department of Electrical and Computer Engineering, Duke University",
        "aff_domain": "DUKE.EDU;STAT.DUKE.EDU;TECHNICOLOR.COM;DUKE.EDU",
        "email": "DUKE.EDU;STAT.DUKE.EDU;TECHNICOLOR.COM;DUKE.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Duke University;Technicolor",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;Research Center",
        "aff_unique_url": "https://www.duke.edu;https://www.technicolor.com",
        "aff_unique_abbr": "Duke;TRC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "United States;France"
    },
    {
        "id": "e16a3fe323",
        "title": "Multi-label Classification via Feature-aware Implicit Label Space Encoding",
        "site": "https://proceedings.mlr.press/v32/linc14.html",
        "author": "Zijia Lin; Guiguang Ding; Mingqing Hu; Jianmin Wang",
        "abstract": "To tackle a multi-label classification problem with many classes, recently label space dimension reduction (LSDR) is proposed. It encodes the original label space to a low-dimensional latent space and uses a decoding process for recovery. In this paper, we propose a novel method termed FaIE to perform LSDR via Feature-aware Implicit label space Encoding. Unlike most previous work, the proposed FaIE makes no assumptions about the encoding process and directly learns a code matrix, i.e. the encoding result of some implicit encoding function, and a linear decoding matrix. To learn both matrices, FaIE jointly maximizes the recoverability of the original label space from the latent space, and the predictability of the latent space from the feature space, thus making itself feature-aware. FaIE can also be specified to learn an explicit encoding function, and extended with kernel tricks to handle non-linear correlations between the feature space and the latent space. Extensive experiments conducted on benchmark datasets well demonstrate its effectiveness.",
        "bibtex": "@InProceedings{pmlr-v32-linc14,\n  title = \t {Multi-label Classification via Feature-aware Implicit Label Space Encoding},\n  author = \t {Lin, Zijia and Ding, Guiguang and Hu, Mingqing and Wang, Jianmin},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {325--333},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/linc14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/linc14.html},\n  abstract = \t {To tackle a multi-label classification problem with many classes, recently label space dimension reduction (LSDR) is proposed. It encodes the original label space to a low-dimensional latent space and uses a decoding process for recovery. In this paper, we propose a novel method termed FaIE to perform LSDR via Feature-aware Implicit label space Encoding. Unlike most previous work, the proposed FaIE makes no assumptions about the encoding process and directly learns a code matrix, i.e. the encoding result of some implicit encoding function, and a linear decoding matrix. To learn both matrices, FaIE jointly maximizes the recoverability of the original label space from the latent space, and the predictability of the latent space from the feature space, thus making itself feature-aware. FaIE can also be specified to learn an explicit encoding function, and extended with kernel tricks to handle non-linear correlations between the feature space and the latent space. Extensive experiments conducted on benchmark datasets well demonstrate its effectiveness.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/linc14.pdf",
        "supp": "",
        "pdf_size": 332096,
        "gs_citation": 171,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9318355272738717197&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science and Technology, Tsinghua University, Beijing, P.R. China+School of Software, Tsinghua University, Beijing, P.R. China; School of Software, Tsinghua University, Beijing, P.R. China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, P.R. China; School of Software, Tsinghua University, Beijing, P.R. China",
        "aff_domain": "TSINGHUA.ORG.CN;TSINGHUA.EDU.CN;ICT.AC.CN;TSINGHUA.EDU.CN",
        "email": "TSINGHUA.ORG.CN;TSINGHUA.EDU.CN;ICT.AC.CN;TSINGHUA.EDU.CN",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+0;0;1;0",
        "aff_unique_norm": "Tsinghua University;Chinese Academy of Sciences",
        "aff_unique_dep": "Department of Computer Science and Technology;Institute of Computing Technology",
        "aff_unique_url": "https://www.tsinghua.edu.cn;http://www.ict.ac.cn",
        "aff_unique_abbr": "THU;CAS",
        "aff_campus_unique_index": "0+0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "7d786d7f4b",
        "title": "Multi-period Trading Prediction Markets with Connections to Machine Learning",
        "site": "https://proceedings.mlr.press/v32/hu14.html",
        "author": "Jinli Hu; Amos Storkey",
        "abstract": "We present a new model for prediction markets, in which we use risk measures to model agents and introduce a market maker to describe the trading process. This specific choice of modelling approach enables us to show that the whole market approaches a global objective, despite the fact that the market is designed such that each agent only cares about its own goal. In addition, the market dynamic provides a sensible algorithm for optimising the global objective. An intimate connection between machine learning and our markets is thus established, such that we could 1) analyse a market by applying machine learning methods to the global objective; and 2) solve machine learning problems by setting up and running certain markets.",
        "bibtex": "@InProceedings{pmlr-v32-hu14,\n  title = \t {Multi-period Trading Prediction Markets with Connections to Machine Learning},\n  author = \t {Hu, Jinli and Storkey, Amos},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1773--1781},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/hu14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/hu14.html},\n  abstract = \t {We present a new model for prediction markets, in which we use risk measures to model agents and introduce a market maker to describe the trading process. This specific choice of modelling approach enables us to show that the whole market approaches a global objective, despite the fact that the market is designed such that each agent only cares about its own goal. In addition, the market dynamic provides a sensible algorithm for optimising the global objective. An intimate connection between machine learning and our markets is thus established, such that we could 1) analyse a market by applying machine learning methods to the global objective; and 2) solve machine learning problems by setting up and running certain markets.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/hu14.pdf",
        "supp": "",
        "pdf_size": 353161,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=61446970102689054&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "School of Informatics, The University of Edinburgh; School of Informatics, The University of Edinburgh",
        "aff_domain": "ED.AC.UK;ED.AC.UK",
        "email": "ED.AC.UK;ED.AC.UK",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Edinburgh",
        "aff_unique_dep": "School of Informatics",
        "aff_unique_url": "https://www.ed.ac.uk",
        "aff_unique_abbr": "Edinburgh",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Edinburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "9b6444ca85",
        "title": "Multimodal Neural Language Models",
        "site": "https://proceedings.mlr.press/v32/kiros14.html",
        "author": "Ryan Kiros; Ruslan Salakhutdinov; Rich Zemel",
        "abstract": "We introduce two multimodal neural language models: models of natural language that can be conditioned on other modalities. An image-text multimodal neural language model can be used to retrieve images given complex sentence queries, retrieve phrase descriptions given image queries, as well as generate text conditioned on images. We show that in the case of image-text modelling we can jointly learn word representations and image features by training our models together with a convolutional network. Unlike many of the existing methods, our approach can generate sentence descriptions for images without the use of templates, structured prediction, and/or syntactic trees. While we focus on image-text modelling, our algorithms can be easily applied to other modalities such as audio.",
        "bibtex": "@InProceedings{pmlr-v32-kiros14,\n  title = \t {Multimodal Neural Language Models},\n  author = \t {Kiros, Ryan and Salakhutdinov, Ruslan and Zemel, Rich},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {595--603},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/kiros14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/kiros14.html},\n  abstract = \t {We introduce two multimodal neural language models: models of natural language that can be conditioned on other modalities. An image-text multimodal neural language model can be used to retrieve images given complex sentence queries, retrieve phrase descriptions given image queries, as well as generate text conditioned on images. We show that in the case of image-text modelling we can jointly learn word representations and image features by training our models together with a convolutional network. Unlike many of the existing methods, our approach can generate sentence descriptions for images without the use of templates, structured prediction, and/or syntactic trees. While we focus on image-text modelling, our algorithms can be easily applied to other modalities such as audio.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/kiros14.pdf",
        "supp": "",
        "pdf_size": 1128141,
        "gs_citation": 927,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3905919677086105959&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto + Canadian Institute for Advanced Research",
        "aff_domain": "CS.TORONTO.EDU;CS.TORONTO.EDU;CS.TORONTO.EDU",
        "email": "CS.TORONTO.EDU;CS.TORONTO.EDU;CS.TORONTO.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "University of Toronto;Canadian Institute for Advanced Research",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.utoronto.ca;https://www.cifar.ca",
        "aff_unique_abbr": "U of T;CIFAR",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Toronto;",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "e21b0e33b5",
        "title": "Multiple Testing under Dependence via Semiparametric Graphical Models",
        "site": "https://proceedings.mlr.press/v32/liue14.html",
        "author": "Jie Liu; Chunming Zhang; Elizabeth Burnside; David Page",
        "abstract": "It has been shown that graphical models can be used to leverage the dependence in large-scale multiple testing problems with significantly improved performance (Sun & Cai, 2009; Liu et al., 2012). These graphical models are fully parametric and require that we know the parameterization of f1, the density function of the test statistic under the alternative hypothesis. However in practice, f1 is often heterogeneous, and cannot be estimated with a simple parametric distribution. We propose a novel semiparametric approach for multiple testing under dependence, which estimates f1 adaptively. This semiparametric approach exactly generalizes the local FDR procedure (Efron et al., 2001) and connects with the BH procedure (Benjamini & Hochberg, 1995). A variety of simulations show that our semiparametric approach outperforms classical procedures which assume independence and the parametric approaches which capture dependence.",
        "bibtex": "@InProceedings{pmlr-v32-liue14,\n  title = \t {Multiple Testing under Dependence via Semiparametric Graphical Models},\n  author = \t {Liu, Jie and Zhang, Chunming and Burnside, Elizabeth and Page, David},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {955--963},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/liue14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/liue14.html},\n  abstract = \t {It has been shown that graphical models can be used to leverage the dependence in large-scale multiple testing problems with significantly improved performance (Sun & Cai, 2009; Liu et al., 2012). These graphical models are fully parametric and require that we know the parameterization of f1, the density function of the test statistic under the alternative hypothesis. However in practice, f1 is often heterogeneous, and cannot be estimated with a simple parametric distribution. We propose a novel semiparametric approach for multiple testing under dependence, which estimates f1 adaptively. This semiparametric approach exactly generalizes the local FDR procedure (Efron et al., 2001) and connects with the BH procedure (Benjamini & Hochberg, 1995). A variety of simulations show that our semiparametric approach outperforms classical procedures which assume independence and the parametric approaches which capture dependence.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/liue14.pdf",
        "supp": "",
        "pdf_size": 809458,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=96444474977458455&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Sciences, University of Wisconsin-Madison; Department of Statistics, University of Wisconsin-Madison; Department of Radiology, University of Wisconsin-Madison; Department of Biostatistics and Medical Informatics, University of Wisconsin-Madison",
        "aff_domain": "CS.WISC.EDU;STAT.WISC.EDU;UWHEALTH.ORG;BIOSTAT.WISC.EDU",
        "email": "CS.WISC.EDU;STAT.WISC.EDU;UWHEALTH.ORG;BIOSTAT.WISC.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Wisconsin-Madison",
        "aff_unique_dep": "Department of Computer Sciences",
        "aff_unique_url": "https://www.wisc.edu",
        "aff_unique_abbr": "UW-Madison",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Madison",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1230078b05",
        "title": "Multiresolution Matrix Factorization",
        "site": "https://proceedings.mlr.press/v32/kondor14.html",
        "author": "Risi Kondor; Nedelina Teneva; Vikas Garg",
        "abstract": "The types of large matrices that appear in modern Machine Learning problems often have complex hierarchical structures that go beyond what can be found by traditional linear algebra tools, such as eigendecompositions. Inspired by ideas from multiresolution analysis,   this paper introduces a new notion of matrix factorization that can capture structure in matrices at multiple different scales. The resulting Multiresolution Matrix Factorizations (MMFs) not only provide a wavelet basis for sparse approximation, but can also be used for matrix compression (similar to Nystrom approximations) and as a prior for matrix completion.",
        "bibtex": "@InProceedings{pmlr-v32-kondor14,\n  title = \t {Multiresolution Matrix Factorization},\n  author = \t {Kondor, Risi and Teneva, Nedelina and Garg, Vikas},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1620--1628},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/kondor14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/kondor14.html},\n  abstract = \t {The types of large matrices that appear in modern Machine Learning problems often have complex hierarchical structures that go beyond what can be found by traditional linear algebra tools, such as eigendecompositions. Inspired by ideas from multiresolution analysis,   this paper introduces a new notion of matrix factorization that can capture structure in matrices at multiple different scales. The resulting Multiresolution Matrix Factorizations (MMFs) not only provide a wavelet basis for sparse approximation, but can also be used for matrix compression (similar to Nystrom approximations) and as a prior for matrix completion.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/kondor14.pdf",
        "supp": "",
        "pdf_size": 249305,
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7026887099848312863&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science + Department of Statistics, The University of Chicago; Department of Computer Science + Department of Statistics, The University of Chicago; Toyota Technological Institute at Chicago",
        "aff_domain": "UCHICAGO.EDU;UCHICAGO.EDU;TTIC.EDU",
        "email": "UCHICAGO.EDU;UCHICAGO.EDU;TTIC.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;2",
        "aff_unique_norm": "Unknown Institution;University of Chicago;Toyota Technological Institute at Chicago",
        "aff_unique_dep": "Department of Computer Science;Department of Statistics;",
        "aff_unique_url": ";https://www.uchicago.edu;https://www.tti-chicago.org",
        "aff_unique_abbr": ";UChicago;TTI Chicago",
        "aff_campus_unique_index": ";;1",
        "aff_campus_unique": ";Chicago",
        "aff_country_unique_index": "1;1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "4d0d2e1c5d",
        "title": "Multivariate Maximal Correlation Analysis",
        "site": "https://proceedings.mlr.press/v32/nguyenc14.html",
        "author": "Hoang Vu Nguyen; Emmanuel M\u00fcller; Jilles Vreeken; Pavel Efros; Klemens B\u00f6hm",
        "abstract": "Correlation analysis is one of the key elements of statistics, and has various applications in data analysis. Whereas most existing measures can only detect pairwise correlations between two dimensions, modern analysis aims at detecting correlations in multi-dimensional spaces.    We propose MAC, a novel multivariate correlation measure designed for discovering multi-dimensional patterns. It belongs to the powerful class of maximal correlation analysis, for which we propose a generalization to multivariate domains. We highlight the limitations of current methods in this class, and address these with MAC. Our experiments show that MAC outperforms existing solutions, is robust to noise, and discovers interesting and useful patterns.",
        "bibtex": "@InProceedings{pmlr-v32-nguyenc14,\n  title = \t {Multivariate Maximal Correlation Analysis},\n  author = \t {Nguyen, Hoang Vu and M\u00fcller, Emmanuel and Vreeken, Jilles and Efros, Pavel and B\u00f6hm, Klemens},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {775--783},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/nguyenc14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/nguyenc14.html},\n  abstract = \t {Correlation analysis is one of the key elements of statistics, and has various applications in data analysis. Whereas most existing measures can only detect pairwise correlations between two dimensions, modern analysis aims at detecting correlations in multi-dimensional spaces.    We propose MAC, a novel multivariate correlation measure designed for discovering multi-dimensional patterns. It belongs to the powerful class of maximal correlation analysis, for which we propose a generalization to multivariate domains. We highlight the limitations of current methods in this class, and address these with MAC. Our experiments show that MAC outperforms existing solutions, is robust to noise, and discovers interesting and useful patterns.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/nguyenc14.pdf",
        "supp": "",
        "pdf_size": 776273,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7302104483159789931&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff": "Karlsruhe Institute of Technology, Germany; Karlsruhe Institute of Technology, Germany + University of Antwerp, Belgium; Max-Planck Institute for Informatics & Saarland University, Germany; Karlsruhe Institute of Technology, Germany; Karlsruhe Institute of Technology, Germany",
        "aff_domain": "KIT.EDU;KIT.EDU;MPI-INF.MPG .DE;KIT.EDU;KIT.EDU",
        "email": "KIT.EDU;KIT.EDU;MPI-INF.MPG .DE;KIT.EDU;KIT.EDU",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;2;0;0",
        "aff_unique_norm": "Karlsruhe Institute of Technology;University of Antwerp;Max-Planck Institute for Informatics",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.kit.edu;https://www.uantwerp.be;https://mpi-inf.mpg.de",
        "aff_unique_abbr": "KIT;UA;MPII",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+1;0;0;0",
        "aff_country_unique": "Germany;Belgium"
    },
    {
        "id": "d98c5984ad",
        "title": "Narrowing the Gap: Random Forests In Theory and In Practice",
        "site": "https://proceedings.mlr.press/v32/denil14.html",
        "author": "Misha Denil; David Matheson; Nando De Freitas",
        "abstract": "Despite widespread interest and practical use, the theoretical properties of random forests are still not well understood. In this paper we contribute to this understanding in two ways. We present a new theoreti- cally tractable variant of random regression forests and prove that our algorithm is con- sistent. We also provide an empirical eval- uation, comparing our algorithm and other theoretically tractable random forest models to the random forest algorithm used in prac- tice. Our experiments provide insight into the relative importance of different simplifi- cations that theoreticians have made to ob- tain tractable models for analysis.",
        "bibtex": "@InProceedings{pmlr-v32-denil14,\n  title = \t {Narrowing the Gap: Random Forests In Theory and In Practice},\n  author = \t {Denil, Misha and Matheson, David and De Freitas, Nando},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {665--673},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/denil14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/denil14.html},\n  abstract = \t {Despite widespread interest and practical use, the theoretical properties of random forests are still not well understood. In this paper we contribute to this understanding in two ways. We present a new theoreti- cally tractable variant of random regression forests and prove that our algorithm is con- sistent. We also provide an empirical eval- uation, comparing our algorithm and other theoretically tractable random forest models to the random forest algorithm used in prac- tice. Our experiments provide insight into the relative importance of different simplifi- cations that theoreticians have made to ob- tain tractable models for analysis.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/denil14.pdf",
        "supp": "",
        "pdf_size": 476013,
        "gs_citation": 345,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7580170702378436542&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "University of Oxford, United Kingdom + University of British Columbia, Canada; University of British Columbia, Canada; University of Oxford, United Kingdom + University of British Columbia, Canada",
        "aff_domain": "CS.OX.AC.UK;CS.UBC.CA;CS.OX.AC.UK",
        "email": "CS.OX.AC.UK;CS.UBC.CA;CS.OX.AC.UK",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;0+1",
        "aff_unique_norm": "University of Oxford;University of British Columbia",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ox.ac.uk;https://www.ubc.ca",
        "aff_unique_abbr": "Oxford;UBC",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1;0+1",
        "aff_country_unique": "United Kingdom;Canada"
    },
    {
        "id": "7b70a42d6c",
        "title": "Near-Optimal Joint Object Matching via Convex Relaxation",
        "site": "https://proceedings.mlr.press/v32/chend14.html",
        "author": "Yuxin Chen; Leonidas Guibas; Qixing Huang",
        "abstract": "Joint object matching aims at aggregating information from a large collection of similar instances (e.g. images, graphs, shapes) to improve the correspondences computed between pairs of objects, typically by exploiting global map compatibility. Despite some practical advances on this problem, from the theoretical point of view, the error-correction ability of existing algorithms are limited by a constant barrier \u2014 none of them can provably recover the correct solution when more than a constant fraction of input correspondences are corrupted. Moreover, prior approaches focus mostly on fully similar objects, while it is practically more demanding and realistic to match instances that are only partially similar to each other.      In this paper, we propose an algorithm to jointly match multiple objects that exhibit only partial similarities, where the provided pairwise feature correspondences can be densely corrupted. By encoding a consistent partial map collection into a 0-1 semidefinite matrix, we attempt recovery via a two-step procedure, that is, a spectral technique followed by a parameter-free convex program called MatchLift. Under a natural randomized model, MatchLift exhibits near-optimal error-correction ability, i.e. it guarantees the recovery of the ground-truth maps even when a dominant fraction of the inputs are randomly corrupted. We evaluate the proposed algorithm on various benchmark data sets including synthetic examples and real-world examples, all of which confirm the practical applicability of the proposed algorithm.",
        "bibtex": "@InProceedings{pmlr-v32-chend14,\n  title = \t {Near-Optimal Joint Object Matching via Convex Relaxation},\n  author = \t {Chen, Yuxin and Guibas, Leonidas and Huang, Qixing},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {100--108},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/chend14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/chend14.html},\n  abstract = \t {Joint object matching aims at aggregating information from a large collection of similar instances (e.g. images, graphs, shapes) to improve the correspondences computed between pairs of objects, typically by exploiting global map compatibility. Despite some practical advances on this problem, from the theoretical point of view, the error-correction ability of existing algorithms are limited by a constant barrier \u2014 none of them can provably recover the correct solution when more than a constant fraction of input correspondences are corrupted. Moreover, prior approaches focus mostly on fully similar objects, while it is practically more demanding and realistic to match instances that are only partially similar to each other.      In this paper, we propose an algorithm to jointly match multiple objects that exhibit only partial similarities, where the provided pairwise feature correspondences can be densely corrupted. By encoding a consistent partial map collection into a 0-1 semidefinite matrix, we attempt recovery via a two-step procedure, that is, a spectral technique followed by a parameter-free convex program called MatchLift. Under a natural randomized model, MatchLift exhibits near-optimal error-correction ability, i.e. it guarantees the recovery of the ground-truth maps even when a dominant fraction of the inputs are randomly corrupted. We evaluate the proposed algorithm on various benchmark data sets including synthetic examples and real-world examples, all of which confirm the practical applicability of the proposed algorithm.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/chend14.pdf",
        "supp": "",
        "pdf_size": 8339266,
        "gs_citation": 168,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7325976522011355909&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Electrical Engineering, Stanford University, Stanford, CA 94305, USA; Department of Computer Science, Stanford University, Stanford, CA 94305, USA; Department of Computer Science, Stanford University, Stanford, CA 94305, USA",
        "aff_domain": "STANFORD.EDU;CS.STANFORD.EDU;STANFORD.EDU",
        "email": "STANFORD.EDU;CS.STANFORD.EDU;STANFORD.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c4f4892452",
        "title": "Near-Optimally Teaching the Crowd to Classify",
        "site": "https://proceedings.mlr.press/v32/singla14.html",
        "author": "Adish Singla; Ilija Bogunovic; Gabor Bartok; Amin Karbasi; Andreas Krause",
        "abstract": "How should we present training examples to learners to teach them classification rules? This is a natural problem when training workers for crowdsourcing labeling tasks, and is also motivated by challenges in data-driven online education. We propose a natural stochastic model of the learners, modeling them as randomly switching among hypotheses based on observed feedback. We then develop STRICT, an efficient algorithm for selecting examples to teach to workers. Our solution greedily maximizes a submodular surrogate objective function in order to select examples to show to the learners. We prove that our strategy is competitive with the optimal teaching policy. Moreover, for the special case of linear separators, we prove that an exponential reduction in error probability can be achieved. Our experiments on simulated workers as well as three real image annotation tasks on Amazon Mechanical Turk show the effectiveness of our teaching algorithm.",
        "bibtex": "@InProceedings{pmlr-v32-singla14,\n  title = \t {Near-Optimally Teaching the Crowd to Classify},\n  author = \t {Singla, Adish and Bogunovic, Ilija and Bartok, Gabor and Karbasi, Amin and Krause, Andreas},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {154--162},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/singla14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/singla14.html},\n  abstract = \t {How should we present training examples to learners to teach them classification rules? This is a natural problem when training workers for crowdsourcing labeling tasks, and is also motivated by challenges in data-driven online education. We propose a natural stochastic model of the learners, modeling them as randomly switching among hypotheses based on observed feedback. We then develop STRICT, an efficient algorithm for selecting examples to teach to workers. Our solution greedily maximizes a submodular surrogate objective function in order to select examples to show to the learners. We prove that our strategy is competitive with the optimal teaching policy. Moreover, for the special case of linear separators, we prove that an exponential reduction in error probability can be achieved. Our experiments on simulated workers as well as three real image annotation tasks on Amazon Mechanical Turk show the effectiveness of our teaching algorithm.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/singla14.pdf",
        "supp": "",
        "pdf_size": 1503749,
        "gs_citation": 157,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5015224038304211103&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 20,
        "aff": "ETH Z\u00a8urich, Z\u00a8urich, Switzerland; School of Computer and Communication Sciences, EPFL, Lausanne, Switzerland; ETH Z\u00a8urich, Z\u00a8urich, Switzerland; ETH Z\u00a8urich, Z\u00a8urich, Switzerland; ETH Z\u00a8urich, Z\u00a8urich, Switzerland",
        "aff_domain": "inf.ethz.ch;epfl.ch;inf.ethz.ch;inf.ethz.ch;ethz.ch",
        "email": "inf.ethz.ch;epfl.ch;inf.ethz.ch;inf.ethz.ch;ethz.ch",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0;0",
        "aff_unique_norm": "ETH Zurich;EPFL",
        "aff_unique_dep": ";School of Computer and Communication Sciences",
        "aff_unique_url": "https://www.ethz.ch;https://www.epfl.ch",
        "aff_unique_abbr": "ETHZ;EPFL",
        "aff_campus_unique_index": "0;1;0;0;0",
        "aff_campus_unique": "Z\u00fcrich;Lausanne",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "5a4aa8bf46",
        "title": "Nearest Neighbors Using Compact Sparse Codes",
        "site": "https://proceedings.mlr.press/v32/cherian14.html",
        "author": "Anoop Cherian",
        "abstract": "In this paper, we propose a novel scheme for approximate nearest neighbor (ANN) retrieval based on dictionary learning and sparse coding. Our key innovation is to build compact codes, dubbed SpANN codes, using the active set of sparse coded data. These codes are then used to index an inverted file table for fast retrieval. The active sets are often found to be sensitive to small differences among data points, resulting in only near duplicate retrieval. We show that this sensitivity is related to the coherence of the dictionary; small coherence resulting in better retrieval. To this end, we propose a novel dictionary learning formulation with incoherence constraints and an efficient method to solve it. Experiments are conducted on two state-of-the-art computer vision datasets with 1M data points and show an order of magnitude improvement in retrieval accuracy without sacrificing memory and query time compared to the state-of-the-art methods.",
        "bibtex": "@InProceedings{pmlr-v32-cherian14,\n  title = \t {Nearest Neighbors Using Compact Sparse Codes},\n  author = \t {Cherian, Anoop},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1053--1061},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/cherian14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/cherian14.html},\n  abstract = \t {In this paper, we propose a novel scheme for approximate nearest neighbor (ANN) retrieval based on dictionary learning and sparse coding. Our key innovation is to build compact codes, dubbed SpANN codes, using the active set of sparse coded data. These codes are then used to index an inverted file table for fast retrieval. The active sets are often found to be sensitive to small differences among data points, resulting in only near duplicate retrieval. We show that this sensitivity is related to the coherence of the dictionary; small coherence resulting in better retrieval. To this end, we propose a novel dictionary learning formulation with incoherence constraints and an efficient method to solve it. Experiments are conducted on two state-of-the-art computer vision datasets with 1M data points and show an order of magnitude improvement in retrieval accuracy without sacrificing memory and query time compared to the state-of-the-art methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/cherian14.pdf",
        "supp": "",
        "pdf_size": 313510,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14097877069391632819&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "INRIA, LEAR Project-team, Grenoble, France",
        "aff_domain": "INRIA.FR",
        "email": "INRIA.FR",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "INRIA",
        "aff_unique_dep": "LEAR Project-team",
        "aff_unique_url": "https://www.inria.fr",
        "aff_unique_abbr": "INRIA",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Grenoble",
        "aff_country_unique_index": "0",
        "aff_country_unique": "France"
    },
    {
        "id": "a56b2e0571",
        "title": "Neural Variational Inference and Learning in Belief Networks",
        "site": "https://proceedings.mlr.press/v32/mnih14.html",
        "author": "Andriy Mnih; Karol Gregor",
        "abstract": "Highly expressive directed latent variable models, such as sigmoid belief networks, are difficult to train on large datasets because exact inference in them is intractable and none of the approximate inference methods that have been applied to them scale well. We propose a fast non-iterative approximate inference method that uses a feedforward network to implement efficient exact sampling from the variational posterior. The model and this inference network are trained jointly by maximizing a variational lower bound on the log-likelihood. Although the naive estimator of the inference network gradient is too high-variance to be useful, we make it practical by applying several straightforward model-independent variance reduction techniques. Applying our approach to training sigmoid belief networks and deep autoregressive networks, we show that it outperforms the wake-sleep algorithm on MNIST and achieves state-of-the-art results on the Reuters RCV1 document dataset.",
        "bibtex": "@InProceedings{pmlr-v32-mnih14,\n  title = \t {Neural Variational Inference and Learning in Belief Networks},\n  author = \t {Mnih, Andriy and Gregor, Karol},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1791--1799},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/mnih14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/mnih14.html},\n  abstract = \t {Highly expressive directed latent variable models, such as sigmoid belief networks, are difficult to train on large datasets because exact inference in them is intractable and none of the approximate inference methods that have been applied to them scale well. We propose a fast non-iterative approximate inference method that uses a feedforward network to implement efficient exact sampling from the variational posterior. The model and this inference network are trained jointly by maximizing a variational lower bound on the log-likelihood. Although the naive estimator of the inference network gradient is too high-variance to be useful, we make it practical by applying several straightforward model-independent variance reduction techniques. Applying our approach to training sigmoid belief networks and deep autoregressive networks, we show that it outperforms the wake-sleep algorithm on MNIST and achieves state-of-the-art results on the Reuters RCV1 document dataset.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/mnih14.pdf",
        "supp": "",
        "pdf_size": 235064,
        "gs_citation": 837,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1343451911861500424&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Google DeepMind; Google DeepMind",
        "aff_domain": "google.com;google.com",
        "email": "google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google DeepMind",
        "aff_unique_url": "https://deepmind.com",
        "aff_unique_abbr": "DeepMind",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "496a26c55d",
        "title": "Nonlinear Information-Theoretic Compressive Measurement Design",
        "site": "https://proceedings.mlr.press/v32/wangh14.html",
        "author": "Liming Wang; Abolfazl Razi; Miguel Rodrigues; Robert Calderbank; Lawrence Carin",
        "abstract": "We investigate design of general nonlinear functions for mapping high-dimensional data into a lower-dimensional (compressive) space. The nonlinear measurements are assumed contaminated by additive Gaussian noise. Depending on the application, we are either interested in recovering the high-dimensional data from the nonlinear compressive measurements, or performing classification directly based on these measurements. The latter case corresponds to classification based on nonlinearly constituted and noisy features. The nonlinear measurement functions are designed based on constrained mutual-information optimization. New analytic results are developed for the gradient of mutual information in this setting, for arbitrary input-signal  statistics. We make connections to kernel-based methods, such as the support vector machine. Encouraging results are presented on multiple datasets, for both signal recovery and classification. The nonlinear approach is shown to be particularly valuable in high-noise scenarios.",
        "bibtex": "@InProceedings{pmlr-v32-wangh14,\n  title = \t {Nonlinear Information-Theoretic Compressive Measurement Design},\n  author = \t {Wang, Liming and Razi, Abolfazl and Rodrigues, Miguel and Calderbank, Robert and Carin, Lawrence},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1161--1169},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/wangh14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/wangh14.html},\n  abstract = \t {We investigate design of general nonlinear functions for mapping high-dimensional data into a lower-dimensional (compressive) space. The nonlinear measurements are assumed contaminated by additive Gaussian noise. Depending on the application, we are either interested in recovering the high-dimensional data from the nonlinear compressive measurements, or performing classification directly based on these measurements. The latter case corresponds to classification based on nonlinearly constituted and noisy features. The nonlinear measurement functions are designed based on constrained mutual-information optimization. New analytic results are developed for the gradient of mutual information in this setting, for arbitrary input-signal  statistics. We make connections to kernel-based methods, such as the support vector machine. Encouraging results are presented on multiple datasets, for both signal recovery and classification. The nonlinear approach is shown to be particularly valuable in high-noise scenarios.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/wangh14.pdf",
        "supp": "",
        "pdf_size": 855458,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2897540854452028829&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Electrical & Computer Engineering, Duke University, Durham, NC 27708, USA; Department of Electrical & Computer Engineering, Duke University, Durham, NC 27708, USA; Department of Electronic and Electrical Engineering, University College London, London, UK; Department of Electrical & Computer Engineering, Duke University, Durham, NC 27708, USA; Department of Electrical & Computer Engineering, Duke University, Durham, NC 27708, USA",
        "aff_domain": "DUKE.EDU;DUKE.EDU;UCL.AC.UK;DUKE.EDU;DUKE.EDU",
        "email": "DUKE.EDU;DUKE.EDU;UCL.AC.UK;DUKE.EDU;DUKE.EDU",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "Duke University;University College London",
        "aff_unique_dep": "Department of Electrical & Computer Engineering;Department of Electronic and Electrical Engineering",
        "aff_unique_url": "https://www.duke.edu;https://www.ucl.ac.uk",
        "aff_unique_abbr": "Duke;UCL",
        "aff_campus_unique_index": "0;0;1;0;0",
        "aff_campus_unique": "Durham;London",
        "aff_country_unique_index": "0;0;1;0;0",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "bdd456ab4f",
        "title": "Nonmyopic \u03b5-Bayes-Optimal Active Learning of Gaussian Processes",
        "site": "https://proceedings.mlr.press/v32/hoang14.html",
        "author": "Trong Nghia Hoang; Bryan Kian Hsiang Low; Patrick Jaillet; Mohan Kankanhalli",
        "abstract": "A fundamental issue in active learning of Gaussian processes is that of the exploration-exploitation trade-off. This paper presents a novel nonmyopic \u03b5-Bayes-optimal active learning (\u03b5-BAL) approach that jointly and naturally optimizes the trade-off.   In contrast, existing works have primarily developed myopic/greedy algorithms or performed exploration and exploitation separately. To perform active learning in real time, we then propose an anytime algorithm based on \u03b5-BAL with performance guarantee and empirically demonstrate using synthetic and real-world datasets that, with limited budget, it outperforms the state-of-the-art algorithms.",
        "bibtex": "@InProceedings{pmlr-v32-hoang14,\n  title = \t {Nonmyopic $\\epsilon$-Bayes-Optimal Active Learning of Gaussian Processes},\n  author = \t {Hoang, Trong Nghia and Low, Bryan Kian Hsiang and Jaillet, Patrick and Kankanhalli, Mohan},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {739--747},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/hoang14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/hoang14.html},\n  abstract = \t {A fundamental issue in active learning of Gaussian processes is that of the exploration-exploitation trade-off. This paper presents a novel nonmyopic \u03b5-Bayes-optimal active learning (\u03b5-BAL) approach that jointly and naturally optimizes the trade-off.   In contrast, existing works have primarily developed myopic/greedy algorithms or performed exploration and exploitation separately. To perform active learning in real time, we then propose an anytime algorithm based on \u03b5-BAL with performance guarantee and empirically demonstrate using synthetic and real-world datasets that, with limited budget, it outperforms the state-of-the-art algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/hoang14.pdf",
        "supp": "",
        "pdf_size": 3077207,
        "gs_citation": 73,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7473237774696862600&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 24,
        "aff": "Department of Computer Science, National University of Singapore, Republic of Singapore; Department of Computer Science, National University of Singapore, Republic of Singapore; Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, USA; Department of Computer Science, National University of Singapore, Republic of Singapore",
        "aff_domain": "COMP.NUS.EDU.SG;COMP.NUS.EDU.SG;MIT.EDU;COMP.NUS.EDU.SG",
        "email": "COMP.NUS.EDU.SG;COMP.NUS.EDU.SG;MIT.EDU;COMP.NUS.EDU.SG",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "National University of Singapore;Massachusetts Institute of Technology",
        "aff_unique_dep": "Department of Computer Science;Department of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.nus.edu.sg;https://web.mit.edu",
        "aff_unique_abbr": "NUS;MIT",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "Singapore;United States"
    },
    {
        "id": "3f3c050e61",
        "title": "Nonnegative Sparse PCA with Provable Guarantees",
        "site": "https://proceedings.mlr.press/v32/asteris14.html",
        "author": "Megasthenis Asteris; Dimitris Papailiopoulos; Alexandros Dimakis",
        "abstract": "We introduce a novel algorithm to compute nonnegative sparse principal components of positive semidefinite (PSD) matrices. Our algorithm comes with approximation guarantees   contingent on the spectral profile of the input matrix A:  the sharper the eigenvalue decay, the better the approximation quality.    If the eigenvalues decay like any asymptotically vanishing function, we can approximate nonnegative sparse PCA within any accuracy \u03b5in time polynomial in the matrix size n and desired sparsity k, but not in 1/\u03b5. Further, we obtain a data-dependent bound that is computed by executing an algorithm on a given data set. This bound is significantly tighter than a-priori bounds and can be used to show that for all tested datasets our algorithm is provably within 40%-90% from the unknown optimum.     Our algorithm is combinatorial and explores a subspace defined by the leading eigenvectors of A. We test our scheme on several data sets, showing that it matches or outperforms the previous state of the art.",
        "bibtex": "@InProceedings{pmlr-v32-asteris14,\n  title = \t {Nonnegative Sparse PCA with Provable Guarantees},\n  author = \t {Asteris, Megasthenis and Papailiopoulos, Dimitris and Dimakis, Alexandros},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1728--1736},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/asteris14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/asteris14.html},\n  abstract = \t {We introduce a novel algorithm to compute nonnegative sparse principal components of positive semidefinite (PSD) matrices. Our algorithm comes with approximation guarantees   contingent on the spectral profile of the input matrix A:  the sharper the eigenvalue decay, the better the approximation quality.    If the eigenvalues decay like any asymptotically vanishing function, we can approximate nonnegative sparse PCA within any accuracy \u03b5in time polynomial in the matrix size n and desired sparsity k, but not in 1/\u03b5. Further, we obtain a data-dependent bound that is computed by executing an algorithm on a given data set. This bound is significantly tighter than a-priori bounds and can be used to show that for all tested datasets our algorithm is provably within 40%-90% from the unknown optimum.     Our algorithm is combinatorial and explores a subspace defined by the leading eigenvectors of A. We test our scheme on several data sets, showing that it matches or outperforms the previous state of the art.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/asteris14.pdf",
        "supp": "",
        "pdf_size": 465790,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8174157444393994205&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Electrical and Computer Engineering, The University of Texas at Austin, TX, USA; Department of Electrical and Computer Engineering, The University of Texas at Austin, TX, USA; Department of Electrical and Computer Engineering, The University of Texas at Austin, TX, USA",
        "aff_domain": "utexas.edu;utexas.edu;austin.utexas.edu",
        "email": "utexas.edu;utexas.edu;austin.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "102743b780",
        "title": "Nonparametric Estimation of Multi-View Latent Variable Models",
        "site": "https://proceedings.mlr.press/v32/songa14.html",
        "author": "Le Song; Animashree Anandkumar; Bo Dai; Bo Xie",
        "abstract": "Spectral methods have greatly advanced the estimation of latent variable models, generating a sequence of novel and efficient algorithms with strong theoretical guarantees. However, current spectral algorithms are largely restricted to mixtures of discrete or Gaussian distributions. In this paper, we propose a kernel method for learning multi-view latent variable models, allowing each mixture component to be nonparametric and learned from data in an unsupervised fashion. The key idea of our method is to embed the joint distribution of a multi-view latent variable model into a reproducing kernel Hilbert space, and then the latent parameters are recovered using a robust tensor power method. We establish that the  sample complexity for the proposed method is quadratic in the number of latent components and is a low order polynomial in the other relevant parameters. Thus, our nonparametric tensor approach to learning latent variable models enjoys good sample and computational efficiencies. As a special case of our framework, we also obtain a first unsupervised conditional density estimator of the kind with provable guarantees. In both synthetic and real world datasets, the nonparametric tensor power method compares favorably to EM algorithm and other spectral algorithms.",
        "bibtex": "@InProceedings{pmlr-v32-songa14,\n  title = \t {Nonparametric Estimation of Multi-View Latent Variable Models},\n  author = \t {Song, Le and Anandkumar, Animashree and Dai, Bo and Xie, Bo},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {640--648},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/songa14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/songa14.html},\n  abstract = \t {Spectral methods have greatly advanced the estimation of latent variable models, generating a sequence of novel and efficient algorithms with strong theoretical guarantees. However, current spectral algorithms are largely restricted to mixtures of discrete or Gaussian distributions. In this paper, we propose a kernel method for learning multi-view latent variable models, allowing each mixture component to be nonparametric and learned from data in an unsupervised fashion. The key idea of our method is to embed the joint distribution of a multi-view latent variable model into a reproducing kernel Hilbert space, and then the latent parameters are recovered using a robust tensor power method. We establish that the  sample complexity for the proposed method is quadratic in the number of latent components and is a low order polynomial in the other relevant parameters. Thus, our nonparametric tensor approach to learning latent variable models enjoys good sample and computational efficiencies. As a special case of our framework, we also obtain a first unsupervised conditional density estimator of the kind with provable guarantees. In both synthetic and real world datasets, the nonparametric tensor power method compares favorably to EM algorithm and other spectral algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/songa14.pdf",
        "supp": "",
        "pdf_size": 560958,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18170519817736952769&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Georgia Institute of Technology, Atlanta, GA 30345 USA; University of California, Irvine, CA 92697, USA; Georgia Institute of Technology, Atlanta, GA 30345 USA; Georgia Institute of Technology, Atlanta, GA 30345 USA",
        "aff_domain": "CC.GATECH.EDU;UCI.EDU;GATECH.EDU;GATECH.EDU",
        "email": "CC.GATECH.EDU;UCI.EDU;GATECH.EDU;GATECH.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Georgia Institute of Technology;University of California, Irvine",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.gatech.edu;https://www.uci.edu",
        "aff_unique_abbr": "Georgia Tech;UCI",
        "aff_campus_unique_index": "0;1;0;0",
        "aff_campus_unique": "Atlanta;Irvine",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4af8c79864",
        "title": "Nonparametric Estimation of Renyi Divergence and Friends",
        "site": "https://proceedings.mlr.press/v32/krishnamurthy14.html",
        "author": "Akshay Krishnamurthy; Kirthevasan Kandasamy; Barnabas Poczos; Larry Wasserman",
        "abstract": "We consider nonparametric estimation of L_2, Renyi-\u03b1and Tsallis-\u03b1divergences between continuous distributions. Our approach is to construct estimators for particular integral functionals of two densities and translate them into divergence estimators. For the integral functionals, our estimators are based on corrections of a preliminary plug-in estimator. We show that these estimators achieve the parametric convergence rate of n^-1/2 when the densities\u2019 smoothness, s, are both at least d/4 where d is the dimension. We also derive minimax lower bounds for this problem which confirm that s > d/4 is necessary to achieve the n^-1/2 rate of convergence. We validate our theoretical guarantees with a number of simulations.",
        "bibtex": "@InProceedings{pmlr-v32-krishnamurthy14,\n  title = \t {Nonparametric Estimation of Renyi Divergence and Friends},\n  author = \t {Krishnamurthy, Akshay and Kandasamy, Kirthevasan and Poczos, Barnabas and Wasserman, Larry},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {919--927},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/krishnamurthy14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/krishnamurthy14.html},\n  abstract = \t {We consider nonparametric estimation of L_2, Renyi-\u03b1and Tsallis-\u03b1divergences between continuous distributions. Our approach is to construct estimators for particular integral functionals of two densities and translate them into divergence estimators. For the integral functionals, our estimators are based on corrections of a preliminary plug-in estimator. We show that these estimators achieve the parametric convergence rate of n^-1/2 when the densities\u2019 smoothness, s, are both at least d/4 where d is the dimension. We also derive minimax lower bounds for this problem which confirm that s > d/4 is necessary to achieve the n^-1/2 rate of convergence. We validate our theoretical guarantees with a number of simulations.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/krishnamurthy14.pdf",
        "supp": "",
        "pdf_size": 716403,
        "gs_citation": 98,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2350242363670174970&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": "CS.CMU.EDU;CS.CMU.EDU;CS.CMU.EDU;STAT.CMU.EDU",
        "email": "CS.CMU.EDU;CS.CMU.EDU;CS.CMU.EDU;STAT.CMU.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d3613578eb",
        "title": "Nuclear Norm Minimization via Active Subspace Selection",
        "site": "https://proceedings.mlr.press/v32/hsiehb14.html",
        "author": "Cho-Jui Hsieh; Peder Olsen",
        "abstract": "We describe a novel approach to optimizing matrix problems involving nuclear norm regularization and apply it to the matrix completion problem. We combine methods from non-smooth and smooth optimization. At each step we use the proximal gradient to select an active subspace. We then find a smooth, convex relaxation of the smaller subspace problems and solve these using second order methods. We apply our methods to matrix completion problems including Netflix dataset, and show that they are more than 6 times faster than state-of-the-art nuclear norm solvers. Also, this is the first paper to scale nuclear norm solvers to the Yahoo-Music dataset, and the first time in the literature that the efficiency of nuclear norm solvers can be compared and even compete with non-convex solvers like Alternating Least Squares (ALS).",
        "bibtex": "@InProceedings{pmlr-v32-hsiehb14,\n  title = \t {Nuclear Norm Minimization via Active Subspace Selection},\n  author = \t {Hsieh, Cho-Jui and Olsen, Peder},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {575--583},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/hsiehb14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/hsiehb14.html},\n  abstract = \t {We describe a novel approach to optimizing matrix problems involving nuclear norm regularization and apply it to the matrix completion problem. We combine methods from non-smooth and smooth optimization. At each step we use the proximal gradient to select an active subspace. We then find a smooth, convex relaxation of the smaller subspace problems and solve these using second order methods. We apply our methods to matrix completion problems including Netflix dataset, and show that they are more than 6 times faster than state-of-the-art nuclear norm solvers. Also, this is the first paper to scale nuclear norm solvers to the Yahoo-Music dataset, and the first time in the literature that the efficiency of nuclear norm solvers can be compared and even compete with non-convex solvers like Alternating Least Squares (ALS).}\n}",
        "pdf": "http://proceedings.mlr.press/v32/hsiehb14.pdf",
        "supp": "",
        "pdf_size": 313692,
        "gs_citation": 153,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10324726314863745392&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, The University of Texas, Austin, TX 78721, USA; IBM T.J. Watson Research Center, Yorktown Heights, NY 10598, USA",
        "aff_domain": "CS.UTEXAS.EDU;US.IBM.COM",
        "email": "CS.UTEXAS.EDU;US.IBM.COM",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Texas at Austin;IBM",
        "aff_unique_dep": "Department of Computer Science;IBM T.J. Watson Research Center",
        "aff_unique_url": "https://www.utexas.edu;https://www.ibm.com/research/watson",
        "aff_unique_abbr": "UT Austin;IBM Watson",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Austin;Yorktown Heights",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9da5f409ba",
        "title": "On Measure Concentration of Random Maximum A-Posteriori Perturbations",
        "site": "https://proceedings.mlr.press/v32/orabona14.html",
        "author": "Francesco Orabona; Tamir Hazan; Anand Sarwate; Tommi Jaakkola",
        "abstract": "The maximum a-posteriori (MAP) perturbation framework has emerged as a useful approach for inference and learning in high dimensional complex models.  By maximizing a randomly perturbed potential function, MAP perturbations generate unbiased samples from the Gibbs distribution.  Unfortunately, the computational cost of generating so many high-dimensional random variables can be prohibitive.  More efficient algorithms use sequential sampling strategies based on the expected value of low dimensional MAP perturbations. This paper develops new measure concentration inequalities that bound the number of samples needed to estimate such expected values. Applying the general result to MAP perturbations can yield a more efficient algorithm to approximate sampling from the Gibbs distribution.  The measure concentration result is of general interest and may be applicable to other areas involving Monte Carlo estimation of expectations.",
        "bibtex": "@InProceedings{pmlr-v32-orabona14,\n  title = \t {On Measure Concentration of Random Maximum A-Posteriori Perturbations},\n  author = \t {Orabona, Francesco and Hazan, Tamir and Sarwate, Anand and Jaakkola, Tommi},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {432--440},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/orabona14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/orabona14.html},\n  abstract = \t {The maximum a-posteriori (MAP) perturbation framework has emerged as a useful approach for inference and learning in high dimensional complex models.  By maximizing a randomly perturbed potential function, MAP perturbations generate unbiased samples from the Gibbs distribution.  Unfortunately, the computational cost of generating so many high-dimensional random variables can be prohibitive.  More efficient algorithms use sequential sampling strategies based on the expected value of low dimensional MAP perturbations. This paper develops new measure concentration inequalities that bound the number of samples needed to estimate such expected values. Applying the general result to MAP perturbations can yield a more efficient algorithm to approximate sampling from the Gibbs distribution.  The measure concentration result is of general interest and may be applicable to other areas involving Monte Carlo estimation of expectations.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/orabona14.pdf",
        "supp": "",
        "pdf_size": 378642,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16567451392365137828&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "Toyota Technological Institute at Chicago; Dept. of Computer Science, University of Haifa; Rutgers University, Dept. of Electrical and Computer Engineering; MIT CSAIL",
        "aff_domain": "ttic.edu;cs.haifa.ac.il;ece.rutgers.edu;csail.mit.edu",
        "email": "ttic.edu;cs.haifa.ac.il;ece.rutgers.edu;csail.mit.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "Toyota Technological Institute at Chicago;University of Haifa;Rutgers University;Massachusetts Institute of Technology",
        "aff_unique_dep": ";Dept. of Computer Science;Dept. of Electrical and Computer Engineering;Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.tti-chicago.org;https://www.haifa.ac.il;https://www.rutgers.edu;https://www.csail.mit.edu",
        "aff_unique_abbr": "TTI Chicago;UoH;Rutgers;MIT CSAIL",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Chicago;;Cambridge",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "United States;Israel"
    },
    {
        "id": "2a24c774d5",
        "title": "On Modelling Non-linear Topical Dependencies",
        "site": "https://proceedings.mlr.press/v32/lib14.html",
        "author": "Zhixing Li; Siqiang Wen; Juanzi Li; Peng Zhang; Jie Tang",
        "abstract": "Probabilistic topic models such as Latent Dirichlet Allocation (LDA) discover latent topics from large corpora by exploiting words\u2019 co-occurring relation. By observing the topical similarity between words, we find that some other relations, such as semantic or syntax relation between words, lead to strong dependence between their topics. In this paper, sentences are represented as dependency trees and a Global Topic Random Field (GTRF) is presented to model the non-linear dependencies between words. To infer our model, a new global factor is defined over all edges and the normalization factor of GRF is proven to be a constant. As a result, no independent assumption is needed when inferring our model. Based on it, we develop an efficient expectation-maximization (EM) procedure for parameter estimation. Experimental results on four data sets show that GTRF achieves much lower perplexity than LDA and linear dependency topic models and produces better topic coherence.",
        "bibtex": "@InProceedings{pmlr-v32-lib14,\n  title = \t {On Modelling Non-linear Topical Dependencies},\n  author = \t {Li, Zhixing and Wen, Siqiang and Li, Juanzi and Zhang, Peng and Tang, Jie},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {458--466},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/lib14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/lib14.html},\n  abstract = \t {Probabilistic topic models such as Latent Dirichlet Allocation (LDA) discover latent topics from large corpora by exploiting words\u2019 co-occurring relation. By observing the topical similarity between words, we find that some other relations, such as semantic or syntax relation between words, lead to strong dependence between their topics. In this paper, sentences are represented as dependency trees and a Global Topic Random Field (GTRF) is presented to model the non-linear dependencies between words. To infer our model, a new global factor is defined over all edges and the normalization factor of GRF is proven to be a constant. As a result, no independent assumption is needed when inferring our model. Based on it, we develop an efficient expectation-maximization (EM) procedure for parameter estimation. Experimental results on four data sets show that GTRF achieves much lower perplexity than LDA and linear dependency topic models and produces better topic coherence.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/lib14.pdf",
        "supp": "",
        "pdf_size": 269021,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7418346784967053344&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, Tsinghua University, Beijing, China; Department of Computer Science, Tsinghua University, Beijing, China; Department of Computer Science, Tsinghua University, Beijing, China; Department of Computer Science, Tsinghua University, Beijing, China; Department of Computer Science, Tsinghua University, Beijing, China",
        "aff_domain": "GMAIL.COM;GMAIL.COM;TSINGHUA.EDU.CN;GMAIL.COM;TSINGHUA.EDU.CN",
        "email": "GMAIL.COM;GMAIL.COM;TSINGHUA.EDU.CN;GMAIL.COM;TSINGHUA.EDU.CN",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Tsinghua University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "THU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "4880121e8e",
        "title": "On Robustness and Regularization of Structural Support Vector Machines",
        "site": "https://proceedings.mlr.press/v32/torkamani14.html",
        "author": "Mohamad Ali Torkamani; Daniel Lowd",
        "abstract": "Previous analysis of binary SVMs has demonstrated a deep connection between robustness to perturbations over uncertainty sets and regularization of the  weights.  In this paper, we explore the problem of learning robust  models for structured prediction problems.  We first formulate the problem  of learning robust structural SVMs when there are perturbations in  the feature space.  We consider two different classes of uncertainty sets for the perturbations: ellipsoidal uncertainty sets and polyhedral uncertainty sets. In both cases, we show that the robust optimization problem is equivalent to the non-robust formulation with an additional regularizer. For the ellipsoidal uncertainty set, the additional regularizer is based on the dual norm of the norm that constrains the ellipsoidal uncertainty. For the polyhedral uncertainty set, we show that the robust optimization problem is equivalent to adding a linear regularizer in a transformed weight space related to the linear constraints of the polyhedron. We also show that  these constraint sets can be combined and demonstrate a number of  interesting special cases.  This represents the first theoretical  analysis of robust optimization of structural support vector machines. Our experimental results show that our method outperforms the nonrobust structural SVMs on real world data when the test data distributions is drifted from the training data distribution.",
        "bibtex": "@InProceedings{pmlr-v32-torkamani14,\n  title = \t {On Robustness and Regularization of Structural Support Vector Machines},\n  author = \t {Torkamani, Mohamad Ali and Lowd, Daniel},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {577--585},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/torkamani14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/torkamani14.html},\n  abstract = \t {Previous analysis of binary SVMs has demonstrated a deep connection between robustness to perturbations over uncertainty sets and regularization of the  weights.  In this paper, we explore the problem of learning robust  models for structured prediction problems.  We first formulate the problem  of learning robust structural SVMs when there are perturbations in  the feature space.  We consider two different classes of uncertainty sets for the perturbations: ellipsoidal uncertainty sets and polyhedral uncertainty sets. In both cases, we show that the robust optimization problem is equivalent to the non-robust formulation with an additional regularizer. For the ellipsoidal uncertainty set, the additional regularizer is based on the dual norm of the norm that constrains the ellipsoidal uncertainty. For the polyhedral uncertainty set, we show that the robust optimization problem is equivalent to adding a linear regularizer in a transformed weight space related to the linear constraints of the polyhedron. We also show that  these constraint sets can be combined and demonstrate a number of  interesting special cases.  This represents the first theoretical  analysis of robust optimization of structural support vector machines. Our experimental results show that our method outperforms the nonrobust structural SVMs on real world data when the test data distributions is drifted from the training data distribution.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/torkamani14.pdf",
        "supp": "",
        "pdf_size": 330762,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=242035236928710135&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Computer and Information Science Department, University of Oregon; Computer and Information Science Department, University of Oregon",
        "aff_domain": "CS.UOREGON.EDU;CS.UOREGON.EDU",
        "email": "CS.UOREGON.EDU;CS.UOREGON.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Oregon",
        "aff_unique_dep": "Computer and Information Science Department",
        "aff_unique_url": "https://www.uoregon.edu",
        "aff_unique_abbr": "UO",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "586b32db24",
        "title": "On learning to localize objects with minimal supervision",
        "site": "https://proceedings.mlr.press/v32/songb14.html",
        "author": "Hyun Oh Song; Ross Girshick; Stefanie Jegelka; Julien Mairal; Zaid Harchaoui; Trevor Darrell",
        "abstract": "Learning to localize objects with minimal supervision is an important problem in computer vision, since large fully annotated datasets are extremely costly to obtain. In this paper, we propose a new method that achieves this goal with only image-level labels of whether the objects are present or not. Our approach combines a discriminative submodular cover problem for automatically discovering a set of positive object windows with a smoothed latent SVM formulation. The latter allows us to leverage efficient quasi-Newton optimization techniques. Our experiments demonstrate that the proposed approach provides a 50% relative improvement in mean average precision over the current state-of-the-art on PASCAL VOC 2007 detection.",
        "bibtex": "@InProceedings{pmlr-v32-songb14,\n  title = \t {On learning to localize objects with minimal supervision},\n  author = \t {Song, Hyun Oh and Girshick, Ross and Jegelka, Stefanie and Mairal, Julien and Harchaoui, Zaid and Darrell, Trevor},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1611--1619},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/songb14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/songb14.html},\n  abstract = \t {Learning to localize objects with minimal supervision is an important problem in computer vision, since large fully annotated datasets are extremely costly to obtain. In this paper, we propose a new method that achieves this goal with only image-level labels of whether the objects are present or not. Our approach combines a discriminative submodular cover problem for automatically discovering a set of positive object windows with a smoothed latent SVM formulation. The latter allows us to leverage efficient quasi-Newton optimization techniques. Our experiments demonstrate that the proposed approach provides a 50% relative improvement in mean average precision over the current state-of-the-art on PASCAL VOC 2007 detection.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/songb14.pdf",
        "supp": "",
        "pdf_size": 3914705,
        "gs_citation": 294,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10541375194788429768&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "EECS, UC Berkeley; EECS, UC Berkeley; EECS, UC Berkeley; INRIA; INRIA; EECS, UC Berkeley",
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu;inria.fr;inria.fr;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu;inria.fr;inria.fr;eecs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;1;0",
        "aff_unique_norm": "University of California, Berkeley;INRIA",
        "aff_unique_dep": "Electrical Engineering and Computer Sciences;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.inria.fr",
        "aff_unique_abbr": "UC Berkeley;INRIA",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0;0;1;1;0",
        "aff_country_unique": "United States;France"
    },
    {
        "id": "84504ca9cf",
        "title": "On p-norm Path Following in Multiple Kernel Learning for Non-linear Feature Selection",
        "site": "https://proceedings.mlr.press/v32/jawanpuria14.html",
        "author": "Pratik Jawanpuria; Manik Varma; Saketha Nath",
        "abstract": "Our objective is to develop formulations and algorithms for efficiently computing the feature selection path \u2013 i.e. the variation in classification accuracy as the fraction of selected features is varied from null to unity. Multiple Kernel Learning subject to l_p\\geq1 regularization (l_p-MKL) has been demonstrated to be one of the most effective techniques for non-linear feature selection. However, state-of-the-art l_p-MKL algorithms are too computationally expensive to be invoked thousands of times to determine the entire path.    We propose a novel conjecture which states that, for certain l_p-MKL formulations, the number of features selected in the optimal solution monotonically decreases as p is decreased from an initial value to unity. We prove the conjecture, for a generic family of kernel target alignment based formulations, and show that the feature weights themselves decay (grow) monotonically once they are below (above) a certain threshold at optimality. This allows us to develop a path following algorithm that systematically generates optimal feature sets of decreasing size. The proposed algorithm sets certain feature weights directly to zero for potentially large intervals of p thereby reducing optimization costs while simultaneously providing approximation guarantees.    We empirically demonstrate that our formulation can lead to classification accuracies which are as much as 10% higher on benchmark data sets not only as compared to other l_p-MKL formulations and uniform kernel baselines but also leading feature selection methods. We further demonstrate that our algorithm reduces training time significantly over other path following algorithms and state-of-the-art l_p-MKL optimizers such as SMO-MKL. In particular, we generate the entire feature selection path for data sets with a hundred thousand features in approximately half an hour on standard hardware.",
        "bibtex": "@InProceedings{pmlr-v32-jawanpuria14,\n  title = \t {On p-norm Path Following in Multiple Kernel Learning for Non-linear Feature Selection},\n  author = \t {Jawanpuria, Pratik and Varma, Manik and Nath, Saketha},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {118--126},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/jawanpuria14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/jawanpuria14.html},\n  abstract = \t {Our objective is to develop formulations and algorithms for efficiently computing the feature selection path \u2013 i.e. the variation in classification accuracy as the fraction of selected features is varied from null to unity. Multiple Kernel Learning subject to l_p\\geq1 regularization (l_p-MKL) has been demonstrated to be one of the most effective techniques for non-linear feature selection. However, state-of-the-art l_p-MKL algorithms are too computationally expensive to be invoked thousands of times to determine the entire path.    We propose a novel conjecture which states that, for certain l_p-MKL formulations, the number of features selected in the optimal solution monotonically decreases as p is decreased from an initial value to unity. We prove the conjecture, for a generic family of kernel target alignment based formulations, and show that the feature weights themselves decay (grow) monotonically once they are below (above) a certain threshold at optimality. This allows us to develop a path following algorithm that systematically generates optimal feature sets of decreasing size. The proposed algorithm sets certain feature weights directly to zero for potentially large intervals of p thereby reducing optimization costs while simultaneously providing approximation guarantees.    We empirically demonstrate that our formulation can lead to classification accuracies which are as much as 10% higher on benchmark data sets not only as compared to other l_p-MKL formulations and uniform kernel baselines but also leading feature selection methods. We further demonstrate that our algorithm reduces training time significantly over other path following algorithms and state-of-the-art l_p-MKL optimizers such as SMO-MKL. In particular, we generate the entire feature selection path for data sets with a hundred thousand features in approximately half an hour on standard hardware.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/jawanpuria14.pdf",
        "supp": "",
        "pdf_size": 364042,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7634006646717839377&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Department of Computer Science & Engineering, Indian Institute of Technology Bombay, India 400 076; Microsoft Research India, Bangalore, India 560 001; Department of Computer Science & Engineering, Indian Institute of Technology Bombay, India 400 076",
        "aff_domain": "CSE.IITB.AC.IN;MICROSOFT.COM;CSE.IITB.AC.IN",
        "email": "CSE.IITB.AC.IN;MICROSOFT.COM;CSE.IITB.AC.IN",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Indian Institute of Technology Bombay;Microsoft",
        "aff_unique_dep": "Department of Computer Science & Engineering;Microsoft Research India",
        "aff_unique_url": "https://www.iitb.ac.in;https://www.microsoft.com/en-us/research/group/microsoft-research-india",
        "aff_unique_abbr": "IIT Bombay;MSRI",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Bombay;Bangalore",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "56f0fab880",
        "title": "On the convergence of no-regret learning in selfish routing",
        "site": "https://proceedings.mlr.press/v32/krichene14.html",
        "author": "Walid Krichene; Benjamin Drigh\u00e8s; Alexandre Bayen",
        "abstract": "We study the repeated, non-atomic routing game, in which selfish players make a sequence of routing decisions. We consider a model in which players use regret-minimizing algorithms as the learning mechanism, and study the resulting dynamics. We are concerned in particular with the convergence to the set of Nash equilibria of the routing game. No-regret learning algorithms are known to guarantee convergence of a subsequence of population strategies. We are concerned with convergence of the actual sequence. We show that convergence holds for a large class of online learning algorithms, inspired from the continuous-time replicator dynamics. In particular, the discounted Hedge algorithm is proved to belong to this class, which guarantees its convergence.",
        "bibtex": "@InProceedings{pmlr-v32-krichene14,\n  title = \t {On the convergence of no-regret learning in selfish routing},\n  author = \t {Krichene, Walid and Drigh\u00e8s, Benjamin and Bayen, Alexandre},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {163--171},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/krichene14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/krichene14.html},\n  abstract = \t {We study the repeated, non-atomic routing game, in which selfish players make a sequence of routing decisions. We consider a model in which players use regret-minimizing algorithms as the learning mechanism, and study the resulting dynamics. We are concerned in particular with the convergence to the set of Nash equilibria of the routing game. No-regret learning algorithms are known to guarantee convergence of a subsequence of population strategies. We are concerned with convergence of the actual sequence. We show that convergence holds for a large class of online learning algorithms, inspired from the continuous-time replicator dynamics. In particular, the discounted Hedge algorithm is proved to belong to this class, which guarantees its convergence.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/krichene14.pdf",
        "supp": "",
        "pdf_size": 394444,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15866703215196727515&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "University of California, 652 Sutardja Dai Hall, Berkeley, CA 94720 USA; Ecole Polytechnique, Route de Saclay, 91120 Palaiseau, France; University of California, 642 Sutardja Dai Hall, Berkeley, CA 94720 USA",
        "aff_domain": "eecs.berkeley.edu;polytechnique.edu;berkeley.edu",
        "email": "eecs.berkeley.edu;polytechnique.edu;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of California, Berkeley;Ecole Polytechnique",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.berkeley.edu;https://www.ec-polytechnique.fr",
        "aff_unique_abbr": "UC Berkeley;X",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Berkeley;Palaiseau",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;France"
    },
    {
        "id": "eb32790fe6",
        "title": "One Practical Algorithm for Both Stochastic and Adversarial Bandits",
        "site": "https://proceedings.mlr.press/v32/seldinb14.html",
        "author": "Yevgeny Seldin; Aleksandrs Slivkins",
        "abstract": "We present an algorithm for multiarmed bandits that achieves almost optimal performance in both stochastic and adversarial regimes without prior knowledge about the nature of the environment. Our algorithm is based on augmentation of the EXP3 algorithm with a new control lever in the form of exploration parameters that are tailored individually for each arm. The algorithm simultaneously applies the \u201cold\u201d control lever, the learning rate, to control the regret in the adversarial regime and the new control lever to detect and exploit gaps between the arm losses. This secures problem-dependent \u201clogarithmic\u201d regret when gaps are present without compromising on the worst-case performance guarantee in the adversarial regime. We show that the algorithm can exploit both the usual expected gaps between the arm losses in the stochastic regime and deterministic gaps between the arm losses in the adversarial regime. The algorithm retains \u201clogarithmic\u201d regret guarantee in the stochastic regime even when some observations are contaminated by an adversary, as long as on average the contamination does not reduce the gap by more than a half. Our results for the stochastic regime are supported by experimental validation.",
        "bibtex": "@InProceedings{pmlr-v32-seldinb14,\n  title = \t {One Practical Algorithm for Both Stochastic and Adversarial Bandits},\n  author = \t {Seldin, Yevgeny and Slivkins, Aleksandrs},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1287--1295},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/seldinb14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/seldinb14.html},\n  abstract = \t {We present an algorithm for multiarmed bandits that achieves almost optimal performance in both stochastic and adversarial regimes without prior knowledge about the nature of the environment. Our algorithm is based on augmentation of the EXP3 algorithm with a new control lever in the form of exploration parameters that are tailored individually for each arm. The algorithm simultaneously applies the \u201cold\u201d control lever, the learning rate, to control the regret in the adversarial regime and the new control lever to detect and exploit gaps between the arm losses. This secures problem-dependent \u201clogarithmic\u201d regret when gaps are present without compromising on the worst-case performance guarantee in the adversarial regime. We show that the algorithm can exploit both the usual expected gaps between the arm losses in the stochastic regime and deterministic gaps between the arm losses in the adversarial regime. The algorithm retains \u201clogarithmic\u201d regret guarantee in the stochastic regime even when some observations are contaminated by an adversary, as long as on average the contamination does not reduce the gap by more than a half. Our results for the stochastic regime are supported by experimental validation.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/seldinb14.pdf",
        "supp": "",
        "pdf_size": 812414,
        "gs_citation": 199,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9063212351741103724&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Queensland University of Technology, Brisbane, Australia; Microsoft Research, New York NY, USA",
        "aff_domain": "gmail.com;microsoft.com",
        "email": "gmail.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Queensland University of Technology;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.qut.edu.au;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "QUT;MSR",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Brisbane;New York",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Australia;United States"
    },
    {
        "id": "b7fa0df6d9",
        "title": "Online Bayesian Passive-Aggressive Learning",
        "site": "https://proceedings.mlr.press/v32/shi14.html",
        "author": "Tianlin Shi; Jun Zhu",
        "abstract": "Online Passive-Aggressive (PA) learning is an effective framework for performing max-margin online learning. But the deterministic formulation and estimated single large-margin model could limit its capability in discovering descriptive structures underlying complex data. This paper presents online Bayesian Passive-Aggressive (BayesPA) learning, which subsumes the online PA and extends naturally to incorporate latent variables and perform nonparametric Bayesian inference, thus providing great flexibility for explorative analysis. We apply BayesPA to topic modeling and derive efficient online learning algorithms for max-margin topic models. We further develop nonparametric methods to resolve the number of topics. Experimental results on real datasets show that our approaches significantly improve time efficiency while maintaining comparable results with the batch counterparts.",
        "bibtex": "@InProceedings{pmlr-v32-shi14,\n  title = \t {Online Bayesian Passive-Aggressive Learning},\n  author = \t {Shi, Tianlin and Zhu, Jun},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {378--386},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/shi14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/shi14.html},\n  abstract = \t {Online Passive-Aggressive (PA) learning is an effective framework for performing max-margin online learning. But the deterministic formulation and estimated single large-margin model could limit its capability in discovering descriptive structures underlying complex data. This paper presents online Bayesian Passive-Aggressive (BayesPA) learning, which subsumes the online PA and extends naturally to incorporate latent variables and perform nonparametric Bayesian inference, thus providing great flexibility for explorative analysis. We apply BayesPA to topic modeling and derive efficient online learning algorithms for max-margin topic models. We further develop nonparametric methods to resolve the number of topics. Experimental results on real datasets show that our approaches significantly improve time efficiency while maintaining comparable results with the batch counterparts.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/shi14.pdf",
        "supp": "",
        "pdf_size": 589447,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11555296768971986491&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Institute for Interdisciplinary Information Sciences, Tsinghua University, China; Dept. of Comp. Sci. & Tech., TNList Lab, State Key Lab of Intell. Tech. & Sys., Tsinghua University, China",
        "aff_domain": "GMAIL.COM;MAIL.TSINGHUA.EDU.CN",
        "email": "GMAIL.COM;MAIL.TSINGHUA.EDU.CN",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Tsinghua University",
        "aff_unique_dep": "Institute for Interdisciplinary Information Sciences",
        "aff_unique_url": "https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "Tsinghua",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "ce513c76c0",
        "title": "Online Clustering of Bandits",
        "site": "https://proceedings.mlr.press/v32/gentile14.html",
        "author": "Claudio Gentile; Shuai Li; Giovanni Zappella",
        "abstract": "We introduce a novel algorithmic approach to content recommendation based on adaptive clustering of exploration-exploitation (\u201cbandit\") strategies. We provide a sharp regret analysis of this algorithm in a standard stochastic noise setting, demonstrate its scalability properties, and prove its effectiveness on a number of artificial and real-world datasets. Our experiments show a significant increase in prediction performance over state-of-the-art methods for bandit problems.",
        "bibtex": "@InProceedings{pmlr-v32-gentile14,\n  title = \t {Online Clustering of Bandits},\n  author = \t {Gentile, Claudio and Li, Shuai and Zappella, Giovanni},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {757--765},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/gentile14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/gentile14.html},\n  abstract = \t {We introduce a novel algorithmic approach to content recommendation based on adaptive clustering of exploration-exploitation (\u201cbandit\") strategies. We provide a sharp regret analysis of this algorithm in a standard stochastic noise setting, demonstrate its scalability properties, and prove its effectiveness on a number of artificial and real-world datasets. Our experiments show a significant increase in prediction performance over state-of-the-art methods for bandit problems.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/gentile14.pdf",
        "supp": "",
        "pdf_size": 1367954,
        "gs_citation": 346,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5858497582839693273&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "DiSTA, University of Insubria, Italy; DiSTA, University of Insubria, Italy; Amazon Development Center Germany, Germany + University of Milan",
        "aff_domain": "UNINSUBRIA.IT;GMAIL.COM;AMAZON.COM",
        "email": "UNINSUBRIA.IT;GMAIL.COM;AMAZON.COM",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1+2",
        "aff_unique_norm": "University of Insubria;Amazon;University of Milan",
        "aff_unique_dep": "DiSTA;Amazon Development Center Germany;",
        "aff_unique_url": "https://www.uninsubria.it;https://www.amazon.de;https://www.unimi.it",
        "aff_unique_abbr": ";;UniMi",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1+0",
        "aff_country_unique": "Italy;Germany"
    },
    {
        "id": "00ec5096bc",
        "title": "Online Learning in Markov Decision Processes with Changing Cost Sequences",
        "site": "https://proceedings.mlr.press/v32/dick14.html",
        "author": "Travis Dick; Andras Gyorgy; Csaba Szepesvari",
        "abstract": "In this paper we consider online learning in finite Markov decision processes (MDPs) with changing cost sequences under full and bandit-information.  We propose to view this problem as an instance of online linear optimization.  We propose two methods for this problem: MD^2 (mirror descent with approximate projections) and the continuous exponential weights algorithm with Dikin walks.  We provide a rigorous complexity analysis of these techniques, while providing near-optimal regret-bounds (in particular, we take into account the computational costs of performing approximate projections in MD^2).  In the case of full-information feedback, our results complement existing ones. In the case of bandit-information feedback we consider the online stochastic shortest path problem, a special case of the above MDP problems, and manage to improve the existing results by removing the previous restrictive assumption that the state-visitation probabilities are uniformly bounded away from zero under all policies.",
        "bibtex": "@InProceedings{pmlr-v32-dick14,\n  title = \t {Online Learning in Markov Decision Processes with Changing Cost Sequences},\n  author = \t {Dick, Travis and Gyorgy, Andras and Szepesvari, Csaba},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {512--520},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/dick14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/dick14.html},\n  abstract = \t {In this paper we consider online learning in finite Markov decision processes (MDPs) with changing cost sequences under full and bandit-information.  We propose to view this problem as an instance of online linear optimization.  We propose two methods for this problem: MD^2 (mirror descent with approximate projections) and the continuous exponential weights algorithm with Dikin walks.  We provide a rigorous complexity analysis of these techniques, while providing near-optimal regret-bounds (in particular, we take into account the computational costs of performing approximate projections in MD^2).  In the case of full-information feedback, our results complement existing ones. In the case of bandit-information feedback we consider the online stochastic shortest path problem, a special case of the above MDP problems, and manage to improve the existing results by removing the previous restrictive assumption that the state-visitation probabilities are uniformly bounded away from zero under all policies.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/dick14.pdf",
        "supp": "",
        "pdf_size": 416744,
        "gs_citation": 89,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7177188463131972220&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computing Science, University of Alberta, Edmonton, AB, Canada T6G 2E8; Department of Computing Science, University of Alberta, Edmonton, AB, Canada T6G 2E8; Department of Computing Science, University of Alberta, Edmonton, AB, Canada T6G 2E8",
        "aff_domain": "UALBERTA.CA;UALBERTA.CA;UALBERTA.CA",
        "email": "UALBERTA.CA;UALBERTA.CA;UALBERTA.CA",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "Department of Computing Science",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Edmonton",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "c44fe203ab",
        "title": "Online Multi-Task Learning for Policy Gradient Methods",
        "site": "https://proceedings.mlr.press/v32/ammar14.html",
        "author": "Haitham Bou Ammar; Eric Eaton; Paul Ruvolo; Matthew Taylor",
        "abstract": "Policy gradient algorithms have shown considerable recent success in solving high-dimensional sequential decision making tasks, particularly in robotics.  However, these methods often require extensive experience in a domain to achieve high performance.  To make agents more sample-efficient, we developed a multi-task policy gradient method to learn decision making tasks consecutively, transferring knowledge between tasks to accelerate learning.  Our approach provides robust theoretical guarantees, and we show empirically that it dramatically accelerates learning on a variety of dynamical systems, including an application to quadrotor control.",
        "bibtex": "@InProceedings{pmlr-v32-ammar14,\n  title = \t {Online Multi-Task Learning for Policy Gradient Methods},\n  author = \t {Ammar, Haitham Bou and Eaton, Eric and Ruvolo, Paul and Taylor, Matthew},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1206--1214},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/ammar14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/ammar14.html},\n  abstract = \t {Policy gradient algorithms have shown considerable recent success in solving high-dimensional sequential decision making tasks, particularly in robotics.  However, these methods often require extensive experience in a domain to achieve high performance.  To make agents more sample-efficient, we developed a multi-task policy gradient method to learn decision making tasks consecutively, transferring knowledge between tasks to accelerate learning.  Our approach provides robust theoretical guarantees, and we show empirically that it dramatically accelerates learning on a variety of dynamical systems, including an application to quadrotor control.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/ammar14.pdf",
        "supp": "",
        "pdf_size": 3286494,
        "gs_citation": 202,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5762458291526911782&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "University of Pennsylvania, Computer and Information Science Department, Philadelphia, PA 19104 USA; University of Pennsylvania, Computer and Information Science Department, Philadelphia, PA 19104 USA; Olin College of Engineering, Needham, MA 02492 USA; Washington State University, School of Electrical Engineering and Computer Science, Pullman, WA 99164 USA",
        "aff_domain": "seas.upenn.edu;cis.upenn.edu;olin.edu;eecs.wsu.edu",
        "email": "seas.upenn.edu;cis.upenn.edu;olin.edu;eecs.wsu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "University of Pennsylvania;Olin College of Engineering;Washington State University",
        "aff_unique_dep": "Computer and Information Science Department;;School of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.upenn.edu;https://www.olin.edu;https://wsu.edu",
        "aff_unique_abbr": "UPenn;Olin;WSU",
        "aff_campus_unique_index": "0;0;2",
        "aff_campus_unique": "Philadelphia;;Pullman",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "30003b0295",
        "title": "Online Stochastic Optimization  under Correlated Bandit Feedback",
        "site": "https://proceedings.mlr.press/v32/azar14.html",
        "author": "Mohammad Gheshlaghi azar; Alessandro Lazaric; Emma Brunskill",
        "abstract": "In this paper we consider the problem of online stochastic optimization of a locally smooth function under bandit feedback. We introduce the high-confidence tree (HCT) algorithm, a novel anytime \\mathcal X-armed bandit algorithm, and derive regret bounds matching the performance of state-of-the-art algorithms in terms of the dependency on number of steps and the near-optimality dimension. The main advantage of HCT is that it handles the challenging case of correlated bandit feedback (reward), whereas existing methods require rewards to be conditionally independent. HCT also improves on the state-of-the-art in terms of the memory requirement, as well as requiring a weaker smoothness assumption on the mean-reward function in comparison with the existing anytime algorithms. Finally, we discuss how HCT can be applied to the problem of policy search in reinforcement learning and we report preliminary empirical results.",
        "bibtex": "@InProceedings{pmlr-v32-azar14,\n  title = \t {Online Stochastic Optimization  under Correlated Bandit Feedback},\n  author = \t {azar, Mohammad Gheshlaghi and Lazaric, Alessandro and Brunskill, Emma},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1557--1565},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/azar14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/azar14.html},\n  abstract = \t {In this paper we consider the problem of online stochastic optimization of a locally smooth function under bandit feedback. We introduce the high-confidence tree (HCT) algorithm, a novel anytime \\mathcal X-armed bandit algorithm, and derive regret bounds matching the performance of state-of-the-art algorithms in terms of the dependency on number of steps and the near-optimality dimension. The main advantage of HCT is that it handles the challenging case of correlated bandit feedback (reward), whereas existing methods require rewards to be conditionally independent. HCT also improves on the state-of-the-art in terms of the memory requirement, as well as requiring a weaker smoothness assumption on the mean-reward function in comparison with the existing anytime algorithms. Finally, we discuss how HCT can be applied to the problem of policy search in reinforcement learning and we report preliminary empirical results.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/azar14.pdf",
        "supp": "",
        "pdf_size": 887239,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7972011138351199265&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Rehabilitation Institute of Chicago, Northwestern University; Team SequeL, INRIA Nord Europe; School of Computer Science, CMU",
        "aff_domain": "NORTHWESTERN.EDU;INRIA.FR;CS.CMU.EDU",
        "email": "NORTHWESTERN.EDU;INRIA.FR;CS.CMU.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Northwestern University;INRIA;Carnegie Mellon University",
        "aff_unique_dep": "Rehabilitation Institute;Team SequeL;School of Computer Science",
        "aff_unique_url": "https://www.northwestern.edu;https://www.inria.fr;https://www.cmu.edu",
        "aff_unique_abbr": "NU;INRIA;CMU",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "Chicago;Nord Europe;Pittsburgh",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;France"
    },
    {
        "id": "a2b829aecd",
        "title": "Optimal Budget Allocation: Theoretical Guarantee and Efficient Algorithm",
        "site": "https://proceedings.mlr.press/v32/soma14.html",
        "author": "Tasuku Soma; Naonori Kakimura; Kazuhiro Inaba; Ken-ichi Kawarabayashi",
        "abstract": "We consider the budget allocation problem over bipartite influence model proposed by Alon et al. This problem can be viewed as the well-known influence maximization problem with budget constraints.     We first show that this problem and its much more general form  fall into a general setting; namely the monotone submodular function maximization over integer lattice subject to a knapsack constraint.  Our framework includes Alon et al.\u2019s model, even with a competitor and with cost.  We then give a (1-1/e)-approximation algorithm for this more general problem. Furthermore, when influence probabilities are nonincreasing, we obtain a faster (1-1/e)-approximation algorithm, which runs essentially in linear time in the number of nodes. This allows us to implement our algorithm up to almost 10M edges (indeed, our experiments tell us that we can implement our algorithm up to 1 billion edges. It would approximately take us only 500 seconds.).",
        "bibtex": "@InProceedings{pmlr-v32-soma14,\n  title = \t {Optimal Budget Allocation: Theoretical Guarantee and Efficient Algorithm},\n  author = \t {Soma, Tasuku and Kakimura, Naonori and Inaba, Kazuhiro and Kawarabayashi, Ken-ichi},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {351--359},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/soma14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/soma14.html},\n  abstract = \t {We consider the budget allocation problem over bipartite influence model proposed by Alon et al. This problem can be viewed as the well-known influence maximization problem with budget constraints.     We first show that this problem and its much more general form  fall into a general setting; namely the monotone submodular function maximization over integer lattice subject to a knapsack constraint.  Our framework includes Alon et al.\u2019s model, even with a competitor and with cost.  We then give a (1-1/e)-approximation algorithm for this more general problem. Furthermore, when influence probabilities are nonincreasing, we obtain a faster (1-1/e)-approximation algorithm, which runs essentially in linear time in the number of nodes. This allows us to implement our algorithm up to almost 10M edges (indeed, our experiments tell us that we can implement our algorithm up to 1 billion edges. It would approximately take us only 500 seconds.).}\n}",
        "pdf": "http://proceedings.mlr.press/v32/soma14.pdf",
        "supp": "",
        "pdf_size": 296567,
        "gs_citation": 140,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7061978733788091219&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, 113-8656 + JST, ERATO, Kawarabayashi Large Graph Project, Tokyo, 101-8430; College of Arts and Sciences, The University of Tokyo, Tokyo, 153-8902; Google Inc.; National Institute of Informatics, JST, ERATO, Kawarabayashi Project, Tokyo, 101-8430",
        "aff_domain": "mist.i.u-tokyo.ac.jp;global.c.u-tokyo.ac.jp;google.com;nii.ac.jp",
        "email": "mist.i.u-tokyo.ac.jp;global.c.u-tokyo.ac.jp;google.com;nii.ac.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;2;3",
        "aff_unique_norm": "University of Tokyo;Japan Science and Technology Agency;Google;National Institute of Informatics",
        "aff_unique_dep": "Graduate School of Information Science and Technology;Kawarabayashi Large Graph Project;Google;",
        "aff_unique_url": "https://www.u-tokyo.ac.jp;https://www.jst.go.jp;https://www.google.com;https://www.nii.ac.jp",
        "aff_unique_abbr": "UTokyo;JST;Google;NII",
        "aff_campus_unique_index": "0;0;2;0",
        "aff_campus_unique": "Tokyo;;Mountain View",
        "aff_country_unique_index": "0+0;0;1;0",
        "aff_country_unique": "Japan;United States"
    },
    {
        "id": "9f1589a360",
        "title": "Optimal Mean Robust Principal Component Analysis",
        "site": "https://proceedings.mlr.press/v32/nieb14.html",
        "author": "Feiping Nie; Jianjun Yuan; Heng Huang",
        "abstract": "Dimensionality reduction techniques extract low-dimensional structure from high-dimensional data and are widespread in machine learning research. In practice, due to lacking labeled data, the unsupervised dimensionality reduction algorithms are more desired. Among them, Principal Component Analysis (PCA) is the most widely used approach. In recent research, several robust PCA algorithms were presented to enhance the robustness of PCA model. However, all existing robust PCA methods incorrectly center the data using the L2-norm distance to calculate the mean, which actually is not the optimal mean due to the L1-norm used in the objective functions. It is non-trivial to remove the optimal mean in the robust PCA, because of the sparsity-inducing norms used in the robust formulations. In this paper, we propose novel robust PCA objective functions with removing optimal mean automatically. We naturally integrate the mean calculation into the dimensionality reduction optimization, such that the optimal mean can be obtained to enhance the dimensionality reduction. Both theoretical analysis and empirical studies demonstrate our new methods can more effectively reduce data dimensionality than previous robust PCA methods.",
        "bibtex": "@InProceedings{pmlr-v32-nieb14,\n  title = \t {Optimal Mean Robust Principal Component Analysis},\n  author = \t {Nie, Feiping and Yuan, Jianjun and Huang, Heng},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1062--1070},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/nieb14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/nieb14.html},\n  abstract = \t {Dimensionality reduction techniques extract low-dimensional structure from high-dimensional data and are widespread in machine learning research. In practice, due to lacking labeled data, the unsupervised dimensionality reduction algorithms are more desired. Among them, Principal Component Analysis (PCA) is the most widely used approach. In recent research, several robust PCA algorithms were presented to enhance the robustness of PCA model. However, all existing robust PCA methods incorrectly center the data using the L2-norm distance to calculate the mean, which actually is not the optimal mean due to the L1-norm used in the objective functions. It is non-trivial to remove the optimal mean in the robust PCA, because of the sparsity-inducing norms used in the robust formulations. In this paper, we propose novel robust PCA objective functions with removing optimal mean automatically. We naturally integrate the mean calculation into the dimensionality reduction optimization, such that the optimal mean can be obtained to enhance the dimensionality reduction. Both theoretical analysis and empirical studies demonstrate our new methods can more effectively reduce data dimensionality than previous robust PCA methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/nieb14.pdf",
        "supp": "",
        "pdf_size": 338529,
        "gs_citation": 296,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4350732117259030829&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Computer Science and Engineering Department, University of Texas at Arlington, Arlington, TX, 76019; Computer Science and Engineering Department, University of Texas at Arlington, Arlington, TX, 76019; Computer Science and Engineering Department, University of Texas at Arlington, Arlington, TX, 76019",
        "aff_domain": "gmail.com;gmail.com;uta.edu",
        "email": "gmail.com;gmail.com;uta.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Texas at Arlington",
        "aff_unique_dep": "Computer Science and Engineering Department",
        "aff_unique_url": "https://www.uta.edu",
        "aff_unique_abbr": "UTA",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Arlington",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4f2fa8e44d",
        "title": "Optimal PAC Multiple Arm Identification with Applications to Crowdsourcing",
        "site": "https://proceedings.mlr.press/v32/zhoub14.html",
        "author": "Yuan Zhou; Xi Chen; Jian Li",
        "abstract": "We study the problem of selecting K arms with the highest expected rewards in a stochastic N-armed bandit game.  Instead of using existing evaluation metrics  (e.g.,  misidentification probability or the metric in EXPLORE-K), we propose to use the aggregate regret, which is defined as the gap between the average reward of the optimal solution and that of our solution. Besides being a natural metric by itself, we argue that in many applications, such as our motivating example from crowdsourcing, the aggregate regret bound is more suitable. We propose a new PAC algorithm, which,  with  probability at least 1-\u03b4, identifies a set of K arms with regret at most \u03b5. We provide the sample complexity bound of our algorithm. To complement, we establish the  lower bound and show that the sample complexity of our algorithm matches the lower bound. Finally, we report experimental results on both synthetic and real data sets, which demonstrates the superior performance of the proposed algorithm.",
        "bibtex": "@InProceedings{pmlr-v32-zhoub14,\n  title = \t {Optimal PAC Multiple Arm Identification with Applications to Crowdsourcing},\n  author = \t {Zhou, Yuan and Chen, Xi and Li, Jian},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {217--225},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/zhoub14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/zhoub14.html},\n  abstract = \t {We study the problem of selecting K arms with the highest expected rewards in a stochastic N-armed bandit game.  Instead of using existing evaluation metrics  (e.g.,  misidentification probability or the metric in EXPLORE-K), we propose to use the aggregate regret, which is defined as the gap between the average reward of the optimal solution and that of our solution. Besides being a natural metric by itself, we argue that in many applications, such as our motivating example from crowdsourcing, the aggregate regret bound is more suitable. We propose a new PAC algorithm, which,  with  probability at least 1-\u03b4, identifies a set of K arms with regret at most \u03b5. We provide the sample complexity bound of our algorithm. To complement, we establish the  lower bound and show that the sample complexity of our algorithm matches the lower bound. Finally, we report experimental results on both synthetic and real data sets, which demonstrates the superior performance of the proposed algorithm.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/zhoub14.pdf",
        "supp": "",
        "pdf_size": 485479,
        "gs_citation": 127,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1724546694646576682&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Carnegie Mellon U; UC Berkeley; Tsinghua U",
        "aff_domain": "CS.CMU.EDU;CS.CMU.EDU;MAIL.TSINGHUA.EDU.CN",
        "email": "CS.CMU.EDU;CS.CMU.EDU;MAIL.TSINGHUA.EDU.CN",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Carnegie Mellon University;University of California, Berkeley;Tsinghua University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.cmu.edu;https://www.berkeley.edu;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "CMU;UC Berkeley;Tsinghua",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "0c8c28c626",
        "title": "Optimization Equivalence of Divergences Improves Neighbor Embedding",
        "site": "https://proceedings.mlr.press/v32/yange14.html",
        "author": "Zhirong Yang; Jaakko Peltonen; Samuel Kaski",
        "abstract": "Visualization methods that arrange data objects in 2D or 3D layouts have followed two main schools, methods oriented for graph layout and methods oriented for vectorial embedding. We show the two previously separate approaches are tied by an optimization equivalence, making it possible to relate methods from the two approaches and to build new methods that take the best of both worlds.  In detail, we prove a theorem of optimization equivalences between beta- and gamma-, as well as alpha- and Renyi-divergences through a connection scalar. Through the equivalences we represent several nonlinear dimensionality reduction and graph drawing methods in a generalized stochastic neighbor embedding setting, where information divergences are minimized between similarities in input and output spaces, and the optimal connection scalar provides a natural choice for the tradeoff between attractive and repulsive forces. We give two examples of developing new visualization methods through the equivalences: 1) We develop weighted symmetric stochastic neighbor embedding (ws-SNE) from Elastic Embedding and analyze its benefits, good performance for both vectorial and network data; in experiments ws-SNE has good performance across data sets of different types, whereas comparison methods fail for some of the data sets; 2) we develop a gamma-divergence version of a PolyLog layout method; the new method is scale invariant in the output space and makes it possible to efficiently use large-scale smoothed neighborhoods.",
        "bibtex": "@InProceedings{pmlr-v32-yange14,\n  title = \t {Optimization Equivalence of Divergences Improves Neighbor Embedding},\n  author = \t {Yang, Zhirong and Peltonen, Jaakko and Kaski, Samuel},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {460--468},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/yange14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/yange14.html},\n  abstract = \t {Visualization methods that arrange data objects in 2D or 3D layouts have followed two main schools, methods oriented for graph layout and methods oriented for vectorial embedding. We show the two previously separate approaches are tied by an optimization equivalence, making it possible to relate methods from the two approaches and to build new methods that take the best of both worlds.  In detail, we prove a theorem of optimization equivalences between beta- and gamma-, as well as alpha- and Renyi-divergences through a connection scalar. Through the equivalences we represent several nonlinear dimensionality reduction and graph drawing methods in a generalized stochastic neighbor embedding setting, where information divergences are minimized between similarities in input and output spaces, and the optimal connection scalar provides a natural choice for the tradeoff between attractive and repulsive forces. We give two examples of developing new visualization methods through the equivalences: 1) We develop weighted symmetric stochastic neighbor embedding (ws-SNE) from Elastic Embedding and analyze its benefits, good performance for both vectorial and network data; in experiments ws-SNE has good performance across data sets of different types, whereas comparison methods fail for some of the data sets; 2) we develop a gamma-divergence version of a PolyLog layout method; the new method is scale invariant in the output space and makes it possible to efficiently use large-scale smoothed neighborhoods.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/yange14.pdf",
        "supp": "",
        "pdf_size": 2386178,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17569529039846252842&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Information and Computer Science, Aalto University, Finland; Helsinki Institute for Information Technology HIIT + Department of Computer Science, University of Helsinki, Finland + University of Tampere, Finland; Helsinki Institute for Information Technology HIIT + Department of Computer Science, University of Helsinki, Finland",
        "aff_domain": "AALTO.FI;AALTO.FI;AALTO.FI",
        "email": "AALTO.FI;AALTO.FI;AALTO.FI",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2+3;1+2",
        "aff_unique_norm": "Aalto University;Helsinki Institute for Information Technology;University of Helsinki;University of Tampere",
        "aff_unique_dep": "Department of Information and Computer Science;HIIT;Department of Computer Science;",
        "aff_unique_url": "https://www.aalto.fi;https://www.hiit.fi;https://www.helsinki.fi;https://www.uta.fi",
        "aff_unique_abbr": "Aalto;HIIT;UH;UTA",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0+0;0+0",
        "aff_country_unique": "Finland"
    },
    {
        "id": "e867171544",
        "title": "Outlier Path: A Homotopy Algorithm for Robust SVM",
        "site": "https://proceedings.mlr.press/v32/suzumura14.html",
        "author": "Shinya Suzumura; Kohei Ogawa; Masashi Sugiyama; Ichiro Takeuchi",
        "abstract": "In recent applications with massive but less reliable data (e.g., labels obtained by a semi-supervised learning method or crowdsourcing), non-robustness of the support vector machine (SVM) often causes considerable performance deterioration. Although improving the robustness of SVM has been investigated for long time, robust SVM (RSVM) learning still poses two major challenges: obtaining a good (local) solution from a non-convex optimization problem and optimally controlling the robustness-efficiency trade-off. In this paper, we address these two issues simultaneously in an integrated way by introducing a novel homotopy approach to RSVM learning. Based on theoretical investigation of the geometry of RSVM solutions, we show that a path of local RSVM solutions can be computed efficiently when the influence of outliers is gradually suppressed as simulated annealing. We experimentally demonstrate that our algorithm tends to produce better local solutions than the alternative approach based on the concave-convex procedure, with the ability of stable and efficient model selection for controlling the influence of outliers.",
        "bibtex": "@InProceedings{pmlr-v32-suzumura14,\n  title = \t {Outlier Path: A Homotopy Algorithm for Robust SVM},\n  author = \t {Suzumura, Shinya and Ogawa, Kohei and Sugiyama, Masashi and Takeuchi, Ichiro},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1098--1106},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/suzumura14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/suzumura14.html},\n  abstract = \t {In recent applications with massive but less reliable data (e.g., labels obtained by a semi-supervised learning method or crowdsourcing), non-robustness of the support vector machine (SVM) often causes considerable performance deterioration. Although improving the robustness of SVM has been investigated for long time, robust SVM (RSVM) learning still poses two major challenges: obtaining a good (local) solution from a non-convex optimization problem and optimally controlling the robustness-efficiency trade-off. In this paper, we address these two issues simultaneously in an integrated way by introducing a novel homotopy approach to RSVM learning. Based on theoretical investigation of the geometry of RSVM solutions, we show that a path of local RSVM solutions can be computed efficiently when the influence of outliers is gradually suppressed as simulated annealing. We experimentally demonstrate that our algorithm tends to produce better local solutions than the alternative approach based on the concave-convex procedure, with the ability of stable and efficient model selection for controlling the influence of outliers.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/suzumura14.pdf",
        "supp": "",
        "pdf_size": 346502,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=61237229716041010&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Nagoya Institute of Technology; Nagoya Institute of Technology; Tokyo Institute of Technology; Nagoya Institute of Technology",
        "aff_domain": "gmail.com;gmail.com;cs.titech.ac.jp;nitech.ac.jp",
        "email": "gmail.com;gmail.com;cs.titech.ac.jp;nitech.ac.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Nagoya Institute of Technology;Tokyo Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.nitech.ac.jp;https://www.titech.ac.jp",
        "aff_unique_abbr": "NIT;Titech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "3b81507a37",
        "title": "PAC-inspired Option Discovery in Lifelong Reinforcement Learning",
        "site": "https://proceedings.mlr.press/v32/brunskill14.html",
        "author": "Emma Brunskill; Lihong Li",
        "abstract": "A key goal of AI is to create lifelong learning agents that can leverage prior experience to improve performance on later tasks. In reinforcement-learning problems, one way to summarize prior experience for future use is through options, which are temporally extended actions (subpolicies) for how to behave. Options can then be used to potentially accelerate learning in new reinforcement learning tasks. In this work, we provide the first formal analysis of the sample complexity, a measure of learning speed, of reinforcement learning with options.  This analysis helps shed light on some interesting  prior empirical results on when and how options may accelerate learning. We then quantify the benefit of options in reducing sample complexity of a lifelong learning agent. Finally, the new theoretical insights inspire a novel option-discovery algorithm that aims at minimizing overall sample complexity in lifelong reinforcement learning.",
        "bibtex": "@InProceedings{pmlr-v32-brunskill14,\n  title = \t {PAC-inspired Option Discovery in Lifelong Reinforcement Learning},\n  author = \t {Brunskill, Emma and Li, Lihong},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {316--324},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/brunskill14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/brunskill14.html},\n  abstract = \t {A key goal of AI is to create lifelong learning agents that can leverage prior experience to improve performance on later tasks. In reinforcement-learning problems, one way to summarize prior experience for future use is through options, which are temporally extended actions (subpolicies) for how to behave. Options can then be used to potentially accelerate learning in new reinforcement learning tasks. In this work, we provide the first formal analysis of the sample complexity, a measure of learning speed, of reinforcement learning with options.  This analysis helps shed light on some interesting  prior empirical results on when and how options may accelerate learning. We then quantify the benefit of options in reducing sample complexity of a lifelong learning agent. Finally, the new theoretical insights inspire a novel option-discovery algorithm that aims at minimizing overall sample complexity in lifelong reinforcement learning.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/brunskill14.pdf",
        "supp": "",
        "pdf_size": 324910,
        "gs_citation": 165,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12474049278706153501&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Computer Science Department, Carnegie Mellon University, Pittsburgh, PA 15213; Microsoft Research, One Microsoft Way, Redmond, WA 98052",
        "aff_domain": "CS.CMU.EDU;MICROSOFT.COM",
        "email": "CS.CMU.EDU;MICROSOFT.COM",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Carnegie Mellon University;Microsoft",
        "aff_unique_dep": "Computer Science Department;Microsoft Research",
        "aff_unique_url": "https://www.cmu.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "CMU;MSR",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Pittsburgh;Redmond",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9d7210a78e",
        "title": "Pitfalls in the use of Parallel Inference for the Dirichlet Process",
        "site": "https://proceedings.mlr.press/v32/gal14.html",
        "author": "Yarin Gal; Zoubin Ghahramani",
        "abstract": "Recent work done by Lovell, Adams, and Mansingka (2012) and Williamson, Dubey, and Xing (2013) has suggested an alternative parametrisation for the Dirichlet process in order to derive non-approximate parallel MCMC inference for it - work which has been picked-up and implemented in several different fields. In this paper we show that the approach suggested is impractical due to an extremely unbalanced distribution of the data. We characterise the requirements of efficient parallel inference for the Dirichlet process and show that the proposed inference fails most of these requirements (while approximate approaches often satisfy most of them). We present both theoretical and experimental evidence, analysing the load balance for the inference and showing that it is independent of the size of the dataset and the number of nodes available in the parallel implementation. We end with suggestions of alternative paths of research for efficient non-approximate parallel inference for the Dirichlet process.",
        "bibtex": "@InProceedings{pmlr-v32-gal14,\n  title = \t {Pitfalls in the use of Parallel Inference for the Dirichlet Process},\n  author = \t {Gal, Yarin and Ghahramani, Zoubin},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {208--216},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/gal14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/gal14.html},\n  abstract = \t {Recent work done by Lovell, Adams, and Mansingka (2012) and Williamson, Dubey, and Xing (2013) has suggested an alternative parametrisation for the Dirichlet process in order to derive non-approximate parallel MCMC inference for it - work which has been picked-up and implemented in several different fields. In this paper we show that the approach suggested is impractical due to an extremely unbalanced distribution of the data. We characterise the requirements of efficient parallel inference for the Dirichlet process and show that the proposed inference fails most of these requirements (while approximate approaches often satisfy most of them). We present both theoretical and experimental evidence, analysing the load balance for the inference and showing that it is independent of the size of the dataset and the number of nodes available in the parallel implementation. We end with suggestions of alternative paths of research for efficient non-approximate parallel inference for the Dirichlet process.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/gal14.pdf",
        "supp": "",
        "pdf_size": 674036,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6550919268306731505&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "University of Cambridge; University of Cambridge",
        "aff_domain": "CAM.AC.UK;ENG.CAM.AC.UK",
        "email": "CAM.AC.UK;ENG.CAM.AC.UK",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "08d9ecda9d",
        "title": "Prediction with Limited Advice and Multiarmed Bandits with Paid Observations",
        "site": "https://proceedings.mlr.press/v32/seldin14.html",
        "author": "Yevgeny Seldin; Peter Bartlett; Koby Crammer; Yasin Abbasi-Yadkori",
        "abstract": "We study two problems of online learning under restricted information access. In the first problem, \\emphprediction with limited advice, we consider a game of prediction with expert advice, where on each round of the game we query the advice of a subset of M out of N experts. We present an algorithm that achieves O(\\sqrt(N/M)T\\ln N) regret on T rounds of this game. The second problem, the \\emphmultiarmed bandit with paid  observations, is a variant of the adversarial N-armed bandit game, where on round t of the game we can observe the reward of any number of arms, but each observation has a cost c. We present an algorithm that achieves O((cN\\ln N)^1/3 T^2/3 + \\sqrtT \\ln N) regret on T rounds of this game in the worst case. Furthermore, we present a number of refinements that treat arm- and time-dependent observation costs and achieve lower regret under benign conditions. We present lower bounds that show that, apart from the logarithmic factors, the worst-case regret bounds cannot be improved.",
        "bibtex": "@InProceedings{pmlr-v32-seldin14,\n  title = \t {Prediction with Limited Advice and Multiarmed Bandits with Paid Observations},\n  author = \t {Seldin, Yevgeny and Bartlett, Peter and Crammer, Koby and Abbasi-Yadkori, Yasin},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {280--287},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/seldin14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/seldin14.html},\n  abstract = \t {We study two problems of online learning under restricted information access. In the first problem, \\emphprediction with limited advice, we consider a game of prediction with expert advice, where on each round of the game we query the advice of a subset of M out of N experts. We present an algorithm that achieves O(\\sqrt(N/M)T\\ln N) regret on T rounds of this game. The second problem, the \\emphmultiarmed bandit with paid  observations, is a variant of the adversarial N-armed bandit game, where on round t of the game we can observe the reward of any number of arms, but each observation has a cost c. We present an algorithm that achieves O((cN\\ln N)^1/3 T^2/3 + \\sqrtT \\ln N) regret on T rounds of this game in the worst case. Furthermore, we present a number of refinements that treat arm- and time-dependent observation costs and achieve lower regret under benign conditions. We present lower bounds that show that, apart from the logarithmic factors, the worst-case regret bounds cannot be improved.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/seldin14.pdf",
        "supp": "",
        "pdf_size": 262443,
        "gs_citation": 86,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5268307064033894348&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Queensland University of Technology + UC Berkeley; UC Berkeley + Queensland University of Technology; The Technion; Queensland University of Technology + UC Berkeley",
        "aff_domain": "gmail.com;eecs.berkeley.edu;ee.technion.ac.il;gmail.com",
        "email": "gmail.com;eecs.berkeley.edu;ee.technion.ac.il;gmail.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1+0;2;0+1",
        "aff_unique_norm": "Queensland University of Technology;University of California, Berkeley;Technion - Israel Institute of Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.qut.edu.au;https://www.berkeley.edu;https://www.technion.ac.il/en/",
        "aff_unique_abbr": "QUT;UC Berkeley;Technion",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0+1;1+0;2;0+1",
        "aff_country_unique": "Australia;United States;Israel"
    },
    {
        "id": "73bacb2798",
        "title": "Preference-Based Rank Elicitation using Statistical Models: The Case of Mallows",
        "site": "https://proceedings.mlr.press/v32/busa-fekete14.html",
        "author": "Robert Busa-Fekete; Eyke Huellermeier; Bal\u00e1zs Sz\u00f6r\u00e9nyi",
        "abstract": "We address the problem of rank elicitation assuming that the underlying data generating process is characterized by a probability distribution on the set of all rankings (total orders) of a given set of items. Instead of asking for complete rankings, however, our learner is only allowed to query pairwise preferences. Using information of that kind, the goal of the learner is to reliably predict properties of the distribution, such as the most probable top-item, the most probable ranking, or the distribution itself. More specifically, learning is done in an online manner, and the goal is to minimize sample complexity while guaranteeing a certain level of confidence.",
        "bibtex": "@InProceedings{pmlr-v32-busa-fekete14,\n  title = \t {Preference-Based Rank Elicitation using Statistical Models: The Case of Mallows},\n  author = \t {Busa-Fekete, Robert and Huellermeier, Eyke and Sz\u00f6r\u00e9nyi, Bal\u00e1zs},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1071--1079},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/busa-fekete14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/busa-fekete14.html},\n  abstract = \t {We address the problem of rank elicitation assuming that the underlying data generating process is characterized by a probability distribution on the set of all rankings (total orders) of a given set of items. Instead of asking for complete rankings, however, our learner is only allowed to query pairwise preferences. Using information of that kind, the goal of the learner is to reliably predict properties of the distribution, such as the most probable top-item, the most probable ranking, or the distribution itself. More specifically, learning is done in an online manner, and the goal is to minimize sample complexity while guaranteeing a certain level of confidence.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/busa-fekete14.pdf",
        "supp": "",
        "pdf_size": 569628,
        "gs_citation": 85,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15458948979965901291&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "MTA-SZTE Research Group on Arti\ufb01cial Intelligence, Tisza Lajos krt. 103., H-6720 Szeged, Hungary; Department of Computer Science, University of Paderborn, Warburger Str. 100, 33098 Paderborn, Germany; MTA-SZTE Research Group on Arti\ufb01cial Intelligence, Tisza Lajos krt. 103., H-6720 Szeged, Hungary+INRIA Lille - Nord Europe, SequeL project, 40 avenue Halley, 59650 Villeneuve d\u2019Ascq, France",
        "aff_domain": "INF.U-SZEGED.HU;UPB.DE;INF.U-SZEGED.HU",
        "email": "INF.U-SZEGED.HU;UPB.DE;INF.U-SZEGED.HU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0+2",
        "aff_unique_norm": "Hungarian Academy of Sciences - University of Szeged;University of Paderborn;INRIA Lille - Nord Europe",
        "aff_unique_dep": "Research Group on Arti\ufb01cial Intelligence;Department of Computer Science;SequeL project",
        "aff_unique_url": "https://www.mta.hu/english, https://www.szte.hu/english;https://www.uni-paderborn.de;https://www.inria.fr/lille-nord-europe",
        "aff_unique_abbr": "MTA-SZTE AI RG;UPB;INRIA",
        "aff_campus_unique_index": "0;1;0+2",
        "aff_campus_unique": "Szeged;Paderborn;Lille",
        "aff_country_unique_index": "0;1;0+2",
        "aff_country_unique": "Hungary;Germany;France"
    },
    {
        "id": "7010536a69",
        "title": "Preserving Modes and Messages via Diverse Particle Selection",
        "site": "https://proceedings.mlr.press/v32/pacheco14.html",
        "author": "Jason Pacheco; Silvia Zuffi; Michael Black; Erik Sudderth",
        "abstract": "In applications of graphical models arising in domains such as computer vision and signal processing, we often seek the most likely configurations of high-dimensional, continuous variables.  We develop a particle-based max-product algorithm which maintains a diverse set of posterior mode hypotheses, and is robust to initialization.  At each iteration, the set of hypotheses at each node is augmented via stochastic proposals, and then reduced via an efficient selection algorithm.  The integer program underlying our optimization-based particle selection minimizes errors in subsequent max-product message updates.  This objective automatically encourages diversity in the maintained hypotheses, without requiring tuning of application-specific distances among hypotheses.  By avoiding the stochastic resampling steps underlying particle sum-product algorithms, we also avoid common degeneracies where particles collapse onto a single hypothesis.  Our approach significantly outperforms previous particle-based algorithms in experiments focusing on the estimation of human pose from single images.",
        "bibtex": "@InProceedings{pmlr-v32-pacheco14,\n  title = \t {Preserving Modes and Messages via Diverse Particle Selection},\n  author = \t {Pacheco, Jason and Zuffi, Silvia and Black, Michael and Sudderth, Erik},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1152--1160},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/pacheco14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/pacheco14.html},\n  abstract = \t {In applications of graphical models arising in domains such as computer vision and signal processing, we often seek the most likely configurations of high-dimensional, continuous variables.  We develop a particle-based max-product algorithm which maintains a diverse set of posterior mode hypotheses, and is robust to initialization.  At each iteration, the set of hypotheses at each node is augmented via stochastic proposals, and then reduced via an efficient selection algorithm.  The integer program underlying our optimization-based particle selection minimizes errors in subsequent max-product message updates.  This objective automatically encourages diversity in the maintained hypotheses, without requiring tuning of application-specific distances among hypotheses.  By avoiding the stochastic resampling steps underlying particle sum-product algorithms, we also avoid common degeneracies where particles collapse onto a single hypothesis.  Our approach significantly outperforms previous particle-based algorithms in experiments focusing on the estimation of human pose from single images.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/pacheco14.pdf",
        "supp": "",
        "pdf_size": 4986504,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17209612021463934617&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 20,
        "aff": "Department of Computer Science, Brown University, Providence, RI 02912, USA; Max Planck Institute for Intelligent Systems, 72076 T \u00a8ubingen, Germany + ITC-CNR, 20133 Milan, Italy; Max Planck Institute for Intelligent Systems, 72076 T \u00a8ubingen, Germany; Department of Computer Science, Brown University, Providence, RI 02912, USA",
        "aff_domain": "CS.BROWN.EDU;TUE.MPG.DE;TUE.MPG.DE;CS.BROWN.EDU",
        "email": "CS.BROWN.EDU;TUE.MPG.DE;TUE.MPG.DE;CS.BROWN.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2;1;0",
        "aff_unique_norm": "Brown University;Max Planck Institute for Intelligent Systems;Institute of Technology and Complexity-CNR",
        "aff_unique_dep": "Department of Computer Science;;",
        "aff_unique_url": "https://www.brown.edu;https://www.mpi-is.mpg.de;https://itc.cnr.it",
        "aff_unique_abbr": "Brown;MPI-IS;ITC-CNR",
        "aff_campus_unique_index": "0;1+2;1;0",
        "aff_campus_unique": "Providence;T\u00fcbingen;Milan",
        "aff_country_unique_index": "0;1+2;1;0",
        "aff_country_unique": "United States;Germany;Italy"
    },
    {
        "id": "1084b7b9c1",
        "title": "Probabilistic Matrix Factorization with Non-random Missing Data",
        "site": "https://proceedings.mlr.press/v32/hernandez-lobatob14.html",
        "author": "Jose Miguel Hernandez-Lobato; Neil Houlsby; Zoubin Ghahramani",
        "abstract": "We propose a probabilistic matrix factorization model for collaborative  filtering that learns from data that is missing not at random(MNAR). Matrix factorization models exhibit state-of-the-art predictive performance in collaborative filtering. However, these models usually assume that the data is missing at random (MAR), and this is rarely the case. For example, the data is not MAR if users rate items they like more than ones they dislike. When the MAR assumption is incorrect, inferences are biased and predictive performance can suffer. Therefore, we model both the generative process for the data and the missing data mechanism. By learning these two models jointly we obtain improved performance over state-of-the-art methods when predicting the ratings and when modeling the data observation process. We present the first viable MF model for MNAR data. Our results are promising and we expect that further research on NMAR models will yield large gains in collaborative  filtering.",
        "bibtex": "@InProceedings{pmlr-v32-hernandez-lobatob14,\n  title = \t {Probabilistic Matrix Factorization with Non-random Missing Data},\n  author = \t {Hernandez-Lobato, Jose Miguel and Houlsby, Neil and Ghahramani, Zoubin},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1512--1520},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/hernandez-lobatob14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/hernandez-lobatob14.html},\n  abstract = \t {We propose a probabilistic matrix factorization model for collaborative  filtering that learns from data that is missing not at random(MNAR). Matrix factorization models exhibit state-of-the-art predictive performance in collaborative filtering. However, these models usually assume that the data is missing at random (MAR), and this is rarely the case. For example, the data is not MAR if users rate items they like more than ones they dislike. When the MAR assumption is incorrect, inferences are biased and predictive performance can suffer. Therefore, we model both the generative process for the data and the missing data mechanism. By learning these two models jointly we obtain improved performance over state-of-the-art methods when predicting the ratings and when modeling the data observation process. We present the first viable MF model for MNAR data. Our results are promising and we expect that further research on NMAR models will yield large gains in collaborative  filtering.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/hernandez-lobatob14.pdf",
        "supp": "",
        "pdf_size": 690613,
        "gs_citation": 208,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1793940054746343160&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 10,
        "aff": "University of Cambridge, Department of Engineering, Cambridge CB2 1PZ, UK; University of Cambridge, Department of Engineering, Cambridge CB2 1PZ, UK; University of Cambridge, Department of Engineering, Cambridge CB2 1PZ, UK",
        "aff_domain": "CAM.AC.UK;CAM.AC.UK;ENG.CAM.AC.UK",
        "email": "CAM.AC.UK;CAM.AC.UK;ENG.CAM.AC.UK",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "Department of Engineering",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "fb15378994",
        "title": "Probabilistic Partial Canonical Correlation Analysis",
        "site": "https://proceedings.mlr.press/v32/mukuta14.html",
        "author": "Yusuke Mukuta;  Harada",
        "abstract": "Partial canonical correlation analysis (partial CCA) is a statistical method that estimates a pair of linear projections onto a low dimensional space, where the correlation between two multidimensional variables is maximized after eliminating the influence of a third variable. Partial CCA is known to be closely related to a causality measure between two time series. However, partial CCA requires the inverses of covariance matrices, so the calculation is not stable. This is particularly the case for high-dimensional data or small sample sizes. Additionally, we cannot estimate the optimal dimension of the subspace in the model. In this paper, we have addressed these problems by proposing a probabilistic interpretation of partial CCA and deriving a Bayesian estimation method based on the probabilistic model. Our numerical experiments demonstrated that our methods can stably estimate the model parameters, even in high dimensions or when there are a small number of samples.",
        "bibtex": "@InProceedings{pmlr-v32-mukuta14,\n  title = \t {Probabilistic Partial Canonical Correlation Analysis},\n  author = \t {Mukuta, Yusuke and Harada, },\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1449--1457},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/mukuta14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/mukuta14.html},\n  abstract = \t {Partial canonical correlation analysis (partial CCA) is a statistical method that estimates a pair of linear projections onto a low dimensional space, where the correlation between two multidimensional variables is maximized after eliminating the influence of a third variable. Partial CCA is known to be closely related to a causality measure between two time series. However, partial CCA requires the inverses of covariance matrices, so the calculation is not stable. This is particularly the case for high-dimensional data or small sample sizes. Additionally, we cannot estimate the optimal dimension of the subspace in the model. In this paper, we have addressed these problems by proposing a probabilistic interpretation of partial CCA and deriving a Bayesian estimation method based on the probabilistic model. Our numerical experiments demonstrated that our methods can stably estimate the model parameters, even in high dimensions or when there are a small number of samples.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/mukuta14.pdf",
        "supp": "",
        "pdf_size": 554513,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14756977770787019761&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Graduate School of Information Science and Technology, The University of Tokyo; Graduate School of Information Science and Technology, The University of Tokyo",
        "aff_domain": "MI.T.U-TOKYO.AC.JP;MI.T.U-TOKYO.AC.JP",
        "email": "MI.T.U-TOKYO.AC.JP;MI.T.U-TOKYO.AC.JP",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Tokyo",
        "aff_unique_dep": "Graduate School of Information Science and Technology",
        "aff_unique_url": "https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "UTokyo",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Tokyo",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2426c8fe0c",
        "title": "Programming by Feedback",
        "site": "https://proceedings.mlr.press/v32/schoenauer14.html",
        "author": "Marc Schoenauer; Riad Akrour; Michele Sebag; Jean-Christophe Souplet",
        "abstract": "This paper advocates a new ML-based programming framework, called Programming by Feedback (PF), which involves a sequence of interactions between the active computer and the user. The latter only provides preference judgments on pairs of solutions supplied by the active computer. The active computer involves two components: the learning component estimates the user\u2019s utility function and accounts for the user\u2019s  (possibly limited) competence; the optimization component explores the search space and returns the most appropriate candidate solution. A proof of principle of the approach is proposed, showing that PF requires a handful of interactions in order to solve some discrete and continuous benchmark problems.",
        "bibtex": "@InProceedings{pmlr-v32-schoenauer14,\n  title = \t {Programming by Feedback},\n  author = \t {Schoenauer, Marc and Akrour, Riad and Sebag, Michele and Souplet, Jean-Christophe},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1503--1511},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/schoenauer14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/schoenauer14.html},\n  abstract = \t {This paper advocates a new ML-based programming framework, called Programming by Feedback (PF), which involves a sequence of interactions between the active computer and the user. The latter only provides preference judgments on pairs of solutions supplied by the active computer. The active computer involves two components: the learning component estimates the user\u2019s utility function and accounts for the user\u2019s  (possibly limited) competence; the optimization component explores the search space and returns the most appropriate candidate solution. A proof of principle of the approach is proposed, showing that PF requires a handful of interactions in order to solve some discrete and continuous benchmark problems.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/schoenauer14.pdf",
        "supp": "",
        "pdf_size": 474128,
        "gs_citation": 76,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9871590290629715920&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "TAO, INRIA/CNRS/LRI, Universit \u00b4e Paris-Sud, 91405 France; TAO, INRIA/CNRS/LRI, Universit \u00b4e Paris-Sud, 91405 France; TAO, INRIA/CNRS/LRI, Universit \u00b4e Paris-Sud, 91405 France; TAO, INRIA/CNRS/LRI, Universit \u00b4e Paris-Sud, 91405 France",
        "aff_domain": "lri.fr;inria.fr;lri.fr;lri.fr",
        "email": "lri.fr;inria.fr;lri.fr;lri.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Universit\u00e9 Paris-Sud",
        "aff_unique_dep": "TAO, INRIA/CNRS/LRI",
        "aff_unique_url": "https://www.universite-paris-sud.fr",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "8311242a1c",
        "title": "Provable Bounds for Learning Some Deep Representations",
        "site": "https://proceedings.mlr.press/v32/arora14.html",
        "author": "Sanjeev Arora; Aditya Bhaskara; Rong Ge; Tengyu Ma",
        "abstract": "We give  algorithms with provable guarantees that learn a class of deep nets in the generative model view popularized by Hinton and others. Our generative model is an n node multilayer neural net that has degree at most n^\u03b3 for some \u03b3< 1 and each edge has a random edge weight in [-1,1]. Our algorithm learns  almost all networks in this class with polynomial running time. The sample complexity is quadratic or cubic depending upon the details of the model.  The algorithm uses layerwise learning. It is based upon a novel idea of observing correlations among features and using these to infer the underlying edge structure via a global graph recovery procedure. The analysis  of the algorithm reveals interesting structure of  neural nets with random edge weights.",
        "bibtex": "@InProceedings{pmlr-v32-arora14,\n  title = \t {Provable Bounds for Learning Some Deep Representations},\n  author = \t {Arora, Sanjeev and Bhaskara, Aditya and Ge, Rong and Ma, Tengyu},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {584--592},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/arora14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/arora14.html},\n  abstract = \t {We give  algorithms with provable guarantees that learn a class of deep nets in the generative model view popularized by Hinton and others. Our generative model is an n node multilayer neural net that has degree at most n^\u03b3 for some \u03b3< 1 and each edge has a random edge weight in [-1,1]. Our algorithm learns  almost all networks in this class with polynomial running time. The sample complexity is quadratic or cubic depending upon the details of the model.  The algorithm uses layerwise learning. It is based upon a novel idea of observing correlations among features and using these to infer the underlying edge structure via a global graph recovery procedure. The analysis  of the algorithm reveals interesting structure of  neural nets with random edge weights.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/arora14.pdf",
        "supp": "",
        "pdf_size": 346117,
        "gs_citation": 454,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13466391840389862915&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Princeton University, Computer Science Department and Center for Computational Intractability, Princeton 08540, USA; Google Research, New York, NY 10011, USA; Microsoft Research, Cambridge, MA 02142, USA; Princeton University, Computer Science Department and Center for Computational Intractability, Princeton 08540, USA",
        "aff_domain": "CS.PRINCETON.EDU;CS.PRINCETON.EDU;MICROSOFT.COM;CS.PRINCETON.EDU",
        "email": "CS.PRINCETON.EDU;CS.PRINCETON.EDU;MICROSOFT.COM;CS.PRINCETON.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Princeton University;Google;Microsoft",
        "aff_unique_dep": "Computer Science Department;Google Research;Microsoft Research",
        "aff_unique_url": "https://www.princeton.edu;https://research.google;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "Princeton;Google Research;MSR",
        "aff_campus_unique_index": "0;1;2;0",
        "aff_campus_unique": "Princeton;New York;Cambridge",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ab36e4e337",
        "title": "Pursuit-Evasion Without Regret, with an Application to Trading",
        "site": "https://proceedings.mlr.press/v32/dworkin14.html",
        "author": "Lili Dworkin; Michael Kearns; Yuriy Nevmyvaka",
        "abstract": "We propose a state-based variant of the classical online learning problem of tracking the best expert. In our setting, the actions of the algorithm and experts correspond to local moves through a continuous and bounded state space. At each step, Nature chooses payoffs as a function of each player\u2019s current position and action. Our model therefore integrates the problem of prediction with expert advice with the stateful formalisms of reinforcement learning. Traditional no-regret learning approaches no longer apply, but we propose a simple algorithm that provably achieves no-regret when the state space is any convex Euclidean region. Our algorithm combines techniques from online learning with results from the literature on pursuit-evasion games. We describe a quantitative trading application in which the convex region captures inventory risk constraints, and local moves limit market impact. Using historical market data, we show experimentally that our algorithm has a strong advantage over classic no-regret approaches.",
        "bibtex": "@InProceedings{pmlr-v32-dworkin14,\n  title = \t {Pursuit-Evasion Without Regret, with an Application to Trading},\n  author = \t {Dworkin, Lili and Kearns, Michael and Nevmyvaka, Yuriy},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1521--1529},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/dworkin14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/dworkin14.html},\n  abstract = \t {We propose a state-based variant of the classical online learning problem of tracking the best expert. In our setting, the actions of the algorithm and experts correspond to local moves through a continuous and bounded state space. At each step, Nature chooses payoffs as a function of each player\u2019s current position and action. Our model therefore integrates the problem of prediction with expert advice with the stateful formalisms of reinforcement learning. Traditional no-regret learning approaches no longer apply, but we propose a simple algorithm that provably achieves no-regret when the state space is any convex Euclidean region. Our algorithm combines techniques from online learning with results from the literature on pursuit-evasion games. We describe a quantitative trading application in which the convex region captures inventory risk constraints, and local moves limit market impact. Using historical market data, we show experimentally that our algorithm has a strong advantage over classic no-regret approaches.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/dworkin14.pdf",
        "supp": "",
        "pdf_size": 466628,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6636551371822675351&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Computer and Information Science, University of Pennsylvania; Computer and Information Science, University of Pennsylvania; ",
        "aff_domain": "SEAS.UPENN.EDU;CIS.UPENN.EDU;GMAIL.COM",
        "email": "SEAS.UPENN.EDU;CIS.UPENN.EDU;GMAIL.COM",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "Computer and Information Science",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c7cae3b59e",
        "title": "Putting MRFs on a Tensor Train",
        "site": "https://proceedings.mlr.press/v32/novikov14.html",
        "author": "Alexander Novikov; Anton Rodomanov; Anton Osokin; Dmitry Vetrov",
        "abstract": "In the paper we present a new framework for dealing with probabilistic graphical models. Our approach relies on the recently proposed Tensor Train format (TT-format) of a tensor that while being  compact allows for efficient application of linear algebra operations. We present a way to convert the energy of a Markov random field to the TT-format and show how one can exploit the properties of the TT-format to attack the tasks of the partition function estimation and the MAP-inference. We provide theoretical guarantees on the accuracy of the proposed algorithm for estimating the partition function and compare our methods against several state-of-the-art algorithms.",
        "bibtex": "@InProceedings{pmlr-v32-novikov14,\n  title = \t {Putting MRFs on a Tensor Train},\n  author = \t {Novikov, Alexander and Rodomanov, Anton and Osokin, Anton and Vetrov, Dmitry},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {811--819},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/novikov14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/novikov14.html},\n  abstract = \t {In the paper we present a new framework for dealing with probabilistic graphical models. Our approach relies on the recently proposed Tensor Train format (TT-format) of a tensor that while being  compact allows for efficient application of linear algebra operations. We present a way to convert the energy of a Markov random field to the TT-format and show how one can exploit the properties of the TT-format to attack the tasks of the partition function estimation and the MAP-inference. We provide theoretical guarantees on the accuracy of the proposed algorithm for estimating the partition function and compare our methods against several state-of-the-art algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/novikov14.pdf",
        "supp": "",
        "pdf_size": 438470,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16034618838314678605&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Moscow State University, Moscow, Russia+Higher School of Economics, Moscow, Russia; Moscow State University, Moscow, Russia; Moscow State University, Moscow, Russia; Moscow State University, Moscow, Russia+Higher School of Economics, Moscow, Russia",
        "aff_domain": "BAYESGROUP.RU;GMAIL.COM;BAYESGROUP.RU;YANDEX.RU",
        "email": "BAYESGROUP.RU;GMAIL.COM;BAYESGROUP.RU;YANDEX.RU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0;0+1",
        "aff_unique_norm": "Moscow State University;Higher School of Economics",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.msu.ru;https://www.hse.ru",
        "aff_unique_abbr": "MSU;HSE",
        "aff_campus_unique_index": "0+0;0;0;0+0",
        "aff_campus_unique": "Moscow",
        "aff_country_unique_index": "0+0;0;0;0+0",
        "aff_country_unique": "Russian Federation"
    },
    {
        "id": "9c4e355e42",
        "title": "Quasi-Monte Carlo Feature Maps for Shift-Invariant Kernels",
        "site": "https://proceedings.mlr.press/v32/yangb14.html",
        "author": "Jiyan Yang; Vikas Sindhwani; Haim Avron; Michael Mahoney",
        "abstract": "We consider the problem of improving the efficiency of randomized Fourier feature maps to accelerate training and testing speed of kernel methods on large datasets. These approximate feature maps arise as Monte Carlo approximations to integral representations of shift-invariant kernel functions (e.g., Gaussian kernel). In this paper, we propose to use Quasi-Monte Carlo (QMC) approximations instead  where the relevant integrands are evaluated on a low-discrepancy sequence of points as opposed to random point sets as in the Monte Carlo approach. We derive a new discrepancy measure called box discrepancy based on theoretical characterizations of the integration error with respect to a given sequence. We then propose to learn QMC sequences adapted to our setting based on explicit box discrepancy minimization. Our theoretical analyses are complemented with empirical results that demonstrate the effectiveness of classical and adaptive QMC techniques for this problem.",
        "bibtex": "@InProceedings{pmlr-v32-yangb14,\n  title = \t {Quasi-Monte Carlo Feature Maps for Shift-Invariant Kernels},\n  author = \t {Yang, Jiyan and Sindhwani, Vikas and Avron, Haim and Mahoney, Michael},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {485--493},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/yangb14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/yangb14.html},\n  abstract = \t {We consider the problem of improving the efficiency of randomized Fourier feature maps to accelerate training and testing speed of kernel methods on large datasets. These approximate feature maps arise as Monte Carlo approximations to integral representations of shift-invariant kernel functions (e.g., Gaussian kernel). In this paper, we propose to use Quasi-Monte Carlo (QMC) approximations instead  where the relevant integrands are evaluated on a low-discrepancy sequence of points as opposed to random point sets as in the Monte Carlo approach. We derive a new discrepancy measure called box discrepancy based on theoretical characterizations of the integration error with respect to a given sequence. We then propose to learn QMC sequences adapted to our setting based on explicit box discrepancy minimization. Our theoretical analyses are complemented with empirical results that demonstrate the effectiveness of classical and adaptive QMC techniques for this problem.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/yangb14.pdf",
        "supp": "",
        "pdf_size": 510680,
        "gs_citation": 134,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15101508993479558365&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "ICME, Stanford University, Stanford, CA 94305; IBM T. J. Watson Research Center, Yorktown Heights, NY 10598; IBM T. J. Watson Research Center, Yorktown Heights, NY 10598; International Computer Science Institute and Dept. of Statistics, University of California at Berkeley, Berkeley, CA 94720",
        "aff_domain": "stanford.edu;us.ibm.com;us.ibm.com;icsi.berkeley.edu",
        "email": "stanford.edu;us.ibm.com;us.ibm.com;icsi.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;2",
        "aff_unique_norm": "Stanford University;IBM;University of California, Berkeley",
        "aff_unique_dep": "Institute for Computational and Mathematical Engineering;IBM T. J. Watson Research Center;Dept. of Statistics",
        "aff_unique_url": "https://www.stanford.edu;https://www.ibm.com/research/watson;https://www.berkeley.edu",
        "aff_unique_abbr": "Stanford;IBM Watson;UC Berkeley",
        "aff_campus_unique_index": "0;1;1;2",
        "aff_campus_unique": "Stanford;Yorktown Heights;Berkeley",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8436843290",
        "title": "Randomized Nonlinear Component Analysis",
        "site": "https://proceedings.mlr.press/v32/lopez-paz14.html",
        "author": "David Lopez-Paz; Suvrit Sra; Alex Smola; Zoubin Ghahramani; Bernhard Schoelkopf",
        "abstract": "Classical methods such as Principal Component Analysis (PCA) and Canonical Correlation Analysis (CCA) are ubiquitous in statistics.  However, these techniques are only able to reveal linear relationships in data. Although nonlinear variants of PCA and CCA have been proposed, these are computationally prohibitive in the large scale.     In a separate strand of recent research, randomized methods have been proposed to construct features that help reveal nonlinear patterns in data. For basic tasks such as regression or classification, random features exhibit little or no loss in performance, while achieving drastic savings in computational requirements.    In this paper we leverage randomness to design scalable new variants of nonlinear PCA and CCA; our ideas extend to key multivariate analysis tools such as spectral clustering or LDA. We demonstrate our algorithms through experiments on real-world data, on which we compare against the state-of-the-art. A simple R implementation of the presented algorithms is provided.",
        "bibtex": "@InProceedings{pmlr-v32-lopez-paz14,\n  title = \t {Randomized Nonlinear Component Analysis},\n  author = \t {Lopez-Paz, David and Sra, Suvrit and Smola, Alex and Ghahramani, Zoubin and Schoelkopf, Bernhard},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1359--1367},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/lopez-paz14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/lopez-paz14.html},\n  abstract = \t {Classical methods such as Principal Component Analysis (PCA) and Canonical Correlation Analysis (CCA) are ubiquitous in statistics.  However, these techniques are only able to reveal linear relationships in data. Although nonlinear variants of PCA and CCA have been proposed, these are computationally prohibitive in the large scale.     In a separate strand of recent research, randomized methods have been proposed to construct features that help reveal nonlinear patterns in data. For basic tasks such as regression or classification, random features exhibit little or no loss in performance, while achieving drastic savings in computational requirements.    In this paper we leverage randomness to design scalable new variants of nonlinear PCA and CCA; our ideas extend to key multivariate analysis tools such as spectral clustering or LDA. We demonstrate our algorithms through experiments on real-world data, on which we compare against the state-of-the-art. A simple R implementation of the presented algorithms is provided.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/lopez-paz14.pdf",
        "supp": "",
        "pdf_size": 510909,
        "gs_citation": 243,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13835474440204073152&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff": "Max-Planck-Institute for Intelligent Systems+University of Cambridge; Max-Planck-Institute for Intelligent Systems+Carnegie Mellon University; Carnegie Mellon University+Google Research; University of Cambridge; Max-Planck-Institute for Intelligent Systems",
        "aff_domain": "TUE.MPG.DE;TUE.MPG.DE;SMOLA.ORG;ENG.CAM.AC.UK;TUE.MPG.DE",
        "email": "TUE.MPG.DE;TUE.MPG.DE;SMOLA.ORG;ENG.CAM.AC.UK;TUE.MPG.DE",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+2;2+3;1;0",
        "aff_unique_norm": "Max-Planck-Institute for Intelligent Systems;University of Cambridge;Carnegie Mellon University;Google",
        "aff_unique_dep": ";;;Google Research",
        "aff_unique_url": "https://www.mpi-is.mpg.de;https://www.cam.ac.uk;https://www.cmu.edu;https://research.google",
        "aff_unique_abbr": "MPI-IS;Cambridge;CMU;Google Research",
        "aff_campus_unique_index": "1;;2;1",
        "aff_campus_unique": ";Cambridge;Mountain View",
        "aff_country_unique_index": "0+1;0+2;2+2;1;0",
        "aff_country_unique": "Germany;United Kingdom;United States"
    },
    {
        "id": "e4a48aa121",
        "title": "Rank-One Matrix Pursuit for Matrix Completion",
        "site": "https://proceedings.mlr.press/v32/wanga14.html",
        "author": "Zheng Wang; Ming-Jun Lai; Zhaosong Lu; Wei Fan; Hasan Davulcu; Jieping Ye",
        "abstract": "Low rank matrix completion has been applied successfully in a wide range of machine learning applications, such as collaborative filtering, image inpainting and Microarray data imputation. However, many existing algorithms are not scalable to large-scale problems, as they involve computing singular value decomposition. In this paper, we present an efficient and scalable algorithm for matrix completion. The key idea is to extend the well-known orthogonal matching pursuit from the vector case to the matrix case. In each iteration, we pursue a rank-one matrix basis generated by the top singular vector pair of the current approximation residual and update the weights for all rank-one matrices obtained up to the current iteration. We further propose a novel weight updating rule to reduce the time and storage complexity, making the proposed algorithm scalable to large matrices. We establish the linear convergence of the proposed algorithm. The fast convergence is achieved due to the proposed construction of matrix bases and the estimation of the weights. We empirically evaluate the proposed algorithm on many real-world large scale datasets. Results show that our algorithm is much more efficient than state-of-the-art matrix completion algorithms while achieving similar or better prediction performance.",
        "bibtex": "@InProceedings{pmlr-v32-wanga14,\n  title = \t {Rank-One Matrix Pursuit for Matrix Completion},\n  author = \t {Wang, Zheng and Lai, Ming-Jun and Lu, Zhaosong and Fan, Wei and Davulcu, Hasan and Ye, Jieping},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {91--99},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/wanga14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/wanga14.html},\n  abstract = \t {Low rank matrix completion has been applied successfully in a wide range of machine learning applications, such as collaborative filtering, image inpainting and Microarray data imputation. However, many existing algorithms are not scalable to large-scale problems, as they involve computing singular value decomposition. In this paper, we present an efficient and scalable algorithm for matrix completion. The key idea is to extend the well-known orthogonal matching pursuit from the vector case to the matrix case. In each iteration, we pursue a rank-one matrix basis generated by the top singular vector pair of the current approximation residual and update the weights for all rank-one matrices obtained up to the current iteration. We further propose a novel weight updating rule to reduce the time and storage complexity, making the proposed algorithm scalable to large matrices. We establish the linear convergence of the proposed algorithm. The fast convergence is achieved due to the proposed construction of matrix bases and the estimation of the weights. We empirically evaluate the proposed algorithm on many real-world large scale datasets. Results show that our algorithm is much more efficient than state-of-the-art matrix completion algorithms while achieving similar or better prediction performance.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/wanga14.pdf",
        "supp": "",
        "pdf_size": 348362,
        "gs_citation": 81,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3654805189744363838&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "b9c3c2a207",
        "title": "Rectangular Tiling Process",
        "site": "https://proceedings.mlr.press/v32/nakano14.html",
        "author": "Masahiro Nakano; Katsuhiko Ishiguro; Akisato Kimura; Takeshi Yamada; Naonori Ueda",
        "abstract": "This paper proposes a novel stochastic process that represents the arbitrary rectangular partitioning of an infinite-dimensional matrix as the conditional projective limit. Rectangular partitioning is used in relational data analysis, and is classified into three types: regular grid, hierarchical, and arbitrary. Conventionally, a variety of probabilistic models have been advanced for the first two, including the product of Chinese restaurant processes and the Mondrian process. However, existing models for arbitrary partitioning are too complicated to permit the analysis of the statistical behaviors of models, which places very severe capability limits on relational data analysis. In this paper, we propose a new probabilistic model of arbitrary partitioning called the rectangular tiling process (RTP). Our model has a sound mathematical base in projective systems and infinite extension of conditional probabilities, and is capable of representing partitions of infinite elements as found in ordinary Bayesian nonparametric models.",
        "bibtex": "@InProceedings{pmlr-v32-nakano14,\n  title = \t {Rectangular Tiling Process},\n  author = \t {Nakano, Masahiro and Ishiguro, Katsuhiko and Kimura, Akisato and Yamada, Takeshi and Ueda, Naonori},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {361--369},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/nakano14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/nakano14.html},\n  abstract = \t {This paper proposes a novel stochastic process that represents the arbitrary rectangular partitioning of an infinite-dimensional matrix as the conditional projective limit. Rectangular partitioning is used in relational data analysis, and is classified into three types: regular grid, hierarchical, and arbitrary. Conventionally, a variety of probabilistic models have been advanced for the first two, including the product of Chinese restaurant processes and the Mondrian process. However, existing models for arbitrary partitioning are too complicated to permit the analysis of the statistical behaviors of models, which places very severe capability limits on relational data analysis. In this paper, we propose a new probabilistic model of arbitrary partitioning called the rectangular tiling process (RTP). Our model has a sound mathematical base in projective systems and infinite extension of conditional probabilities, and is capable of representing partitions of infinite elements as found in ordinary Bayesian nonparametric models.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/nakano14.pdf",
        "supp": "",
        "pdf_size": 1287723,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15524895850146635447&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "NTT communication science laboratories; NTT communication science laboratories; NTT communication science laboratories; NTT communication science laboratories; NTT communication science laboratories",
        "aff_domain": "LAB.NTT.CO.JP;LAB.NTT.CO.JP;LAB.NTT.CO.JP;LAB.NTT.CO.JP;LAB.NTT.CO.JP",
        "email": "LAB.NTT.CO.JP;LAB.NTT.CO.JP;LAB.NTT.CO.JP;LAB.NTT.CO.JP;LAB.NTT.CO.JP",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "NTT Communication Science Laboratories",
        "aff_unique_dep": "Communication Science",
        "aff_unique_url": "https://www.ntt-csl.com",
        "aff_unique_abbr": "NTT CSL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "b4918f3921",
        "title": "Recurrent Convolutional Neural Networks for Scene Labeling",
        "site": "https://proceedings.mlr.press/v32/pinheiro14.html",
        "author": "Pedro Pinheiro; Ronan Collobert",
        "abstract": "The goal of the scene labeling task is to assign a class label to each pixel in an image.  To ensure a good visual coherence and a high class accuracy, it is essential for a model to capture long range  pixel) label dependencies in images. In a feed-forward architecture, this can be achieved simply by considering a sufficiently large input context patch, around each pixel to be labeled.  We propose an approach that consists of a recurrent convolutional neural network which allows us to consider a large input context while limiting the capacity of the model. Contrary to most standard approaches, our method does not rely on any segmentation technique nor any task-specific features. The system is trained in an end-to-end manner over raw pixels, and models complex spatial dependencies with low inference cost. As the context size increases with the built-in recurrence, the system identifies and corrects its own errors. Our approach yields state-of-the-art performance on both the Stanford Background Dataset and the SIFT Flow Dataset, while remaining very fast at test time.",
        "bibtex": "@InProceedings{pmlr-v32-pinheiro14,\n  title = \t {Recurrent Convolutional Neural Networks for Scene Labeling},\n  author = \t {Pinheiro, Pedro and Collobert, Ronan},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {82--90},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/pinheiro14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/pinheiro14.html},\n  abstract = \t {The goal of the scene labeling task is to assign a class label to each pixel in an image.  To ensure a good visual coherence and a high class accuracy, it is essential for a model to capture long range  pixel) label dependencies in images. In a feed-forward architecture, this can be achieved simply by considering a sufficiently large input context patch, around each pixel to be labeled.  We propose an approach that consists of a recurrent convolutional neural network which allows us to consider a large input context while limiting the capacity of the model. Contrary to most standard approaches, our method does not rely on any segmentation technique nor any task-specific features. The system is trained in an end-to-end manner over raw pixels, and models complex spatial dependencies with low inference cost. As the context size increases with the built-in recurrence, the system identifies and corrects its own errors. Our approach yields state-of-the-art performance on both the Stanford Background Dataset and the SIFT Flow Dataset, while remaining very fast at test time.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/pinheiro14.pdf",
        "supp": "",
        "pdf_size": 1863309,
        "gs_citation": 1064,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5818920289733010269&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 28,
        "aff": "Ecole Polytechnique F\u00e9d\u00e9r\u00e1le de Lausanne (EPFL), Lausanne, Switzerland+Idiap Research Institute, Martigny, Switzerland; Idiap Research Institute, Martigny, Switzerland",
        "aff_domain": "IDIAP.CH;COLLOBERT.COM",
        "email": "IDIAP.CH;COLLOBERT.COM",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1",
        "aff_unique_norm": "EPFL;Idiap Research Institute",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.epfl.ch;https://www.idiap.ch",
        "aff_unique_abbr": "EPFL;Idiap",
        "aff_campus_unique_index": "0+1;1",
        "aff_campus_unique": "Lausanne;Martigny",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "a846c218ec",
        "title": "Reducing Dueling Bandits to Cardinal Bandits",
        "site": "https://proceedings.mlr.press/v32/ailon14.html",
        "author": "Nir Ailon; Zohar Karnin; Thorsten Joachims",
        "abstract": "We present algorithms for reducing the Dueling Bandits problem to the conventional (stochastic) Multi-Armed Bandits problem. The Dueling Bandits problem is an online model of learning with ordinal feedback of the form \u201cA is preferred to B\u201d (as opposed to cardinal feedback like \u201cA has value 2.5\u201d), giving it wide applicability in learning from implicit user feedback and revealed and stated preferences. In contrast to existing algorithms for the Dueling Bandits problem, our reductions \u2013 named \\Doubler, \\MultiSbm and \\DoubleSbm \u2013 provide a generic schema for translating the extensive body of known results about conventional Multi-Armed Bandit algorithms to the Dueling Bandits setting.     For \\Doubler and \\MultiSbm we prove regret upper bounds in both finite and infinite settings, and conjecture about the performance of \\DoubleSbm which empirically outperforms the other two as well as previous algorithms in our experiments.  In addition, we provide the first almost optimal regret bound in terms of second order terms, such as the differences between the values of the arms.",
        "bibtex": "@InProceedings{pmlr-v32-ailon14,\n  title = \t {Reducing Dueling Bandits to Cardinal Bandits},\n  author = \t {Ailon, Nir and Karnin, Zohar and Joachims, Thorsten},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {856--864},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/ailon14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/ailon14.html},\n  abstract = \t {We present algorithms for reducing the Dueling Bandits problem to the conventional (stochastic) Multi-Armed Bandits problem. The Dueling Bandits problem is an online model of learning with ordinal feedback of the form \u201cA is preferred to B\u201d (as opposed to cardinal feedback like \u201cA has value 2.5\u201d), giving it wide applicability in learning from implicit user feedback and revealed and stated preferences. In contrast to existing algorithms for the Dueling Bandits problem, our reductions \u2013 named \\Doubler, \\MultiSbm and \\DoubleSbm \u2013 provide a generic schema for translating the extensive body of known results about conventional Multi-Armed Bandit algorithms to the Dueling Bandits setting.     For \\Doubler and \\MultiSbm we prove regret upper bounds in both finite and infinite settings, and conjecture about the performance of \\DoubleSbm which empirically outperforms the other two as well as previous algorithms in our experiments.  In addition, we provide the first almost optimal regret bound in terms of second order terms, such as the differences between the values of the arms.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/ailon14.pdf",
        "supp": "",
        "pdf_size": 457055,
        "gs_citation": 160,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12906853935277175891&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Technion, Dept. of Computer Science, Haifa 32000, Israel; Yahoo Labs, Haifa 31905, Israel; Cornell University, Dept. of Computer Science, Ithaca, NY 14850, USA",
        "aff_domain": "TECHNION.AC.IL;YMAIL.COM;CS.CORNELL.EDU",
        "email": "TECHNION.AC.IL;YMAIL.COM;CS.CORNELL.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Technion;Yahoo Labs;Cornell University",
        "aff_unique_dep": "Dept. of Computer Science;;Dept. of Computer Science",
        "aff_unique_url": "https://www.technion.ac.il;https://labs.yahoo.com;https://www.cornell.edu",
        "aff_unique_abbr": "Technion;Yahoo Labs;Cornell",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Haifa;Ithaca",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Israel;United States"
    },
    {
        "id": "7a501ac533",
        "title": "Relative Upper Confidence Bound for the K-Armed Dueling Bandit Problem",
        "site": "https://proceedings.mlr.press/v32/zoghi14.html",
        "author": "Masrour Zoghi; Shimon Whiteson; Remi Munos; Maarten Rijke",
        "abstract": "This paper proposes a new method for the K-armed dueling bandit problem, a variation on the regular K-armed bandit problem that offers only relative feedback about pairs of arms. Our approach extends the Upper Confidence Bound algorithm to the relative setting by using estimates of the pairwise probabilities to select a promising arm and applying Upper Confidence Bound with the winner as a benchmark. We prove a sharp finite-time regret bound of order O(K log t) on a very general class of dueling bandit problems that matches a lower bound proven in (Yue et al., 2012). In addition, our empirical results using real data from an information retrieval application show that it greatly outperforms the state of the art.",
        "bibtex": "@InProceedings{pmlr-v32-zoghi14,\n  title = \t {Relative Upper Confidence Bound for the K-Armed Dueling Bandit Problem},\n  author = \t {Zoghi, Masrour and Whiteson, Shimon and Munos, Remi and Rijke, Maarten},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {10--18},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/zoghi14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/zoghi14.html},\n  abstract = \t {This paper proposes a new method for the K-armed dueling bandit problem, a variation on the regular K-armed bandit problem that offers only relative feedback about pairs of arms. Our approach extends the Upper Confidence Bound algorithm to the relative setting by using estimates of the pairwise probabilities to select a promising arm and applying Upper Confidence Bound with the winner as a benchmark. We prove a sharp finite-time regret bound of order O(K log t) on a very general class of dueling bandit problems that matches a lower bound proven in (Yue et al., 2012). In addition, our empirical results using real data from an information retrieval application show that it greatly outperforms the state of the art.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/zoghi14.pdf",
        "supp": "",
        "pdf_size": 1165817,
        "gs_citation": 186,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4909099288602776994&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "ISLA, University of Amsterdam, Netherlands; ISLA, University of Amsterdam, Netherlands; INRIA Lille - Nord Europe / MSR-NE; ISLA, University of Amsterdam, Netherlands",
        "aff_domain": "UV A.NL;UV A.NL;INRIA.FR;UV A.NL",
        "email": "UV A.NL;UV A.NL;INRIA.FR;UV A.NL",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Amsterdam;INRIA Lille - Nord Europe",
        "aff_unique_dep": "ISLA;",
        "aff_unique_url": "https://www.uva.nl;https://www.inria.fr/lille",
        "aff_unique_abbr": ";INRIA",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Lille",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "Netherlands;France"
    },
    {
        "id": "a6b8b97cac",
        "title": "Riemannian Pursuit for Big Matrix Recovery",
        "site": "https://proceedings.mlr.press/v32/tan14.html",
        "author": "Mingkui Tan; Ivor W. Tsang; Li Wang; Bart Vandereycken; Sinno Jialin Pan",
        "abstract": "Low rank matrix recovery is a fundamental task in many real-world  applications. The performance of existing methods, however,   deteriorates significantly when applied to ill-conditioned or large-scale matrices.  In this paper, we therefore propose an efficient method, called  Riemannian Pursuit (RP), that aims to address these two problems  simultaneously. Our method consists of a sequence of fixed-rank  optimization problems. Each subproblem, solved by a nonlinear  Riemannian conjugate gradient method, aims to correct the solution  in the most important subspace of increasing size.   Theoretically, RP converges linearly under mild conditions and  experimental results show that it substantially outperforms existing  methods when applied to   large-scale and ill-conditioned matrices.",
        "bibtex": "@InProceedings{pmlr-v32-tan14,\n  title = \t {Riemannian Pursuit for Big Matrix Recovery},\n  author = \t {Tan, Mingkui and Tsang, Ivor W. and Wang, Li and Vandereycken, Bart and Pan, Sinno Jialin},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1539--1547},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/tan14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/tan14.html},\n  abstract = \t {Low rank matrix recovery is a fundamental task in many real-world  applications. The performance of existing methods, however,   deteriorates significantly when applied to ill-conditioned or large-scale matrices.  In this paper, we therefore propose an efficient method, called  Riemannian Pursuit (RP), that aims to address these two problems  simultaneously. Our method consists of a sequence of fixed-rank  optimization problems. Each subproblem, solved by a nonlinear  Riemannian conjugate gradient method, aims to correct the solution  in the most important subspace of increasing size.   Theoretically, RP converges linearly under mild conditions and  experimental results show that it substantially outperforms existing  methods when applied to   large-scale and ill-conditioned matrices.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/tan14.pdf",
        "supp": "",
        "pdf_size": 519068,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13444147716744706211&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "School of Computer Science, The University of Adelaide, Ingkarni Wardli North Terrace Campus 5005, Australia; Center for Quantum Computation & Intelligent Systems, University of Technology Sydney, Australia; Department of Mathematics, University of California, San Diego, USA; Department of Mathematics, Princeton University, Fine Hall, Washington Road, Princeton NJ 08544-1000, USA; Institute for Infocomm Research, 1 Fusionopolis Way, #21-01 Connexis (South) 138632, Singapore",
        "aff_domain": "gmail.com;gmail.com;ucsd.edu;princeton.edu;i2r.a-star.edu.sg",
        "email": "gmail.com;gmail.com;ucsd.edu;princeton.edu;i2r.a-star.edu.sg",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3;4",
        "aff_unique_norm": "University of Adelaide;University of Technology Sydney;University of California, San Diego;Princeton University;Institute for Infocomm Research",
        "aff_unique_dep": "School of Computer Science;Center for Quantum Computation & Intelligent Systems;Department of Mathematics;Department of Mathematics;",
        "aff_unique_url": "https://www.adelaide.edu.au;https://www.uts.edu.au;https://www.ucsd.edu;https://www.princeton.edu;https://www.i2r.a-star.edu.sg",
        "aff_unique_abbr": "Adelaide;UTS;UCSD;Princeton;I2R",
        "aff_campus_unique_index": "0;1;2;3;4",
        "aff_campus_unique": "Ingkarni Wardli North Terrace Campus;Sydney;San Diego;Princeton;Fusionopolis",
        "aff_country_unique_index": "0;0;1;1;2",
        "aff_country_unique": "Australia;United States;Singapore"
    },
    {
        "id": "85fea2ac2c",
        "title": "Robust Distance Metric Learning via Simultaneous L1-Norm Minimization and Maximization",
        "site": "https://proceedings.mlr.press/v32/wangj14.html",
        "author": "Hua Wang; Feiping Nie; Heng Huang",
        "abstract": "Traditional distance metric learning with side information usually formulates the objectives using the covariance matrices of the data point pairs in the two constraint sets of must-links and cannot-links. Because the covariance matrix computes the sum of the squared L2-norm distances, it is prone to both outlier samples and outlier features. To develop a robust distance metric learning method, in this paper we propose a new objective for distance metric learning using the L1-norm distances. However, the resulted objective is very challenging to solve, because it simultaneously minimizes and maximizes (minmax) a number of non-smooth L1-norm terms. As an important theoretical contribution of this paper, we systematically derive an efficient iterative algorithm to solve the general L1-norm minmax problem, which is rarely studied in literature. We have performed extensive empirical evaluations, where our new distance metric learning method outperforms related state-of-the-art methods in a variety of experimental settings to cluster both noiseless and noisy data.",
        "bibtex": "@InProceedings{pmlr-v32-wangj14,\n  title = \t {Robust Distance Metric Learning via Simultaneous L1-Norm Minimization and Maximization},\n  author = \t {Wang, Hua and Nie, Feiping and Huang, Heng},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1836--1844},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/wangj14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/wangj14.html},\n  abstract = \t {Traditional distance metric learning with side information usually formulates the objectives using the covariance matrices of the data point pairs in the two constraint sets of must-links and cannot-links. Because the covariance matrix computes the sum of the squared L2-norm distances, it is prone to both outlier samples and outlier features. To develop a robust distance metric learning method, in this paper we propose a new objective for distance metric learning using the L1-norm distances. However, the resulted objective is very challenging to solve, because it simultaneously minimizes and maximizes (minmax) a number of non-smooth L1-norm terms. As an important theoretical contribution of this paper, we systematically derive an efficient iterative algorithm to solve the general L1-norm minmax problem, which is rarely studied in literature. We have performed extensive empirical evaluations, where our new distance metric learning method outperforms related state-of-the-art methods in a variety of experimental settings to cluster both noiseless and noisy data.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/wangj14.pdf",
        "supp": "",
        "pdf_size": 264313,
        "gs_citation": 118,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16813797167353581751&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 8,
        "aff": "Colorado School of Mines, Department of Electrical Engineering and Computer Science, Golden, Colorado 80401; Computer Science and Engineering Department, University of Texas at Arlington, Arlington, TX, 76019; Computer Science and Engineering Department, University of Texas at Arlington, Arlington, TX, 76019",
        "aff_domain": "usc.edu;gmail.com;uta.edu",
        "email": "usc.edu;gmail.com;uta.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Colorado School of Mines;University of Texas at Arlington",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Science;Computer Science and Engineering Department",
        "aff_unique_url": "https://www.mines.edu;https://www.uta.edu",
        "aff_unique_abbr": "CSM;UTA",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Golden;Arlington",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "bc82b88504",
        "title": "Robust Inverse Covariance Estimation under Noisy Measurements",
        "site": "https://proceedings.mlr.press/v32/wangf14.html",
        "author": "Jun-Kun Wang; Shou-de Lin",
        "abstract": "This paper proposes a robust method to estimate the inverse covariance under noisy measurements. The method is based on the estimation of each column in the inverse covariance matrix independently via robust regression, which enables parallelization. Different from previous linear programming based methods that cannot guarantee a positive semi-definite covariance matrix, our method adjusts the learned matrix to satisfy this condition, which further facilitates the tasks of forecasting future values. Experiments on time series prediction and classification under  noisy condition demonstrate the effectiveness of the approach.",
        "bibtex": "@InProceedings{pmlr-v32-wangf14,\n  title = \t {Robust Inverse Covariance Estimation under Noisy Measurements},\n  author = \t {Wang, Jun-Kun and Lin, Shou-de},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {928--936},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/wangf14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/wangf14.html},\n  abstract = \t {This paper proposes a robust method to estimate the inverse covariance under noisy measurements. The method is based on the estimation of each column in the inverse covariance matrix independently via robust regression, which enables parallelization. Different from previous linear programming based methods that cannot guarantee a positive semi-definite covariance matrix, our method adjusts the learned matrix to satisfy this condition, which further facilitates the tasks of forecasting future values. Experiments on time series prediction and classification under  noisy condition demonstrate the effectiveness of the approach.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/wangf14.pdf",
        "supp": "",
        "pdf_size": 146217,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2339025755557095858&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Intel-NTU, National Taiwan University, Taiwan; Intel-NTU, National Taiwan University, Taiwan",
        "aff_domain": "gmail.com;csie.ntu.edu.tw",
        "email": "gmail.com;csie.ntu.edu.tw",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "National Taiwan University",
        "aff_unique_dep": "Intel-NTU",
        "aff_unique_url": "https://www.ntu.edu.tw",
        "aff_unique_abbr": "NTU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Taiwan",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "c7ce47a1da",
        "title": "Robust Learning under Uncertain Test Distributions: Relating Covariate Shift to Model Misspecification",
        "site": "https://proceedings.mlr.press/v32/wen14.html",
        "author": "Junfeng Wen; Chun-Nam Yu; Russell Greiner",
        "abstract": "Many learning situations involve learning the conditional distribution p(y|x) when the training instances are drawn from the training distribution p_tr(x), even though it will later be used to predict for instances drawn from a different test distribution p_te(x).   Most current approaches focus on learning how to reweigh the training examples, to make them resemble the test distribution.   However, reweighing does not always help, because (we show that) the test error also depends on the correctness of the underlying model class.   This paper analyses this situation by viewing the problem of learning under changing distributions as a game between a learner and an adversary.   We characterize when such reweighing is needed, and also provide an algorithm, robust covariate shift adjustment (RCSA), that provides relevant weights.   Our empirical studies, on UCI datasets and a real-world cancer prognostic prediction dataset, show that our analysis applies, and that our RCSA works effectively.",
        "bibtex": "@InProceedings{pmlr-v32-wen14,\n  title = \t {Robust Learning under Uncertain Test Distributions: Relating Covariate Shift to Model Misspecification},\n  author = \t {Wen, Junfeng and Yu, Chun-Nam and Greiner, Russell},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {631--639},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/wen14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/wen14.html},\n  abstract = \t {Many learning situations involve learning the conditional distribution p(y|x) when the training instances are drawn from the training distribution p_tr(x), even though it will later be used to predict for instances drawn from a different test distribution p_te(x).   Most current approaches focus on learning how to reweigh the training examples, to make them resemble the test distribution.   However, reweighing does not always help, because (we show that) the test error also depends on the correctness of the underlying model class.   This paper analyses this situation by viewing the problem of learning under changing distributions as a game between a learner and an adversary.   We characterize when such reweighing is needed, and also provide an algorithm, robust covariate shift adjustment (RCSA), that provides relevant weights.   Our empirical studies, on UCI datasets and a real-world cancer prognostic prediction dataset, show that our analysis applies, and that our RCSA works effectively.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/wen14.pdf",
        "supp": "",
        "pdf_size": 353062,
        "gs_citation": 139,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13465424009045281279&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computing Science, University of Alberta, Edmonton, AB T6G 2E8 CANADA; Bell Labs, Alcatel-Lucent, 600 Mountain Avenue, Murray Hill, NJ 07974 USA; Department of Computing Science, University of Alberta, Edmonton, AB T6G 2E8 CANADA",
        "aff_domain": "UALBERTA.CA;ALCATEL-LUCENT.COM;UALBERTA.CA",
        "email": "UALBERTA.CA;ALCATEL-LUCENT.COM;UALBERTA.CA",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Alberta;Bell Labs",
        "aff_unique_dep": "Department of Computing Science;",
        "aff_unique_url": "https://www.ualberta.ca;",
        "aff_unique_abbr": "UAlberta;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Edmonton;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "7008ff5cf4",
        "title": "Robust Principal Component Analysis with Complex Noise",
        "site": "https://proceedings.mlr.press/v32/zhao14.html",
        "author": "Qian Zhao; Deyu Meng; Zongben Xu; Wangmeng Zuo; Lei Zhang",
        "abstract": "The research on robust principal component analysis (RPCA) has been attracting much attention recently. The original RPCA model assumes sparse noise, and use the L_1-norm to characterize the error term. In practice, however, the noise is much more complex and it is not appropriate to simply use a certain L_p-norm for noise modeling. We propose a generative RPCA model under the Bayesian framework by modeling data noise as a mixture of Gaussians (MoG). The MoG is a universal approximator to continuous distributions and thus our model is able to fit a wide range of noises such as Laplacian, Gaussian, sparse noises and any combinations of them. A variational Bayes algorithm is presented to infer the posterior of the proposed model. All involved parameters can be recursively updated in closed form. The advantage of our method is demonstrated by extensive experiments on synthetic data, face modeling and background subtraction.",
        "bibtex": "@InProceedings{pmlr-v32-zhao14,\n  title = \t {Robust Principal Component Analysis with Complex Noise},\n  author = \t {Zhao, Qian and Meng, Deyu and Xu, Zongben and Zuo, Wangmeng and Zhang, Lei},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {55--63},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/zhao14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/zhao14.html},\n  abstract = \t {The research on robust principal component analysis (RPCA) has been attracting much attention recently. The original RPCA model assumes sparse noise, and use the L_1-norm to characterize the error term. In practice, however, the noise is much more complex and it is not appropriate to simply use a certain L_p-norm for noise modeling. We propose a generative RPCA model under the Bayesian framework by modeling data noise as a mixture of Gaussians (MoG). The MoG is a universal approximator to continuous distributions and thus our model is able to fit a wide range of noises such as Laplacian, Gaussian, sparse noises and any combinations of them. A variational Bayes algorithm is presented to infer the posterior of the proposed model. All involved parameters can be recursively updated in closed form. The advantage of our method is demonstrated by extensive experiments on synthetic data, face modeling and background subtraction.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/zhao14.pdf",
        "supp": "",
        "pdf_size": 4163399,
        "gs_citation": 277,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12167431714111803391&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "School of Mathematics and Statistics, Xi\u2019an Jiaotong University, Xi\u2019an, China; School of Mathematics and Statistics, Xi\u2019an Jiaotong University, Xi\u2019an, China; School of Mathematics and Statistics, Xi\u2019an Jiaotong University, Xi\u2019an, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China",
        "aff_domain": "gmail.com;mail.xjtu.edu.cn;mail.xjtu.edu.cn;gmail.com;comp.polyu.edu.hk",
        "email": "gmail.com;mail.xjtu.edu.cn;mail.xjtu.edu.cn;gmail.com;comp.polyu.edu.hk",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;2",
        "aff_unique_norm": "Xi'an Jiao Tong University;Harbin Institute of Technology;Hong Kong Polytechnic University",
        "aff_unique_dep": "School of Mathematics and Statistics;School of Computer Science and Technology;Department of Computing",
        "aff_unique_url": "http://en.xjtu.edu.cn/;http://www.hit.edu.cn/;https://www.polyu.edu.hk",
        "aff_unique_abbr": "XJTU;HIT;PolyU",
        "aff_campus_unique_index": "0;0;0;1;2",
        "aff_campus_unique": "Xi'an;Harbin;Hong Kong",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2d10cbefec",
        "title": "Robust RegBayes: Selectively Incorporating First-Order Logic Domain Knowledge into Bayesian Models",
        "site": "https://proceedings.mlr.press/v32/mei14.html",
        "author": "Shike Mei; Jun Zhu; Jerry Zhu",
        "abstract": "Much research in Bayesian modeling has been done to elicit a prior distribution that incorporates domain knowledge. We present a novel and more direct approach by imposing First-Order Logic (FOL) rules on the posterior distribution. Our approach unifies FOL and Bayesian modeling under the regularized Bayesian framework. In addition, our approach automatically estimates the uncertainty of FOL rules when they are produced by humans, so that reliable rules are incorporated while unreliable ones are ignored. We apply our approach to latent topic modeling tasks and demonstrate that by combining FOL knowledge and Bayesian modeling, we both improve the task performance and discover more structured latent representations in unsupervised and supervised learning.",
        "bibtex": "@InProceedings{pmlr-v32-mei14,\n  title = \t {Robust RegBayes: Selectively Incorporating First-Order Logic Domain Knowledge into Bayesian Models},\n  author = \t {Mei, Shike and Zhu, Jun and Zhu, Jerry},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {253--261},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/mei14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/mei14.html},\n  abstract = \t {Much research in Bayesian modeling has been done to elicit a prior distribution that incorporates domain knowledge. We present a novel and more direct approach by imposing First-Order Logic (FOL) rules on the posterior distribution. Our approach unifies FOL and Bayesian modeling under the regularized Bayesian framework. In addition, our approach automatically estimates the uncertainty of FOL rules when they are produced by humans, so that reliable rules are incorporated while unreliable ones are ignored. We apply our approach to latent topic modeling tasks and demonstrate that by combining FOL knowledge and Bayesian modeling, we both improve the task performance and discover more structured latent representations in unsupervised and supervised learning.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/mei14.pdf",
        "supp": "",
        "pdf_size": 569315,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=424347159147146758&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Sciences, University of Wisconsin-Madison, Madison, WI, USA 53706; Dept. of Comp. Sci. & Tech., TNList Lab, State Key Lab of Intell. Tech. & Sys., Tsinghua University, China; Department of Computer Sciences, University of Wisconsin-Madison, Madison, WI, USA 53706",
        "aff_domain": "CS.WISC.EDU;MAIL.TSINGHUA.EDU.CN;CS.WISC.EDU",
        "email": "CS.WISC.EDU;MAIL.TSINGHUA.EDU.CN;CS.WISC.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Wisconsin-Madison;Tsinghua University",
        "aff_unique_dep": "Department of Computer Sciences;Dept. of Comp. Sci. & Tech.",
        "aff_unique_url": "https://www.wisc.edu;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "UW-Madison;THU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Madison;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "d805671d0c",
        "title": "Robust and Efficient Kernel Hyperparameter Paths with Guarantees",
        "site": "https://proceedings.mlr.press/v32/giesen14.html",
        "author": "Joachim Giesen; Soeren Laue; Patrick Wieschollek",
        "abstract": "Algorithmically, many machine learning tasks boil down to solving  parameterized optimization problems. Finding good values for the  parameters has significant influence on the statistical performance  of these methods. Thus supporting the choice of parameter values  algorithmically has received quite some attention recently,  especially algorithms for computing the whole solution path of  parameterized optimization problem. These algorithms can be used,  for instance, to track the solution of a regularized learning  problem along the regularization parameter path, or for tracking the  solution of kernelized problems along a kernel hyperparameter  path. Since exact path following algorithms can be numerically  unstable, robust and efficient approximate path tracking algorithms  became popular for regularized learning problems. By now algorithms  with optimal path complexity are known for many regularized learning  problems. That is not the case for kernel hyperparameter path  tracking algorithms, where the exact path tracking algorithms can  also suffer from numerical instabilities. The robust approximation  algorithms for regularization path tracking can not be used directly  for kernel hyperparameter path tracking problems since the latter  fall into a different problem class. Here we address this problem by  devising a robust and efficient path tracking algorithm that can  also handle kernel hyperparameter paths and has asymptotically  optimal complexity. We use this algorithm to compute approximate  kernel hyperparamter solution paths for support vector machines and  robust kernel regression. Experimental results for this problem  applied to various data sets confirms the theoretical complexity  analysis.",
        "bibtex": "@InProceedings{pmlr-v32-giesen14,\n  title = \t {Robust and Efficient Kernel Hyperparameter Paths with Guarantees},\n  author = \t {Giesen, Joachim and Laue, Soeren and Wieschollek, Patrick},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1296--1304},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/giesen14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/giesen14.html},\n  abstract = \t {Algorithmically, many machine learning tasks boil down to solving  parameterized optimization problems. Finding good values for the  parameters has significant influence on the statistical performance  of these methods. Thus supporting the choice of parameter values  algorithmically has received quite some attention recently,  especially algorithms for computing the whole solution path of  parameterized optimization problem. These algorithms can be used,  for instance, to track the solution of a regularized learning  problem along the regularization parameter path, or for tracking the  solution of kernelized problems along a kernel hyperparameter  path. Since exact path following algorithms can be numerically  unstable, robust and efficient approximate path tracking algorithms  became popular for regularized learning problems. By now algorithms  with optimal path complexity are known for many regularized learning  problems. That is not the case for kernel hyperparameter path  tracking algorithms, where the exact path tracking algorithms can  also suffer from numerical instabilities. The robust approximation  algorithms for regularization path tracking can not be used directly  for kernel hyperparameter path tracking problems since the latter  fall into a different problem class. Here we address this problem by  devising a robust and efficient path tracking algorithm that can  also handle kernel hyperparameter paths and has asymptotically  optimal complexity. We use this algorithm to compute approximate  kernel hyperparamter solution paths for support vector machines and  robust kernel regression. Experimental results for this problem  applied to various data sets confirms the theoretical complexity  analysis.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/giesen14.pdf",
        "supp": "",
        "pdf_size": 313719,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7030856530305811810&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Friedrich-Schiller-Universit \u00a8at Jena, Germany; Friedrich-Schiller-Universit \u00a8at Jena, Germany; Friedrich-Schiller-Universit \u00a8at Jena, Germany",
        "aff_domain": "UNI-JENA.DE;UNI-JENA.DE;WIESCHOLLEK.INFO",
        "email": "UNI-JENA.DE;UNI-JENA.DE;WIESCHOLLEK.INFO",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Friedrich-Schiller-Universit\u00e4t Jena",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uni-jena.de",
        "aff_unique_abbr": "FSU Jena",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "9e186096b1",
        "title": "Saddle Points and Accelerated Perceptron Algorithms",
        "site": "https://proceedings.mlr.press/v32/yuc14.html",
        "author": "Adams Wei Yu; Fatma Kilinc-Karzan; Jaime Carbonell",
        "abstract": "In this paper, we consider the problem of finding a linear (binary) classifier or providing a near-infeasibility certificate if there is none. We bring a new perspective to addressing these two problems simultaneously in a single efficient process, by investigating a related Bilinear Saddle Point Problem (BSPP). More specifically, we show that a BSPP-based approach provides either a linear classifier or an \u03b5-infeasibility certificate. We show that the accelerated primal-dual algorithm, Mirror Prox, can be used for this purpose and achieves the best known convergence rate of O(\\sqrt\\log n\\over\u03c1(A)) (O(\\sqrt\\log n\\over\u03b5)), which is \\emphalmost independent of the problem size, n. Our framework also solves kernelized and conic versions of the problem, with the same rate of convergence. We support our theoretical findings with an empirical study on synthetic and real data, highlighting the efficiency and numerical stability of our algorithms, especially on  large-scale instances.",
        "bibtex": "@InProceedings{pmlr-v32-yuc14,\n  title = \t {Saddle Points and Accelerated Perceptron Algorithms},\n  author = \t {Yu, Adams Wei and Kilinc-Karzan, Fatma and Carbonell, Jaime},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1827--1835},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/yuc14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/yuc14.html},\n  abstract = \t {In this paper, we consider the problem of finding a linear (binary) classifier or providing a near-infeasibility certificate if there is none. We bring a new perspective to addressing these two problems simultaneously in a single efficient process, by investigating a related Bilinear Saddle Point Problem (BSPP). More specifically, we show that a BSPP-based approach provides either a linear classifier or an \u03b5-infeasibility certificate. We show that the accelerated primal-dual algorithm, Mirror Prox, can be used for this purpose and achieves the best known convergence rate of O(\\sqrt\\log n\\over\u03c1(A)) (O(\\sqrt\\log n\\over\u03b5)), which is \\emphalmost independent of the problem size, n. Our framework also solves kernelized and conic versions of the problem, with the same rate of convergence. We support our theoretical findings with an empirical study on synthetic and real data, highlighting the efficiency and numerical stability of our algorithms, especially on  large-scale instances.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/yuc14.pdf",
        "supp": "",
        "pdf_size": 438826,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4937951417298421171&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA; Tepper School of Business, Carnegie Mellon University, Pittsburgh, PA, USA; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA",
        "aff_domain": "CS.CMU.EDU;ANDREW.CMU.EDU;CS.CMU.EDU",
        "email": "CS.CMU.EDU;ANDREW.CMU.EDU;CS.CMU.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1cc153b03f",
        "title": "Safe Screening with Variational Inequalities and Its Application to Lasso",
        "site": "https://proceedings.mlr.press/v32/liuc14.html",
        "author": "Jun Liu; Zheng Zhao; Jie Wang; Jieping Ye",
        "abstract": "Sparse learning techniques have been routinely used for feature selection as the resulting model usually has a small number of non-zero entries.  Safe screening, which eliminates the features that are guaranteed to have zero coefficients for a certain value of the regularization parameter, is a technique for improving the computational efficiency. Safe screening is gaining increasing attention since 1) solving sparse learning formulations usually has a high computational cost especially when the number of features is large and 2) one needs to try several regularization parameters to select a suitable model. In this paper, we propose an approach called \u201cSasvi\" (Safe screening with variational inequalities). Sasvi makes use of the variational inequality that provides the sufficient and necessary optimality condition for the dual problem. Several existing approaches for Lasso screening can be casted as relaxed versions of the proposed Sasvi, thus Sasvi provides a stronger safe screening rule. We further study the monotone properties of Sasvi for Lasso, based on which a sure removal regularization parameter can be identified for each feature. Experimental results on both synthetic and real data sets are reported to demonstrate the effectiveness of the proposed Sasvi for Lasso screening.",
        "bibtex": "@InProceedings{pmlr-v32-liuc14,\n  title = \t {Safe Screening with Variational Inequalities and Its Application to Lasso},\n  author = \t {Liu, Jun and Zhao, Zheng and Wang, Jie and Ye, Jieping},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {289--297},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/liuc14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/liuc14.html},\n  abstract = \t {Sparse learning techniques have been routinely used for feature selection as the resulting model usually has a small number of non-zero entries.  Safe screening, which eliminates the features that are guaranteed to have zero coefficients for a certain value of the regularization parameter, is a technique for improving the computational efficiency. Safe screening is gaining increasing attention since 1) solving sparse learning formulations usually has a high computational cost especially when the number of features is large and 2) one needs to try several regularization parameters to select a suitable model. In this paper, we propose an approach called \u201cSasvi\" (Safe screening with variational inequalities). Sasvi makes use of the variational inequality that provides the sufficient and necessary optimality condition for the dual problem. Several existing approaches for Lasso screening can be casted as relaxed versions of the proposed Sasvi, thus Sasvi provides a stronger safe screening rule. We further study the monotone properties of Sasvi for Lasso, based on which a sure removal regularization parameter can be identified for each feature. Experimental results on both synthetic and real data sets are reported to demonstrate the effectiveness of the proposed Sasvi for Lasso screening.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/liuc14.pdf",
        "supp": "",
        "pdf_size": 4153342,
        "gs_citation": 88,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11635278769824762772&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "SAS Institute Inc., Cary, NC 27513; SAS Institute Inc., Cary, NC 27513; Arizona State University, Tempe, AZ 85287; Arizona State University, Tempe, AZ 85287",
        "aff_domain": "SAS.COM;SAS.COM;USTC;ASU.EDU",
        "email": "SAS.COM;SAS.COM;USTC;ASU.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;1",
        "aff_unique_norm": "SAS Institute Inc.;Arizona State University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sas.com;https://www.asu.edu",
        "aff_unique_abbr": "SAS;ASU",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Tempe",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b8d5e37e1a",
        "title": "Sample Efficient Reinforcement Learning with Gaussian Processes",
        "site": "https://proceedings.mlr.press/v32/grande14.html",
        "author": "Robert Grande; Thomas Walsh; Jonathan How",
        "abstract": "This paper derives sample complexity results for using Gaussian Processes (GPs) in both model-based and model-free reinforcement learning (RL). We show that GPs are KWIK learnable, proving for the first time that a model-based RL approach using GPs, GP-Rmax, is sample efficient (PAC-MDP). However, we then show that previous approaches to model-free RL using GPs take an exponential number of steps to find an optimal policy, and are therefore not sample efficient. The third and main contribution is the introduction of a model-free RL algorithm using GPs, DGPQ, which is sample efficient and, in contrast to model-based algorithms, capable of acting in real time, as demonstrated on a five-dimensional aircraft simulator.",
        "bibtex": "@InProceedings{pmlr-v32-grande14,\n  title = \t {Sample Efficient Reinforcement Learning with Gaussian Processes},\n  author = \t {Grande, Robert and Walsh, Thomas and How, Jonathan},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1332--1340},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/grande14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/grande14.html},\n  abstract = \t {This paper derives sample complexity results for using Gaussian Processes (GPs) in both model-based and model-free reinforcement learning (RL). We show that GPs are KWIK learnable, proving for the first time that a model-based RL approach using GPs, GP-Rmax, is sample efficient (PAC-MDP). However, we then show that previous approaches to model-free RL using GPs take an exponential number of steps to find an optimal policy, and are therefore not sample efficient. The third and main contribution is the introduction of a model-free RL algorithm using GPs, DGPQ, which is sample efficient and, in contrast to model-based algorithms, capable of acting in real time, as demonstrated on a five-dimensional aircraft simulator.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/grande14.pdf",
        "supp": "",
        "pdf_size": 425876,
        "gs_citation": 87,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11023378997429495608&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Massachusetts Institute of Technology; Massachusetts Institute of Technology; Massachusetts Institute of Technology",
        "aff_domain": "MIT.EDU;GMAIL.COM;MIT.EDU",
        "email": "MIT.EDU;GMAIL.COM;MIT.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "badf18295b",
        "title": "Sample-based approximate regularization",
        "site": "https://proceedings.mlr.press/v32/bachman14.html",
        "author": "Philip Bachman; Amir-Massoud Farahmand; Doina Precup",
        "abstract": "We introduce a method for regularizing linearly parameterized functions using general derivative-based penalties, which relies on sampling as well as finite-difference approximations of the relevant derivatives. We call this approach sample-based approximate regularization (SAR). We provide theoretical guarantees on the fidelity of such regularizers, compared to those they approximate, and prove that the approximations converge efficiently. We also examine the empirical performance of SAR on several datasets.",
        "bibtex": "@InProceedings{pmlr-v32-bachman14,\n  title = \t {Sample-based approximate regularization},\n  author = \t {Bachman, Philip and Farahmand, Amir-Massoud and Precup, Doina},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1926--1934},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/bachman14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/bachman14.html},\n  abstract = \t {We introduce a method for regularizing linearly parameterized functions using general derivative-based penalties, which relies on sampling as well as finite-difference approximations of the relevant derivatives. We call this approach sample-based approximate regularization (SAR). We provide theoretical guarantees on the fidelity of such regularizers, compared to those they approximate, and prove that the approximations converge efficiently. We also examine the empirical performance of SAR on several datasets.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/bachman14.pdf",
        "supp": "",
        "pdf_size": 1020097,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14605762738478117622&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "School of Computer Science, McGill University, Montreal, Canada+Carnegie Mellon University, Pittsburgh, USA; School of Computer Science, McGill University, Montreal, Canada; School of Computer Science, McGill University, Montreal, Canada",
        "aff_domain": "GMAIL.COM;ANDREW.CMU.EDU;CS.MCGILL.CA",
        "email": "GMAIL.COM;ANDREW.CMU.EDU;CS.MCGILL.CA",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "McGill University;Carnegie Mellon University",
        "aff_unique_dep": "School of Computer Science;",
        "aff_unique_url": "https://www.mcgill.ca;https://www.cmu.edu",
        "aff_unique_abbr": "McGill;CMU",
        "aff_campus_unique_index": "0+1;0;0",
        "aff_campus_unique": "Montreal;Pittsburgh",
        "aff_country_unique_index": "0+1;0;0",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "191f4ca495",
        "title": "Scalable Bayesian Low-Rank Decomposition of Incomplete Multiway Tensors",
        "site": "https://proceedings.mlr.press/v32/rai14.html",
        "author": "Piyush Rai; Yingjian Wang; Shengbo Guo; Gary Chen; David Dunson; Lawrence Carin",
        "abstract": "We present a scalable Bayesian framework for low-rank decomposition of multiway tensor data with missing observations. The key issue of pre-specifying the rank of the decomposition is sidestepped in a principled manner using a multiplicative gamma process prior. Both continuous and binary data can be analyzed under the framework, in a coherent way using fully conjugate Bayesian analysis. In particular, the analysis in the non-conjugate binary case is facilitated via the use of the P\u00f3lya-Gamma sampling strategy which elicits closed-form Gibbs sampling updates. The resulting samplers are efficient and enable us to apply our framework to large-scale problems, with time-complexity that is linear in the number of observed entries in the tensor. This is especially attractive in analyzing very large but sparsely observed tensors with very few known entries. Moreover, our method admits easy extension to the supervised setting where entities in one or more tensor modes have labels. Our method outperforms several state-of-the-art tensor decomposition methods on various synthetic and benchmark real-world datasets.",
        "bibtex": "@InProceedings{pmlr-v32-rai14,\n  title = \t {Scalable Bayesian Low-Rank Decomposition of Incomplete Multiway Tensors},\n  author = \t {Rai, Piyush and Wang, Yingjian and Guo, Shengbo and Chen, Gary and Dunson, David and Carin, Lawrence},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1800--1808},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/rai14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/rai14.html},\n  abstract = \t {We present a scalable Bayesian framework for low-rank decomposition of multiway tensor data with missing observations. The key issue of pre-specifying the rank of the decomposition is sidestepped in a principled manner using a multiplicative gamma process prior. Both continuous and binary data can be analyzed under the framework, in a coherent way using fully conjugate Bayesian analysis. In particular, the analysis in the non-conjugate binary case is facilitated via the use of the P\u00f3lya-Gamma sampling strategy which elicits closed-form Gibbs sampling updates. The resulting samplers are efficient and enable us to apply our framework to large-scale problems, with time-complexity that is linear in the number of observed entries in the tensor. This is especially attractive in analyzing very large but sparsely observed tensors with very few known entries. Moreover, our method admits easy extension to the supervised setting where entities in one or more tensor modes have labels. Our method outperforms several state-of-the-art tensor decomposition methods on various synthetic and benchmark real-world datasets.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/rai14.pdf",
        "supp": "",
        "pdf_size": 751036,
        "gs_citation": 163,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9911275260617249888&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Electrical and Computer Engineering, Duke University; Department of Electrical and Computer Engineering, Duke University; Samsung Research America; Samsung Research America; Department of Statistical Science, Duke University; Department of Electrical and Computer Engineering, Duke University",
        "aff_domain": "duke.edu;duke.edu;samsung.com;samsung.com;duke.edu;duke.edu",
        "email": "duke.edu;duke.edu;samsung.com;samsung.com;duke.edu;duke.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;1;0;0",
        "aff_unique_norm": "Duke University;Samsung",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;Samsung Research America",
        "aff_unique_url": "https://www.duke.edu;https://www.samsung.com/us/careers/research/",
        "aff_unique_abbr": "Duke;SRA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0dbf726263",
        "title": "Scalable Gaussian Process Structured Prediction for Grid Factor Graph Applications",
        "site": "https://proceedings.mlr.press/v32/bratieres14.html",
        "author": "Sebastien Bratieres; Novi Quadrianto; Sebastian Nowozin; Zoubin Ghahramani",
        "abstract": "Structured prediction is an important and well studied problem with many applications across machine learning. GPstruct is a recently proposed structured prediction model that offers appealing properties such as being kernelised, non-parametric, and supporting Bayesian inference (Brati\u00e8res et al. 2013).   The model places a Gaussian process prior over energy functions which describe relationships between input variables and structured output variables.  However, the memory demand of GPstruct is quadratic in the number of latent variables and training runtime scales cubically.   This prevents GPstruct from being applied to problems involving grid factor graphs, which are prevalent in computer vision and spatial statistics applications.     Here we explore a scalable approach to learning GPstruct models based on ensemble learning, with weak learners (predictors) trained on subsets of the latent variables and bootstrap data, which can easily be distributed.  We show experiments with 4M latent variables on image segmentation.  Our method outperforms widely-used conditional random field models trained with pseudo-likelihood.   Moreover, in image segmentation problems it improves over recent state-of-the-art marginal optimisation methods in terms of predictive performance and uncertainty calibration. Finally, it generalises well on all training set sizes.",
        "bibtex": "@InProceedings{pmlr-v32-bratieres14,\n  title = \t {Scalable Gaussian Process Structured Prediction for Grid Factor Graph Applications},\n  author = \t {Bratieres, Sebastien and Quadrianto, Novi and Nowozin, Sebastian and Ghahramani, Zoubin},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {334--342},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/bratieres14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/bratieres14.html},\n  abstract = \t {Structured prediction is an important and well studied problem with many applications across machine learning. GPstruct is a recently proposed structured prediction model that offers appealing properties such as being kernelised, non-parametric, and supporting Bayesian inference (Brati\u00e8res et al. 2013).   The model places a Gaussian process prior over energy functions which describe relationships between input variables and structured output variables.  However, the memory demand of GPstruct is quadratic in the number of latent variables and training runtime scales cubically.   This prevents GPstruct from being applied to problems involving grid factor graphs, which are prevalent in computer vision and spatial statistics applications.     Here we explore a scalable approach to learning GPstruct models based on ensemble learning, with weak learners (predictors) trained on subsets of the latent variables and bootstrap data, which can easily be distributed.  We show experiments with 4M latent variables on image segmentation.  Our method outperforms widely-used conditional random field models trained with pseudo-likelihood.   Moreover, in image segmentation problems it improves over recent state-of-the-art marginal optimisation methods in terms of predictive performance and uncertainty calibration. Finally, it generalises well on all training set sizes.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/bratieres14.pdf",
        "supp": "",
        "pdf_size": 867918,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2551459460219828279&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "SMiLe CLiNiC, Department of Informatics, University of Sussex, UK+Machine Learning Group, Department of Engineering, University of Cambridge, UK; SMiLe CLiNiC, Department of Informatics, University of Sussex, UK+Machine Learning Group, Department of Engineering, University of Cambridge, UK; Microsoft Research, Cambridge, UK; Machine Learning Group, Department of Engineering, University of Cambridge, UK",
        "aff_domain": "CANTAB.NET;SUSSEX.AC.UK;MICROSOFT.COM;ENG.CAM.AC.UK",
        "email": "CANTAB.NET;SUSSEX.AC.UK;MICROSOFT.COM;ENG.CAM.AC.UK",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;2;1",
        "aff_unique_norm": "University of Sussex;University of Cambridge;Microsoft",
        "aff_unique_dep": "Department of Informatics;Department of Engineering;Microsoft Research",
        "aff_unique_url": "https://www.sussex.ac.uk;https://www.cam.ac.uk;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "Sussex;Cambridge;MSR",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0+0;0+0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "d124b1515c",
        "title": "Scalable Semidefinite Relaxation for Maximum A Posterior Estimation",
        "site": "https://proceedings.mlr.press/v32/huang14.html",
        "author": "Qixing Huang; Yuxin Chen; Leonidas Guibas",
        "abstract": "Maximum a posteriori (MAP) inference over discrete Markov random fields is a central task spanning a wide spectrum of real-world applications but known to be NP-hard for general graphs. In this paper, we propose a novel semidefinite relaxation formulation (referred to as SDR) to estimate the MAP assignment. Algorithmically, we develop an accelerated variant of the alternating direction method of multipliers (referred to as SDPAD-LR) that can effectively exploit the special structure of SDR. Encouragingly, the proposed procedure allows solving SDR for large-scale problems,  e.g. problems comprising hundreds of thousands of variables with multiple states on a grid graph. Compared with prior SDP solvers, SDPAD-LR is capable of attaining comparable accuracy while exhibiting remarkably improved scalability. This contradicts the commonly held belief that semidefinite relaxation can only been applied on small-scale problems. We have evaluated the performance of SDR on various benchmark datasets including OPENGM2 and PIC. Experimental results demonstrate that for a broad class of problems, SDPAD-LR  outperforms state-of-the-art algorithms in producing better MAP assignments.",
        "bibtex": "@InProceedings{pmlr-v32-huang14,\n  title = \t {Scalable Semidefinite Relaxation for Maximum A Posterior Estimation},\n  author = \t {Huang, Qixing and Chen, Yuxin and Guibas, Leonidas},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {64--72},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/huang14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/huang14.html},\n  abstract = \t {Maximum a posteriori (MAP) inference over discrete Markov random fields is a central task spanning a wide spectrum of real-world applications but known to be NP-hard for general graphs. In this paper, we propose a novel semidefinite relaxation formulation (referred to as SDR) to estimate the MAP assignment. Algorithmically, we develop an accelerated variant of the alternating direction method of multipliers (referred to as SDPAD-LR) that can effectively exploit the special structure of SDR. Encouragingly, the proposed procedure allows solving SDR for large-scale problems,  e.g. problems comprising hundreds of thousands of variables with multiple states on a grid graph. Compared with prior SDP solvers, SDPAD-LR is capable of attaining comparable accuracy while exhibiting remarkably improved scalability. This contradicts the commonly held belief that semidefinite relaxation can only been applied on small-scale problems. We have evaluated the performance of SDR on various benchmark datasets including OPENGM2 and PIC. Experimental results demonstrate that for a broad class of problems, SDPAD-LR  outperforms state-of-the-art algorithms in producing better MAP assignments.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/huang14.pdf",
        "supp": "",
        "pdf_size": 432990,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11603237265009135850&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Computer Science, Stanford University, Stanford, CA 94305, USA; Department of Electrical Engineering, Stanford University, Stanford, CA 94305, USA; Department of Computer Science, Stanford University, Stanford, CA 94305 USA",
        "aff_domain": "STANFORD.EDU;STANFORD.EDU;CS.STANFORD.EDU",
        "email": "STANFORD.EDU;STANFORD.EDU;CS.STANFORD.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5554c78116",
        "title": "Scalable and Robust Bayesian Inference via the Median Posterior",
        "site": "https://proceedings.mlr.press/v32/minsker14.html",
        "author": "Stanislav Minsker; Sanvesh Srivastava; Lizhen Lin; David Dunson",
        "abstract": "Many Bayesian learning methods for massive data benefit from working with small subsets of observations.  In particular, significant progress has been made in scalable Bayesian learning via stochastic approximation.  However, Bayesian learning methods in distributed computing environments are often problem- or distribution-specific and use ad hoc techniques.   We propose a novel general approach to Bayesian inference that is scalable and robust to corruption in the data.  Our technique is based on the idea of splitting the data into several non-overlapping subgroups, evaluating the posterior distribution given each independent subgroup, and then combining the results.  The main novelty is the proposed aggregation step which is based on finding the geometric median of posterior distributions.    We present both theoretical and numerical results illustrating the advantages of our approach.",
        "bibtex": "@InProceedings{pmlr-v32-minsker14,\n  title = \t {Scalable and Robust Bayesian Inference via the Median Posterior},\n  author = \t {Minsker, Stanislav and Srivastava, Sanvesh and Lin, Lizhen and Dunson, David},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1656--1664},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/minsker14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/minsker14.html},\n  abstract = \t {Many Bayesian learning methods for massive data benefit from working with small subsets of observations.  In particular, significant progress has been made in scalable Bayesian learning via stochastic approximation.  However, Bayesian learning methods in distributed computing environments are often problem- or distribution-specific and use ad hoc techniques.   We propose a novel general approach to Bayesian inference that is scalable and robust to corruption in the data.  Our technique is based on the idea of splitting the data into several non-overlapping subgroups, evaluating the posterior distribution given each independent subgroup, and then combining the results.  The main novelty is the proposed aggregation step which is based on finding the geometric median of posterior distributions.    We present both theoretical and numerical results illustrating the advantages of our approach.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/minsker14.pdf",
        "supp": "",
        "pdf_size": 1372182,
        "gs_citation": 127,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10433244812322572045&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Departments of Mathematics; Departments of Statistical Science+Statistical and Applied Mathematical Sciences Institute; Departments of Statistical Science; Departments of Statistical Science",
        "aff_domain": "MATH.DUKE.EDU;STAT.DUKE.EDU;STAT.DUKE.EDU;STAT.DUKE.EDU",
        "email": "MATH.DUKE.EDU;STAT.DUKE.EDU;STAT.DUKE.EDU;STAT.DUKE.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2;1;1",
        "aff_unique_norm": "University Affiliation Not Specified;Departments of Statistical Science;Statistical and Applied Mathematical Sciences Institute",
        "aff_unique_dep": "Departments of Mathematics;Statistical Science;Statistical and Applied Mathematical Sciences",
        "aff_unique_url": ";;",
        "aff_unique_abbr": ";;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "f8884a5b83",
        "title": "Scaling SVM and Least Absolute Deviations via Exact Data Reduction",
        "site": "https://proceedings.mlr.press/v32/wangd14.html",
        "author": "Jie Wang; Peter Wonka; Jieping Ye",
        "abstract": "The support vector machine (SVM) is a widely used method for classification. Although many efforts have been devoted to develop efficient solvers, it remains challenging to apply SVM to large-scale problems. A nice property of SVM is that the non-support vectors have no effect on the resulting classifier. Motivated by this observation, we present fast and efficient screening rules to discard non-support vectors by analyzing the dual problem of SVM via variational inequalities (DVI). As a result, the number of data instances to be entered into the optimization can be substantially reduced. Some appealing features of our screening method are: (1) DVI is safe in the sense that the vectors discarded by DVI are guaranteed to be non-support vectors; (2) the data set needs to be scanned only once to run the screening, and its computational cost is negligible compared to that of solving the SVM problem; (3) DVI is independent of the solvers and can be integrated with any existing efficient solver. We also show that the DVI technique can be extended to detect non-support vectors in the least absolute deviations regression (LAD). To the best of our knowledge, there are currently no screening methods for LAD.  We have evaluated DVI on both synthetic and real data sets. Experiments indicate that DVI significantly outperforms the existing state-of-the-art screening rules for SVM, and it is very effective in discarding non-support vectors for LAD. The speedup gained by DVI rules can be up to two orders of magnitude.",
        "bibtex": "@InProceedings{pmlr-v32-wangd14,\n  title = \t {Scaling SVM and Least Absolute Deviations via Exact Data Reduction},\n  author = \t {Wang, Jie and Wonka, Peter and Ye, Jieping},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {523--531},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/wangd14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/wangd14.html},\n  abstract = \t {The support vector machine (SVM) is a widely used method for classification. Although many efforts have been devoted to develop efficient solvers, it remains challenging to apply SVM to large-scale problems. A nice property of SVM is that the non-support vectors have no effect on the resulting classifier. Motivated by this observation, we present fast and efficient screening rules to discard non-support vectors by analyzing the dual problem of SVM via variational inequalities (DVI). As a result, the number of data instances to be entered into the optimization can be substantially reduced. Some appealing features of our screening method are: (1) DVI is safe in the sense that the vectors discarded by DVI are guaranteed to be non-support vectors; (2) the data set needs to be scanned only once to run the screening, and its computational cost is negligible compared to that of solving the SVM problem; (3) DVI is independent of the solvers and can be integrated with any existing efficient solver. We also show that the DVI technique can be extended to detect non-support vectors in the least absolute deviations regression (LAD). To the best of our knowledge, there are currently no screening methods for LAD.  We have evaluated DVI on both synthetic and real data sets. Experiments indicate that DVI significantly outperforms the existing state-of-the-art screening rules for SVM, and it is very effective in discarding non-support vectors for LAD. The speedup gained by DVI rules can be up to two orders of magnitude.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/wangd14.pdf",
        "supp": "",
        "pdf_size": 478028,
        "gs_citation": 72,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8351289829625412920&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 19,
        "aff": "Arizona State University, Tempe, AZ 85287 USA; King Abdullah University of Science and Technology, Thuwal, Saudi Arabia + Arizona State University, Tempe, AZ 85287 USA; Arizona State University, Tempe, AZ 85287 USA",
        "aff_domain": "asu.edu;gmail.com;asu.edu",
        "email": "asu.edu;gmail.com;asu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;0",
        "aff_unique_norm": "Arizona State University;King Abdullah University of Science and Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.asu.edu;https://www.kast.kau.edu.sa",
        "aff_unique_abbr": "ASU;KAUST",
        "aff_campus_unique_index": "0;1+0;0",
        "aff_campus_unique": "Tempe;Thuwal",
        "aff_country_unique_index": "0;1+0;0",
        "aff_country_unique": "United States;Saudi Arabia"
    },
    {
        "id": "8ef37461c5",
        "title": "Scaling Up Approximate Value Iteration with Options: Better Policies with Fewer Iterations",
        "site": "https://proceedings.mlr.press/v32/mann14.html",
        "author": "Timothy Mann; Shie Mannor",
        "abstract": "We show how options, a class of control structures encompassing primitive and temporally extended actions, can play a valuable role in planning in MDPs with continuous state-spaces. Analyzing the convergence rate of Approximate Value Iteration with options reveals that for pessimistic initial value function estimates, options can speed up convergence compared to planning with only primitive actions even when the temporally extended actions are suboptimal and sparsely scattered throughout the state-space. Our experimental results in an optimal replacement task and a complex inventory management task demonstrate the potential for options to speed up convergence in practice. We show that options induce faster convergence to the optimal value function, which implies deriving better policies with fewer iterations.",
        "bibtex": "@InProceedings{pmlr-v32-mann14,\n  title = \t {Scaling Up Approximate Value Iteration with Options: Better Policies with Fewer Iterations},\n  author = \t {Mann, Timothy and Mannor, Shie},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {127--135},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/mann14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/mann14.html},\n  abstract = \t {We show how options, a class of control structures encompassing primitive and temporally extended actions, can play a valuable role in planning in MDPs with continuous state-spaces. Analyzing the convergence rate of Approximate Value Iteration with options reveals that for pessimistic initial value function estimates, options can speed up convergence compared to planning with only primitive actions even when the temporally extended actions are suboptimal and sparsely scattered throughout the state-space. Our experimental results in an optimal replacement task and a complex inventory management task demonstrate the potential for options to speed up convergence in practice. We show that options induce faster convergence to the optimal value function, which implies deriving better policies with fewer iterations.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/mann14.pdf",
        "supp": "",
        "pdf_size": 594147,
        "gs_citation": 70,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13178145968688488644&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Electrical Engineering, The Technion - Israel Institute of Technology, Haifa, Israel 32000; Department of Electrical Engineering, The Technion - Israel Institute of Technology, Haifa, Israel 32000",
        "aff_domain": "ee.technion.ac.il;ee.technion.ac.il",
        "email": "ee.technion.ac.il;ee.technion.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Technion - Israel Institute of Technology",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.technion.ac.il",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Haifa",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "f306ce1a75",
        "title": "Scaling Up Robust MDPs using Function Approximation",
        "site": "https://proceedings.mlr.press/v32/tamar14.html",
        "author": "Aviv Tamar; Shie Mannor; Huan Xu",
        "abstract": "We consider large-scale Markov decision processes (MDPs) with parameter uncertainty, under the robust MDP paradigm. Previous studies showed that robust MDPs, based on a minimax approach to handling uncertainty, can be solved using dynamic programming for small to medium sized problems. However, due to the \"curse of dimensionality\", MDPs that model real-life problems are typically prohibitively large for such approaches. In this work we employ a reinforcement learning approach to tackle this planning problem: we develop a robust approximate dynamic programming method based on a projected fixed point equation to approximately solve large scale robust MDPs. We show that the proposed method provably succeeds under certain technical conditions, and demonstrate its effectiveness through simulation of an option pricing problem. To the best of our knowledge, this is the first attempt to scale up the robust MDP paradigm.",
        "bibtex": "@InProceedings{pmlr-v32-tamar14,\n  title = \t {Scaling Up Robust MDPs using Function Approximation},\n  author = \t {Tamar, Aviv and Mannor, Shie and Xu, Huan},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {181--189},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/tamar14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/tamar14.html},\n  abstract = \t {We consider large-scale Markov decision processes (MDPs) with parameter uncertainty, under the robust MDP paradigm. Previous studies showed that robust MDPs, based on a minimax approach to handling uncertainty, can be solved using dynamic programming for small to medium sized problems. However, due to the \"curse of dimensionality\", MDPs that model real-life problems are typically prohibitively large for such approaches. In this work we employ a reinforcement learning approach to tackle this planning problem: we develop a robust approximate dynamic programming method based on a projected fixed point equation to approximately solve large scale robust MDPs. We show that the proposed method provably succeeds under certain technical conditions, and demonstrate its effectiveness through simulation of an option pricing problem. To the best of our knowledge, this is the first attempt to scale up the robust MDP paradigm.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/tamar14.pdf",
        "supp": "",
        "pdf_size": 386603,
        "gs_citation": 176,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13643350134809493712&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Electrical Engineering Department, The Technion - Israel Institute of Technology, Haifa 32000, Israel; Electrical Engineering Department, The Technion - Israel Institute of Technology, Haifa 32000, Israel; Mechanical Engineering Department, National University of Singapore, Singapore 117575, Singapore",
        "aff_domain": "TX.TECHNION.AC.IL;EE.TECHNION.AC.IL;NUS.EDU.SG",
        "email": "TX.TECHNION.AC.IL;EE.TECHNION.AC.IL;NUS.EDU.SG",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Technion - Israel Institute of Technology;National University of Singapore",
        "aff_unique_dep": "Electrical Engineering Department;Mechanical Engineering Department",
        "aff_unique_url": "https://www.technion.ac.il;https://www.nus.edu.sg",
        "aff_unique_abbr": "Technion;NUS",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Haifa;",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Israel;Singapore"
    },
    {
        "id": "1d285cf1e2",
        "title": "Signal recovery from Pooling Representations",
        "site": "https://proceedings.mlr.press/v32/estrach14.html",
        "author": "Joan Bruna Estrach; Arthur Szlam; Yann LeCun",
        "abstract": "Pooling operators construct non-linear representations  by cascading a redundant linear transform, followed by   a point-wise nonlinearity and a local aggregation, typically  implemented with a \\ell_p norm.   Their efficiency in recognition architectures is based   on their ability to locally contract the input space,   but also on their capacity to retain as much stable information   as possible.  We address this latter question by computing the upper and   lower Lipschitz bounds of \\ell_p pooling operators for p=1, 2, \u221eas well as their half-rectified equivalents, which give  sufficient conditions for the design of invertible pooling layers.  Numerical experiments on MNIST and image patches confirm that  pooling layers can be inverted with phase recovery algorithms. Moreover,  the regularity of the inverse pooling, controlled by the lower Lipschitz constant,   is empirically verified with a nearest neighbor regression.",
        "bibtex": "@InProceedings{pmlr-v32-estrach14,\n  title = \t {Signal recovery from Pooling Representations},\n  author = \t {Estrach, Joan Bruna and Szlam, Arthur and LeCun, Yann},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {307--315},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/estrach14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/estrach14.html},\n  abstract = \t {Pooling operators construct non-linear representations  by cascading a redundant linear transform, followed by   a point-wise nonlinearity and a local aggregation, typically  implemented with a \\ell_p norm.   Their efficiency in recognition architectures is based   on their ability to locally contract the input space,   but also on their capacity to retain as much stable information   as possible.  We address this latter question by computing the upper and   lower Lipschitz bounds of \\ell_p pooling operators for p=1, 2, \u221eas well as their half-rectified equivalents, which give  sufficient conditions for the design of invertible pooling layers.  Numerical experiments on MNIST and image patches confirm that  pooling layers can be inverted with phase recovery algorithms. Moreover,  the regularity of the inverse pooling, controlled by the lower Lipschitz constant,   is empirically verified with a nearest neighbor regression.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/estrach14.pdf",
        "supp": "",
        "pdf_size": 375106,
        "gs_citation": 133,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16764596505323789178&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Courant Institute, New York University; The City College of New York, CUNY; Courant Institute, New York University",
        "aff_domain": "CIMS.NYU.EDU;CCNY.CUNY.EDU;CS.NYU.EDU",
        "email": "CIMS.NYU.EDU;CCNY.CUNY.EDU;CS.NYU.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "New York University;City College of New York",
        "aff_unique_dep": "Courant Institute;",
        "aff_unique_url": "https://www.courant.nyu.edu;https://www.ccny.cuny.edu",
        "aff_unique_abbr": "NYU;CCNY",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "New York",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5b95d4f464",
        "title": "Skip Context Tree Switching",
        "site": "https://proceedings.mlr.press/v32/bellemare14.html",
        "author": "Marc Bellemare; Joel Veness; Erik Talvitie",
        "abstract": "Context Tree Weighting (CTW) is a powerful probabilistic sequence prediction technique that efficiently performs Bayesian model averaging over the class of all prediction suffix trees of bounded depth. In this paper we show how to generalize this technique to the class of K-skip prediction suffix trees. Contrary to regular prediction suffix trees, K-skip prediction suffix trees are permitted to ignore up to K contiguous portions of the context. This allows for significant improvements in predictive accuracy when irrelevant variables are present, a case which often occurs within record-aligned data and images. We provide a regret-based analysis of our approach, and empirically evaluate it on the Calgary corpus and a set of Atari 2600 screen prediction tasks.",
        "bibtex": "@InProceedings{pmlr-v32-bellemare14,\n  title = \t {Skip Context Tree Switching},\n  author = \t {Bellemare, Marc and Veness, Joel and Talvitie, Erik},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1458--1466},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/bellemare14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/bellemare14.html},\n  abstract = \t {Context Tree Weighting (CTW) is a powerful probabilistic sequence prediction technique that efficiently performs Bayesian model averaging over the class of all prediction suffix trees of bounded depth. In this paper we show how to generalize this technique to the class of K-skip prediction suffix trees. Contrary to regular prediction suffix trees, K-skip prediction suffix trees are permitted to ignore up to K contiguous portions of the context. This allows for significant improvements in predictive accuracy when irrelevant variables are present, a case which often occurs within record-aligned data and images. We provide a regret-based analysis of our approach, and empirically evaluate it on the Calgary corpus and a set of Atari 2600 screen prediction tasks.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/bellemare14.pdf",
        "supp": "",
        "pdf_size": 582014,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8195637401085576755&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Google DeepMind; Google DeepMind; Franklin and Marshall College",
        "aff_domain": "GOOGLE.COM;GOOGLE.COM;FANDM.EDU",
        "email": "GOOGLE.COM;GOOGLE.COM;FANDM.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Google;Franklin and Marshall College",
        "aff_unique_dep": "Google DeepMind;",
        "aff_unique_url": "https://deepmind.com;https://www.fandm.edu",
        "aff_unique_abbr": "DeepMind;F&M",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "c2eb13a648",
        "title": "Sparse Reinforcement Learning via Convex Optimization",
        "site": "https://proceedings.mlr.press/v32/qin14.html",
        "author": "Zhiwei Qin; Weichang Li; Firdaus Janoos",
        "abstract": "We propose two new algorithms for the sparse reinforcement learning problem based on different formulations.  The first algorithm is an off-line method based on the alternating direction method of multipliers for solving a constrained formulation that explicitly controls the projected Bellman residual.  The second algorithm is an online stochastic approximation algorithm that employs the regularized dual averaging technique, using the Lagrangian formulation.  The convergence of both algorithms are established. We demonstrate the performance of these algorithms through two classical examples.",
        "bibtex": "@InProceedings{pmlr-v32-qin14,\n  title = \t {Sparse Reinforcement Learning via Convex Optimization},\n  author = \t {Qin, Zhiwei and Li, Weichang and Janoos, Firdaus},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {424--432},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/qin14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/qin14.html},\n  abstract = \t {We propose two new algorithms for the sparse reinforcement learning problem based on different formulations.  The first algorithm is an off-line method based on the alternating direction method of multipliers for solving a constrained formulation that explicitly controls the projected Bellman residual.  The second algorithm is an online stochastic approximation algorithm that employs the regularized dual averaging technique, using the Lagrangian formulation.  The convergence of both algorithms are established. We demonstrate the performance of these algorithms through two classical examples.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/qin14.pdf",
        "supp": "",
        "pdf_size": 344892,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16294681845545310675&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "@WalmartLabs, 850 Cherry Ave, San Bruno, CA 94066 + ExxonMobil Corporate Strategic Research, 1545 Rt. 22 East, Annandale, NJ 08801; ExxonMobil Corporate Strategic Research, 1545 Rt. 22 East, Annandale, NJ 08801; ExxonMobil Corporate Strategic Research, 1545 Rt. 22 East, Annandale, NJ 08801",
        "aff_domain": "WALMARTLABS.COM;EXXONMOBIL.COM;EXXONMOBIL.COM",
        "email": "WALMARTLABS.COM;EXXONMOBIL.COM;EXXONMOBIL.COM",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;1",
        "aff_unique_norm": "Walmart Labs;ExxonMobil",
        "aff_unique_dep": ";Corporate Strategic Research",
        "aff_unique_url": "https://walmartlabs.com;https://www.exxonmobil.com",
        "aff_unique_abbr": "WalmartLabs;ExxonMobil",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "San Bruno;",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "856bd5a3fd",
        "title": "Sparse meta-Gaussian information bottleneck",
        "site": "https://proceedings.mlr.press/v32/rey14.html",
        "author": "Melani Rey; Volker Roth; Thomas Fuchs",
        "abstract": "We present a new sparse compression technique based on the information  bottleneck (IB) principle, which takes into account side information. This is achieved by introducing a sparse variant of IB which preserves the information in only a few selected dimensions of the original data through compression. By assuming a Gaussian copula we can capture arbitrary non-Gaussian margins, continuous or discrete. We apply our model to select a sparse number of biomarkers relevant to the evolution of malignant melanoma and show that our sparse selection  provides reliable predictors.",
        "bibtex": "@InProceedings{pmlr-v32-rey14,\n  title = \t {Sparse meta-Gaussian information bottleneck},\n  author = \t {Rey, Melani and Roth, Volker and Fuchs, Thomas},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {910--918},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/rey14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/rey14.html},\n  abstract = \t {We present a new sparse compression technique based on the information  bottleneck (IB) principle, which takes into account side information. This is achieved by introducing a sparse variant of IB which preserves the information in only a few selected dimensions of the original data through compression. By assuming a Gaussian copula we can capture arbitrary non-Gaussian margins, continuous or discrete. We apply our model to select a sparse number of biomarkers relevant to the evolution of malignant melanoma and show that our sparse selection  provides reliable predictors.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/rey14.pdf",
        "supp": "",
        "pdf_size": 689589,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11673965588382078479&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "University of Basel, Basel, Switzerland; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, USA + University of Basel, Basel, Switzerland; University of Basel, Basel, Switzerland",
        "aff_domain": "UNIBAS.CH;CALTECH.EDU;UNIBAS.CH",
        "email": "UNIBAS.CH;CALTECH.EDU;UNIBAS.CH",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;0",
        "aff_unique_norm": "University of Basel;California Institute of Technology",
        "aff_unique_dep": ";Jet Propulsion Laboratory",
        "aff_unique_url": "https://www.unibas.ch;https://www.caltech.edu",
        "aff_unique_abbr": "UniBas;Caltech",
        "aff_campus_unique_index": "0;1+0;0",
        "aff_campus_unique": "Basel;Pasadena",
        "aff_country_unique_index": "0;1+0;0",
        "aff_country_unique": "Switzerland;United States"
    },
    {
        "id": "57e004e8fb",
        "title": "Spectral Bandits for Smooth Graph Functions",
        "site": "https://proceedings.mlr.press/v32/valko14.html",
        "author": "Michal Valko; Remi Munos; Branislav Kveton; Tom\u00e1\u0161 Koc\u00e1k",
        "abstract": "Smooth functions on graphs have wide applications in manifold and semi-supervised learning. In this paper, we study a bandit problem where the payoffs of arms are smooth on a graph. This framework is suitable for solving online learning problems that involve graphs, such as content-based recommendation. In this problem, each item we can recommend is a node and its expected rating is similar to its neighbors. The goal is to recommend items that have high expected ratings. We aim for the algorithms where the cumulative regret with respect to the optimal policy would not scale poorly with the number of nodes. In particular, we introduce the notion of an effective dimension, which is small in real-world graphs, and propose two algorithms for solving our problem that scale linearly and sublinearly in this dimension. Our experiments on real-world content recommendation problem show that a good estimator of user preferences for thousands of items can be learned from just tens of nodes evaluations.",
        "bibtex": "@InProceedings{pmlr-v32-valko14,\n  title = \t {Spectral Bandits for Smooth Graph Functions},\n  author = \t {Valko, Michal and Munos, Remi and Kveton, Branislav and Koc\u00e1k, Tom\u00e1\u0161},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {46--54},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/valko14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/valko14.html},\n  abstract = \t {Smooth functions on graphs have wide applications in manifold and semi-supervised learning. In this paper, we study a bandit problem where the payoffs of arms are smooth on a graph. This framework is suitable for solving online learning problems that involve graphs, such as content-based recommendation. In this problem, each item we can recommend is a node and its expected rating is similar to its neighbors. The goal is to recommend items that have high expected ratings. We aim for the algorithms where the cumulative regret with respect to the optimal policy would not scale poorly with the number of nodes. In particular, we introduce the notion of an effective dimension, which is small in real-world graphs, and propose two algorithms for solving our problem that scale linearly and sublinearly in this dimension. Our experiments on real-world content recommendation problem show that a good estimator of user preferences for thousands of items can be learned from just tens of nodes evaluations.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/valko14.pdf",
        "supp": "",
        "pdf_size": 697047,
        "gs_citation": 140,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17924586872162153128&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff": "INRIA Lille - Nord Europe, SequeL team, 40 avenue Halley 59650, Villeneuve d\u2019Ascq, France; INRIA Lille - Nord Europe, SequeL team, France + Microsoft Research New England, Cambridge, MA, USA; Technicolor Research Center, 735 Emerson St, Palo Alto, CA 94301, USA; INRIA Lille - Nord Europe, SequeL team, 40 avenue Halley 59650, Villeneuve d\u2019Ascq, France",
        "aff_domain": "inria.fr;inria.fr;technicolor.com;inria.fr",
        "email": "inria.fr;inria.fr;technicolor.com;inria.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2;3;0",
        "aff_unique_norm": "INRIA Lille - Nord Europe;INRIA;Microsoft;Technicolor Research Center",
        "aff_unique_dep": "SequeL team;SequeL team;Microsoft Research New England;",
        "aff_unique_url": "https://www.inria.fr/lille-nord-europe;https://www.inria.fr;https://www.microsoft.com/en-us/research/group/new-england;https://www.technicolor.com/en",
        "aff_unique_abbr": "INRIA;INRIA;MSR NE;",
        "aff_campus_unique_index": "0;1+2;3;0",
        "aff_campus_unique": "Lille;Lille - Nord Europe;Cambridge;Palo Alto",
        "aff_country_unique_index": "0;0+1;1;0",
        "aff_country_unique": "France;United States"
    },
    {
        "id": "d2bd9c8a03",
        "title": "Spectral Regularization for Max-Margin Sequence Tagging",
        "site": "https://proceedings.mlr.press/v32/quattoni14.html",
        "author": "Ariadna Quattoni; Borja Balle; Xavier Carreras; Amir Globerson",
        "abstract": "We frame max-margin learning of latent variable structured prediction models as a convex optimization problem, making use of scoring functions computed by input-output observable operator models. This learning problem can be expressed as an optimization involving a low-rank Hankel matrix that represents the input-output operator model. The direct outcome of our work is a new spectral regularization method for max-margin structured prediction.  Our experiments confirm that our proposed regularization framework leads to an effective way of controlling the capacity of structured prediction models.",
        "bibtex": "@InProceedings{pmlr-v32-quattoni14,\n  title = \t {Spectral Regularization for Max-Margin Sequence Tagging},\n  author = \t {Quattoni, Ariadna and Balle, Borja and Carreras, Xavier and Globerson, Amir},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1710--1718},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/quattoni14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/quattoni14.html},\n  abstract = \t {We frame max-margin learning of latent variable structured prediction models as a convex optimization problem, making use of scoring functions computed by input-output observable operator models. This learning problem can be expressed as an optimization involving a low-rank Hankel matrix that represents the input-output operator model. The direct outcome of our work is a new spectral regularization method for max-margin structured prediction.  Our experiments confirm that our proposed regularization framework leads to an effective way of controlling the capacity of structured prediction models.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/quattoni14.pdf",
        "supp": "",
        "pdf_size": 316801,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16140261958956397593&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Universitat Polit `ecnica de Catalunya, Barcelona, Catalunya; McGill University, Montreal, QC, Canada; Universitat Polit `ecnica de Catalunya, Barcelona, Catalunya; The Hebrew University of Jerusalem, Jerusalem, Israel",
        "aff_domain": "LSI.UPC.EDU;CS.MCGILL.CA;LSI.UPC.EDU;CS.HUJI.AC.IL",
        "email": "LSI.UPC.EDU;CS.MCGILL.CA;LSI.UPC.EDU;CS.HUJI.AC.IL",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "Universitat Polit\u00e8cnica de Catalunya;McGill University;Hebrew University of Jerusalem",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.upc.edu;https://www.mcgill.ca;https://www.huji.ac.il",
        "aff_unique_abbr": "UPC;McGill;HUJI",
        "aff_campus_unique_index": "0;1;0;2",
        "aff_campus_unique": "Barcelona;Montreal;Jerusalem",
        "aff_country_unique_index": "0;1;0;2",
        "aff_country_unique": "Spain;Canada;Israel"
    },
    {
        "id": "d8295711b8",
        "title": "Spherical Hamiltonian Monte Carlo for Constrained Target Distributions",
        "site": "https://proceedings.mlr.press/v32/lan14.html",
        "author": "Shiwei Lan; Bo Zhou; Babak Shahbaba",
        "abstract": "Statistical models with constrained probability distributions are abundant in machine learning. Some examples include regression models with norm constraints (e.g., Lasso), probit models, many copula models, and Latent Dirichlet Allocation (LDA) models. Bayesian inference involving probability distributions confined to constrained domains could be quite challenging for commonly used sampling algorithms. For such problems, we propose a novel Markov Chain Monte Carlo (MCMC) method that provides a general and computationally efficient framework for handling boundary conditions. Our method first maps the D-dimensional constrained domain of parameters to the unit ball \\bf B_0^D(1), then augments it to the D-dimensional sphere \\bf S^D such that the original boundary corresponds to the equator of \\bf S^D. This way, our method handles the constraints implicitly by moving freely on sphere generating proposals that remain within boundaries when mapped back to the original space. To improve the computational efficiency of our algorithm, we divide the dynamics into several parts such that the resulting split dynamics has a partial analytical solution as a geodesic flow on the sphere. We apply our method to several examples including truncated Gaussian, Bayesian Lasso, Bayesian bridge regression, and a copula model for identifying synchrony among multiple neurons. Our results show that the proposed method can provide a natural and efficient framework for handling several types of constraints on target distributions.",
        "bibtex": "@InProceedings{pmlr-v32-lan14,\n  title = \t {Spherical Hamiltonian Monte Carlo for Constrained Target Distributions},\n  author = \t {Lan, Shiwei and Zhou, Bo and Shahbaba, Babak},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {629--637},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/lan14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/lan14.html},\n  abstract = \t {Statistical models with constrained probability distributions are abundant in machine learning. Some examples include regression models with norm constraints (e.g., Lasso), probit models, many copula models, and Latent Dirichlet Allocation (LDA) models. Bayesian inference involving probability distributions confined to constrained domains could be quite challenging for commonly used sampling algorithms. For such problems, we propose a novel Markov Chain Monte Carlo (MCMC) method that provides a general and computationally efficient framework for handling boundary conditions. Our method first maps the D-dimensional constrained domain of parameters to the unit ball \\bf B_0^D(1), then augments it to the D-dimensional sphere \\bf S^D such that the original boundary corresponds to the equator of \\bf S^D. This way, our method handles the constraints implicitly by moving freely on sphere generating proposals that remain within boundaries when mapped back to the original space. To improve the computational efficiency of our algorithm, we divide the dynamics into several parts such that the resulting split dynamics has a partial analytical solution as a geodesic flow on the sphere. We apply our method to several examples including truncated Gaussian, Bayesian Lasso, Bayesian bridge regression, and a copula model for identifying synchrony among multiple neurons. Our results show that the proposed method can provide a natural and efficient framework for handling several types of constraints on target distributions.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/lan14.pdf",
        "supp": "",
        "pdf_size": 508400,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=487721705863198962&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Statistics, University of California, Irvine, CA 92697, USA; Department of Statistics, University of California, Irvine, CA 92697, USA; Department of Statistics, University of California, Irvine, CA 92697, USA",
        "aff_domain": "UCI.EDU;UCI.EDU;UCI.EDU",
        "email": "UCI.EDU;UCI.EDU;UCI.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Irvine",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.uci.edu",
        "aff_unique_abbr": "UCI",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Irvine",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c6a1951bbf",
        "title": "Square Deal: Lower Bounds and Improved Relaxations for Tensor Recovery",
        "site": "https://proceedings.mlr.press/v32/mu14.html",
        "author": "Cun Mu; Bo Huang; John Wright; Donald Goldfarb",
        "abstract": "Recovering a low-rank tensor from incomplete information is a recurring problem in signal processing and machine learning. The most popular convex relaxation of this problem minimizes the sum of the nuclear norms (SNN) of the unfolding matrices of the tensor. We show that this approach can be substantially suboptimal: reliably recovering a K-way n\\timesn\\times\u22ef\\times n tensor of Tucker rank (r, r, \\ldots, r) from Gaussian measurements requires \u03a9( r n^K-1 ) observations. In contrast, a certain (intractable) nonconvex formulation needs only O(r^K + nrK) observations. We introduce a simple, new convex relaxation, which partially bridges this gap. Our new formulation succeeds with O(r^\u230aK/2 \u230bn^\u2308K/2 \u2309) observations. The lower bound for the SNN model follows from our new result on recovering signals with multiple structures (e.g. sparse, low rank), which indicates the significant suboptimality of the common approach of minimizing the sum of individual sparsity inducing norms (e.g. \\ell_1, nuclear norm). Our new tractable formulation for low-rank tensor recovery shows how the sample complexity can be reduced by designing convex regularizers that exploit several structures jointly.",
        "bibtex": "@InProceedings{pmlr-v32-mu14,\n  title = \t {Square Deal: Lower Bounds and Improved Relaxations for Tensor Recovery},\n  author = \t {Mu, Cun and Huang, Bo and Wright, John and Goldfarb, Donald},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {73--81},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/mu14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/mu14.html},\n  abstract = \t {Recovering a low-rank tensor from incomplete information is a recurring problem in signal processing and machine learning. The most popular convex relaxation of this problem minimizes the sum of the nuclear norms (SNN) of the unfolding matrices of the tensor. We show that this approach can be substantially suboptimal: reliably recovering a K-way n\\timesn\\times\u22ef\\times n tensor of Tucker rank (r, r, \\ldots, r) from Gaussian measurements requires \u03a9( r n^K-1 ) observations. In contrast, a certain (intractable) nonconvex formulation needs only O(r^K + nrK) observations. We introduce a simple, new convex relaxation, which partially bridges this gap. Our new formulation succeeds with O(r^\u230aK/2 \u230bn^\u2308K/2 \u2309) observations. The lower bound for the SNN model follows from our new result on recovering signals with multiple structures (e.g. sparse, low rank), which indicates the significant suboptimality of the common approach of minimizing the sum of individual sparsity inducing norms (e.g. \\ell_1, nuclear norm). Our new tractable formulation for low-rank tensor recovery shows how the sample complexity can be reduced by designing convex regularizers that exploit several structures jointly.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/mu14.pdf",
        "supp": "",
        "pdf_size": 570245,
        "gs_citation": 388,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13080058510704821442&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Industrial Engineering and Operations Research, Columbia University; Department of Industrial Engineering and Operations Research, Columbia University; Department of Electrical Engineering, Columbia University; Department of Industrial Engineering and Operations Research, Columbia University",
        "aff_domain": "COLUMBIA.EDU;COLUMBIA.EDU;EE.COLUMBIA.EDU;COLUMBIA.EDU",
        "email": "COLUMBIA.EDU;COLUMBIA.EDU;EE.COLUMBIA.EDU;COLUMBIA.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "Department of Industrial Engineering and Operations Research",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f40fbd78b2",
        "title": "Stable and Efficient Representation Learning with Nonnegativity Constraints",
        "site": "https://proceedings.mlr.press/v32/line14.html",
        "author": "Tsung-Han Lin; H. T. Kung",
        "abstract": "Orthogonal matching pursuit (OMP) is an efficient approximation algorithm for computing sparse representations. However, prior research has shown that the representations computed by OMP may be of inferior quality, as they deliver suboptimal classification accuracy on several im- age datasets. We have found that this problem is caused by OMP\u2019s relatively weak stability under data variations, which leads to unreliability in supervised classifier training. We show that by imposing a simple nonnegativity constraint, this nonnegative variant of OMP (NOMP) can mitigate OMP\u2019s stability issue and is resistant to noise overfitting. In this work, we provide extensive analysis and experimental results to examine and validate the stability advantage of NOMP. In our experiments, we use a multi-layer deep architecture for representation learning, where we use K-means for feature learning and NOMP for representation encoding. The resulting learning framework is not only efficient and scalable to large feature dictionaries, but also is robust against input noise. This framework achieves the state-of-the-art accuracy on the STL-10 dataset.",
        "bibtex": "@InProceedings{pmlr-v32-line14,\n  title = \t {Stable and Efficient Representation Learning with Nonnegativity Constraints},\n  author = \t {Lin, Tsung-Han and Kung, H. T.},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1323--1331},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/line14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/line14.html},\n  abstract = \t {Orthogonal matching pursuit (OMP) is an efficient approximation algorithm for computing sparse representations. However, prior research has shown that the representations computed by OMP may be of inferior quality, as they deliver suboptimal classification accuracy on several im- age datasets. We have found that this problem is caused by OMP\u2019s relatively weak stability under data variations, which leads to unreliability in supervised classifier training. We show that by imposing a simple nonnegativity constraint, this nonnegative variant of OMP (NOMP) can mitigate OMP\u2019s stability issue and is resistant to noise overfitting. In this work, we provide extensive analysis and experimental results to examine and validate the stability advantage of NOMP. In our experiments, we use a multi-layer deep architecture for representation learning, where we use K-means for feature learning and NOMP for representation encoding. The resulting learning framework is not only efficient and scalable to large feature dictionaries, but also is robust against input noise. This framework achieves the state-of-the-art accuracy on the STL-10 dataset.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/line14.pdf",
        "supp": "",
        "pdf_size": 480424,
        "gs_citation": 72,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2136895037000297869&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "School of Engineering and Applied Sciences, Harvard University, Cambridge MA, USA + Intel Labs, Santa Clara CA, USA; School of Engineering and Applied Sciences, Harvard University, Cambridge MA, USA",
        "aff_domain": "EECS.HARVARD.EDU;HARVARD.EDU",
        "email": "EECS.HARVARD.EDU;HARVARD.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0",
        "aff_unique_norm": "Harvard University;Intel",
        "aff_unique_dep": "School of Engineering and Applied Sciences;Intel Labs",
        "aff_unique_url": "https://www.harvard.edu;https://www.intel.com/research",
        "aff_unique_abbr": "Harvard;Intel",
        "aff_campus_unique_index": "0+1;0",
        "aff_campus_unique": "Cambridge;Santa Clara",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "820a22c4d5",
        "title": "Standardized Mutual Information for Clustering Comparisons: One Step Further in Adjustment for Chance",
        "site": "https://proceedings.mlr.press/v32/romano14.html",
        "author": "Simone Romano; James Bailey; Vinh Nguyen; Karin Verspoor",
        "abstract": "Mutual information is a very popular measure for comparing clusterings. Previous work has shown that it is beneficial to make an adjustment for chance to this measure, by subtracting an expected value and normalizing via an upper bound. This yields the constant baseline property that enhances intuitiveness. In this paper, we argue that a further type of statistical adjustment for the mutual information is also beneficial - an adjustment to correct selection bias. This type of adjustment is useful when carrying out many clustering comparisons, to select one or more preferred clusterings. It reduces the tendency for the mutual information to choose clustering solutions i) with more clusters, or ii) induced on fewer data points, when compared to a reference one. We term our new adjusted measure the *standardized mutual information*. It requires computation of the variance of mutual information under a hypergeometric model of randomness, which is technically challenging. We derive an analytical formula for this variance and analyze its complexity. We then experimentally assess how our new measure can address selection bias and also increase interpretability. We recommend using the standardized mutual information when making multiple clustering comparisons in situations where the number of records is small compared to the number of clusters considered.",
        "bibtex": "@InProceedings{pmlr-v32-romano14,\n  title = \t {Standardized Mutual Information for Clustering Comparisons: One Step Further in Adjustment for Chance},\n  author = \t {Romano, Simone and Bailey, James and Nguyen, Vinh and Verspoor, Karin},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1143--1151},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/romano14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/romano14.html},\n  abstract = \t {Mutual information is a very popular measure for comparing clusterings. Previous work has shown that it is beneficial to make an adjustment for chance to this measure, by subtracting an expected value and normalizing via an upper bound. This yields the constant baseline property that enhances intuitiveness. In this paper, we argue that a further type of statistical adjustment for the mutual information is also beneficial - an adjustment to correct selection bias. This type of adjustment is useful when carrying out many clustering comparisons, to select one or more preferred clusterings. It reduces the tendency for the mutual information to choose clustering solutions i) with more clusters, or ii) induced on fewer data points, when compared to a reference one. We term our new adjusted measure the *standardized mutual information*. It requires computation of the variance of mutual information under a hypergeometric model of randomness, which is technically challenging. We derive an analytical formula for this variance and analyze its complexity. We then experimentally assess how our new measure can address selection bias and also increase interpretability. We recommend using the standardized mutual information when making multiple clustering comparisons in situations where the number of records is small compared to the number of clusters considered.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/romano14.pdf",
        "supp": "",
        "pdf_size": 359125,
        "gs_citation": 168,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5836867598858639422&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computing and Information Systems, The University of Melbourne, Victoria, Australia; Department of Computing and Information Systems, The University of Melbourne, Victoria, Australia; Department of Computing and Information Systems, The University of Melbourne, Victoria, Australia; Department of Computing and Information Systems, The University of Melbourne, Victoria, Australia",
        "aff_domain": "unimelb.edu.au;unimelb.edu.au;unimelb.edu.au;unimelb.edu.au",
        "email": "unimelb.edu.au;unimelb.edu.au;unimelb.edu.au;unimelb.edu.au",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Melbourne",
        "aff_unique_dep": "Department of Computing and Information Systems",
        "aff_unique_url": "https://www.unimelb.edu.au",
        "aff_unique_abbr": "UniMelb",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Melbourne",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "ec28709867",
        "title": "Statistical analysis of stochastic gradient methods for generalized linear models",
        "site": "https://proceedings.mlr.press/v32/toulis14.html",
        "author": "Panagiotis Toulis; Edoardo Airoldi; Jason Rennie",
        "abstract": "We study the statistical properties of stochastic gradient descent (SGD) using   explicit and implicit updates for fitting generalized linear models (GLMs).  Initially, we develop a computationally   efficient algorithm to implement implicit SGD learning of GLMs.  Next, we obtain exact formulas for the bias and variance  of both updates which leads to two important observations on their   comparative statistical properties.  First, in small samples, the estimates from the implicit procedure   are more biased than the estimates from the explicit one,   but their empirical variance is smaller and they are more robust to   learning rate misspecification.   Second, the two procedures are statistically identical in the limit:   they are both unbiased, converge at the same rate and have the   same asymptotic variance. Our set of experiments confirm our theory and   more broadly suggest that the implicit procedure can be a competitive choice   for fitting large-scale  models, especially when robustness is a concern.",
        "bibtex": "@InProceedings{pmlr-v32-toulis14,\n  title = \t {Statistical analysis of stochastic gradient methods for generalized linear models},\n  author = \t {Toulis, Panagiotis and Airoldi, Edoardo and Rennie, Jason},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {667--675},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/toulis14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/toulis14.html},\n  abstract = \t {We study the statistical properties of stochastic gradient descent (SGD) using   explicit and implicit updates for fitting generalized linear models (GLMs).  Initially, we develop a computationally   efficient algorithm to implement implicit SGD learning of GLMs.  Next, we obtain exact formulas for the bias and variance  of both updates which leads to two important observations on their   comparative statistical properties.  First, in small samples, the estimates from the implicit procedure   are more biased than the estimates from the explicit one,   but their empirical variance is smaller and they are more robust to   learning rate misspecification.   Second, the two procedures are statistically identical in the limit:   they are both unbiased, converge at the same rate and have the   same asymptotic variance. Our set of experiments confirm our theory and   more broadly suggest that the implicit procedure can be a competitive choice   for fitting large-scale  models, especially when robustness is a concern.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/toulis14.pdf",
        "supp": "",
        "pdf_size": 461712,
        "gs_citation": 94,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2425178013476910528&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Statistics, Harvard University; Google, Inc.; Department of Statistics, Harvard University",
        "aff_domain": "FAS.HARVARD.EDU;GOOGLE.COM;FAS.HARVARD.EDU",
        "email": "FAS.HARVARD.EDU;GOOGLE.COM;FAS.HARVARD.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Harvard University;Google",
        "aff_unique_dep": "Department of Statistics;Google",
        "aff_unique_url": "https://www.harvard.edu;https://www.google.com",
        "aff_unique_abbr": "Harvard;Google",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Cambridge;Mountain View",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f75bedae6f",
        "title": "Statistical-Computational Phase Transitions in Planted Models: The High-Dimensional Setting",
        "site": "https://proceedings.mlr.press/v32/chene14.html",
        "author": "Yudong Chen; Jiaming Xu",
        "abstract": "The planted models assume that a graph is generated from some unknown clusters by randomly placing edges between nodes according to their cluster memberships; the task is to recover the clusters given the graph. Special cases include planted clique, planted partition, planted densest subgraph and planted coloring. Of particular interest is the High-Dimensional setting where the number of clusters is allowed to grow with the number of nodes. We show that the space of model parameters can be partitioned into four disjoint regions corresponding to decreasing statistical and computational complexities: (1) the impossible regime, where all algorithms fail; (2) the hard regime, where the exponential-time Maximum Likelihood Estimator (MLE) succeeds, and no polynomial-time method is known; (3) the easy regime, where the polynomial-time convexified MLE succeeds; (4) the simple regime, where a simple counting/thresholding procedure succeeds. Moreover, each of these algorithms provably fails in the previous harder regimes. Our theorems establish the first minimax recovery results for the high-dimensional setting, and provide the best known guarantees for polynomial-time algorithms. Our results extend to the related problem of submatrix localization, a.k.a. bi-clustering. These results demonstrate the tradeoffs between statistical and computational considerations.",
        "bibtex": "@InProceedings{pmlr-v32-chene14,\n  title = \t {Statistical-Computational Phase Transitions in Planted Models: The High-Dimensional Setting},\n  author = \t {Chen, Yudong and Xu, Jiaming},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {244--252},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/chene14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/chene14.html},\n  abstract = \t {The planted models assume that a graph is generated from some unknown clusters by randomly placing edges between nodes according to their cluster memberships; the task is to recover the clusters given the graph. Special cases include planted clique, planted partition, planted densest subgraph and planted coloring. Of particular interest is the High-Dimensional setting where the number of clusters is allowed to grow with the number of nodes. We show that the space of model parameters can be partitioned into four disjoint regions corresponding to decreasing statistical and computational complexities: (1) the impossible regime, where all algorithms fail; (2) the hard regime, where the exponential-time Maximum Likelihood Estimator (MLE) succeeds, and no polynomial-time method is known; (3) the easy regime, where the polynomial-time convexified MLE succeeds; (4) the simple regime, where a simple counting/thresholding procedure succeeds. Moreover, each of these algorithms provably fails in the previous harder regimes. Our theorems establish the first minimax recovery results for the high-dimensional setting, and provide the best known guarantees for polynomial-time algorithms. Our results extend to the related problem of submatrix localization, a.k.a. bi-clustering. These results demonstrate the tradeoffs between statistical and computational considerations.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/chene14.pdf",
        "supp": "",
        "pdf_size": 313952,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3272772491753210685&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of EECS, University of California, Berkeley, Berkeley, CA 94704, USA; Department of ECE, University of Illinois at Urbana-Champaign, Urbana, IL 61801, USA",
        "aff_domain": "EECS.BERKELEY.EDU;ILLINOIS.EDU",
        "email": "EECS.BERKELEY.EDU;ILLINOIS.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of California, Berkeley;University of Illinois Urbana-Champaign",
        "aff_unique_dep": "Department of EECS;Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.berkeley.edu;https://illinois.edu",
        "aff_unique_abbr": "UC Berkeley;UIUC",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Berkeley;Urbana",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5e81051826",
        "title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models",
        "site": "https://proceedings.mlr.press/v32/rezende14.html",
        "author": "Danilo Jimenez Rezende; Shakir Mohamed; Daan Wierstra",
        "abstract": "We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning.   Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound.  We develop stochastic backpropagation \u2013 rules for gradient backpropagation through stochastic variables \u2013 and   derive an algorithm that allows for joint optimisation of the parameters of both the generative and recognition models.  We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to  generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.",
        "bibtex": "@InProceedings{pmlr-v32-rezende14,\n  title = \t {Stochastic Backpropagation and Approximate Inference in Deep Generative Models},\n  author = \t {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1278--1286},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/rezende14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/rezende14.html},\n  abstract = \t {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning.   Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound.  We develop stochastic backpropagation \u2013 rules for gradient backpropagation through stochastic variables \u2013 and   derive an algorithm that allows for joint optimisation of the parameters of both the generative and recognition models.  We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to  generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/rezende14.pdf",
        "supp": "",
        "pdf_size": 4281475,
        "gs_citation": 6349,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16288343975294201511&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Google DeepMind, London, United Kingdom; Google DeepMind, London, United Kingdom; Google DeepMind, London, United Kingdom",
        "aff_domain": "google.com;google.com;google.com",
        "email": "google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google DeepMind",
        "aff_unique_url": "https://deepmind.com",
        "aff_unique_abbr": "DeepMind",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "6cfda647d6",
        "title": "Stochastic Dual Coordinate Ascent with Alternating Direction Method of Multipliers",
        "site": "https://proceedings.mlr.press/v32/suzuki14.html",
        "author": "Taiji Suzuki",
        "abstract": "We propose a new stochastic dual coordinate ascent technique  that can be applied to a wide range of regularized learning problems. Our method is based on  alternating direction method of multipliers (ADMM) to deal with complex regularization functions such as structured regularizations. Although the original ADMM is a batch method,  the proposed method offers a stochastic update rule where each iteration requires only one or few sample observations. Moreover, our method can naturally afford mini-batch update and it gives speed up of convergence. We show that, under mild assumptions, our method converges exponentially. The numerical experiments show that our method actually performs efficiently.",
        "bibtex": "@InProceedings{pmlr-v32-suzuki14,\n  title = \t {Stochastic Dual Coordinate Ascent with Alternating Direction Method of Multipliers},\n  author = \t {Suzuki, Taiji},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {736--744},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/suzuki14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/suzuki14.html},\n  abstract = \t {We propose a new stochastic dual coordinate ascent technique  that can be applied to a wide range of regularized learning problems. Our method is based on  alternating direction method of multipliers (ADMM) to deal with complex regularization functions such as structured regularizations. Although the original ADMM is a batch method,  the proposed method offers a stochastic update rule where each iteration requires only one or few sample observations. Moreover, our method can naturally afford mini-batch update and it gives speed up of convergence. We show that, under mild assumptions, our method converges exponentially. The numerical experiments show that our method actually performs efficiently.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/suzuki14.pdf",
        "supp": "",
        "pdf_size": 284206,
        "gs_citation": 83,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14283950603358305284&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Mathematical and Computing Sciences, Tokyo Institute of Technology, Tokyo 152-8552, JAPAN",
        "aff_domain": "IS.TITECH.AC.JP",
        "email": "IS.TITECH.AC.JP",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Tokyo Institute of Technology",
        "aff_unique_dep": "Department of Mathematical and Computing Sciences",
        "aff_unique_url": "https://www.titech.ac.jp",
        "aff_unique_abbr": "Titech",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Tokyo",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "d41b1b0284",
        "title": "Stochastic Gradient Hamiltonian Monte Carlo",
        "site": "https://proceedings.mlr.press/v32/cheni14.html",
        "author": "Tianqi Chen; Emily Fox; Carlos Guestrin",
        "abstract": "Hamiltonian Monte Carlo (HMC) sampling methods provide a mechanism for defining distant proposals with high acceptance probabilities in a Metropolis-Hastings framework, enabling more efficient exploration of the state space than standard random-walk proposals.  The popularity of such methods has grown significantly in recent years.  However, a limitation of HMC methods is the required gradient computation for simulation of the Hamiltonian dynamical system-such computation is infeasible in problems involving a large sample size or streaming data. Instead, we must rely on a noisy gradient estimate computed from a subset of the data.  In this paper, we explore the properties of such a stochastic gradient HMC approach. Surprisingly, the natural implementation of the stochastic approximation can be arbitrarily bad.  To address this problem we introduce a variant that uses second-order Langevin dynamics with a friction term that counteracts the effects of the noisy gradient, maintaining the desired target distribution as the invariant distribution.  Results on simulated data validate our theory.  We also provide an application of our methods to a classification task using neural networks and to online Bayesian matrix factorization.",
        "bibtex": "@InProceedings{pmlr-v32-cheni14,\n  title = \t {Stochastic Gradient Hamiltonian Monte Carlo},\n  author = \t {Chen, Tianqi and Fox, Emily and Guestrin, Carlos},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1683--1691},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/cheni14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/cheni14.html},\n  abstract = \t {Hamiltonian Monte Carlo (HMC) sampling methods provide a mechanism for defining distant proposals with high acceptance probabilities in a Metropolis-Hastings framework, enabling more efficient exploration of the state space than standard random-walk proposals.  The popularity of such methods has grown significantly in recent years.  However, a limitation of HMC methods is the required gradient computation for simulation of the Hamiltonian dynamical system-such computation is infeasible in problems involving a large sample size or streaming data. Instead, we must rely on a noisy gradient estimate computed from a subset of the data.  In this paper, we explore the properties of such a stochastic gradient HMC approach. Surprisingly, the natural implementation of the stochastic approximation can be arbitrarily bad.  To address this problem we introduce a variant that uses second-order Langevin dynamics with a friction term that counteracts the effects of the noisy gradient, maintaining the desired target distribution as the invariant distribution.  Results on simulated data validate our theory.  We also provide an application of our methods to a classification task using neural networks and to online Bayesian matrix factorization.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/cheni14.pdf",
        "supp": "",
        "pdf_size": 411877,
        "gs_citation": 1205,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5348802385924354608&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "MODE Lab, University of Washington, Seattle, WA; MODE Lab, University of Washington, Seattle, WA; MODE Lab, University of Washington, Seattle, WA",
        "aff_domain": "CS.WASHINGTON.EDU;STAT.WASHINGTON.EDU;CS.WASHINGTON.EDU",
        "email": "CS.WASHINGTON.EDU;STAT.WASHINGTON.EDU;CS.WASHINGTON.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "MODE Lab",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c212fd18fa",
        "title": "Stochastic Inference for Scalable Probabilistic Modeling of Binary Matrices",
        "site": "https://proceedings.mlr.press/v32/hernandez-lobatoa14.html",
        "author": "Jose Miguel Hernandez-Lobato; Neil Houlsby; Zoubin Ghahramani",
        "abstract": "Fully observed large binary matrices appear in a wide variety of contexts. To model them, probabilistic matrix factorization (PMF) methods are an attractive solution. However, current batch algorithms for PMF can be inefficient because they need to analyze the entire data matrix before producing any parameter updates. We derive an efficient stochastic inference algorithm for PMF models of fully observed binary matrices. Our method exhibits faster convergence rates than more expensive batch approaches and has better predictive performance than scalable alternatives.  The proposed method includes new data subsampling strategies which produce large gains over standard uniform subsampling. We also address the task  of automatically selecting the size of the minibatches of data used by our method. For this, we derive an algorithm that adjusts this hyper-parameter online.",
        "bibtex": "@InProceedings{pmlr-v32-hernandez-lobatoa14,\n  title = \t {Stochastic Inference for Scalable Probabilistic Modeling of Binary Matrices},\n  author = \t {Hernandez-Lobato, Jose Miguel and Houlsby, Neil and Ghahramani, Zoubin},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {379--387},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/hernandez-lobatoa14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/hernandez-lobatoa14.html},\n  abstract = \t {Fully observed large binary matrices appear in a wide variety of contexts. To model them, probabilistic matrix factorization (PMF) methods are an attractive solution. However, current batch algorithms for PMF can be inefficient because they need to analyze the entire data matrix before producing any parameter updates. We derive an efficient stochastic inference algorithm for PMF models of fully observed binary matrices. Our method exhibits faster convergence rates than more expensive batch approaches and has better predictive performance than scalable alternatives.  The proposed method includes new data subsampling strategies which produce large gains over standard uniform subsampling. We also address the task  of automatically selecting the size of the minibatches of data used by our method. For this, we derive an algorithm that adjusts this hyper-parameter online.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/hernandez-lobatoa14.pdf",
        "supp": "",
        "pdf_size": 508117,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13499846413815680645&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "University of Cambridge, Department of Engineering, Cambridge CB2 1PZ, UK; University of Cambridge, Department of Engineering, Cambridge CB2 1PZ, UK; University of Cambridge, Department of Engineering, Cambridge CB2 1PZ, UK",
        "aff_domain": "CAM.AC.UK;CAM.AC.UK;ENG.CAM.AC.UK",
        "email": "CAM.AC.UK;CAM.AC.UK;ENG.CAM.AC.UK",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "Department of Engineering",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "de4998a0e8",
        "title": "Stochastic Neighbor Compression",
        "site": "https://proceedings.mlr.press/v32/kusner14.html",
        "author": "Matt Kusner; Stephen Tyree; Kilian Weinberger; Kunal Agrawal",
        "abstract": "We present Stochastic Neighborhood Compression (SNC), an algorithm to compress a dataset for the purpose of k-nearest neighbor (kNN) classification. Given training data, SNC learns a much smaller synthetic data set, that minimizes the stochastic 1-nearest neighbor classification error on the training data. This approach has several appealing properties: due to its small size, the compressed set speeds up kNN testing drastically (up to several orders of magnitude, in our experiments); it makes the kNN classifier substantially more robust to label noise; on 4 of 7 data sets it yields lower test error than kNN on the entire training set, even at compression ratios as low as 2%; finally, the SNC compression leads to impressive speed ups over kNN even when kNN and SNC are both used with ball-tree data structures, hashing, and LMNN dimensionality reduction, demonstrating that it is complementary to existing state-of-the-art algorithms to speed up kNN classification and leads to substantial further improvements.",
        "bibtex": "@InProceedings{pmlr-v32-kusner14,\n  title = \t {Stochastic Neighbor Compression},\n  author = \t {Kusner, Matt and Tyree, Stephen and Weinberger, Kilian and Agrawal, Kunal},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {622--630},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/kusner14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/kusner14.html},\n  abstract = \t {We present Stochastic Neighborhood Compression (SNC), an algorithm to compress a dataset for the purpose of k-nearest neighbor (kNN) classification. Given training data, SNC learns a much smaller synthetic data set, that minimizes the stochastic 1-nearest neighbor classification error on the training data. This approach has several appealing properties: due to its small size, the compressed set speeds up kNN testing drastically (up to several orders of magnitude, in our experiments); it makes the kNN classifier substantially more robust to label noise; on 4 of 7 data sets it yields lower test error than kNN on the entire training set, even at compression ratios as low as 2%; finally, the SNC compression leads to impressive speed ups over kNN even when kNN and SNC are both used with ball-tree data structures, hashing, and LMNN dimensionality reduction, demonstrating that it is complementary to existing state-of-the-art algorithms to speed up kNN classification and leads to substantial further improvements.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/kusner14.pdf",
        "supp": "",
        "pdf_size": 836187,
        "gs_citation": 92,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11335507386770174609&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Washington University in St. Louis; Washington University in St. Louis; Washington University in St. Louis; Washington University in St. Louis",
        "aff_domain": "WUSTL.EDU;WUSTL.EDU;WUSTL.EDU;WUSTL.EDU",
        "email": "WUSTL.EDU;WUSTL.EDU;WUSTL.EDU;WUSTL.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Washington University in St. Louis",
        "aff_unique_dep": "",
        "aff_unique_url": "https://wustl.edu",
        "aff_unique_abbr": "WashU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "St. Louis",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "eb5ba34ba2",
        "title": "Stochastic Variational Inference for Bayesian Time Series Models",
        "site": "https://proceedings.mlr.press/v32/johnson14.html",
        "author": "Matthew Johnson; Alan Willsky",
        "abstract": "Bayesian models provide powerful tools for analyzing complex time series data, but performing inference with large datasets is a challenge.  Stochastic variational inference (SVI) provides a new framework for approximating model posteriors with only a small number of passes through the data, enabling such models to be fit at scale.  However, its application to time series models has not been studied.    In this paper we develop SVI algorithms for several common Bayesian time series models, namely the hidden Markov model (HMM), hidden semi-Markov model (HSMM), and the nonparametric HDP-HMM and HDP-HSMM.  In addition, because HSMM inference can be expensive even in the minibatch setting of SVI, we develop fast approximate updates for HSMMs with durations distributions that are negative binomials or mixtures of negative binomials.",
        "bibtex": "@InProceedings{pmlr-v32-johnson14,\n  title = \t {Stochastic Variational Inference for Bayesian Time Series Models},\n  author = \t {Johnson, Matthew and Willsky, Alan},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1854--1862},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/johnson14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/johnson14.html},\n  abstract = \t {Bayesian models provide powerful tools for analyzing complex time series data, but performing inference with large datasets is a challenge.  Stochastic variational inference (SVI) provides a new framework for approximating model posteriors with only a small number of passes through the data, enabling such models to be fit at scale.  However, its application to time series models has not been studied.    In this paper we develop SVI algorithms for several common Bayesian time series models, namely the hidden Markov model (HMM), hidden semi-Markov model (HSMM), and the nonparametric HDP-HMM and HDP-HSMM.  In addition, because HSMM inference can be expensive even in the minibatch setting of SVI, we develop fast approximate updates for HSMMs with durations distributions that are negative binomials or mixtures of negative binomials.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/johnson14.pdf",
        "supp": "",
        "pdf_size": 2063917,
        "gs_citation": 96,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15398151090208850032&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Massachusetts Institute of Technology, 77 Massachusetts Ave, Cambridge, MA USA; Massachusetts Institute of Technology, 77 Massachusetts Ave, Cambridge, MA USA",
        "aff_domain": "CSAIL.MIT.EDU;MIT.EDU",
        "email": "CSAIL.MIT.EDU;MIT.EDU",
        "github": "github.com/mattjj/pyhsmm",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "463abc95f4",
        "title": "Structured Generative Models of Natural Source Code",
        "site": "https://proceedings.mlr.press/v32/maddison14.html",
        "author": "Chris Maddison; Daniel Tarlow",
        "abstract": "We study the problem of building generative models of natural source code (NSC); that is, source code written and understood by humans. Our primary contribution is to describe a family of generative models for NSC that have two key properties: First, they incorporate both sequential and hierarchical structure. Second, they are capable of integrating closely with a compiler, which allows leveraging compiler logic and abstractions when building structure into the model. We also develop an extension that includes more complex structure, refining how the model generates identifier tokens based on what variables are currently in scope.  Our models can be learned efficiently, and we show empirically that including appropriate structure greatly improves the probability of generating test programs.",
        "bibtex": "@InProceedings{pmlr-v32-maddison14,\n  title = \t {Structured Generative Models of Natural Source Code},\n  author = \t {Maddison, Chris and Tarlow, Daniel},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {649--657},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/maddison14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/maddison14.html},\n  abstract = \t {We study the problem of building generative models of natural source code (NSC); that is, source code written and understood by humans. Our primary contribution is to describe a family of generative models for NSC that have two key properties: First, they incorporate both sequential and hierarchical structure. Second, they are capable of integrating closely with a compiler, which allows leveraging compiler logic and abstractions when building structure into the model. We also develop an extension that includes more complex structure, refining how the model generates identifier tokens based on what variables are currently in scope.  Our models can be learned efficiently, and we show empirically that including appropriate structure greatly improves the probability of generating test programs.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/maddison14.pdf",
        "supp": "",
        "pdf_size": 559099,
        "gs_citation": 198,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1058446411967143184&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "University of Toronto; Microsoft Research",
        "aff_domain": "CS.TORONTO.EDU;MICROSOFT.COM",
        "email": "CS.TORONTO.EDU;MICROSOFT.COM",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Toronto;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.utoronto.ca;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "U of T;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "28ee9dbacd",
        "title": "Structured Low-Rank Matrix Factorization: Optimality, Algorithm, and Applications to Image Processing",
        "site": "https://proceedings.mlr.press/v32/haeffele14.html",
        "author": "Benjamin Haeffele; Eric Young; Rene Vidal",
        "abstract": "Recently, convex solutions to low-rank matrix factorization problems have received increasing attention in machine learning. However, in many applications the data can display other structures beyond simply being low-rank. For example, images and videos present complex spatio-temporal structures, which are largely ignored by current low-rank methods. In this paper we explore a matrix factorization technique suitable for large datasets that captures additional structure in the factors by using a projective tensor norm, which includes classical image regularizers such as total variation and the nuclear norm as particular cases. Although the resulting optimization problem is not convex, we show that under certain conditions on the factors, any local minimizer for the factors yields a global minimizer for their product. Examples in biomedical video segmentation and hyperspectral compressed recovery show the advantages of our approach on high-dimensional datasets.",
        "bibtex": "@InProceedings{pmlr-v32-haeffele14,\n  title = \t {Structured Low-Rank Matrix Factorization: Optimality, Algorithm, and Applications to Image Processing},\n  author = \t {Haeffele, Benjamin and Young, Eric and Vidal, Rene},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {2007--2015},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/haeffele14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/haeffele14.html},\n  abstract = \t {Recently, convex solutions to low-rank matrix factorization problems have received increasing attention in machine learning. However, in many applications the data can display other structures beyond simply being low-rank. For example, images and videos present complex spatio-temporal structures, which are largely ignored by current low-rank methods. In this paper we explore a matrix factorization technique suitable for large datasets that captures additional structure in the factors by using a projective tensor norm, which includes classical image regularizers such as total variation and the nuclear norm as particular cases. Although the resulting optimization problem is not convex, we show that under certain conditions on the factors, any local minimizer for the factors yields a global minimizer for their product. Examples in biomedical video segmentation and hyperspectral compressed recovery show the advantages of our approach on high-dimensional datasets.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/haeffele14.pdf",
        "supp": "",
        "pdf_size": 1799551,
        "gs_citation": 161,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8781216573787073477&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Biomedical Engineering, Johns Hopkins University, Baltimore, Maryland USA; Department of Biomedical Engineering, Johns Hopkins University, Baltimore, Maryland USA; Department of Biomedical Engineering, Johns Hopkins University, Baltimore, Maryland USA",
        "aff_domain": "JHU.EDU;JHU.EDU;JHU.EDU",
        "email": "JHU.EDU;JHU.EDU;JHU.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Johns Hopkins University",
        "aff_unique_dep": "Department of Biomedical Engineering",
        "aff_unique_url": "https://www.jhu.edu",
        "aff_unique_abbr": "JHU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Baltimore",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "15dbeeb7e7",
        "title": "Structured Prediction of Network Response",
        "site": "https://proceedings.mlr.press/v32/su14.html",
        "author": "Hongyu Su; Aristides Gionis; Juho Rousu",
        "abstract": "We introduce the following network response problem: given a complex network and an action, predict the subnetwork that responds to action, that is, which nodes perform the action and which directed edges relay the action to the adjacent nodes.     We approach the problem through max-margin structured learning, in which a compatibility score is learned between the actions and their activated  subnetworks. Thus, unlike the most popular influence network approaches, our method, called SPIN,  is context-sensitive, namely, the presence, the direction and the dynamics of influences depend on the properties of the actions.     The inference problems of finding the highest scoring as well as the worst margin violating networks, are proven to be NP-hard. To solve the problems, we present an approximate inference method through a semi-definite programming relaxation (SDP), as well as a more scalable greedy heuristic algorithm.    In our experiments, we demonstrate that taking advantage of the context given by the actions and the network structure leads SPIN to a markedly better predictive performance over competing methods.",
        "bibtex": "@InProceedings{pmlr-v32-su14,\n  title = \t {Structured Prediction of Network Response},\n  author = \t {Su, Hongyu and Gionis, Aristides and Rousu, Juho},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {442--450},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/su14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/su14.html},\n  abstract = \t {We introduce the following network response problem: given a complex network and an action, predict the subnetwork that responds to action, that is, which nodes perform the action and which directed edges relay the action to the adjacent nodes.     We approach the problem through max-margin structured learning, in which a compatibility score is learned between the actions and their activated  subnetworks. Thus, unlike the most popular influence network approaches, our method, called SPIN,  is context-sensitive, namely, the presence, the direction and the dynamics of influences depend on the properties of the actions.     The inference problems of finding the highest scoring as well as the worst margin violating networks, are proven to be NP-hard. To solve the problems, we present an approximate inference method through a semi-definite programming relaxation (SDP), as well as a more scalable greedy heuristic algorithm.    In our experiments, we demonstrate that taking advantage of the context given by the actions and the network structure leads SPIN to a markedly better predictive performance over competing methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/su14.pdf",
        "supp": "",
        "pdf_size": 505676,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12500609428603061888&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Helsinki Institute for Information Technology (HIIT) + Department of Information and Computer Science, Aalto University, Finland; Helsinki Institute for Information Technology (HIIT) + Department of Information and Computer Science, Aalto University, Finland; Helsinki Institute for Information Technology (HIIT) + Department of Information and Computer Science, Aalto University, Finland",
        "aff_domain": "AALTO.FI;AALTO.FI;AALTO.FI",
        "email": "AALTO.FI;AALTO.FI;AALTO.FI",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "Helsinki Institute for Information Technology;Aalto University",
        "aff_unique_dep": ";Department of Information and Computer Science",
        "aff_unique_url": "https://www.hiit.fi;https://www.aalto.fi",
        "aff_unique_abbr": "HIIT;Aalto",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "Finland"
    },
    {
        "id": "6a04b7dc5f",
        "title": "Structured Recurrent Temporal Restricted Boltzmann Machines",
        "site": "https://proceedings.mlr.press/v32/mittelman14.html",
        "author": "Roni Mittelman; Benjamin Kuipers; Silvio Savarese; Honglak Lee",
        "abstract": "The Recurrent temporal restricted Boltzmann machine (RTRBM) is a probabilistic model for temporal data, that has been shown to effectively capture both short and long-term dependencies in time-series. The topology of the RTRBM graphical model, however, assumes full connectivity between all the pairs of visible and hidden units, therefore ignoring the dependency structure between the different observations. Learning this structure has the potential to not only improve the prediction performance, but it can also reveal important patterns in the data. For example, given an econometric dataset, we could identify interesting dependencies between different market sectors; given a meteorological dataset, we could identify regional weather patterns. In this work we propose a new class of RTRBM, which explicitly uses a dependency graph to model the structure in the problem and to define the energy function. We refer to the new model as the structured RTRBM (SRTRBM). Our technique is related to methods such as graphical lasso, which are used to learn the topology of Gaussian graphical models. We also develop a spike-and-slab version of the RTRBM, and combine it with our method to learn structure in datasets with real valued observations. Our experimental results using synthetic and real datasets, demonstrate that the SRTRBM can improve the prediction performance of the RTRBM, particularly when the number of visible units is large and the size of the training set is small. It also reveals the structure underlying our benchmark datasets.",
        "bibtex": "@InProceedings{pmlr-v32-mittelman14,\n  title = \t {Structured Recurrent Temporal Restricted Boltzmann Machines},\n  author = \t {Mittelman, Roni and Kuipers, Benjamin and Savarese, Silvio and Lee, Honglak},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1647--1655},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/mittelman14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/mittelman14.html},\n  abstract = \t {The Recurrent temporal restricted Boltzmann machine (RTRBM) is a probabilistic model for temporal data, that has been shown to effectively capture both short and long-term dependencies in time-series. The topology of the RTRBM graphical model, however, assumes full connectivity between all the pairs of visible and hidden units, therefore ignoring the dependency structure between the different observations. Learning this structure has the potential to not only improve the prediction performance, but it can also reveal important patterns in the data. For example, given an econometric dataset, we could identify interesting dependencies between different market sectors; given a meteorological dataset, we could identify regional weather patterns. In this work we propose a new class of RTRBM, which explicitly uses a dependency graph to model the structure in the problem and to define the energy function. We refer to the new model as the structured RTRBM (SRTRBM). Our technique is related to methods such as graphical lasso, which are used to learn the topology of Gaussian graphical models. We also develop a spike-and-slab version of the RTRBM, and combine it with our method to learn structure in datasets with real valued observations. Our experimental results using synthetic and real datasets, demonstrate that the SRTRBM can improve the prediction performance of the RTRBM, particularly when the number of visible units is large and the size of the training set is small. It also reveals the structure underlying our benchmark datasets.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/mittelman14.pdf",
        "supp": "",
        "pdf_size": 1241023,
        "gs_citation": 98,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16093425108681238378&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI; Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI; Computer Science Department, Stanford University, Stanford, CA; Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI",
        "aff_domain": "UMICH.EDU;UMICH.EDU;STANFORD.EDU;UMICH.EDU",
        "email": "UMICH.EDU;UMICH.EDU;STANFORD.EDU;UMICH.EDU",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Michigan;Stanford University",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Science;Computer Science Department",
        "aff_unique_url": "https://www.umich.edu;https://www.stanford.edu",
        "aff_unique_abbr": "UM;Stanford",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Ann Arbor;Stanford",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "396e72b091",
        "title": "Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits",
        "site": "https://proceedings.mlr.press/v32/agarwalb14.html",
        "author": "Alekh Agarwal; Daniel Hsu; Satyen Kale; John Langford; Lihong Li; Robert Schapire",
        "abstract": "We present a new algorithm for the contextual bandit learning problem,  where the learner repeatedly takes one of K \\emphactions in response to the  observed \\emphcontext, and observes the \\emphreward only for that  action. Our method assumes access to an oracle for solving fully  supervised cost-sensitive classification problems and achieves the  statistically optimal regret guarantee with only \\otil(\\sqrtKT)  oracle calls across all T rounds. By doing so, we obtain the most  practical contextual bandit learning algorithm amongst approaches that  work for general policy classes.  We conduct a  proof-of-concept experiment which demonstrates the excellent  computational and statistical performance of (an online variant of) our  algorithm relative to several strong baselines.",
        "bibtex": "@InProceedings{pmlr-v32-agarwalb14,\n  title = \t {Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits},\n  author = \t {Agarwal, Alekh and Hsu, Daniel and Kale, Satyen and Langford, John and Li, Lihong and Schapire, Robert},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1638--1646},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/agarwalb14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/agarwalb14.html},\n  abstract = \t {We present a new algorithm for the contextual bandit learning problem,  where the learner repeatedly takes one of K \\emphactions in response to the  observed \\emphcontext, and observes the \\emphreward only for that  action. Our method assumes access to an oracle for solving fully  supervised cost-sensitive classification problems and achieves the  statistically optimal regret guarantee with only \\otil(\\sqrtKT)  oracle calls across all T rounds. By doing so, we obtain the most  practical contextual bandit learning algorithm amongst approaches that  work for general policy classes.  We conduct a  proof-of-concept experiment which demonstrates the excellent  computational and statistical performance of (an online variant of) our  algorithm relative to several strong baselines.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/agarwalb14.pdf",
        "supp": "",
        "pdf_size": 302852,
        "gs_citation": 622,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3644651126463241945&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "Microsoft Research, New York, NY; Columbia University, New York, NY; Yahoo! Labs, New York, NY; Microsoft Research, New York, NY; Microsoft Research, Redmond, WA; Princeton University, Princeton, NJ",
        "aff_domain": "microsoft.com;cs.columbia.edu;satyenkale.com;microsoft.com;microsoft.com;cs.princeton.edu",
        "email": "microsoft.com;cs.columbia.edu;satyenkale.com;microsoft.com;microsoft.com;cs.princeton.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0;0;3",
        "aff_unique_norm": "Microsoft;Columbia University;Yahoo!;Princeton University",
        "aff_unique_dep": "Microsoft Research;;Yahoo! Labs;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://www.columbia.edu;https://yahoo.com;https://www.princeton.edu",
        "aff_unique_abbr": "MSR;Columbia;Yahoo!;Princeton",
        "aff_campus_unique_index": "0;0;0;0;1;2",
        "aff_campus_unique": "New York;Redmond;Princeton",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5fd22c0e4d",
        "title": "The Coherent Loss Function for Classification",
        "site": "https://proceedings.mlr.press/v32/yanga14.html",
        "author": "Wenzhuo Yang; Melvyn Sim; Huan Xu",
        "abstract": "A prediction rule in binary classification that aims to achieve the lowest probability of misclassification involves minimizing over a non-convex, 0-1 loss function, which is typically a computationally intractable optimization problem. To address the intractability, previous methods consider minimizing the cumulative loss \u2013 the sum of convex surrogates of the 0-1 loss of each sample. In this paper, we revisit this paradigm and develop instead an axiomatic framework by proposing a set of salient properties on functions for binary classification and then propose the coherent loss approach, which is a tractable upper-bound of the empirical classification error over the entire sample set. We show that the proposed approach yields a strictly tighter approximation to the empirical classification error than any convex cumulative loss approach while preserving the convexity of the underlying optimization problem, and this approach for binary classification also has a robustness interpretation which builds a connection to robust SVMs. The experimental results show that our approach outperforms the standard SVM when additional constraints are imposed.",
        "bibtex": "@InProceedings{pmlr-v32-yanga14,\n  title = \t {The Coherent Loss Function for Classification},\n  author = \t {Yang, Wenzhuo and Sim, Melvyn and Xu, Huan},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {37--45},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/yanga14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/yanga14.html},\n  abstract = \t {A prediction rule in binary classification that aims to achieve the lowest probability of misclassification involves minimizing over a non-convex, 0-1 loss function, which is typically a computationally intractable optimization problem. To address the intractability, previous methods consider minimizing the cumulative loss \u2013 the sum of convex surrogates of the 0-1 loss of each sample. In this paper, we revisit this paradigm and develop instead an axiomatic framework by proposing a set of salient properties on functions for binary classification and then propose the coherent loss approach, which is a tractable upper-bound of the empirical classification error over the entire sample set. We show that the proposed approach yields a strictly tighter approximation to the empirical classification error than any convex cumulative loss approach while preserving the convexity of the underlying optimization problem, and this approach for binary classification also has a robustness interpretation which builds a connection to robust SVMs. The experimental results show that our approach outperforms the standard SVM when additional constraints are imposed.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/yanga14.pdf",
        "supp": "",
        "pdf_size": 151297,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11710262076493404888&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Mechanical Engineering, National University of Singapore, Singapore 117576; Department of Decision Sciences, National University of Singapore, Singapore 117576; Department of Mechanical Engineering, National University of Singapore, Singapore 117576",
        "aff_domain": "NUS.EDU.SG;NUS.EDU.SG;NUS.EDU.SG",
        "email": "NUS.EDU.SG;NUS.EDU.SG;NUS.EDU.SG",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "National University of Singapore",
        "aff_unique_dep": "Department of Mechanical Engineering",
        "aff_unique_url": "https://www.nus.edu.sg",
        "aff_unique_abbr": "NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "33437b5571",
        "title": "The Falling Factorial Basis and Its Statistical Applications",
        "site": "https://proceedings.mlr.press/v32/wange14.html",
        "author": "Yu-Xiang Wang; Alex Smola; Ryan Tibshirani",
        "abstract": "We study a novel spline-like basis, which we name the   \\it falling factorial basis, bearing many similarities to the  classic truncated power basis.  The advantage of the falling factorial  basis is that it enables rapid, linear-time computations in basis  matrix multiplication and basis matrix inversion.  The falling  factorial functions are not actually splines, but are close enough  to splines that they provably retain some of the favorable properties  of the latter functions.  We examine their application in two  problems: trend filtering over arbitrary input points, and a  higher-order variant of the two-sample Kolmogorov-Smirnov test.",
        "bibtex": "@InProceedings{pmlr-v32-wange14,\n  title = \t {The Falling Factorial Basis and Its Statistical Applications},\n  author = \t {Wang, Yu-Xiang and Smola, Alex and Tibshirani, Ryan},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {730--738},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/wange14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/wange14.html},\n  abstract = \t {We study a novel spline-like basis, which we name the   \\it falling factorial basis, bearing many similarities to the  classic truncated power basis.  The advantage of the falling factorial  basis is that it enables rapid, linear-time computations in basis  matrix multiplication and basis matrix inversion.  The falling  factorial functions are not actually splines, but are close enough  to splines that they provably retain some of the favorable properties  of the latter functions.  We examine their application in two  problems: trend filtering over arbitrary input points, and a  higher-order variant of the two-sample Kolmogorov-Smirnov test.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/wange14.pdf",
        "supp": "",
        "pdf_size": 325043,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1440371641760628828&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": "CS.CMU.EDU;SMOLA.ORG;STAT.CMU.EDU",
        "email": "CS.CMU.EDU;SMOLA.ORG;STAT.CMU.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7192444814",
        "title": "The Inverse Regression Topic Model",
        "site": "https://proceedings.mlr.press/v32/rabinovich14.html",
        "author": "Maxim Rabinovich; David Blei",
        "abstract": "\\citettaddy13mnir proposed multinomial inverse regression (MNIR) as a new model of annotated text based on the influence of metadata and response variables on the distribution of words in a document. While effective, MNIR has no way to exploit structure in the corpus to improve its predictions or facilitate exploratory data analysis. On the other hand, traditional probabilistic topic models (like latent Dirichlet allocation) capture natural heterogeneity in a collection but do not account for external variables. In this paper, we introduce the inverse regression topic model (IRTM), a mixed-membership extension of MNIR that combines the strengths of both methodologies. We present two inference algorithms for the IRTM: an efficient batch estimation  algorithm and an online variant, which is suitable for large corpora.  We apply these methods to a corpus of 73K Congressional press releases  and another of 150K Yelp reviews, demonstrating that the IRTM  outperforms both MNIR and supervised topic models on the prediction task.  Further, we give examples showing that the IRTM enables systematic  discovery of in-topic lexical variation, which is not possible with previous supervised topic models.",
        "bibtex": "@InProceedings{pmlr-v32-rabinovich14,\n  title = \t {The Inverse Regression Topic Model},\n  author = \t {Rabinovich, Maxim and Blei, David},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {199--207},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/rabinovich14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/rabinovich14.html},\n  abstract = \t {\\citettaddy13mnir proposed multinomial inverse regression (MNIR) as a new model of annotated text based on the influence of metadata and response variables on the distribution of words in a document. While effective, MNIR has no way to exploit structure in the corpus to improve its predictions or facilitate exploratory data analysis. On the other hand, traditional probabilistic topic models (like latent Dirichlet allocation) capture natural heterogeneity in a collection but do not account for external variables. In this paper, we introduce the inverse regression topic model (IRTM), a mixed-membership extension of MNIR that combines the strengths of both methodologies. We present two inference algorithms for the IRTM: an efficient batch estimation  algorithm and an online variant, which is suitable for large corpora.  We apply these methods to a corpus of 73K Congressional press releases  and another of 150K Yelp reviews, demonstrating that the IRTM  outperforms both MNIR and supervised topic models on the prediction task.  Further, we give examples showing that the IRTM enables systematic  discovery of in-topic lexical variation, which is not possible with previous supervised topic models.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/rabinovich14.pdf",
        "supp": "",
        "pdf_size": 464791,
        "gs_citation": 76,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8150487678184038224&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Engineering, University of Cambridge; Department of Computer Science, Princeton University",
        "aff_domain": "CAM.AC.UK;CS.PRINCETON.EDU",
        "email": "CAM.AC.UK;CS.PRINCETON.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Cambridge;Princeton University",
        "aff_unique_dep": "Department of Engineering;Department of Computer Science",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.princeton.edu",
        "aff_unique_abbr": "Cambridge;Princeton",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "a2f4e3c077",
        "title": "The f-Adjusted Graph Laplacian: a Diagonal Modification with a Geometric Interpretation",
        "site": "https://proceedings.mlr.press/v32/kurras14.html",
        "author": "Sven Kurras; Ulrike Luxburg; Gilles Blanchard",
        "abstract": "Consider a neighborhood graph, for example a k-nearest neighbor graph, that is constructed on sample points drawn according to some density p. Our goal is to re-weight the graph\u2019s edges such that all cuts and volumes behave as if the graph was built on a different sample drawn from an alternative density q. We introduce the f-adjusted graph and prove that it provides the correct cuts and volumes as the sample size tends to infinity. From an algebraic perspective, we show that its normalized Laplacian, denoted as the f-adjusted Laplacian, represents a natural family of diagonal perturbations of the original normalized Laplacian. Our technique allows to apply any cut and volume based algorithm to the f-adjusted graph, for example spectral clustering, in order to study the given graph as if it were built on an unaccessible sample from a different density. We point out applications in sample bias correction, data uniformization, and multi-scale analysis of graphs.",
        "bibtex": "@InProceedings{pmlr-v32-kurras14,\n  title = \t {The f-Adjusted Graph Laplacian: a Diagonal Modification with a Geometric Interpretation},\n  author = \t {Kurras, Sven and Luxburg, Ulrike and Blanchard, Gilles},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1530--1538},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/kurras14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/kurras14.html},\n  abstract = \t {Consider a neighborhood graph, for example a k-nearest neighbor graph, that is constructed on sample points drawn according to some density p. Our goal is to re-weight the graph\u2019s edges such that all cuts and volumes behave as if the graph was built on a different sample drawn from an alternative density q. We introduce the f-adjusted graph and prove that it provides the correct cuts and volumes as the sample size tends to infinity. From an algebraic perspective, we show that its normalized Laplacian, denoted as the f-adjusted Laplacian, represents a natural family of diagonal perturbations of the original normalized Laplacian. Our technique allows to apply any cut and volume based algorithm to the f-adjusted graph, for example spectral clustering, in order to study the given graph as if it were built on an unaccessible sample from a different density. We point out applications in sample bias correction, data uniformization, and multi-scale analysis of graphs.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/kurras14.pdf",
        "supp": "",
        "pdf_size": 2285347,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15377480918499385494&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, University of Hamburg, Germany; Department of Computer Science, University of Hamburg, Germany; Department of Mathematics, University of Potsdam, Germany",
        "aff_domain": "INFORMATIK.UNI-HAMBURG.DE;INFORMATIK.UNI-HAMBURG.DE;MATH.UNI-POTSDAM.DE",
        "email": "INFORMATIK.UNI-HAMBURG.DE;INFORMATIK.UNI-HAMBURG.DE;MATH.UNI-POTSDAM.DE",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Hamburg;University of Potsdam",
        "aff_unique_dep": "Department of Computer Science;Department of Mathematics",
        "aff_unique_url": "https://www.uni-hamburg.de;https://www.uni-potsdam.de",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "7b4f6b14c6",
        "title": "Thompson Sampling for Complex Online Problems",
        "site": "https://proceedings.mlr.press/v32/gopalan14.html",
        "author": "Aditya Gopalan; Shie Mannor; Yishay Mansour",
        "abstract": "We consider stochastic multi-armed bandit problems with complex actions over a set of basic arms, where the decision maker plays a complex action rather than a basic arm in each round. The reward of the complex action is some function of the basic arms\u2019 rewards, and the feedback observed may not necessarily be the reward per-arm. For instance, when the complex actions are subsets of the arms, we may only observe the maximum reward over the chosen subset. Thus, feedback across complex actions may be coupled due to the nature of the reward function. We prove a frequentist regret bound for Thompson sampling in a very general setting involving parameter, action and observation spaces and a likelihood function over them. The bound holds for discretely-supported priors over the parameter space and without additional structural properties such as closed-form posteriors, conjugate prior structure or independence across arms. The regret bound scales logarithmically with time but, more importantly, with an improved constant that non-trivially captures the coupling across complex actions due to the structure of the rewards. As applications, we derive improved regret bounds for classes of complex bandit problems involving selecting subsets of arms, including the first nontrivial regret bounds for nonlinear MAX reward feedback from subsets. Using particle filters for computing posterior distributions which lack an explicit closed-form, we present numerical results for the performance of Thompson sampling for subset-selection and job scheduling problems.",
        "bibtex": "@InProceedings{pmlr-v32-gopalan14,\n  title = \t {Thompson Sampling for Complex Online Problems},\n  author = \t {Gopalan, Aditya and Mannor, Shie and Mansour, Yishay},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {100--108},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/gopalan14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/gopalan14.html},\n  abstract = \t {We consider stochastic multi-armed bandit problems with complex actions over a set of basic arms, where the decision maker plays a complex action rather than a basic arm in each round. The reward of the complex action is some function of the basic arms\u2019 rewards, and the feedback observed may not necessarily be the reward per-arm. For instance, when the complex actions are subsets of the arms, we may only observe the maximum reward over the chosen subset. Thus, feedback across complex actions may be coupled due to the nature of the reward function. We prove a frequentist regret bound for Thompson sampling in a very general setting involving parameter, action and observation spaces and a likelihood function over them. The bound holds for discretely-supported priors over the parameter space and without additional structural properties such as closed-form posteriors, conjugate prior structure or independence across arms. The regret bound scales logarithmically with time but, more importantly, with an improved constant that non-trivially captures the coupling across complex actions due to the structure of the rewards. As applications, we derive improved regret bounds for classes of complex bandit problems involving selecting subsets of arms, including the first nontrivial regret bounds for nonlinear MAX reward feedback from subsets. Using particle filters for computing posterior distributions which lack an explicit closed-form, we present numerical results for the performance of Thompson sampling for subset-selection and job scheduling problems.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/gopalan14.pdf",
        "supp": "",
        "pdf_size": 3328463,
        "gs_citation": 266,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3648373378029744311&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Electrical Engineering, Technion - Israel Institute of Technology, Haifa 32000, Israel; Department of Electrical Engineering, Technion - Israel Institute of Technology, Haifa 32000, Israel; School of Computer Science, Tel Aviv University, Tel Aviv 69978, Israel",
        "aff_domain": "ee.technion.ac.il;ee.technion.ac.il;tau.ac.il",
        "email": "ee.technion.ac.il;ee.technion.ac.il;tau.ac.il",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Technion - Israel Institute of Technology;Tel Aviv University",
        "aff_unique_dep": "Department of Electrical Engineering;School of Computer Science",
        "aff_unique_url": "https://www.technion.ac.il;https://www.tau.ac.il",
        "aff_unique_abbr": "Technion;TAU",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Haifa;Tel Aviv",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "5053ef6839",
        "title": "Time-Regularized Interrupting Options (TRIO)",
        "site": "https://proceedings.mlr.press/v32/mannb14.html",
        "author": "Timothy Mann; Daniel Mankowitz; Shie Mannor",
        "abstract": "High-level skills relieve planning algorithms from low-level details. But when the skills are poorly designed for the domain, the resulting plan may be severely suboptimal. Sutton et al. 1999 made an important step towards resolving this problem by introducing a rule that automatically improves a set of skills called options. This rule terminates an option early whenever switching to another option gives a higher value than continuing with the current option. However, they only analyzed the case where the improvement rule is applied once. We show conditions where this rule converges to the optimal set of options. A new Bellman-like operator that simultaneously improves the set of options is at the core of our analysis. One problem with the update rule is that it tends to favor lower-level skills. Therefore we introduce a regularization term that favors longer duration skills. Experimental results demonstrate that this approach can derive a good set of high-level skills even when the original set of skills cannot solve the problem.",
        "bibtex": "@InProceedings{pmlr-v32-mannb14,\n  title = \t {Time-Regularized Interrupting Options (TRIO)},\n  author = \t {Mann, Timothy and Mankowitz, Daniel and Mannor, Shie},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1350--1358},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/mannb14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/mannb14.html},\n  abstract = \t {High-level skills relieve planning algorithms from low-level details. But when the skills are poorly designed for the domain, the resulting plan may be severely suboptimal. Sutton et al. 1999 made an important step towards resolving this problem by introducing a rule that automatically improves a set of skills called options. This rule terminates an option early whenever switching to another option gives a higher value than continuing with the current option. However, they only analyzed the case where the improvement rule is applied once. We show conditions where this rule converges to the optimal set of options. A new Bellman-like operator that simultaneously improves the set of options is at the core of our analysis. One problem with the update rule is that it tends to favor lower-level skills. Therefore we introduce a regularization term that favors longer duration skills. Experimental results demonstrate that this approach can derive a good set of high-level skills even when the original set of skills cannot solve the problem.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/mannb14.pdf",
        "supp": "",
        "pdf_size": 1515017,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4347128180187930116&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 3,
        "aff": "Electrical Engineering Department, The Technion - Israel Institute of Technology, Haifa 32000, Israel; Electrical Engineering Department, The Technion - Israel Institute of Technology, Haifa 32000, Israel; Electrical Engineering Department, The Technion - Israel Institute of Technology, Haifa 32000, Israel",
        "aff_domain": "TX.TECHNION.AC.IL;EE.TECHNION.AC.IL;EE.TECHNION.AC.IL",
        "email": "TX.TECHNION.AC.IL;EE.TECHNION.AC.IL;EE.TECHNION.AC.IL",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Technion - Israel Institute of Technology",
        "aff_unique_dep": "Electrical Engineering Department",
        "aff_unique_url": "https://www.technion.ac.il",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Haifa",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "598fd9410a",
        "title": "Topic Modeling using Topics from Many Domains, Lifelong Learning and Big Data",
        "site": "https://proceedings.mlr.press/v32/chenf14.html",
        "author": "Zhiyuan Chen; Bing Liu",
        "abstract": "Topic modeling has been commonly used to discover topics from document collections. However, unsupervised models can generate many incoherent topics. To address this problem, several knowledge-based topic models have been proposed to incorporate prior domain knowledge from the user. This work advances this research much further and shows that without any user input, we can mine the prior knowledge automatically and dynamically from topics already found from a large number of domains. This paper first proposes a novel method to mine such prior knowledge dynamically in the modeling process, and then a new topic model to use the knowledge to guide the model inference. What is also interesting is that this approach offers a novel lifelong learning algorithm for topic discovery, which exploits the big (past) data and knowledge gained from such data for subsequent modeling. Our experimental results using product reviews from 50 domains demonstrate the effectiveness of the proposed approach.",
        "bibtex": "@InProceedings{pmlr-v32-chenf14,\n  title = \t {Topic Modeling using Topics from Many Domains, Lifelong Learning and Big Data},\n  author = \t {Chen, Zhiyuan and Liu, Bing},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {703--711},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/chenf14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/chenf14.html},\n  abstract = \t {Topic modeling has been commonly used to discover topics from document collections. However, unsupervised models can generate many incoherent topics. To address this problem, several knowledge-based topic models have been proposed to incorporate prior domain knowledge from the user. This work advances this research much further and shows that without any user input, we can mine the prior knowledge automatically and dynamically from topics already found from a large number of domains. This paper first proposes a novel method to mine such prior knowledge dynamically in the modeling process, and then a new topic model to use the knowledge to guide the model inference. What is also interesting is that this approach offers a novel lifelong learning algorithm for topic discovery, which exploits the big (past) data and knowledge gained from such data for subsequent modeling. Our experimental results using product reviews from 50 domains demonstrate the effectiveness of the proposed approach.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/chenf14.pdf",
        "supp": "",
        "pdf_size": 320512,
        "gs_citation": 242,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18301692158887632313&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, University of Illinois at Chicago; Department of Computer Science, University of Illinois at Chicago",
        "aff_domain": "gmail.com;cs.uic.edu",
        "email": "gmail.com;cs.uic.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Illinois at Chicago",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uic.edu",
        "aff_unique_abbr": "UIC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chicago",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "40ba698d49",
        "title": "Towards End-To-End Speech Recognition with Recurrent Neural Networks",
        "site": "https://proceedings.mlr.press/v32/graves14.html",
        "author": "Alex Graves; Navdeep Jaitly",
        "abstract": "This paper presents a speech recognition system that directly transcribes audio data with text, without requiring an intermediate phonetic representation. The system is based on a combination of the deep bidirectional LSTM recurrent neural network architecture and the Connectionist Temporal Classification objective function. A modification to the objective function is introduced that trains the network to minimise the expectation of an arbitrary transcription loss function. This allows a direct optimisation of the word error rate, even in the absence of a lexicon or language model. The system achieves a word error rate of 27.3% on the Wall Street Journal corpus with no prior linguistic information, 21.9% with only a lexicon of allowed words, and 8.2% with a trigram language model. Combining the network with a baseline system further reduces the error rate to 6.7%.",
        "bibtex": "@InProceedings{pmlr-v32-graves14,\n  title = \t {Towards End-To-End Speech Recognition with Recurrent Neural Networks},\n  author = \t {Graves, Alex and Jaitly, Navdeep},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1764--1772},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/graves14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/graves14.html},\n  abstract = \t {This paper presents a speech recognition system that directly transcribes audio data with text, without requiring an intermediate phonetic representation. The system is based on a combination of the deep bidirectional LSTM recurrent neural network architecture and the Connectionist Temporal Classification objective function. A modification to the objective function is introduced that trains the network to minimise the expectation of an arbitrary transcription loss function. This allows a direct optimisation of the word error rate, even in the absence of a lexicon or language model. The system achieves a word error rate of 27.3% on the Wall Street Journal corpus with no prior linguistic information, 21.9% with only a lexicon of allowed words, and 8.2% with a trigram language model. Combining the network with a baseline system further reduces the error rate to 6.7%.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/graves14.pdf",
        "supp": "",
        "pdf_size": 442855,
        "gs_citation": 3143,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17027777407928493461&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Google DeepMind, London, United Kingdom; Department of Computer Science, University of Toronto, Canada",
        "aff_domain": "CS.TORONTO.EDU;CS.TORONTO.EDU",
        "email": "CS.TORONTO.EDU;CS.TORONTO.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Google;University of Toronto",
        "aff_unique_dep": "Google DeepMind;Department of Computer Science",
        "aff_unique_url": "https://deepmind.com;https://www.utoronto.ca",
        "aff_unique_abbr": "DeepMind;U of T",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "London;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United Kingdom;Canada"
    },
    {
        "id": "a55eb1f95d",
        "title": "Towards Minimax Online Learning with Unknown Time Horizon",
        "site": "https://proceedings.mlr.press/v32/luo14.html",
        "author": "Haipeng Luo; Robert Schapire",
        "abstract": "We consider online learning when the time horizon is unknown. We apply a minimax analysis, beginning with the fixed horizon case, and then moving on to two unknown-horizon settings, one that assumes the horizon is chosen randomly according to some distribution, and the other which allows the adversary full control over the horizon. For the random horizon setting with restricted losses, we derive a fully optimal minimax algorithm. And for the adversarial horizon setting, we prove a nontrivial lower bound which shows that the adversary obtains strictly more power than when the horizon is fixed and known. Based on the minimax solution of the random horizon setting, we then propose a new adaptive algorithm which \u201cpretends\u201d that the horizon is drawn from a distribution from a special family, but no matter how the actual horizon is chosen,  the worst-case regret is of the optimal rate. Furthermore, our algorithm can be combined and applied in many ways, for instance, to online convex optimization, follow the perturbed leader, exponential weights algorithm and first order bounds. Experiments show that our algorithm outperforms many other existing algorithms in an online linear optimization setting.",
        "bibtex": "@InProceedings{pmlr-v32-luo14,\n  title = \t {Towards Minimax Online Learning with Unknown Time Horizon},\n  author = \t {Luo, Haipeng and Schapire, Robert},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {226--234},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/luo14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/luo14.html},\n  abstract = \t {We consider online learning when the time horizon is unknown. We apply a minimax analysis, beginning with the fixed horizon case, and then moving on to two unknown-horizon settings, one that assumes the horizon is chosen randomly according to some distribution, and the other which allows the adversary full control over the horizon. For the random horizon setting with restricted losses, we derive a fully optimal minimax algorithm. And for the adversarial horizon setting, we prove a nontrivial lower bound which shows that the adversary obtains strictly more power than when the horizon is fixed and known. Based on the minimax solution of the random horizon setting, we then propose a new adaptive algorithm which \u201cpretends\u201d that the horizon is drawn from a distribution from a special family, but no matter how the actual horizon is chosen,  the worst-case regret is of the optimal rate. Furthermore, our algorithm can be combined and applied in many ways, for instance, to online convex optimization, follow the perturbed leader, exponential weights algorithm and first order bounds. Experiments show that our algorithm outperforms many other existing algorithms in an online linear optimization setting.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/luo14.pdf",
        "supp": "",
        "pdf_size": 499642,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1162906612832155640&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computer Science, Princeton University, Princeton, NJ 08540; Department of Computer Science, Princeton University, Princeton, NJ 08540",
        "aff_domain": "CS.PRINCETON.EDU;CS.PRINCETON.EDU",
        "email": "CS.PRINCETON.EDU;CS.PRINCETON.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Princeton University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.princeton.edu",
        "aff_unique_abbr": "Princeton",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Princeton",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a23d7bec40",
        "title": "Towards an optimal stochastic alternating direction method of multipliers",
        "site": "https://proceedings.mlr.press/v32/azadi14.html",
        "author": "Samaneh Azadi; Suvrit Sra",
        "abstract": "We study regularized stochastic convex optimization subject to linear equality constraints. This class of problems was recently also studied by Ouyang et al. (2013) and Suzuki (2013); both introduced similar stochastic alternating direction method of multipliers (SADMM) algorithms. However, the analysis of both papers led to suboptimal convergence rates. This paper presents two new SADMM methods: (i) the first attains the minimax optimal rate of O(1/k) for nonsmooth strongly-convex stochastic problems; while (ii) the second progresses towards an optimal rate by exhibiting an O(1/k^2) rate for the smooth part. We present several experiments with our new methods; the results indicate improved performance over competing ADMM methods.",
        "bibtex": "@InProceedings{pmlr-v32-azadi14,\n  title = \t {Towards an optimal stochastic alternating direction method of multipliers},\n  author = \t {Azadi, Samaneh and Sra, Suvrit},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {620--628},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/azadi14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/azadi14.html},\n  abstract = \t {We study regularized stochastic convex optimization subject to linear equality constraints. This class of problems was recently also studied by Ouyang et al. (2013) and Suzuki (2013); both introduced similar stochastic alternating direction method of multipliers (SADMM) algorithms. However, the analysis of both papers led to suboptimal convergence rates. This paper presents two new SADMM methods: (i) the first attains the minimax optimal rate of O(1/k) for nonsmooth strongly-convex stochastic problems; while (ii) the second progresses towards an optimal rate by exhibiting an O(1/k^2) rate for the smooth part. We present several experiments with our new methods; the results indicate improved performance over competing ADMM methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/azadi14.pdf",
        "supp": "",
        "pdf_size": 219576,
        "gs_citation": 72,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12563465726512356688&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "UC Berkeley, Berkeley, CA + School of ECE, Shiraz University, Shiraz, Iran; Carnegie Mellon University, Pittsburgh + Max Planck Institute for Intelligent Systems, T\u00fcbingen, Germany",
        "aff_domain": "gmail.com;tuebingen.mpg.de",
        "email": "gmail.com;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2+3",
        "aff_unique_norm": "University of California, Berkeley;Shiraz University;Carnegie Mellon University;Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": ";School of Electrical and Computer Engineering;;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.s\u0169.shirazu.ac.ir;https://www.cmu.edu;https://www.mpi-is.mpg.de",
        "aff_unique_abbr": "UC Berkeley;S\u0169;CMU;MPI-IS",
        "aff_campus_unique_index": "0+1;2+3",
        "aff_campus_unique": "Berkeley;Shiraz;Pittsburgh;T\u00fcbingen",
        "aff_country_unique_index": "0+1;0+2",
        "aff_country_unique": "United States;Iran;Germany"
    },
    {
        "id": "8e79b31e86",
        "title": "Towards scaling up Markov chain Monte Carlo: an adaptive subsampling approach",
        "site": "https://proceedings.mlr.press/v32/bardenet14.html",
        "author": "R\u00e9mi Bardenet; Arnaud Doucet; Chris Holmes",
        "abstract": "Markov chain Monte Carlo (MCMC) methods are often deemed far too computationally intensive to be of any practical use for large datasets. This paper describes a methodology that aims to scale up the Metropolis-Hastings (MH) algorithm in this context. We propose an approximate implementation of the accept/reject step of MH that only requires evaluating the likelihood of a random subset of the data, yet is guaranteed to coincide with the accept/reject step based on the full dataset with a probability superior to a user-specified tolerance level. This adaptive subsampling technique is an alternative to the recent approach developed in (Korattikara et al, ICML\u201914), and it allows us to establish rigorously that the resulting approximate MH algorithm samples from a perturbed version of the target distribution of interest, whose total variation distance to this very target is controlled explicitly. We explore the benefits and limitations of this scheme on several examples.",
        "bibtex": "@InProceedings{pmlr-v32-bardenet14,\n  title = \t {Towards scaling up Markov chain Monte Carlo: an adaptive subsampling approach },\n  author = \t {Bardenet, R\u00e9mi and Doucet, Arnaud and Holmes, Chris},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {405--413},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/bardenet14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/bardenet14.html},\n  abstract = \t {Markov chain Monte Carlo (MCMC) methods are often deemed far too computationally intensive to be of any practical use for large datasets. This paper describes a methodology that aims to scale up the Metropolis-Hastings (MH) algorithm in this context. We propose an approximate implementation of the accept/reject step of MH that only requires evaluating the likelihood of a random subset of the data, yet is guaranteed to coincide with the accept/reject step based on the full dataset with a probability superior to a user-specified tolerance level. This adaptive subsampling technique is an alternative to the recent approach developed in (Korattikara et al, ICML\u201914), and it allows us to establish rigorously that the resulting approximate MH algorithm samples from a perturbed version of the target distribution of interest, whose total variation distance to this very target is controlled explicitly. We explore the benefits and limitations of this scheme on several examples.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/bardenet14.pdf",
        "supp": "",
        "pdf_size": 1278012,
        "gs_citation": 225,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7985359798773776734&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Statistics, University of Oxford, Oxford OX1 3TG, UK; Department of Statistics, University of Oxford, Oxford OX1 3TG, UK; Department of Statistics, University of Oxford, Oxford OX1 3TG, UK",
        "aff_domain": "GMAIL.COM;STATS.OX.AC.UK;STATS.OX.AC.UK",
        "email": "GMAIL.COM;STATS.OX.AC.UK;STATS.OX.AC.UK",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Oxford",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.ox.ac.uk",
        "aff_unique_abbr": "Oxford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Oxford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "f9491d66d0",
        "title": "Tracking Adversarial Targets",
        "site": "https://proceedings.mlr.press/v32/abbasi-yadkori14.html",
        "author": "Yasin Abbasi-Yadkori; Peter Bartlett; Varun Kanade",
        "abstract": "We study linear control problems with quadratic losses and adversarially chosen tracking targets. We present an efficient algorithm for this problem and show that, under standard conditions on the linear system, its regret with respect to an optimal linear policy grows as O(\\log^2 T), where T is the number of rounds of the game.  We also study a problem with adversarially chosen transition dynamics; we present an exponentially-weighted average algorithm for this problem, and we give regret bounds that grow as O(\\sqrt T).",
        "bibtex": "@InProceedings{pmlr-v32-abbasi-yadkori14,\n  title = \t {Tracking Adversarial Targets},\n  author = \t {Abbasi-Yadkori, Yasin and Bartlett, Peter and Kanade, Varun},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {369--377},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/abbasi-yadkori14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/abbasi-yadkori14.html},\n  abstract = \t {We study linear control problems with quadratic losses and adversarially chosen tracking targets. We present an efficient algorithm for this problem and show that, under standard conditions on the linear system, its regret with respect to an optimal linear policy grows as O(\\log^2 T), where T is the number of rounds of the game.  We also study a problem with adversarially chosen transition dynamics; we present an exponentially-weighted average algorithm for this problem, and we give regret bounds that grow as O(\\sqrt T).}\n}",
        "pdf": "http://proceedings.mlr.press/v32/abbasi-yadkori14.pdf",
        "supp": "",
        "pdf_size": 348387,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12720616418257306653&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Queensland University of Technology; University of California, Berkeley + QUT; University of California, Berkeley",
        "aff_domain": "QUT.EDU.AU;EECS.BERKELEY.EDU;EECS.BERKELEY.EDU",
        "email": "QUT.EDU.AU;EECS.BERKELEY.EDU;EECS.BERKELEY.EDU",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;1",
        "aff_unique_norm": "Queensland University of Technology;University of California, Berkeley",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.qut.edu.au;https://www.berkeley.edu",
        "aff_unique_abbr": "QUT;UC Berkeley",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0;1+0;1",
        "aff_country_unique": "Australia;United States"
    },
    {
        "id": "178d306ecd",
        "title": "Transductive Learning with Multi-class Volume Approximation",
        "site": "https://proceedings.mlr.press/v32/niu14.html",
        "author": "Gang Niu; Bo Dai; Christoffel Plessis; Masashi Sugiyama",
        "abstract": "Given a hypothesis space, the large volume principle by Vladimir Vapnik prioritizes equivalence classes according to their volume in the hypothesis space. The volume approximation has hitherto been successfully applied to binary learning problems. In this paper, we propose a novel generalization to multiple classes, allowing applications of the large volume principle on more learning problems such as multi-class, multi-label and serendipitous learning in a transductive manner. Although the resultant learning method involves a non-convex optimization problem, the globally optimal solution is almost surely unique and can be obtained using O(n^3) time. Novel theoretical analyses are presented for the proposed method, and experimental results show it compares favorably with the one-vs-rest extension.",
        "bibtex": "@InProceedings{pmlr-v32-niu14,\n  title = \t {Transductive Learning with Multi-class Volume Approximation},\n  author = \t {Niu, Gang and Dai, Bo and Plessis, Christoffel and Sugiyama, Masashi},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1377--1385},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/niu14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/niu14.html},\n  abstract = \t {Given a hypothesis space, the large volume principle by Vladimir Vapnik prioritizes equivalence classes according to their volume in the hypothesis space. The volume approximation has hitherto been successfully applied to binary learning problems. In this paper, we propose a novel generalization to multiple classes, allowing applications of the large volume principle on more learning problems such as multi-class, multi-label and serendipitous learning in a transductive manner. Although the resultant learning method involves a non-convex optimization problem, the globally optimal solution is almost surely unique and can be obtained using O(n^3) time. Novel theoretical analyses are presented for the proposed method, and experimental results show it compares favorably with the one-vs-rest extension.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/niu14.pdf",
        "supp": "",
        "pdf_size": 628774,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3605993082518446547&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Tokyo Institute of Technology + Baidu Inc.; Georgia Institute of Technology; Tokyo Institute of Technology; Tokyo Institute of Technology",
        "aff_domain": "baidu.com;gatech.edu;sg.cs.titech.ac.jp;cs.titech.ac.jp",
        "email": "baidu.com;gatech.edu;sg.cs.titech.ac.jp;cs.titech.ac.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;0;0",
        "aff_unique_norm": "Tokyo Institute of Technology;Baidu;Georgia Institute of Technology",
        "aff_unique_dep": ";Baidu Inc.;",
        "aff_unique_url": "https://www.titech.ac.jp;https://www.baidu.com;https://www.gatech.edu",
        "aff_unique_abbr": "Titech;Baidu;Georgia Tech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;2;0;0",
        "aff_country_unique": "Japan;China;United States"
    },
    {
        "id": "033fdc324d",
        "title": "True Online TD(lambda)",
        "site": "https://proceedings.mlr.press/v32/seijen14.html",
        "author": "Harm Seijen; Rich Sutton",
        "abstract": "TD(lambda) is a core algorithm of modern reinforcement learning. Its appeal comes from its equivalence to a clear and conceptually simple forward view, and the fact that it can be implemented online in an inexpensive manner. However, the equivalence between TD(lambda) and the forward view is exact only for the off-line version of the algorithm (in which updates are made only at the end of each episode). In the online version of TD(lambda) (in which updates are made at each step, which generally performs better and is always used in applications) the match to the forward view is only approximate. In a sense this is unavoidable for the conventional forward view, as it itself presumes that the estimates are unchanging during an episode. In this paper we introduce a new forward view that takes into account the possibility of changing estimates and a new variant of TD(lambda) that exactly achieves it. Our algorithm uses a new form of eligibility trace similar to but different from conventional accumulating and replacing traces. The overall computational complexity is the same as TD(lambda), even when using function approximation. In our empirical comparisons, our algorithm outperformed TD(lambda) in all of its variations. It seems, by adhering more truly to the original goal of TD(lambda)\u2014matching an intuitively clear forward view even in the online case\u2014that we have found a new algorithm that simply improves on classical TD(lambda).",
        "bibtex": "@InProceedings{pmlr-v32-seijen14,\n  title = \t {True Online TD(lambda)},\n  author = \t {Seijen, Harm and Sutton, Rich},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {692--700},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/seijen14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/seijen14.html},\n  abstract = \t {TD(lambda) is a core algorithm of modern reinforcement learning. Its appeal comes from its equivalence to a clear and conceptually simple forward view, and the fact that it can be implemented online in an inexpensive manner. However, the equivalence between TD(lambda) and the forward view is exact only for the off-line version of the algorithm (in which updates are made only at the end of each episode). In the online version of TD(lambda) (in which updates are made at each step, which generally performs better and is always used in applications) the match to the forward view is only approximate. In a sense this is unavoidable for the conventional forward view, as it itself presumes that the estimates are unchanging during an episode. In this paper we introduce a new forward view that takes into account the possibility of changing estimates and a new variant of TD(lambda) that exactly achieves it. Our algorithm uses a new form of eligibility trace similar to but different from conventional accumulating and replacing traces. The overall computational complexity is the same as TD(lambda), even when using function approximation. In our empirical comparisons, our algorithm outperformed TD(lambda) in all of its variations. It seems, by adhering more truly to the original goal of TD(lambda)\u2014matching an intuitively clear forward view even in the online case\u2014that we have found a new algorithm that simply improves on classical TD(lambda).}\n}",
        "pdf": "http://proceedings.mlr.press/v32/seijen14.pdf",
        "supp": "",
        "pdf_size": 390519,
        "gs_citation": 139,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=490776451188860989&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Computing Science, University of Alberta, Edmonton, Alberta, T6G 2E8, Canada; Department of Computing Science, University of Alberta, Edmonton, Alberta, T6G 2E8, Canada",
        "aff_domain": "UALBERTA.CA;CS.UALBERTA.CA",
        "email": "UALBERTA.CA;CS.UALBERTA.CA",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "Department of Computing Science",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Edmonton",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "d6568dc1de",
        "title": "Two-Stage Metric Learning",
        "site": "https://proceedings.mlr.press/v32/wangc14.html",
        "author": "Jun Wang; Ke Sun; Fei Sha; St\u00e9phane Marchand-Maillet; Alexandros Kalousis",
        "abstract": "In this paper, we present a novel two-stage metric learning algorithm. We first map each learning instance to a probability distribution by computing its similarities to a set of fixed anchor points. Then, we define the distance in the input data space as the Fisher information distance on the associated statistical manifold. This induces in the input data space a new family of distance metric which presents unique properties. Unlike kernelized metric learning, we do not require the similarity measure to be positive semi-definite. Moreover, it can also be interpreted as a local metric learning algorithm with well defined distance approximation. We evaluate its performance on a number of datasets. It outperforms significantly other metric learning methods and SVM.",
        "bibtex": "@InProceedings{pmlr-v32-wangc14,\n  title = \t {Two-Stage Metric Learning},\n  author = \t {Wang, Jun and Sun, Ke and Sha, Fei and Marchand-Maillet, St\u00e9phane and Kalousis, Alexandros},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {370--378},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/wangc14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/wangc14.html},\n  abstract = \t {In this paper, we present a novel two-stage metric learning algorithm. We first map each learning instance to a probability distribution by computing its similarities to a set of fixed anchor points. Then, we define the distance in the input data space as the Fisher information distance on the associated statistical manifold. This induces in the input data space a new family of distance metric which presents unique properties. Unlike kernelized metric learning, we do not require the similarity measure to be positive semi-definite. Moreover, it can also be interpreted as a local metric learning algorithm with well defined distance approximation. We evaluate its performance on a number of datasets. It outperforms significantly other metric learning methods and SVM.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/wangc14.pdf",
        "supp": "",
        "pdf_size": 711389,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11889849157824840918&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computer Science, University of Geneva, Switzerland; Department of Computer Science, University of Geneva, Switzerland; Department of Computer Science, University of Southern California, Los Angeles, CA, USA; Department of Computer Science, University of Geneva, Switzerland; Department of Business Informatics,University of Applied Sciences,Western Switzerland+Department of Computer Science, University of Geneva, Switzerland",
        "aff_domain": "UNIGE.CH;UNIGE.CH;USC.EDU;UNIGE.CH;HESGE.CH",
        "email": "UNIGE.CH;UNIGE.CH;USC.EDU;UNIGE.CH;HESGE.CH",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0;2+0",
        "aff_unique_norm": "University of Geneva;University of Southern California;University of Applied Sciences Western Switzerland",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science;Department of Business Informatics",
        "aff_unique_url": "https://www.unige.ch;https://www.usc.edu;https://www.hes-so.ch/en",
        "aff_unique_abbr": "UNIGE;USC;",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";Los Angeles",
        "aff_country_unique_index": "0;0;1;0;0+0",
        "aff_country_unique": "Switzerland;United States"
    },
    {
        "id": "6117861043",
        "title": "Understanding Protein Dynamics with L1-Regularized Reversible Hidden Markov Models",
        "site": "https://proceedings.mlr.press/v32/mcgibbon14.html",
        "author": "Robert McGibbon; Bharath Ramsundar; Mohammad Sultan; Gert Kiss; Vijay Pande",
        "abstract": "We present a machine learning framework for modeling protein dynamics. Our  approach uses L1-regularized, reversible hidden Markov models to  understand large protein datasets generated via molecular dynamics  simulations. Our model is motivated by three design principles: (1) the requirement of massive scalability; (2) the need to adhere to relevant physical law; and (3) the necessity of providing accessible interpretations, critical for rational protein engineering and drug design. We present an EM algorithm for learning and introduce a model selection criteria based on the physical notion of relaxation timescales. We contrast our model with standard methods in biophysics and demonstrate improved robustness. We implement our algorithm on GPUs and apply the method to two large protein simulation datasets generated respectively on the NCSA Bluewaters supercomputer and the Folding@Home distributed computing network. Our analysis identifies the conformational dynamics of the ubiquitin protein responsible for signaling, and elucidates the stepwise activation mechanism of the c-Src kinase protein.",
        "bibtex": "@InProceedings{pmlr-v32-mcgibbon14,\n  title = \t {Understanding Protein Dynamics with L1-Regularized Reversible Hidden Markov Models},\n  author = \t {McGibbon, Robert and Ramsundar, Bharath and Sultan, Mohammad and Kiss, Gert and Pande, Vijay},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1197--1205},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/mcgibbon14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/mcgibbon14.html},\n  abstract = \t {We present a machine learning framework for modeling protein dynamics. Our  approach uses L1-regularized, reversible hidden Markov models to  understand large protein datasets generated via molecular dynamics  simulations. Our model is motivated by three design principles: (1) the requirement of massive scalability; (2) the need to adhere to relevant physical law; and (3) the necessity of providing accessible interpretations, critical for rational protein engineering and drug design. We present an EM algorithm for learning and introduce a model selection criteria based on the physical notion of relaxation timescales. We contrast our model with standard methods in biophysics and demonstrate improved robustness. We implement our algorithm on GPUs and apply the method to two large protein simulation datasets generated respectively on the NCSA Bluewaters supercomputer and the Folding@Home distributed computing network. Our analysis identifies the conformational dynamics of the ubiquitin protein responsible for signaling, and elucidates the stepwise activation mechanism of the c-Src kinase protein.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/mcgibbon14.pdf",
        "supp": "",
        "pdf_size": 2471586,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2965106374169061655&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Chemistry, Stanford University, Stanford CA 94305, USA; Department of Computer Science, Stanford University, Stanford CA 94305, USA; Department of Chemistry, Stanford University, Stanford CA 94305, USA; Department of Chemistry, Stanford University, Stanford CA 94305, USA; Department of Chemistry, Stanford University, Stanford CA 94305, USA",
        "aff_domain": "STANFORD.EDU;STANFORD.EDU;STANFORD.EDU;STANFORD.EDU;STANFORD.EDU",
        "email": "STANFORD.EDU;STANFORD.EDU;STANFORD.EDU;STANFORD.EDU;STANFORD.EDU",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Chemistry",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "eb25de4648",
        "title": "Understanding the Limiting Factors of Topic Modeling via Posterior Contraction Analysis",
        "site": "https://proceedings.mlr.press/v32/tang14.html",
        "author": "Jian Tang; Zhaoshi Meng; Xuanlong Nguyen; Qiaozhu Mei; Ming Zhang",
        "abstract": "Topic models such as the latent Dirichlet allocation (LDA) have become a standard staple in the modeling toolbox of machine learning. They have been applied to a vast variety of data sets, contexts, and tasks to varying degrees of success. However, to date there is almost no formal theory explicating the LDA\u2019s behavior, and despite its familiarity there is very little systematic analysis of and guidance on the properties of the data that affect the inferential performance of the model. This paper seeks to address this gap, by providing a systematic analysis of factors which characterize the LDA\u2019s performance.  We present theorems elucidating the posterior contraction rates of the topics as the amount of data increases, and a thorough supporting empirical study using synthetic and real data sets, including news and web-based articles and tweet messages. Based on these results we provide practical guidance on how to identify suitable data sets for topic models, and how to specify particular model parameters.",
        "bibtex": "@InProceedings{pmlr-v32-tang14,\n  title = \t {Understanding the Limiting Factors of Topic Modeling via Posterior Contraction Analysis},\n  author = \t {Tang, Jian and Meng, Zhaoshi and Nguyen, Xuanlong and Mei, Qiaozhu and Zhang, Ming},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {190--198},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/tang14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/tang14.html},\n  abstract = \t {Topic models such as the latent Dirichlet allocation (LDA) have become a standard staple in the modeling toolbox of machine learning. They have been applied to a vast variety of data sets, contexts, and tasks to varying degrees of success. However, to date there is almost no formal theory explicating the LDA\u2019s behavior, and despite its familiarity there is very little systematic analysis of and guidance on the properties of the data that affect the inferential performance of the model. This paper seeks to address this gap, by providing a systematic analysis of factors which characterize the LDA\u2019s performance.  We present theorems elucidating the posterior contraction rates of the topics as the amount of data increases, and a thorough supporting empirical study using synthetic and real data sets, including news and web-based articles and tweet messages. Based on these results we provide practical guidance on how to identify suitable data sets for topic models, and how to specify particular model parameters.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/tang14.pdf",
        "supp": "",
        "pdf_size": 557118,
        "gs_citation": 404,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12938288684714385295&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 16,
        "aff": "School of EECS, Peking University; Department of EECS, University of Michigan; Department of Statistics, University of Michigan+Department of EECS, University of Michigan; School of Information, University of Michigan+Department of EECS, University of Michigan; School of EECS, Peking University",
        "aff_domain": "NET.PKU.EDU.CN;UMICH.EDU;UMICH.EDU;UMICH.EDU;NET.PKU.EDU.CN",
        "email": "NET.PKU.EDU.CN;UMICH.EDU;UMICH.EDU;UMICH.EDU;NET.PKU.EDU.CN",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1+1;1+1;0",
        "aff_unique_norm": "Peking University;University of Michigan",
        "aff_unique_dep": "School of EECS;Department of Electrical Engineering and Computer Science",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.umich.edu",
        "aff_unique_abbr": "PKU;UM",
        "aff_campus_unique_index": "1;1+1;1+1",
        "aff_campus_unique": ";Ann Arbor",
        "aff_country_unique_index": "0;1;1+1;1+1;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "28a18326f1",
        "title": "Unimodal Bandits: Regret Lower Bounds and Optimal Algorithms",
        "site": "https://proceedings.mlr.press/v32/combes14.html",
        "author": "Richard Combes; Alexandre Proutiere",
        "abstract": "We consider stochastic multi-armed bandits where the expected reward is a unimodal function over partially ordered arms. This important class of problems has been recently investigated in (Cope 2009, Yu 2011). The set of arms is either discrete, in which case arms correspond to the vertices of a finite graph whose structure represents similarity in rewards, or continuous, in which case arms belong to a bounded interval. For discrete unimodal bandits, we derive asymptotic lower bounds for the regret achieved under any algorithm, and propose OSUB, an algorithm whose regret matches this lower bound. Our algorithm optimally exploits the unimodal structure of the problem, and surprisingly, its asymptotic regret does not depend on the number of arms. We also provide a regret upper bound for OSUB in non-stationary environments where the expected rewards smoothly evolve over time. The analytical results are supported by numerical experiments showing that OSUB performs significantly better than the state-of-the-art algorithms. For continuous sets of arms, we provide a brief discussion. We show that combining an appropriate discretization of the set of arms with the UCB algorithm yields an order-optimal regret, and in practice, outperforms recently proposed algorithms designed to exploit the unimodal structure.",
        "bibtex": "@InProceedings{pmlr-v32-combes14,\n  title = \t {Unimodal Bandits: Regret Lower Bounds and Optimal Algorithms},\n  author = \t {Combes, Richard and Proutiere, Alexandre},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {521--529},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/combes14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/combes14.html},\n  abstract = \t {We consider stochastic multi-armed bandits where the expected reward is a unimodal function over partially ordered arms. This important class of problems has been recently investigated in (Cope 2009, Yu 2011). The set of arms is either discrete, in which case arms correspond to the vertices of a finite graph whose structure represents similarity in rewards, or continuous, in which case arms belong to a bounded interval. For discrete unimodal bandits, we derive asymptotic lower bounds for the regret achieved under any algorithm, and propose OSUB, an algorithm whose regret matches this lower bound. Our algorithm optimally exploits the unimodal structure of the problem, and surprisingly, its asymptotic regret does not depend on the number of arms. We also provide a regret upper bound for OSUB in non-stationary environments where the expected rewards smoothly evolve over time. The analytical results are supported by numerical experiments showing that OSUB performs significantly better than the state-of-the-art algorithms. For continuous sets of arms, we provide a brief discussion. We show that combining an appropriate discretization of the set of arms with the UCB algorithm yields an order-optimal regret, and in practice, outperforms recently proposed algorithms designed to exploit the unimodal structure.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/combes14.pdf",
        "supp": "",
        "pdf_size": 286066,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "KTH, Royal Institute of technology, Stockholm, Sweden; KTH, Royal Institute of technology, Stockholm, Sweden",
        "aff_domain": "KTH.SE;KTH.SE",
        "email": "KTH.SE;KTH.SE",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "KTH Royal Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.kth.se",
        "aff_unique_abbr": "KTH",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stockholm",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Sweden"
    },
    {
        "id": "c3c0bcdd95",
        "title": "Universal Matrix Completion",
        "site": "https://proceedings.mlr.press/v32/bhojanapalli14.html",
        "author": "Srinadh Bhojanapalli; Prateek Jain",
        "abstract": "The problem of low-rank matrix completion has recently generated a lot of interest leading to several results that offer exact solutions to the problem. However, in order to do so, these methods make assumptions that can be quite restrictive in practice. More specifically, the methods assume that: a) the observed indices are sampled uniformly at random, and b) for every new matrix, the observed indices are sampled \\emphafresh. In this work, we address these issues by providing a universal recovery guarantee for matrix completion that works for a variety of sampling schemes. In particular, we show that if the set of sampled indices come from the edges of a bipartite graph with large spectral gap (i.e. gap between the first and the second singular value), then the nuclear norm minimization based method exactly recovers all low-rank matrices that satisfy certain incoherence properties.Moreover, we also show that under certain stricter incoherence conditions, O(nr^2) uniformly sampled entries are enough to recover any rank-r n\\times n matrix, in contrast to the O(nr\\log n) sample complexity required by other matrix completion algorithms as well as existing analyses of the nuclear norm method.",
        "bibtex": "@InProceedings{pmlr-v32-bhojanapalli14,\n  title = \t {Universal Matrix Completion},\n  author = \t {Bhojanapalli, Srinadh and Jain, Prateek},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1881--1889},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/bhojanapalli14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/bhojanapalli14.html},\n  abstract = \t {The problem of low-rank matrix completion has recently generated a lot of interest leading to several results that offer exact solutions to the problem. However, in order to do so, these methods make assumptions that can be quite restrictive in practice. More specifically, the methods assume that: a) the observed indices are sampled uniformly at random, and b) for every new matrix, the observed indices are sampled \\emphafresh. In this work, we address these issues by providing a universal recovery guarantee for matrix completion that works for a variety of sampling schemes. In particular, we show that if the set of sampled indices come from the edges of a bipartite graph with large spectral gap (i.e. gap between the first and the second singular value), then the nuclear norm minimization based method exactly recovers all low-rank matrices that satisfy certain incoherence properties.Moreover, we also show that under certain stricter incoherence conditions, O(nr^2) uniformly sampled entries are enough to recover any rank-r n\\times n matrix, in contrast to the O(nr\\log n) sample complexity required by other matrix completion algorithms as well as existing analyses of the nuclear norm method.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/bhojanapalli14.pdf",
        "supp": "",
        "pdf_size": 2042976,
        "gs_citation": 135,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6441528321071104410&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "The University of Texas at Austin; Microsoft Research, India",
        "aff_domain": "UTEXAS.EDU;MICROSOFT.COM",
        "email": "UTEXAS.EDU;MICROSOFT.COM",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Texas at Austin;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.utexas.edu;https://www.microsoft.com/en-us/research/group/india.aspx",
        "aff_unique_abbr": "UT Austin;MSR India",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Austin;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;India"
    },
    {
        "id": "163806e964",
        "title": "Variational Inference for Sequential Distance Dependent Chinese Restaurant Process",
        "site": "https://proceedings.mlr.press/v32/bartunov14.html",
        "author": "Sergey Bartunov; Dmitry Vetrov",
        "abstract": "Recently proposed distance dependent Chinese Restaurant Process (ddCRP) generalizes extensively used Chinese Restaurant Process (CRP) by accounting for dependencies between data points. Its posterior is intractable and so far only MCMC methods were used for inference. Because of very different nature of ddCRP no prior developments in variational methods for Bayesian nonparametrics are appliable. In this paper we propose novel variational inference for important sequential case of ddCRP (seqddCRP) by revealing its connection with Laplacian of random graph constructed by the process. We develop efficient algorithm for optimizing variational lower bound and demonstrate its efficiency comparing to Gibbs sampler. We also apply our variational approximation to CRP-equivalent seqddCRP-mixture model, where it could be considered as alternative to one based on truncated stick-breaking representation. This allowed us to achieve significantly better variational lower bound than variational approximation based on truncated stick breaking for Dirichlet process.",
        "bibtex": "@InProceedings{pmlr-v32-bartunov14,\n  title = \t {Variational Inference for Sequential Distance Dependent Chinese Restaurant Process},\n  author = \t {Bartunov, Sergey and Vetrov, Dmitry},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1404--1412},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/bartunov14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/bartunov14.html},\n  abstract = \t {Recently proposed distance dependent Chinese Restaurant Process (ddCRP) generalizes extensively used Chinese Restaurant Process (CRP) by accounting for dependencies between data points. Its posterior is intractable and so far only MCMC methods were used for inference. Because of very different nature of ddCRP no prior developments in variational methods for Bayesian nonparametrics are appliable. In this paper we propose novel variational inference for important sequential case of ddCRP (seqddCRP) by revealing its connection with Laplacian of random graph constructed by the process. We develop efficient algorithm for optimizing variational lower bound and demonstrate its efficiency comparing to Gibbs sampler. We also apply our variational approximation to CRP-equivalent seqddCRP-mixture model, where it could be considered as alternative to one based on truncated stick-breaking representation. This allowed us to achieve significantly better variational lower bound than variational approximation based on truncated stick breaking for Dirichlet process.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/bartunov14.pdf",
        "supp": "",
        "pdf_size": 975277,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16371757698361543028&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Dorodnicyn Computing Centre of the Russian Academy of Sciences, Moscow RUSSIA; Moscow State University, Moscow RUSSIA + Higher School of Economics, Moscow RUSSIA",
        "aff_domain": "SBOS.IN;YANDEX.RU",
        "email": "SBOS.IN;YANDEX.RU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+2",
        "aff_unique_norm": "Dorodnicyn Computing Centre;Moscow State University;Higher School of Economics",
        "aff_unique_dep": "Russian Academy of Sciences;;",
        "aff_unique_url": ";https://www.msu.ru;https://www.hse.ru",
        "aff_unique_abbr": ";MSU;HSE",
        "aff_campus_unique_index": "0;0+0",
        "aff_campus_unique": "Moscow",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "Russian Federation"
    },
    {
        "id": "356ee4d0b6",
        "title": "Von Mises-Fisher Clustering Models",
        "site": "https://proceedings.mlr.press/v32/gopal14.html",
        "author": "Siddharth Gopal; Yiming Yang",
        "abstract": "This paper proposes a suite of models for clustering high-dimensional data on a unit sphere based on Von Mises-Fisher (vMF) distribution and for discovering more intuitive clusters than existing approaches. The proposed models include  a) A Bayesian formulation of vMF mixture that enables information sharing among clusters,  b) a Hierarchical vMF mixture that provides multi-scale shrinkage and tree structured view of the data and c) a Temporal vMF mixture that captures evolution of clusters in temporal data.  For posterior inference, we develop fast variational methods  as well as collapsed Gibbs sampling techniques for all three models. Our experiments on six datasets provide strong empirical support in favour of vMF based clustering models over other popular tools such as K-means, Multinomial Mixtures and Latent Dirichlet Allocation.",
        "bibtex": "@InProceedings{pmlr-v32-gopal14,\n  title = \t {Von Mises-Fisher Clustering Models},\n  author = \t {Gopal, Siddharth and Yang, Yiming},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {154--162},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/gopal14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/gopal14.html},\n  abstract = \t {This paper proposes a suite of models for clustering high-dimensional data on a unit sphere based on Von Mises-Fisher (vMF) distribution and for discovering more intuitive clusters than existing approaches. The proposed models include  a) A Bayesian formulation of vMF mixture that enables information sharing among clusters,  b) a Hierarchical vMF mixture that provides multi-scale shrinkage and tree structured view of the data and c) a Temporal vMF mixture that captures evolution of clusters in temporal data.  For posterior inference, we develop fast variational methods  as well as collapsed Gibbs sampling techniques for all three models. Our experiments on six datasets provide strong empirical support in favour of vMF based clustering models over other popular tools such as K-means, Multinomial Mixtures and Latent Dirichlet Allocation.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/gopal14.pdf",
        "supp": "",
        "pdf_size": 439265,
        "gs_citation": 163,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9760197927278661020&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Carnegie Mellon University, Pittsburgh, PA 15213 USA; Carnegie Mellon University, Pittsburgh, PA 15213 USA",
        "aff_domain": "CS.CMU.EDU;CS.CMU.EDU",
        "email": "CS.CMU.EDU;CS.CMU.EDU",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7cfc06d690",
        "title": "Wasserstein Propagation for Semi-Supervised Learning",
        "site": "https://proceedings.mlr.press/v32/solomon14.html",
        "author": "Justin Solomon; Raif Rustamov; Leonidas Guibas; Adrian Butscher",
        "abstract": "Probability distributions and histograms are natural representations for product ratings, traffic measurements, and other data considered in many machine learning applications.  Thus, this paper introduces a technique for graph-based semi-supervised learning of histograms, derived from the theory of optimal transportation. Our method has several properties making it suitable for this application; in particular, its behavior can be characterized by the moments and shapes of the histograms at the labeled nodes. In addition, it can be used for histograms on non-standard domains like circles, revealing a strategy for manifold-valued semi-supervised learning. We also extend this technique to related problems such as smoothing distributions on graph nodes.",
        "bibtex": "@InProceedings{pmlr-v32-solomon14,\n  title = \t {Wasserstein Propagation for Semi-Supervised Learning},\n  author = \t {Solomon, Justin and Rustamov, Raif and Guibas, Leonidas and Butscher, Adrian},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {306--314},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {1},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/solomon14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/solomon14.html},\n  abstract = \t {Probability distributions and histograms are natural representations for product ratings, traffic measurements, and other data considered in many machine learning applications.  Thus, this paper introduces a technique for graph-based semi-supervised learning of histograms, derived from the theory of optimal transportation. Our method has several properties making it suitable for this application; in particular, its behavior can be characterized by the moments and shapes of the histograms at the labeled nodes. In addition, it can be used for histograms on non-standard domains like circles, revealing a strategy for manifold-valued semi-supervised learning. We also extend this technique to related problems such as smoothing distributions on graph nodes.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/solomon14.pdf",
        "supp": "",
        "pdf_size": 4949660,
        "gs_citation": 174,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6013434377261286560&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Computer Science, Stanford University; Department of Computer Science, Stanford University; Department of Computer Science, Stanford University; Max Planck Center for Visual Computing and Communication",
        "aff_domain": "STANFORD.EDU;STANFORD.EDU;CS.STANFORD.EDU;GMAIL.COM",
        "email": "STANFORD.EDU;STANFORD.EDU;CS.STANFORD.EDU;GMAIL.COM",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Stanford University;Max Planck Society",
        "aff_unique_dep": "Department of Computer Science;Center for Visual Computing and Communication",
        "aff_unique_url": "https://www.stanford.edu;https://www.mpi-inf.mpg.de",
        "aff_unique_abbr": "Stanford;MPG",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "United States;Germany"
    },
    {
        "id": "3f187a718b",
        "title": "Weighted Graph Clustering with Non-Uniform Uncertainties",
        "site": "https://proceedings.mlr.press/v32/chenh14.html",
        "author": "Yudong Chen; Shiau Hong Lim; Huan Xu",
        "abstract": "We study the graph clustering problem where each observation (edge or no-edge between a pair of nodes) may have a different level of confidence/uncertainty. We propose a clustering algorithm that is based on optimizing an appropriate weighted objective, where larger weights are given to observations with lower uncertainty. Our approach leads to a convex optimization problem that is efficiently solvable. We analyze our approach under a natural generative model, and establish theoretical guarantees for recovering the underlying clusters. Our main result is a general theorem that applies to any given weight and distribution for the uncertainty. By optimizing over the weights, we derive a provably optimal weighting scheme, which matches the information theoretic lower bound up to logarithmic factors and leads to strong performance bounds in several specific settings. By optimizing over the uncertainty distribution, we show that non-uniform uncertainties can actually help. In particular, if the graph is built by spending a limited amount of resource to take measurement on each node pair, then it is beneficial to allocate the resource in a non-uniform fashion to obtain accurate measurements on a few pairs of nodes, rather than obtaining inaccurate measurements on many pairs. We provide simulation results that validate our theoretical findings.",
        "bibtex": "@InProceedings{pmlr-v32-chenh14,\n  title = \t {Weighted Graph Clustering with Non-Uniform Uncertainties},\n  author = \t {Chen, Yudong and Lim, Shiau Hong and Xu, Huan},\n  booktitle = \t {Proceedings of the 31st International Conference on Machine Learning},\n  pages = \t {1566--1574},\n  year = \t {2014},\n  editor = \t {Xing, Eric P. and Jebara, Tony},\n  volume = \t {32},\n  number =       {2},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Bejing, China},\n  month = \t {22--24 Jun},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v32/chenh14.pdf},\n  url = \t {https://proceedings.mlr.press/v32/chenh14.html},\n  abstract = \t {We study the graph clustering problem where each observation (edge or no-edge between a pair of nodes) may have a different level of confidence/uncertainty. We propose a clustering algorithm that is based on optimizing an appropriate weighted objective, where larger weights are given to observations with lower uncertainty. Our approach leads to a convex optimization problem that is efficiently solvable. We analyze our approach under a natural generative model, and establish theoretical guarantees for recovering the underlying clusters. Our main result is a general theorem that applies to any given weight and distribution for the uncertainty. By optimizing over the weights, we derive a provably optimal weighting scheme, which matches the information theoretic lower bound up to logarithmic factors and leads to strong performance bounds in several specific settings. By optimizing over the uncertainty distribution, we show that non-uniform uncertainties can actually help. In particular, if the graph is built by spending a limited amount of resource to take measurement on each node pair, then it is beneficial to allocate the resource in a non-uniform fashion to obtain accurate measurements on a few pairs of nodes, rather than obtaining inaccurate measurements on many pairs. We provide simulation results that validate our theoretical findings.}\n}",
        "pdf": "http://proceedings.mlr.press/v32/chenh14.pdf",
        "supp": "",
        "pdf_size": 1641454,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5088428756313952748&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "University of California, Berkeley; National University of Singapore; National University of Singapore",
        "aff_domain": "EECS.BERKELEY.EDU;NUS.EDU.SG;NUS.EDU.SG",
        "email": "EECS.BERKELEY.EDU;NUS.EDU.SG;NUS.EDU.SG",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of California, Berkeley;National University of Singapore",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.berkeley.edu;https://www.nus.edu.sg",
        "aff_unique_abbr": "UC Berkeley;NUS",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "United States;Singapore"
    }
]